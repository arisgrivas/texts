A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis
Sorouralsadat Fatemi sfatem6@uic.edu University of Illinois at Chicago USA
Yuheng Hu yuhenghu@uic.edu University of Illinois at Chicago USA
ABSTRACT
Financial sentiment analysis plays a crucial role in uncovering latent patterns and detecting emerging trends, enabling individuals to make well-informed decisions that may yield substantial advantages within the constantly changing realm of finance. Recently, Large Language Models (LLMs) have demonstrated their effectiveness in diverse domains, showcasing remarkable capabilities even in zero-shot and few-shot in-context learning for various Natural Language Processing (NLP) tasks. Nevertheless, their potential and applicability in the context of financial sentiment analysis have not been thoroughly explored yet. To bridge this gap, we employ two approaches: in-context learning (with a focus on gpt-3.5-turbo model) and fine-tuning LLMs on a finance-domain dataset. Given the computational costs associated with fine-tuning LLMs with large parameter sizes, our focus lies on smaller LLMs, spanning from 250M to 3B parameters for fine-tuning. We then compare the performances with state-of-the-art results to evaluate their effectiveness in the finance-domain. Our results demonstrate that fine-tuned smaller LLMs can achieve comparable performance to state-of-the-art fine-tuned LLMs, even with models having fewer parameters and a smaller training dataset. Additionally, the zero-shot and one-shot performance of LLMs produces comparable results with fine-tuned smaller LLMs and state-of-the-art outcomes. Furthermore, our analysis demonstrates that there is no observed enhancement in performance for financedomain sentiment analysis when the number of shots for in-context learning is increased.
CCS CONCEPTS
• Computing methodologies → Natural language processing.
KEYWORDS
Sentiment Analysis, Finance, Large Language Model, Natural Language Processing
ACM Reference Format:
Sorouralsadat Fatemi and Yuheng Hu. 2018. A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment Analysis. In Proceedings of Make sure to enter the correct conference title from
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Conference acronym ’XX, November 27–29, 2018, Woodstock, NY © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX
your rights confirmation emai (Conference acronym ’XX). ACM, New York, NY, USA, 8 pages. https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
The performance of various Large Language Models (LLMs) such as GPT3 [1], LLaMA [19], and T5 [17] has been remarkable across a diverse set of Natural Language Processing (NLP) tasks. Sentiment analysis is an NLP task that utilizes advancements in language modeling to achieve enhanced outcomes. LLMs are trained on a wide range of corpora, enabling them to understand language patterns and perform tasks using zero-shot or few-shot prompting without explicit supervised training [2, 25]. Therefore, there exists considerable potential for the application of LLMs in sentiment analysis tasks, particularly in the financial domain where a limited availability of annotated data. Recent studies have highlighted the promising performance of LLMs in various NLP tasks, particularly in the zero-shot and fewshot settings. Notably, GPT-3 has shown competitive performance and, in some cases, even outperformed state-of-the-art models in few-shot in-context learning [2, 28]. However, when it comes to sentiment analysis tasks, some studies have explored the zero-shot and in-context learning capabilities of LLMs and found that, in certain instances, in-context learning may not achieve comparable performance compared to fine-tuned and state-of-the-art models [10, 20]. Therefore, it becomes crucial to investigate the zero-shot and few-shot in-context learning capabilities of LLMs in the financial domain, as it encompasses specific jargon and unique linguistic characteristics that necessitate special consideration. The aforementioned aspect has yet to be thoroughly examined in prior research, rendering it an important domain for further investigation. Additionally, a number of studies indicate that in-context learning can yield inferior performance compared to fine-tuning [2, 10]. To delve further into this, we extend our experiments by fine-tuning LLMs on a finance-domain dataset and evaluate their performance on various financial datasets. For this purpose, we focus on a range of smaller language models known as FLAN-T5 models, with parameter sizes varying from 770 million to 3 billion parameters. Our goal is to determine whether their performance is comparable to state-of-the-art fine-tuned LLMs with larger parameter sizes. To comprehensively assess the performance of LLMs in financial sentiment analysis, we conduct comparisons between the fine-tuned models and their counterparts in zero-shot and few-shot settings. In summary, our study involves conducting an empirical examination of the zero-shot and few-shot learning abilities of LLMs. In addition, we extend our investigation by performing fine-tuning on three Flan-T5 models (base, large, and xl) using finance-specific
arXiv:2312.08725v1 [cs.LG] 14 Dec 2023


Conference acronym ’XX, November 27–29, 2018, Woodstock, NY Sorouralsadat Fatemi and Yuheng Hu
data. Additionally, we compare the results with state-of-the-art models to assess their performance comprehensively.
2 RELATED WORK
Sentiment analysis in the financial domain has gained importance in NLP due to its impact on the stock prediction task. It plays a crucial role in providing valuable insights for investors’ decisionmaking [15]. previous literature explores diverse methodologies for conducting financial sentiment analysis, including lexicon-based techniques [3, 13]. However, such approaches often struggle to capture the semantic nuances arising from specific word sequences. Alternatively, the advent of pre-trained language models, particularly models like BERT have revolutionized the field of NLP, including sentiment analysis [2]. They leverage large-scale datasets and self-supervised learning to develop a robust understanding of context. This contextual awareness allows them to capture the subtle nuances and idiomatic expressions in sentiment-rich text [7]. Although these models are trained on general domain corpora like news articles and Wikipedia, they lack specific knowledge in the finance domain. To address this limitation, researchers have undertaken further fine-tuning of BERT specifically using financial text, leading to promising outcomes in financial sentiment classification. Nevertheless, it is crucial to acknowledge that the fine-tuning procedure for language models such as BERT requires a substantial quantity of financial data and computational resources [1, 16]. In a specific study, BERT performed further pre-training by utilizing a financial communication corpus consisting of 4.9 billion finance-domain tokens [16]. This problem has been mitigated by the most recent advancements in NLP through the utilization of LLMs. These models are trained on vast amounts of textual data, spanning various domains and comprising hundreds of millions to billions of words, surpassing the scale of previous language models. In addition to leveraging larger amounts of pre-training data, large language models (LLMs) integrate various training techniques, including instruction tuning and reinforcement learning from human feedback (RLHF) [4, 21]. This combination enables LLMs to achieve impressive performance in zero-shot or few-shot learning scenarios, occasionally surpassing the state-of-the-art benchmarks [2]. In light of these advancements, efforts have been made to develop financial-focused LLMs. One notable example is BloombergGPT, a 50 billion parameter language model specifically trained on a diverse range of financial data [23]. Although, it demonstrated impressive performance in sentiment analysis, its closed-source nature limits its use in the research community. To address this, a new study proposed an open-source alternative called FinGPT, finetuned on extensive financial data from various sources [24]. FinGPT achieved comparable performance in financial sentiment analysis to BloombergGPT, with lower adaptation costs and computational requirements. In a recent study, researchers introduced an instruction-tuned LLaMA-7B model, Instruct-FinGPT-7B, designed to overcome challenges faced by LLMs in accurately interpreting numerical values and comprehending financial context. The evaluation results showed that this model outperformed state-of-the-art methods in financial sentiment analysis, underscoring the promising potential of
LLMs in this domain [26]. However, the focus of the present study is on fine-tuning smaller LLMs to achieve comparable performance while minimizing computational and memory resources. Furthermore, there have been notable efforts to explore the zeroshot and in-context learning capabilities of LLMs in sentiment analysis tasks [20, 27]. Results indicate that LLMs, like ChatGPT, exhibit robust zero-shot performance, even comparable to finetuned models like T5-large. However, they may not perform as well in few-shot learning, particularly with an increasing number of shots, especially on certain datasets like Twitter and MR [27]. On the other hand, a separate study suggests that long-tail domains could benefit from few-shot prompting [20]. In light of the varying outcomes observed in few-shot learning when applied to different sentiment analysis datasets, we aim to conduct a comprehensive examination to evaluate the effectiveness of zero-shot and few-shot learning methods for sentiment analysis in the finance-domain.
3 METHOD AND EXPERIMENTAL SETUP
In this section, we outline the methodologies and datasets employed to investigate the objectives of our study.
3.1 Zero-shot and Few-shot Settings
In this investigation, we explore the zero-shot and few-shot learning capabilities of LLMs by conducting sentiment analysis inference without any specific training or modifying LLM parameters [9]. We use three models from Flan-T5 with different parameters, namely Flan-T5-Base (250M), Flan-T5-Large (780M), and Flan-T5XL (3B), to study the impact of model size on zero-shot and fewshot learning performance. Additionally, we utilize the ChatGPT (gpt-3.5-turbo) model from OpenAI, known for its proficiency and cost-effectiveness within the GPT-3.5 family. To ensure consistency in our evaluations, we set the temperature parameter to zero, resulting in deterministic predictions. The performance of Flan-T5 models and ChatGPT is assessed under these conditions:
• zero-shot learning: The model is provided with only task name, task definition and label space, which serves as a set of options that enable the model to generate its response accordingly. • few-shot learning: The model is exposed to task definitions and input-output pairs containing K randomly selected labeled samples per class. We evaluated the few-shot learning capabilities of LLMs across three k-shot settings: 1-shot, 5-shot, and 10-shot. In each setting, we sampled K examples for each sentiment label.
3.2 Fine-tuning LLMs
For fine-tuning, we take three Flan-T5 models: Flan-T5 base (250M), Flan-T5-large (780M), and Flan-T5-xl (3B parameters). FLAN-T5 is an open-source finetuned version of Google’s T5 model with instruct-finetuning [5]. They instruction-finetune T5 model which is a decoder-encoder model on a collection of data sources with a variety of instruction template types such as on different T5 model sizes. The Flan-T5 series models have indeed demonstrated remarkable performance compared to T5 on specific benchmarks. For example, Flan-T5-XL, with only 3B parameters, achieved an impressive Massive Multi-task Language Understanding (MMLU)


A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment AnalysisConference acronym ’XX, November 27–29, 2018, Woodstock, NY
score of 52.4%, surpassing the score of GPT-3 with 175B parameters by 8.5% [5]. Furthermore, the authors evaluate Flan instruction tuning as an intermediate step before single target fine-tuning, aiming to determine whether Flan-T5 could serve as a superior starting checkpoint for subsequent fine-tuning. The results indicate that employing Flan-T5 as a starting checkpoint offers an additional advantage in terms of training efficiency. Additionally, during single target fine-tuning, Flan-T5 converges much faster than T5 and achieves higher accuracies (e.g., increased accuracy by 7.8% in the RTE task). These findings strongly suggest that Flan-T5 is an excellent choice for further fine-tuning in our sentiment analysis task [12].
3.3 Fine-tuning Procedure
Fine-tuning pre-trained language models is a widely used technique to adapt LLMs for specific tasks by training them on task-specific data. However, this process becomes computationally intensive as it requires updating all pre-trained model parameters. [8]. To address this challenge, Microsoft researchers introduced a solution known as Low-Rank-Adaption (LoRA). LoRA introduces pairs of rank-decomposition weight matrices into the existing weights. During fine-tuning, only these newly added weights are trained, while the pre-trained model weights remain unchanged [8]. This method was further enhanced by the QLoRA technique, which involves backpropagating gradients through a frozen, 4-bit quantized pretrained language model into LoRA [6]. By integrating QLoRA, the fine-tuning process becomes more memory-efficient and computationally faster, while outperforming other efficient fine-tuning techniques [6]. Given that we are dealing with LLMs with 250M, 780M, and 3B parameters, we utilize the QLoRA method for fine-tuning. This approach, which offers enhanced memory efficiency and faster computation, is well-suited for optimizing the fine-tuning process for models of such sizes. Table 1 displays the reduced number of trainable parameters for each model.
4 PERFORMANCE EVALUATION
In this section, we assess the efficacy of fine-tuning smaller LLMs and in-context learning of ChatGPT (GPT-3.5-turbo) and Flan-T5 models for financial sentiment analysis task. We compare the results with state-of-the-art approaches from previous studies, including FinBert and Instruct-FinGPT. In order to assess the effectiveness of each model, we report accuracy and F1-Macro for sentiment analysis tasks including fin-tuned models and in-context learning settings.
4.1 Datasets
The dataset used for fine-tuning Flan-T5 models is Twitter Financial News Sentiment (Twitter Train), which comprises tweets related to financial topics and is accessible through HuggingFace. The training dataset consists of 9540 samples, with each sample labeled as Positive, Negative, or Neutral based on the sentiment expressed in the tweets. Testing dataset consists of two datasets (all datasets are available through Hugging Face):
• Twitter financial news sentiment validation (TFSN): The dataset comprises 2,390 samples of an annotated corpus of finance-related tweets. Each sample is labeled as Positive, Negative, or Neutral.
• Financial PhraseBank (FPB): The dataset consists of samples randomly extracted from financial news articles annotated by a team of 16 domain experts. Additionally, the dataset includes information on the agreement levels among the annotators for each sentence. We select the sentences with a 50% agreement level between annotators which consists of 4845 financial news and their sentiment label of positive, negative, or neutral [14].
We utilize the entire training dataset for fine-tuning. Subsequently, we perform inference for fine-tuned model, and in-context settings on both test datasets.
4.2 Model Training Details
For each Flan-T5 model (base, large, and xl), we follow the same fine-tuning procedure with the same hyperparameters. To compress the pretrained language models, we load the Flan-T5 models in a 4-bit format. To further reduce memory requirements during finetuning, we employ LoRa with an attention dimension (r) of 8, an alpha parameter of 32, and a dropout probability of 0.05, resulting in a reduction in the number of trainable parameters, as illustrated in Table 1. Then, we perform fine-tuning over 3 epochs with a learning
Model All Parameters Trainable Parameters Flan-T5-Base 248M 0.88M Flan-T5-Large 785M 2.36M Flan-T5-XL 2.85B 4.72M
Table 1: Number of trainable parameters after using QLoRA.
rate of 1e-4 and a maximum input text length of 256 tokens. To prevent CUDA Out-of-Memory errors, we set the batch size to fit into memory automatically through exponential decay. We conduct fine-tuning using one A100 GPU, which leads to the following total training times for each model: 28 minutes for the base model, 54 minutes for the large model, and 65 minutes for the XL model.
4.3 Benchmark Models
In this section, we present outcomes obtained from previous studies that have demonstrated superior results.
• FinBERT: This model is a variant of BERT, pre-trained on a financial text corpus comprising 1.8M news articles from Reuters TRC2 dataset. For inference, the model was accessed via the HuggingFace API pipeline. The sentences are initially tokenized and then fed into FinBERT for inference. FinBERT generates sentiment analysis outcomes for each textual input, classifying them as positive, negative, or neutral. • Instruct-FinGPT-7B: The model is fine-tuned on LLaMA7B model, employing the instruction tuning dataset. For evaluation purposes, we rely on the results reported in the paper that originally introduced Instruct-FinGPT-7B [26].


Conference acronym ’XX, November 27–29, 2018, Woodstock, NY Sorouralsadat Fatemi and Yuheng Hu
Flan-T5-Base Flan-T5-Large Flan-T5-XL ChatGPT TFSN
0.0
0.2
0.4
0.6
0.8
Accuracy
Flan-T5-Base Flan-T5-Large Flan-T5-XL ChatGPT FPB
0.0
0.2
0.4
0.6
0.8
Accuracy
w Label Description w/o Label Description
Figure 1: Zero-shot performance of Flan-T5 models and ChatGPT with and without label description in the prompt.
Dataset Metric FinBert Instruct-Llama-7B Fine-tuned-Flan-T5
Base Large XL
ChatGPT 0-shot 1-shot 5-shot 10-shot FPB Acc.
F1

0.758 0.739
0.769 0.793 0.807 0.726 0.759 0.780
0.727 0.791 0.795 0.779 0.680 0.774 0.794 0.783
TFSN Acc.
F1
0.725 0.668
0.881 0.842
0.874 0.894 0.903 0.838 0.867 0.878
0.821 0.823 0.768 0.724 0.818 0.819 0.775 0.732
Table 2: Performance comparison between fine-tuned Flan-T5 models, in-context learning of ChatGPT, and state-of-the-art models. Instruct-FinGPT-7B [26] results are taken from its respective papers. FinBERT results for the FPB dataset are not reported due to its use as the training set. The best results are in bold.
4.4 Prompt Design
The effectiveness and coherence of prompt formats can vary based on the training methodology and data used during the training process. In our experiments, we started with a straightforward prompt for zero-shot inference, as shown below:
• You are an AI language model trained to detect the sentiment of each sentence for stock prediction. Analyze the following sentence and determine if the sentiment is: positive or negative or neutral. Return only a single word, either Positive or Negative or Neutral.
Subsequently, we introduced label descriptions along with various prompt formats1, such as """ to separate the instruction and context, to assess whether they could enhance the model’s performance. The example of zero-shot and one-shot prompt is shown in Figure 1. We evaluated the performance of all three Flan-T5 models and the ChatGPT model using zero-shot inference. The results, shown in Figure 1, indicate higher accuracy across most models with the prompt containing label descriptions compared to the prompt without label descriptions. Therefore, we adopted the prompt format shown in Table 4 for all zero-shot and few-shot settings inference.
1Examples of prompt formats can be found at the following link:https://help.openai. com/en/articles/6654000- best- practices- for- prompt- engineering- with- openai- api
Model TFSN
Acc. F1
FPB Acc. F1 Base 0-Shot 1-Shot 5-Shot 10-Shot
0.665 0.694 0.707 0.726
0.593 0.624 0.643 0.659
0.741 0.726 0.734 0.764
0.729 0.727 0.742 0.742 Large 0-Shot 1-Shot 5-Shot 10-Shot
0.687 0.742 0.765 0.778
0.647 0.688 0.701 0.704
0.797 0.809 0.813 0.804
0.798 0.807 0.810 0.801 XL 0-Shot 1-Shot 5-Shot 10-Shot
0.683 0.791 0.760 0.771
0.678 0.753 0.730 0.729
0.815 0.847 0.832 0.780
0.823 0.836 0.833 0.796
Table 3: Few-shot performance of Flan-T5 models on TFSB and FPB datasets.
5 EVALUATION RESULTS
As mentioned in section 2, our main focus is on evaluating the zeroshot and few-shot learning performance of LLMs, including both


A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment AnalysisConference acronym ’XX, November 27–29, 2018, Woodstock, NY
0 1 5 10 Number of Shots
0.70
0.75
0.80
0.85
Accuracy
Base
0 1 5 10 Number of Shots
0.70
0.75
0.80
0.85
0.90
Accuracy
Large
0 1 5 10 Number of Shots
0.70
0.75
0.80
0.85
0.90
Accuracy
XL
0 1 5 10 Number of Shots
0.75
0.80
0.85
0.90
Accuracy
ChatGPT
Flan-T5 FinBert Instruct-FinGPT Fine-tuned-Flan-T5
Figure 2: Few-shot prompting results on TFSN dataset compared with FinBERT, Fine-tuned-Flan-T5, and Instruct-FinGPT results. Utilizing the best-performing fine-tuned Flan-T5-XL model for ChatGPT.
0 1 5 10 Number of Shots
0.74
0.75
0.76
0.77
Accuracy
Base
0 1 5 10 Number of Shots
0.76
0.77
0.78
0.79
0.80
0.81
Accuracy
Large
0 1 5 10 Number of Shots
0.76
0.78
0.80
0.82
0.84
0.86
0.88
Accuracy
XL
0 1 5 10 Number of Shots
0.725
0.750
0.775
0.800
0.825
0.850
0.875
Accuracy
ChatGPT
Flan-T5 Instruct-FinGPT Fine-tuned-Flan-T5
Figure 3: Few-shot prompting results on FPB dataset compared with Fine-tuned-Flan-T5 and Instruct-FinGPT results. Utilizing the best-performing fine-tuned Flan-T5-XL model for ChatGPT.
smaller and larger models in terms of parameter size, in comparison to fine-tuned smaller LLMs. The results of Flan-T5 fine-tuned models on both TFSN and FPB datasets, along with the benchmark models, and the zero-shot and few-shot performance of ChatGPT, are presented in Table 2. Additionally, the results of the zero-shot and few-shot performance of Flan-T5 models are shown in Table 3. Fine-tuned LLMs Results. As the results indicate in Figure 2 and Table 2, on the TFSN dataset, the performance of fine-tuned Flan-T5 models is comparable to the state-of-the-art model, InstructFinGPT ,and significatly outperforms FinBert results. It is noteworthy that we achieved this level of performance with significantly fewer computational resources (1 A100 GPU compared to 8 A100 GPUs) and a comparable or even shorter training time by utilizing QLoRA method , especially for the Flan-T5-Base model (28 minutes). These findings align with a previous study that highlights the efficiency advantage of using Flan-T5 as a starting checkpoint for further fine-tuning, as discussed in section 3.2.
Zero-shot Results. As shown in Figure 2, the zero-shot learning results of Flan-T5 models (Base, Largr, and XL) on the TFSN dataset fall significantly behind those of all fine-tuned models (FinBert, Instruct-FinGPT, and all fine-tuned Flan-T5 models). Notably, Instruct-FinGPT and fine-tuned Flan-T5 models outperform LLMs by a clear margin of roughly 20%. However, the zero-shot performance of ChatGPT reaches 82%, surpassing the FinBert model but still remaining inferior to Instruct-FinGPT and all fine-tuned Flan-T5 models. As depicted in Figure 3 and Table 3, the zero-shot results of all Flan-T5 models on the FPB dataset show more promising outcomes, even performing comparably to the corresponding fine-tuned FlanT5 models. This observation aligns with previous research findings [27]. Additionally, a noteworthy finding on the FPB dataset is that larger models, with a greater number of parameters, tend to outperform the smaller ones in zero-shot inference. For instance, when


Conference acronym ’XX, November 27–29, 2018, Woodstock, NY Sorouralsadat Fatemi and Yuheng Hu
comparing the performance between Flan-T5-Base and Flan-T5-XL, there is an increase of roughly 7% in performance. When comparing the zero-shot performance of Flan-T5 models to ChatGPT, ChatGPT appears to be less accurate despite having larger parameters, which contradicts the zero-shot inference on the TFSN dataset. This discrepancy in performance can be explained by the different language and jargon used in the FPB news headlines and the TFSN social media posts. ChatGPT, being trained on a large corpus of social media posts, is better equipped to extract the true sentiment from social media text, which is reflected in its zero-shot performance on the TFSN dataset. On the other hand, smaller LLMs like Flan-T5 models outperform ChatGPT in the zero-shot setting on the FPB dataset, possibly due to their large-scale instruction tuning, which enhances their reasoning capabilities and allows them to extract sentiment from more complex texts. Few-shot Results. Results comparing few-shot prompting performance with zero-shot prompting performance are presented in Figure 2, Figure 3, and Table 3. Notably, we observe that 1-shot prompting can significantly improve the performance across almost all models and datasets, except for Flan-T5-Base (250M), where there is a slight decrease in performance. This decrease can be attributed to the impact of a single unrelated example, which can have an adverse effect on the smaller LLMs. Interestingly, we find that smaller LLMs (Flan-T5) benefit more on average from one demonstration compared to ChatGPT. The impact of increasing shots to 5 and 10 exhibited variability across various models and datasets. On the TFSN dataset, it was observed that a marginal improvement in performance was achieved for both Flan-T5-Base and Flan-T5-Large models with an increase in the number of shots. However, the performance of Flan-T5-Large and Flan-T5-XL was impeded by the increase in shots. The variation in outcomes can be ascribed to the difficulties associated with managing excessively lengthy contexts, which have the potential to lead the LLMs misguided, as revealed in a new study [27]. These findings are consistent with a previous study that highlighted the sensitivity issue of LLMs when exposed to few-shot examples during inference [27]. In order to mitigate these disparities and obtain consistent enhancements in performance, it is imperative to employ more efficacious techniques for few-shot learning. For example, a recent study suggested the retrieval of examples that possess semantic similarity to a test query sample in order to generate its corresponding prompt [11]. Moreover, the introduction of the Chain of Thought (CoT) and Clue And Reasoning Prompting (CARP) methods aimed to improve the reasoning capabilities of LLMs. which could be beneficial for extracting sentiment from financial corpora [18, 22]. Our future work will encompass an exploration of these approaches
6 CONCLUSION AND FUTURE DIRECTIONS
In this study, we extensively compared the performance of finetuned LLMs with different parameter sizes (ranging from 250M to 3B) and their in-context learning capabilities, for financial sentiment analysis. Our experimental results demonstrate that fine-tuned LLMs achieved comparable performance to state-of-the-art models while utilizing significantly fewer computational resources. The zero-shot and one-shot settings performed impressively, especially
on the FPB dataset, which can be attributed to the corpus used for pre-training the LLMs. Moreover, larger LLMs demonstrated better performance in the zero-shot and one-shot settings. The remarkable results obtained from fine-tuned, zero-shot, and one-shot inferences of Flan-T5 models can be attributed to their instruct fine-tuning. However, we observed inconsistent performance in the five-shot and ten-shot settings across all models and datasets. Notably, increasing the number of shots did not lead to improved performance on average. Nevertheless, the models demonstrated reasonable performance on both datasets and across all three Flan-T5 models and ChatGPT, indicating their potential for financial sentiment analysis considering the scarcity of labeled data in finance-domain. In conclusion, our study showcases the remarkable capabilities of LLMs, even smaller models, in both fine-tuning and in-context learning for financial sentiment analysis task. These findings provide valuable insights into potential avenues for further investigation in the field of financial sentiment analysis. For future studies, we plan to assess the new methods of prompt formatting, such as CoT and CARP, and prompt selection by retrieving semantically-similar examples to the test queries, rather than random sampling for few-shot settings [18, 22]. The objective of this study is to determine the extent to which these methods can consistently enhance the performance of LLMs in financial sentiment analysis tasks.
REFERENCES
[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.
[3] Chung-Chi Chen, Hen-Hsen Huang, and Hsin-Hsi Chen. 2018. NTUSD-Fin: a market sentiment dictionary for financial social media data applications. In Proceedings of the 1st Financial Narrative Processing Workshop (FNP 2018). 37–43. [4] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017).
[5] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022). [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314 (2023). [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).
[9] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).
[10] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems 35 (2022), 1950–1965. [11] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What Makes Good In-Context Examples for GPT-3? arXiv preprint arXiv:2101.06804 (2021).
[12] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688 (2023).
[13] Tim Loughran and Bill McDonald. 2011. When is a liability not a liability? Textual analysis, dictionaries, and 10-Ks. The Journal of finance 66, 1 (2011), 35–65.


A Comparative Analysis of Fine-Tuned LLMs and Few-Shot Learning of LLMs for Financial Sentiment AnalysisConference acronym ’XX, November 27–29, 2018, Woodstock, NY
[14] Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology 65, 4 (2014), 782–796. [15] Kostadin Mishev, Ana Gjorgjevikj, Irena Vodenska, Lubomir T Chitkushev, and Dimitar Trajanov. 2020. Evaluation of sentiment analysis in finance: from lexicons to transformers. IEEE access 8 (2020), 131662–131682. [16] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9. [17] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485–5551. [18] Xiaofei Sun, Xiaoya Li, Jiwei Li, Fei Wu, Shangwei Guo, Tianwei Zhang, and Guoyin Wang. 2023. Text Classification via Large Language Models. arXiv preprint arXiv:2305.08377 (2023).
[19] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).
[20] Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui Xia. 2023. Is ChatGPT a good sentiment analyzer? A preliminary study. arXiv preprint arXiv:2304.04339 (2023). [21] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824–24837.
[22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 (2022), 24824–24837. [23] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: A large language model for finance. arXiv preprint arXiv:2303.17564 (2023).
[24] Hongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023. FinGPT: OpenSource Financial Large Language Models. arXiv preprint arXiv:2306.06031 (2023). [25] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712 (2023). [26] Boyu Zhang, Hongyang Yang, and Xiao-Yang Liu. 2023. Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models. arXiv preprint arXiv:2306.12659 (2023).
[27] Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment Analysis in the Era of Large Language Models: A Reality Check. arXiv preprint arXiv:2305.15005 (2023).
[28] Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and Dacheng Tao. 2023. Can chatgpt understand too? a comparative study on chatgpt and fine-tuned bert. arXiv preprint arXiv:2302.10198 (2023).
A APPENDICES
We introduce a zero-shot and 1-shot prompt designed for the financial sentiment analysis task, displayed on the subsequent page.


Conference acronym ’XX, November 27–29, 2018, Woodstock, NY Sorouralsadat Fatemi and Yuheng Hu
Zero-shot prompt
You are an AI language model trained to detect the sentiment of sentences for stock prediction See below all the possible labels and their descriptions """ description: Bearish sentiment label: Negative """ description: neutral sentiment label: Neutral """ description: Bullish sentiment label: Positive """ Here is the sentence that needs to be classified sentence: {sentence} label:
One-shot Prompt You are an AI language model trained to detect the sentiment of sentences for stock prediction See below all the possible labels and their descriptions """ description: Bearish sentiment label: Negative """ description: neutral sentiment label: Neutral """ description: Bullish sentiment label: Positive """ See below a couple of examples """ text: Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook label: Negative """ text: ululemon Falls on Conservative View But Analysts Keep Faith label: Neutral """ text: Wells Fargo Downgrades Netflix $NFLX to Underperform but sees as a takeover target. NFLX could get acquired label: Positive """ Here is the sentence that needs to be classified sentence: {sentence} label:
Table 4: Prompts for zero-shot and one-shot settings. In the Five-Shot and Ten-Shot settings, 5 and 10 samples were added for each class, respectively.