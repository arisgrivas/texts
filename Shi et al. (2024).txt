Applied Intelligence (2024) 54:6415–6432 https://doi.org/10.1007/s10489-024-05492-0
Aspect based sentiment analysis with instruction tuning and external knowledge enhanced dependency graph
Xuefeng Shi1,2 · Min Hu1,2 · Fuji Ren3 · Piao Shi1,2 · Satoshi Nakagawa4
Accepted: 29 April 2024 / Published online: 14 May 2024 © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024
Abstract
Aspect-Based Sentiment Analysis (ABSA) is generally defined as a fine-grained task in Natural Language Processing (NLP). Recently, the integration of the Large Language Model (LLM) and Graph Convolutional Network (GCN) has been widely studied to excavate the underlying contextual information and support the sentiment polarity prediction. However, in existing research, the LLM is usually employed directly to generate the contextual feature representation without any specific instructions, which is not suitable for learning the domain language corpus. In addition, the existing works usually fuse the contextual feature and graph feature by GCN simply, and it ignores further specific processing to highlight the sentiment representations before the model’s final outputting. To tackle these two imperfections, this work proposes a novel ABSA model Instruction Tuning-based Graph Convolutional Network (ITGCN) to implement the subtask of predicting sentiment polaritiesR2, which leverages the instructed LLM to generate the task-oriented contextual representation and the GCN to exploit the external affective knowledge-assisted syntactic features. In the proposed ITGCN, firstly, the inputting sentence is reconstructed with the designed task-specific instructions, which tell the LLM what is the target in the input. Secondly, this work’s dependency graph, before being processed by GCN, is weighted by the affective knowledge extracted from SenticNet. This kind of dependency graph is endowed with affective information, which is closer to the intention of the related study. Finally, to learn more structured knowledge, a bi-layer sentiment representation module is proposed and utilized to enhance the feature representation. To validate the effectiveness of the proposed ITGCN, extensive experiments have been conducted on five public and available datasets. The proposed ITGCN achieves competitive performance and outperforms the selected state-of-the-art baselines, obviously.
Keywords ABSA · Instruction tuning · GCN · Affective knowledge · Bi-layer sentiment representation
1 Introduction
In the past decades, with the rapid development of data mining, the information extractedR1 from various modalities has been universallyR1 involved in assisting Human-Computer Interaction (HCI) [1, 25, 50]. Specifically, Natural Language Processing (NLP) has attracted a lot of attention from many researchers, and it has been studied widely to help the computer understand the rules used in human languages. Moreover, detecting the underlying sentiment in the text can encourage the computers to recognize a human’s emotional
B Xuefeng Shi 2020010107@mail.hfut.edu.cn
Extended author information available on the last page of the article
state [7]. As a research hot-spot, Aspect-Based Sentiment Analysis (ABSA) is a challenging but attractive task in the NLP community [4, 51] , which aimsR2 to analyze the sentiment polarity (e.g. {Positive, Negative, Neutral}) according to the mentioned aspect and opinion terms in a given sentence. Although impressive progress has been achieved in previous ABSA works, several problems are still not solved well, such as the gap between the pre-trained language model and downstream tasks, the effective utility of external knowledge, and so onR2. Generally, the reviews from electronic product reviews, comments on restaurants, and texts published on social media platforms are usually involved in the ABSA task. Through thoroughly studying the underlying regularity in these texts, will encourage the merchants to improve their services and the quality of their commodities. Additionally, the valid reviews on these issues will greatly
123


6416 X. Shi et al.
influence those consumers who have similar opinions, and generate a strong impact on the relevant economy [10]R1. Recently, steady progress has been achieved toward ABSA by leveraging modern Large Language Models (LLM) and dependency graphs [45]. In existing works, ABSA models can be roughly summarized in two parts: context-based approaches and syntax-based approaches. Context-based approaches aim to model the relationships between linguistic information and the specific aspect, which are usually generated by an LLM (e.g. BERT (Bidirectional Encoder Representation from Transformers) [8], RoBERTa (A Robustly Optimized BERT Pre-training Approach) [20], GPT (Generative Pre-Trained Transformer) [27], et al.). From these works, it is obviously knownR1 that the contextbased ABSA models seek to express the precise importance of the aspects in the whole sentence and model the relation between the aspect term and the contextual wordsR1. However, in previous LLM-based works, the related methods usually fine-tune a unique LLM checkpoint for their specific task directly. In such a paradigm, the LLM can only generate the general contextual representation via learning the raw training corpus, and such contextual representation is weak in linking the learning samples to the task destination, which means that the obtained feature information can not represent the original text very wellR1. Therefore, providing the designed exact instructions to the model is quite essential, which will help ABSA model fill the gap between the generated contextual representation and the downstream tasksR2. On the other side, syntax-based approaches mainly complete the ABSA task with the assistance of syntax dependency. Not only is semantic feature vital to understanding a sentence but also syntaxR2 influences the comprehension of a context significantly [2, 46]. The dependency tree parses texts with syntactic rules, and it provides a more clear way for the computer to grasp the key meaning with respect to syntactic structure. Besides, existing graph-based models usually fuse contextual representation with dependency treeR2 directly before the output layer through Graph Convolutional Networks (GCN). However, this manner of information integrationR2 is without the enhancement of external affective knowledge and further feature extraction and transformation, which means that the compulsory features are not highlighted obviously. And finally, the performance of the proposed models is obtained at a discountR1. To obtain the task-oriented contextual representation and leverage the information learned by GCN effectively, a novel method Instruction Tuning-based Graph Convolutional Network (ITGCN) is proposedR1 to instruct the LLM’s learning and enrich the affective representation in the dependency graph. Compared to priorR2 ABSA models, ITGCN has two obvious improvements: (1) Inspired by the thought of prompt learning, several different instructions are preparedR1 to
tell the model which type of feature is required in the correspondingR1 subtask. Different from original prompt learning providing specific words or embedding space to learn the model, instruction tuning tells LLM directly what the key information is in current training material, which largely exploits the potential capability of LLM and carefully models the semantic relations among different tokens; (2) To highlight the affective nodes in the dependency graph, SenticNet is employed to provide professional affective knowledge for the relevant words. Thus, an affective dependency graph is constructed and achieved, and it is an essential precondition for further sentiment analysis in ABSA taskR1. Although the semantic and syntactic information will be integrated by GCN, it still needs further operation to transform the integrated feature into a more unambiguous representation. Thus, a bi-layer sentiment representation module is designed to complete this work in this paperR1, which refers to the attention mechanism to highlight the needed sentiment information correctlyR1. Except above two improvements, another two existing techniques are also utilized to enhance the performance of theR1 proposed ITGCN. Firstly, position-aware informationR2 has been proven effective in strengthening the relations among words and improving the model’s performance in modeling feature representation in ABSA task [48]. Thus, ITGCN refers to its working mechanism and embeds the word’s position information into the representation obtained by LLM, which can make the token’s representation more accurate. Secondly, to cooperate with the topic of theR1 task, highlighting the aspect terms is essential in learning the text corpus. Inspired by prior works, aspect-specific masking is also introduced to ITGCN. With the assistance of task-oriented masking, ITGCN achieves aspect-related feature representation, which R2 strongly enhances the performance of the proposed model. To verify ITGCN’s effectiveness, extensive experiments are conducted on five widely used ABSA datasets. The impressive experimental results demonstrate that ITGCN can generate the sentiment abundant and taskoriented contextual representation. The main contributions of thisR1 work can be summarized as follows:
• A novel solution ITGCN for ABSA is proposed, which adopts instruction tuning as the training guidanceR2 for the large language model. With this approach, ITGCN can construct task-oriented contextual representations and extract the aspect-specific features for the ABSA task. • This work proposesR1 to further process the output of graph convolutional networks. A bi-layer sentiment representation module is designed to further highlight the
123


Aspect based sentiment analysis with instruction tuning... 6417
affective nodes in the knowledge-enhanced dependency graph. • Position-aware information and aspect-masking are also considered powerful supports to represent the specific aspect terms. The former provides the corresponding relationships between the aspect words and the other contextual words, and the latter highlights the aspect words by masking the others. • Experimental results on five benchmark datasets illustrate ITGCN’s effectiveness in exploring the ABSA task.
The remainder of this paper is as follows: Section 2 introduces the related works of aspect-based sentiment analysis and prompt learning in recent years. The detailed model framework is described in Section 3. Experimental settings and all experimental results and analyses are presented in Section 4. Finally, Section 5 gives a brief summary of this work.
2 Related works
In recent years, various approaches have been designed to deal with ABSA tasks, and most of them have benefited from the development of LLM and GCN. Thus, in this section, itR1 focuses on summarizing the works related to ABSA based on deep learning and dependency graph shortly. Besides, the concept and utilization of instruction tuning are also reviewed briefly.
2.1 Aspect-based sentiment analysis
ABSA is a fine-grained sentiment analysis sub-taskR2 in the fundamental NLP community, which aims to predict the sentiment polarity depending on the mentioned aspect or opinion terms. According to the investigation of existing ABSArelated works, early works can be categorizedR1 into two classes: context-based and syntax-based methods. Furthermore, the incorporation of these two manners achieves more and more attention from researchers currently. Since getting rid of the dependence on manually crafted features, the models based onR2 Deep Neural Networks (DNN) benefit a lot from the contextual representation learned from the training data, and show their promising performance on ABSA task. In the early times, the mainstream DNN models Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) were widely used to extract the aspect-specific contextual representation. And a variety of variants R2 are proposed based on their ability to extract sequential features (e.g. Sentic LSTM [23], KGCapsAN [39], DBi-LSTM [49]).
Later, the transformer was proposed in [31], which had an architecture of stacked attention layers. Unlike previous works, transformer-based ABSA models aim to understand the input review from its linguistic characteristics, and this pattern is quite similar to the thinking way of human beings. Thus, currently, most of the LLMs are constructed upon the transformer, such as BERT [8], RoBERTa [20], GPT [27]R2. As the representative of transformer-based LLM, BERT achieves excellent progress in modeling contextualized representation. With the specific fine-tuning mechanism, BERT can be adjusted to complete various downstream tasks without any more specific required structure. Likewise, BERT-based models have been adopted in ABSA tasks in recent years. In [30], Sun. et al. fine-tuned BERT via reconstructing the input text with an auxiliary sentence from the aspect, and achieved more abundant feature information in dealing with the ABSA task. The method SPAN-BERT proposed in [17] also took BERT as the backbone network, which provided rich feature representations for the following aspect extraction and sentiment analysis task. On the other hand, dependency graph also has a significant influence on contextual comprehension [36, 44]. Since GCN is powerful in handling the structured graph, ASGCN [41] is designed to implement the ABSA task, which proves it is suitable for the relevant task well. On the basis of GCN, Wang et al. [32] proposed R-GAT to learn the aspectoriented dependency tree, and the designed graph attention mechanism could highlight the targeted sentiment nodes forcefully.R2 Furthermore, the integration of contextual and syntactic representation also contributes a lot to the ABSA task. Minh et al.R2 [26] combined part-of-speech embedding, dependency-based embedding, and contextualized embedding to model comprehensive representation for the ABSA task. The results proved the integration of these three aspects of information was beneficial in improving the model’s performance. Feng et al.R2 [9] proposed to use GCN generating attention-assisted graph-based representation, which collected effective features from the sentence globally. As a robust version of BERT, RoBERTa is also introduced into the ABSA task after it has been proposed. In [6], through comparing the effectiveness of the parsed dependency tree and the induced tree from fine-tuned RoBERTa, Dai et al. revealed that the syntactic tree was important in improving RoBERTa’s performance in the ABSA task. Of course, there are still many outstanding studies about cooperating contextual and syntactic representation to enhance the performance of ABSA task [11, 21, 24, 28], which are not described detailedly here due to length limitation. The above-mentioned investigation shows that the cooperation of context-based and syntax-based feature representation has an impressive result on the ABSA task. However, two main shortcomings are also concludedR1 from the review of previous studies: First, general LLMs generate the contextual
123


6418 X. Shi et al.
representation only based on the raw training data, without any task-oriented instructions, which are hard to fill the gap between pre-trained models and downstream tasks; Second, existing syntax-based works suffer from the lack of further process after GCN layer, which impacts the detection of the key nodes in a sentence. Intuitively, a brief comparison of context-based and syntax-based ABSA models are reported in Table 1.R1 Thus, to mitigate these two problems, ITGCN is proposedR1 to generate instructed feature representation and further process the output of GCN with the bi-layer sentiment representation module.
2.2 Prompt learning
For original learning text x, prompt learning aims to guide the LLM to generate the expected representation through constructing x′ with several additional and instructed strings, which is different from common fine-tuning, and cloze prompts [5] and prefix prompts [18] are the two main varieties. Specifically, prefix prompts tend to be more conducive, as they are adjusted easily to different tasks with different prompt templates. Remarkably, this has been successfully applied toR2 a large range of NLP tasks. In their work [14], Han et al. designed sub-prompts according to relevant languageR1 rules, which are encoded with the prior knowledge of a classification task. Through this innovationR2, more effective prompts are generated in less time. Instead of adopting hard templates to learn different vectors from different domains, Wu et al. [35] proposed to use separate soft prompts to complete modeling cross-domain knowledge representation, which alleviates the domain discrepancy of the special tokens in different masked language modeling tasks. The above concise survey has shown the effectiveness of prompt learning in instructing LLMs while modeling the contextual representation. This paper focuses on transferring this superiority into the ABSA task and improving the performance of LLMs with different manually instructed templates. And according to the exact utilization, this paper prefersR1 to call this learning method as instruction tuning.
3 Instruction tuning-based graph convolutional networkR2
In this section, the detailed working mechanism of ITGCN will be described progressively. The overall architecture is illustrated in Fig. 1.
3.1 Overall architecture of ITGCN
From the above investigation of the existing works, it is obvious that there is a distinct gap between the feature
representation generated by LLM and the downstream task needed. Providing task-specific guidance to the LLM in its fine-tuning can minimize the gap and achieve task-oriented sentiment information. Additionally, it is necessary that further exploration should be conducted on the feature representation extracted by multi-layer GCN, and this work designs a bi-layer mechanism to complete this essential processing.R1 Generally, this part will introduceR1 the proposed ITGCN detailedly via four subsections. Firstly, there are several different manual templates are designed,R1 and they are separately inserted into the raw training reviews to support instruction tuning, and the instructed reviews are fed to the backbone LLM (e.g., RoBERTa) to generate task-oriented contextual representation. Besides, positionaware relations are also adopted to enhance the generated feature maps. Secondly, the affective graph achieved based on conventional syntax dependency graph is derived with the assistance of SenticNetR1, and the graph will be sent to multi-layer GCNs to cooperate with the generated instructed semantic feature representation tightly. Thus, the GCN module will grasp more sentiment information for ABSA task expectantlyR1. Thirdly, the effective bi-layer sentiment representation module is utilized to further extract the key information related to aspect terms, which strengthens the relation between opinion and aspect terms in the final feature representationR1. Finally, before outputting the predicted sentiment probabilities, aspect-specific masking masks the non-aspect words and highlights the significant features in the review of the output vectors learned by the GCN module. Hereto, the task-oriented sentiment information is achieved. And with a comprehensive softmax layer, the distribution of sentiment polarities is presented clearlyR1.
3.2 Task definition
The definition of the ABSA task can be formulated as follows. Given a review S = {w1, w2, ..., wa1, wa2, ...waj , ..., wk }, where the length of S is k, waj is the a j-th aspect term, and {wa1, wa2, ...waj } is a subset of the whole review S, the length of the aspect terms is j, and it can be known this relation 1 < j < k.R2 The target of the ABSA task in this workR2 is to learn the review and extract the aspect-related sentiment information for predicting the sentiment polarity (i.e., Positive, Negative, or Neutral).
3.3 Instruction tuning for task-oriented contextual representation
As aforementioned, specific instructions can tell the LLM what type of features are needed in the current taskR1. In [29], BERT-SPC reconstructed the inputting via appending the aspect terms after the raw review additionally. The experimental results demonstrated it was effective to improve the
123


Aspect based sentiment analysis with instruction tuning... 6419
Table 1 Comparison of context-based and syntax-based ABSA approaches
Category Classical models Main features & Limitation
Context-based Sentic-LSTM [23], KGCapsAN [39], DTML [49],
BERT-pair[30], SPAN [17], CSAE [26], AG-VSR [9]
Main Features: (a) Excellent ability in modeling the contextual
representation for inputting text. (b) Providing a comprehensive under
standing of the sentence for the next specific operation. Limitation: (a)
A obvious gap between the generated universal representation and the
downstream specific tasks.
Syntax-based ASGCN [41], DE-GCN [44], PD-RGAT [36], R-GAT
[32]
Main Features: (a) It is strong in handling graph structure data, and
promising performance has been achieved in its involved tasks. (b) Easy
to be applied and beneficial to ABSA tasks. Limitation: (a) The general
syntax dependency tree can not highlight the significant aspect terms
and opinion words, which contain abundant sentiment information. (b)
Further sentiment processing on the output of GCN is absent.
Ours ITGCN Main Features: (a) Reconstructing the inputting reviews with the man
ually designed task-oriented instruction templates, which can minimize
the gap between LLM and ABSA tasks. (b) External affective knowl
edge is leveraged and injected into the general syntax dependency graph,
which can highlight the sentiment features. (c) A bi-layer sentiment rep
resentation module is designed to further enhance the related affective
features in the final contextual representation.
123


6420 X. Shi et al.
Fig. 1 The overview of the proposed ABSA architecture
performance of the proposed model. Therefore, inspired by this manner of modification and the idea of prompts learning, this workR1 reorganizes the review by inserting the manual instruction templates: S′ = {S, I T , A}, where S is the raw review, I T = {I1, I2, I3, ..., Im} denotes the instructions, and A = {wa1, wa2, ...waj } is the aspect terms. For example, on the basis of the example in Fig. 2 and the instruction IT4 in Table 4, the reconstructed inputting review S′ will be shown as {The battery life is good, but the whole computer is just so so. The expected target aspects are the battery and computer}R2. In this paper, RoBERTa is utilized as the encoder backbone for obtaining semantic feature information. The transformation can be denoted as
HˆR = RoB E RT a(S′), (1)
where HˆR = {hˆ1, hˆ2, ..., hˆi , ..., hˆn} ∈ Rd×|N| means the embedding vector achieved by RoBERTa, n is the length of S′, hi ∈ Rd is the i-th word embedding of contextual word wi , d is the dimension of word vectors, |N | is the
size of vocabulary. Due to the import of introductions in review S′, the task-oriented and aspect-specific contextual representation is obtained by RoBERTa eventually, and this representation is the semantic backbone of the further sentiment extractionR1. Moreover, implementing the position relations of the aspect word and other contextual words as a weight has been validated practical in processing ABSA tasks [33]. Therefore, this workR1 also leverages position relations to enhance the relevantR1 generated contextual embedding. The process can be presented as:
Pi,a j =
{
1 − |i−a j|
N , if wi is not in aspect terms,
0, otherwise, (2)
where i is the the physical positionR2 of i-th word in S′, a j is the the physical positionR2 of a j-th aspect term in S′. In this definition, the aspect terms are centrally located, and the distance from each contextual word to aspect terms
Fig. 2 An example with dependency graph and sentiment score from SenticNet
123


Aspect based sentiment analysis with instruction tuning... 6421
is measured directlyR2. Subsequently, As the collection of Pi,aj , P ∈ RN×N is utilized as a position score matrix to enhance the structural representation of HˆR, and the operation is depicted as:
HR = P × HˆR (3)
Ultimately, the general contextual representation is updated by the position relation explicitly. And with this design, more exact position information is fused into the feature representation, which will enhance the links between aspect terms and the contextual words from the absolute positionR1.
3.4 External affective knowledge enhancing dependency graph
In existing studies, syntax dependency tree has been widely applied in NLP tasks. Thus, the dependency graph is also employed in the proposedR1 method. To leverage the superiority of syntactic knowledge, an adjacency matrix of the review is constructedR1 depending on dependency tree firstlyR1.1 And the derived matrix M′ ∈ Rk×k is computed as follows:
M′
i,i′ =
{
1, if wi , wi′ contains dependency,
0, otherwise. (4)
Hereto, the common dependency graph has been derived from the existing syntax parserR1 toolkit. FurthermoreR1, external knowledge has been proven useful in strengthening the representation of features. Inspired by the work [19], to make the dependency graph more affective, this workR1 also applies SenticNet [22] into theR1 proposed ITGCN to enhance the relations of the dependency matrix. The affective score of the related words is obtained by the formula:
ASi,aj = Scor e(wi ) + Scor e(waj ), (5)
where Scor e(·) is the collection of the affective score derived from SenticNet, and Scor e(wi ) ∈ [−1, 1], which means the emotional intensity of the word wi R2. Besides, if the word wi is neutral or not in SenticNet, the affective score Scor e(wi ) will be set to 0 (An example is shown in Fig. 2). Moreover, to emphasize the importance of aspect terms in the review, an additional dependency matrix T Ri,aj is considered to replen
ish the common dependency graph M′
i,i′ , which is defined as:
T Ri,i′ =
{
1, if wi′ ∈ {wa1, wa2, ...waj },
0, otherwise, (6)
1 In this work, the spaCy toolkit is used to derive the dependency tree of the review: https://spacy.io/.
where T R is the collection of the relation matrix between contextual word wi and aspect term waj . Since the affective score derived from SenticNet and the relation defined by (6) have been obtained, the dependency matrix M′
i,i′ can be
rewritten as:R1
Mi,i′ = M ′
i,i′ × ( ASi,aj + T Ri,i′ + 1), (7)
where M is the set of Mi,i′ . It is worth noting that i and i′ have the same theoretical significance, which both mean the random index of the word in the given reviewR2. Through the introduction of external affective knowledge and the relation between aspect words and other words, the dependency graph has been refined totally.
3.5 Multi-layer graph convolutional networks
GCN can be considered an adaptation of the conventional Convolutional Neural Networks (CNNs) for encoding local information of unstructured data. In previous works, GCN is always used to process and integrate the semantic and syntactic features simultaneously. Naturally, in this work, GCN is also employedR1 to process the derived adjacency matrix M and the generated feature representation HRR2. For convenience, the detailed transformation is shown R1 on a single graph node Mi,i′ , and HR denotes R1 the semantic representation G1 for the first GCNR2 layer. Besides, the output of
φ-th layer for node i is denoted as Gφ
i , which is transferred
from basic GCN layerR2 G1. For a -layer GCN moduleR2, φ ∈ [1, 2, ..., ], and the final state of node i is denoted as Gi . Subsequently, this computing process of GCN can be
formulated R1 as follows:
Gφ
i = F(
N ∑
i ′=1
Mi,i′ W φ Gφ−1
i + bφ), (8)
where W φ is the weight matrix of linear transformation, bφ is the bias, and F is the activation function (e.g., ReLU).
3.6 Bi-layer sentiment representation
Unlike the sentence-level sentiment classification, ABSA aims to predict the sentiment polarity of the specific aspect terms in a review. Thus, modeling the particular representation for aspect and sentiment information is significant to completing this task successfully. The above transformations have integrated the semantic and syntactic features by GCN entirely, but there is a lack of further consideration to enhance the representation of the specific aspect. Therefore, a bi-layer sentiment representation module is proposed to implement the further extraction on the aspect termsR1, which regards the outcome G of the final GCN layer as the key unit in its
123


6422 X. Shi et al.
transformation. The process can be denoted as:
GS′ = G × G
√dg1
, (9)
GS = GS′ × G
√dg2
, (10)
where dg1 and dg2 are the scale parameters, and is the
symbol of transposeR2. This progress is designed to address the model’s attention to the correlation between aspect terms and the sentiment information in a review, which will assist the model in capturing the vital sentiment features during learning the textual corpusR2. Furthermore, it is worth noting that ITGCN masksR1 out hidden state vectors G S of non-aspect words to highlight the significant aspect features. And the mask is definedR1 as:
HMask =
{
G Saj , if word is in aspect terms,
0, otherwise. (11)
Thus, the feature representation of masked aspect terms are obtained, and it can be denoted asR1 HMask = {0, 0, ..., G Sa1, G Sa2, ..., G Saj , 0, 0}, where HMask ∈ Rd×|N|. Subsequently, depending on the final feature representation extracted by the GCN module, and inspired by [41], the retrieval-based attention mechanism is adoptedR1 to retrieve significant features from the hidden state vectors, which are semantically relevant to the aspect terms. The implementation is conducted as:
ζt = ∑
aj∈A
hr
t G Saj , (12)
θt = exp(ζt )
∑N
i=1 exp(ζi ) , (13)
HF =
N ∑
t =1
θt hr
t , (14)
where htr is the representation of the t-th word learned by the final GCN layer. ζt is the dependency between t-th aspect term and the contextual words. θt is the obtained aspectbased affective dependency of the whole sentence. As the description by (12), the weight of each relation between aspect terms and contextual words, is measured in the high dimensional representation. Then, the weights are normalized by (13), which are treated as the feedback to weight the feature representation htr subsequently in (14)R1. Hereto, the affective information and semantic relatedness between aspect terms and contextual words in the review are modeled by the aspect-specific masking attention. And the affective commonsense knowledge also can be highlighted
in the final constructed contextual representation HF (i.e., (14)). Lastly, a fully connected layer and a softmax normalization layer are utilized to yield a probability distribution Y over the sentiment polarity category (i.e., {Positive, Negative, Neutral}). The transformation is depicted as:
Y = so f tmax(Wy HF + by), (15)
where Wy and by are the trainable weights and bias, respectively.
3.7 Training
The standard gradient descent algorithm is adopted to optimize and update the parameters of the proposedR1 model. Also, the model is trained by minimizing the cross-entropy loss with L2 regularization between the ground truth labelR2 yˆ and the predicted label y:
L=−
V ∑
v=1
C ∑
c=1
yˆ c
k log yc
k + λ ‖ ‖2 , (16)
where V is the number of training samples, C is the number of sentiment classes. λ is the coefficient of L2 regularization term. means all trainable parameters.
4 Experiments and results
In this section, extensive experiments are conducted on five benchmark datasets to validate the effectiveness of the proposed model ITGCNR1. Besides, except for presenting the remarkably main results, other five aspects are also taken into account in the ablation study, which are the impacts of instructions, the number of GCN layers, the learning rate of the model, the optimizer, and the performance affective knowledge, respectivelyR1.
4.1 Datasets and experimental settings
4.1.1 Datasets
The datasets utilized in theR1 experiments are five widely used and available benchmark corpora. Concretely, Laptop14 and Restaurant14 datasets are taken from SemEval 2014 task 4, and the other two are from SemEval 2015 task 12 and SemEval 2016 task 5, which are both in the restaurant domain. The fifth dataset Twitter is collected from the social media platform Twitter. The statistics of these five datasets are shown in Table 2.
123


Aspect based sentiment analysis with instruction tuning... 6423
Table 2 Statistics of datasets
Dataset Positive Negative Neutral Train Test Train Test Train Test
Twitter 1561 173 1560 173 3127 346
Laptop14 994 341 870 128 464 169
Restaurant14 2164 728 807 196 637 196
Restaurant15 912 326 256 182 36 34
Restaurant16 1240 469 439 117 69 30
4.1.2 Parameter configuration
To conduct the experiments successfully, several hyperparameters of theR1 model should be predefined first. The trained weight W and bias b in ITGCN are all randomly initialized with uniform distribution for all neural network layers. RoBERTa is chosenR1 as the backbone network of the encoder, and the number of epochs and batch size are 100 and 16, respectively. The coefficient λ of L2 regularization is 1e −5. ITGCN selects Adam as the major optimizer to optimize and update the parameters of the proposedR1 model, and the learning rate is 1e − 5. The dropout is 0.1. Besides, 6 instruction templates are designed to validate the effectiveness of the model’s idea, which are collected in Table 3. Furthermore, Accuracy (Acc) and Macro-averaged F1 (F1) are adopted as the evaluation metrics.
4.2 Comparison models
To comprehensively evaluate the effectiveness of theR1 model, several models are selectedR1 as the baselines to compare with this work’s experimental results. These baselines mainly consist of LSTM-, BERT- and RoBERTa-based approaches. The selected baselines are describedR1 as follows:
• DTML [49] proposes to boost the performance of ABSA via taking the position prediction of the aspects into account, which fuses the designed position-aware
Table 3 The explanation of the prefixed instructions
Instructions Explanation
IT1 The exactly target aspect in this sentence is
IT2 The target aspect in this sentence is
IT3 The aspect in this sentence is
IT4 The expected target aspect is
IT5 The target aspect is
IT6 The purpose is
attention and the deep bi-LSTM to capture the position information effectively. • Sentic-LSTM [23] considers injectingR2 commonsense knowledge into ABSA data with an exploited version of LSTM. • BERT-SPC [8] obtains the semantics of review and provides sentence pairs with a two-way encoder based on the transformer. • SA-GCN+BERT [16] is a BERT-based model with the GCN module operating on the dependency tree. Selective attention is used to select the top K words with the highest attention scores to enhance the feature representation. • KGAN-BERT [47] captures external knowledge from multiple different perspectives and integrates them with aspect-specific knowledge via an attention mechanism. • EK-GCN-BERT [12] employs GCN to learn the sentiment information from the external sentiment lexicon and part-of-speech information, which can highlight the sentiment words in ABSA data. • MLM-BERT [43] is a multitask learning model combining the Aspect Polarity Classification (APC) task and Aspect Term Extraction (ATE) task that can extract aspect terms and predict aspect polarity simultaneously. • KDGN [37] incorporates domain knowledge, dependency labels, and syntax path with dependency graph to learn a more structured model. • HGCN-BERT [38] exploits the synthesized information from the constituency tree and dependency tree to enrich the textual representation, which enhances the model’s comprehension of training data. • HRLN [3] is proposed to alleviate the pressure of confirming the mapping between the aspect and the core context, which contains a heterogeneous network module, and a knowledge graph-based reinforcement learning module. • RGAT-RoBERTa [6] fuses relational graph attention Network (RAGT) with the feature representation learned from RoBERTa, and finetunes RoBERTa to gain the induced tree which benefits the ABSA task. • DMGGAT [34] jointly considers semantics and syntactic information of multiple granularity features, which adjusts the obtained information with the assistance of GAT and BERT. • IA-HiNET [13] is a novel model, which introduces partof-speech information and position information as a priori knowledge to mine and strengthen the relationship between different aspect words.
To investigate the superiority of the model, it is compared to the above-mentioned LSTM-, BERT- and RoBERTa-based methods. The results of baselines are all copied from original
123


6424 X. Shi et al.
papers or are optimized by the same source datasets for a fair comparison.
4.3 Main results
The experimental results are reported on 5 benchmark datasets in Table 4. For convenience, the results obtained from the experiments based on IT4, are shown in this table for further comparison, which denotes the nearly average level of the experimental resultsR2. From observing the results, four conclusions can be achieved explicitlyR1. Firstly, Table 4 shows that LSTMbased models have the worst performance in all compared groups, because the pre-trained word vector is limited in representing the sentiment information, and the lack of further aspect-specific information exploitation by LSTM makes the results collapse seriously. Secondly, as the outstanding representative of LLM, BERT learns reviews from three aspects (i.e., position, semantic, syntactic) through the stacked transformers. Table 4 shows BERT-based models achieve the most sub-optimal results (e.g., KGANBERT, EK-GCN-BERT, HRLN-BERT-large), and they even achieve the optimal results on the dataset Twitter. For example, the experiment conducted on KGAN-BERT achieves 79.12% and 78.14% for Acc and F1, respectively. Besides,
HRLN-BERT-large also performs extraordinary work in Twitter, which achieves 78.9% for Acc and 79.4% for F1. Moreover, EK-GCN-BERT and SA-GCN+BERT also improve the performance of ABSA tasks within their ability. But, it is evident that the ideal results are not achieved by a single BERT-based method in the table. This indicates that each BERT-based method has its merits in different aspects while modeling the feature information of reviews. Thirdly, unexpected results are obtained by RoBERTa-based methods. The recent worksR2 mentioned on RoBERTa all failed to recognize the correct sentiment information in a review. Finally, the proposed model ITGCN outperforms the reported baseline methods on most datasets, which is also the promotional advantage of the ITGCNR2. For instance, in the Laptop14 dataset, compared to the current state-of-theart (SOTA) KGAN-BERT (Acc: 82.66%, F1:78.98%), the improvements of performance achieved by ITGCN (Acc: 82.76%, F1: 79.37%) are 0.10% and 0.18%. Additionally, in the Restaurant14, ITGCN (Acc: 88.21%, F1: 82.35%) achieves vital improvements (Acc:0.41%, F1:2.15%) than HRLN-BERT-large (Acc:87.8%, F1:80.2% ). Moreover, in the currently listed models, SA-GCN+BERT achieves the SOTA performance (Acc:84.0%, F1:71.0%) on Restaurant15, but the proposedR1 model gets 87.64% for Acc and 75.53% for F1, which are obviously higher than the results of SA-GCN+BERT with the improvements (Acc:3.56%,
Table 4 Experimental results on five benchmark datasets in ABSA
Models Twitter Laptop14 Restaurant14 Restaurant15 Restaurant16 Acc (%) F1 (%) Acc (%) F1 (%) Acc (%) F1 (%) Acc (%) F1 (%) Acc (%) F1 (%)
LSTM DTML − − 73.35 68.93 82.23 72.49 82.72 66.94 88.13 71.48
Sentic-LSTM 70.66 67.87 70.88 67.19 79.43 70.32 79.55 60.56 83.01 68.22
BERT BERT-SPC 72.88 71.78 76.19 70.84 83.74 74.77 − − − −
SA-GCN+BERT − − 80.31 76.99 86.16 80.54 84.18 69.42 91.41 80.39
KGAN-BERT 79.12 78.14 82.66 78.98 87.15 82.05 − − − −
EK-GCN-BERT 75.89 75.16 81.30 79.19 87.65 82.55 − − − −
MLM-BERT 76.59 74.67 80.56 77.00 86.88 81.16 − − − −
KDGN 77.64 75.55 81.32 77.59 87.01 81.94 − − − −
HGCN-BERT 76.52 75.37 79.59 76.24 86.45 80.60 83.91 68.68 91.72 78.71
HRLN-BERT 78.2 77.4 80.0 76.5 87.1 79.9 83.2 69.3 92.5 79.3
HRLN-BERT-large 78.9 79.4 81.1 77.8 87.8 80.2 83.6 69.7 93.0 80.0
DMGGAT 75.99 74.56 80.78 77.57 87.13 81.19 − − − −
IA-HiNET 75.88 75.36 79.45 76.57 86.79 81.84 − − − −
RoBERTa HRLN-RoBERTa 68.9 67.0 66.2 58.4 66.9 35.9 60.5 26.2 76.5 31.0
RGAT-RoBERTa 75.43 74.04 77.43 74.21 82.76 75.25 − − − −
RoBERTa4GCN 74.75 74.00 81.80 78.16 86.23 78.61 − − − −
RoBERTa-MLP 72.76 71.73 81.11 77.11 86.79 79.76 − − − −
Ours ITGCN 76.45 75.41 82.76 79.37 88.21 82.35 87.64 75.53 93.51 79.75
Note: The experimental results of ITGCN are selected from the performance of IT4, which is the moderate level in all mentioned instructions. The bold denotes the highest score, and the underline means the sub-optimal score. Three groups models of LSTM, BERT and RoBERTa are listed for comparison
123


Aspect based sentiment analysis with instruction tuning... 6425
Table 5 The computational efficiency of ITGCN on five datasets in ABSA
IT Twitter Laptop14 Restaurant14 Restaurant15 Restaurant16 Train (s) Test (s) Train (s) Test (s) Train (s) Test (s) Train (s) F1 (%) Train (s) Test (s)
IT1 3497.55 1.04 2185.55 1.67 1356.50 0.95 750.22 0.80 1056.24 0.92
IT2 3497.89 1.03 2146.00 1.68 1357.78 0.95 752.69 0.80 1060.47 0.91
IT3 3505.24 1.04 2129.83 1.67 1374.04 0.96 749.94 0.80 1058.56 0.91
IT4 3525.02 1.04 2150.75 1.69 1379.94 0.95 750.67 0.80 1084.67 0.92
IT5 3507.78 1.02 2137.89 1.66 1376.07 0.95 746.30 0.80 1069.82 0.91
IT6 3493.81 1.04 2163.45 1.66 1391.94 0.96 763.76 0.81 1070.73 0.92
Avg. 3504.55 1.04 2152.25 1.67 1372.71 0.95 752.26 0.80 1065.95 0.92
Note: The IT1−6 are the prefixed task-oriented instructions for current ABSA, and the specific explanation is presented in Table 3
F1:6.11%). The same situation happens on dataset Restaurant16, where ITGCN achieves nearly the same level of results with HRLN-BERT-large in the ABSA task. Although ITGCN does not do very well on dataset Twitter, it still gets average level performance among all listed comparedR2 models. These findings demonstrate that the proposedR1 ITGCN can further implement the SenticNet-based graph very well and effectively extract aspect-specific sentiment information with the designed instructions.
4.4 Computational efficiency
To evaluate the training efficiency of the proposed ITGCN, the train and test times are collected and reported in Table 5. From the table, it is learned that the computational time costs on different manual instruction templates are nearly similar. Under the same configuration of epoch number and batch size, the differences in train and test times are minor, which could be ignored completely because of the significant differences in the performances with regard to Acc and F1 among various instructions. As an obvious phenomenon, the test times on five datasets are all less than 2 seconds, which is
highly proper to utilize the model in practical application in the future. It should be mentioned that the involved total computational parameters in this study are nearly 517.6 M. Thus, to complete this ABSA task, a system on NVIDIA GeForce RTX 3080Ti with 12GB graphics memory is employed effectively, which is enough to provide a working platform to finish the inference of sentiment polaritiesR1.
4.5 Ablation study
Note that extensive ablation studies are conducted to investigate the effects of the individual modules in the ITGCN, and the obtained Acc and F1 scores are reported in Table 6. In the ablation study, this workR1 mainly considers the effects caused by three aspects (i.e., IT: instructions; PS: position-aware relation; BI: bi-layer sentiment representation). Besides, more external experiments have been conducted to validate the cross-impacts from them. For convenience, the unexpected results are marked with the black boxes, which have more than a 2% difference from the baseline ITGCN. From observing Table 6, there are 7 groups of ablation experiments listed in the table. Firstly, IT, BI,
Table 6 Overall ablation results on five datasets
Methods Twitter Laptop14 Restaurant14 Restaurant15 Restaurant16 Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%)
ITGCN 76.45 75.41 82.76 79.37 88.21 82.35 87.64 75.53 93.51 79.75
w/o BI 74.57 73.81 82.29 79.31 82.59 71.95 85.61 71.93 91.88 78.95
w/o IT 76.16 75.57 77.27 73.82 87.59 81.42 85.79 72.58 93.51 81.61
w/o PS 75.72 74.61 82.29 79.46 87.23 81.73 88.75 74.60 93.34 82.08
w/o BI & IT 76.30 75.44 79.62 75.20 87.50 79.59 85.98 70.41 93.18 80.78
w/o PS & IT 76.01 75.00 82.92 79.40 86.79 79.76 85.42 70.56 93.18 79.66
w/o PS & BI 71.97 70.26 81.97 78.66 87.77 81.60 87.08 71.68 93.18 78.19
w/o PS & BI & IT 72.54 70.62 82.60 79.93 88.12 82.32 86.53 71.06 93.02 77.81
Note: BI denotes Bi-layer sentiment representation, PS means position, IT is the abbreviation of instructions, and w/o denoted without. And the results boxed by mean they have more than 2% difference with the compared baseline ITGCN
123


6426 X. Shi et al.
Table 7 The performances of the pre-designed instruction templates on five datasets in ABSA
IT Twitter Laptop14 Restaurant14 Restaurant15 Restaurant16
Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%)
IT1 76.73+0.21 76.05+0.49 82.92+0.16 80.16+0.63 86.25−0.51 79.08−0.99 87.64+0.34 72.62+0.02 94.16+0.55 81.13−0.35
IT2 77.02+0.50 75.90+0.34 81.82−0.92 78.25−1.28 81.79−4.97 73.23−6.84 86.72−0.58 71.45−1.15 93.99+0.38 82.89+1.41
IT3 75.72−0.80 74.91−0.65 84.33+1.59 81.18+1.65 87.59+0.83 80.67+0.60 86.72−0.58 70.14−2.46 92.69−0.92 78.18−3.30
IT4 76.45−0.07 75.41−0.15 82.76−0.02 79.37−0.16 88.21+1.45 82.35+2.28 87.64+0.34 75.53+2.96 93.51−0.10 79.75−1.73
IT5 76.45−0.07 75.33−0.23 82.45−0.29 79.19−0.34 88.66+1.9 83.36+3.29 88.01+0.71 72.52−0.08 93.99+0.38 84.87+3.39
IT6 76.73+0.21 75.77+0.21 82.13−0.61 79.02−0.51 88.39+1.63 82.79+2.72 87.08−0.22 73.36+0.76 93.34−0.27 82.05+0.57
Avg. 76.52 75.56 82.74 79.53 86.76 80.07 87.30 72.60 93.61 81.48
Note: The IT1−6 are the prefixed task-oriented instructions for the current ABSA task, and the specific explanation is presented in Table 2
123


Aspect based sentiment analysis with instruction tuning... 6427
and PS are removed alternately, to probe their impact separately. It can be learned that the model without BI performs worse on Restaurant14 and Restaurant15, and the model performs worse on Laptop14 and Restaurant15 without IT. From another perspective, this indicates that BI and IT can promote the improvement of the model’s robustness, which means the proposed model has a stronger generalization ability with these two units. However, it is also found that the lack of PS for ITGCN has little influence on the overall experimental results. This finding implies that the contribution of PS to ITGCN is limited due to the powerful position embedding ability of the backbone encoder RoBERTa. Secondly, while analyzing the cross-impacts of IT, BI, and PS, it is dramatically that the comparative models all fail to model an excellent representation on dataset Restaurant15, which leads to a collapse of the relevant experimental resultsR1. Besides, the dataset Twitter is also difficult to learn for the model without PS and BI. Therefore, with the above description of ablation experimental results, it can be concluded that the proposed operations instruction tuning and bi-layer sentiment representation contribute a lot to improving the performance of ABSA task, and the position-aware operation can be considered as a strong auxiliary unit to the proposed ITGCN.
4.6 Impact of instructions
From the above ablation studies about the instructions, it is known that the designed instruction tuning impacts the model’s performance obviously. Thus, in this section, this work constructs 6 groups of instructions (i.e., Table 3) with different intensities of tone (which is denoted via the different sentence lengths (ranges from 3 to 8).) for the detailed comparison. And the results are shown in Table 7. From an overall point of view of the table, different instructions will influence the effect of ITGCN more or less, and the improvements motivated by different instructions are distributed on different review datasets. For example, while comparing to the average level on Laptop14 (shown as Avg. in Table 7), IT3 improves the performance of ITGCN with
remarkable growths for Acc (+1.59%) and F1 (+1.65%). However, it weakens the model’s effect on Restaurant15 and Restaurant16 seriously. The same observation can be acquired from the results instructed by IT1, which has worse work on Restaurant14 with a drop (Acc: -0.51%, F1: 0.99%), but it dramatically performs better on the other four datasets. Besides, it is worth noting that IT2 has a large failure in learning Restaurant14, and the results on Laptop14 and Restaurant15 are also lower than the ideal expectation, but it still performs well on Twitter with a slight increase than the Average level (Avg.). From the above illustration, it can be learned that the paradigm of instructions impacts the performance of ITGCN largely. Two reasons may be summarized to explain this phenomenon: (1) The semantic difference between the designed instruction template and the train samples still interferes with the learning of LLMs. (2) It is hard to construct an absolute perfect instruction template for all validated datasets. According to the difficulty, how to design the ideal instruction template for the specific task is still the research hot spot in the community of data miningR2. Due to the different instructing abilities of each instruction to ITGCN, IT4 is selected as the instruction template for the following experiments and analysis, which achieves the results nearly at the average level.
4.7 Impact of GCN layers
Generally, GCN with different layers would learn sparse or dense feature representation from the training corpus, which will impact the extraction of sentiment information completelyR2. On this basis, this work also explores how many GCN layers employed by ITGCN could achieve optimal performance in the ABSA task. Specifically, it sets the number of layers ranges from 1 to 4 with a step of 1. Likewise, extensive experiments are conducted on five datasets, and the results are depicted in Fig. 3. From the observation of the figure, it can be found that different numbers of GCN layers have slight effects on the experimental results, and the differences are distributed on different datasets. Thus, the change caused by layers is marginal. Anyway, it is interesting that
Fig. 3 The impact from different GCN layers in current ABSA task
123


6428 X. Shi et al.
Fig. 4 The experimental results of ABSA with different learning rates
four candidate models with different GCN layers all perform worst on Twitter for Acc, but they eventually get the worst performance on Restaurant15 for F1 scores. Although there is no common optimal value for the number of GCN layers over five datasets, this work chooses 3 layers of GCN to produce the experiments for other modules’ validation, which is based on its average performance on five benchmark datasets and the low computing resource. From the results conducted on other validated experiments, 3 layers of GCN have been proven effective in completing the work of extracting enough knowledge from the affective-based graph for sentiment analysis.
4.8 Impact of learning rate
Currently, LLMs are deep, wide, and nonconvex, and they are effective for representation learning, and become the core components for downstream tasks. While fine-tuning an LLM, selecting a proper learning rate can help it filter the noisy data and keep the useful information simultaneously [15]. Thus, to adjust the backbone RoBERTa to the optimum mode, this work trains it with five different learning
rates and reports the experimental results over five datasets, which are reported via Fig. 4. Through observing the figure, it can be learned that the learning rate impacts the training effect of LLM largely. In the process of fine-tuning, the curves depicted for 3e − 5 and 5e − 5 fluctuate wildly, which indicate that they totally fail to optimize the pre-trained model’s parameters and also imply the models trained by them have a worse generalization ability. Besides, while the learning rate is set to 1e − 5, the performance obviously outperforms the trained checkpoints learned through the other four learning rates. This finding implies that 1e − 5 is the most appropriate learning rate for the proposed ITGCN. Through these extensive experiments, it is clearly revealed that the correct selection of learning rate will have a positive impact on extracting sentiment information efficiency via LLM (e.g. RoBERTa).
4.9 Impact of optimizers
Optimizer impacts the model’s training quality largely. To investigate the influence on the proposed ITGCN from the optimizer, this work conducts extensive experiments on eight
Table 8 The performance of the model with different optimizers on five datasets in current ABSA
Optimizer Twitter Laptop14 Restaurant14 Restaurant15 Restaurant16 Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%)
Adam 76.45 75.41 82.76 79.37 88.21 82.35 87.64 75.53 93.51 79.75
Adamax 74.57 73.30 81.50 77.41 87.23 80.94 87.08 65.88 93.18 77.85
Adadelta
Adadelta 50.00 22.22 53.61 23.76 65.00 26.26 60.15 25.04 76.14 28.82
Adagrad 71.82 71.08 79.78 76.23 81.61 70.26 82.29 59.09 89.61 67.30
Asgd
Rmsprop 72.25 69.57 82.92 80.30 88.39 83.10 87.64 73.88 92.05 79.65
Sgd
Adamw 70.38 69.20 81.50 78.50 86.88 80.11 84.87 70.68 93.18 81.65
Note: The experiments are conducted with the learning rate 1e-5 and the instruction “the expected target aspect is”
123


Aspect based sentiment analysis with instruction tuning... 6429
widely used optimizers to find the most proper one for the proposed ITGCN in the current ABSA task. And the results are reported in Table 8. From the table, it can summarize two conclusions exactly. Firstly, three optimizers fail to train ITGCN completely, which are SGD, ASGD, and Adadelta, respectively. Their experimental results illustrate that they can not help ITGCN extract enoughR2 sentiment information from all five datasets totally. Secondly, it is easy to find that the optimizer Adam performs best in promoting the training of ITGCN. Besides, the performances achieved by the remaining four optimizers (e.g. Adamax, adagrad, Rmsprop, Adamw)R2 are slightly worse than the selected Adam, which suggests they are not completely suitable for the ABSA task currently. Thus, in other extensive experiments, Adam is employed as the optimizer to learn a better version of ITGCN.
4.10 Impact of affective knowledge
Through the investigation of existing works, external knowledge is considered a capable supplement to the original training data and enhances the feature representation learned by the LLM [40, 42]. To validate the effectiveness of SenticNet in the proposed model, two different approaches for knowledge integration are mentioned in this section, which aims to prove the credibility of the graph utilized in ITGCN. They are formulated as follows:
Mi,i′ = ASi,i′ , (17)
̂Mi,i′ = M ′
i,i′ , (18)
where Mi,i′ denotes the graph is totally constructed by the
affective scores indexed from SenticNet, and ̂Mi,i′ means the original dependency graph learned from (4). Moreover, the results conducted on five datasets are collected in Table 9. It is obvious that Mi,i′ based ITGCN achieves the best results in recognizing the relation between aspect terms and contextualR2 sentiment information. Besides, it is found that the performance of Mi,i′ -based approach is close to common ITGCN on four datasets (i.e., Twitter,
Laotop14, Resraurant14, and Restaurant16), which suggests that the operation of taking affective knowledge into consideration has a large impact on re-constructing the dependency graph and is useful to strengthen the sentiment feature representation. On another side, the original dependency graph ̂Mi,i′ performs a little worse without the enhancement of external knowledge, which also proves the significant effectiveness of SenticNet in the proposed ITGCN. In general, the experimental results demonstrate that the knowledgeenabled dependency graph is able to improve LLM’s ability to model the representation of aspect-specific sentiment features. It can be concluded that the joint of LLM and external knowledge-based graph brings improved performance to ABSA task.
4.11 Case study
To gain insight into why the proposed model is more effective, this work selects cases from the five evaluated datasets for a closer look. Specifically, this work visualizes the selected five challenging samples with different shades of color according to their attention weights learned by ITGCN. And the results of visualization are shown in Table 10. Besides, the comparisons of predicted polarity and the ground truth label are also collected in the table. Detailedly, for the two “Negative” samples, the first sample “the dinner was ok, nothing i would have again.” has one aspect term “dinner” R2. The model gives higher scores to “was ok, nothing”, which has an obvious transition on the tone about the perspective for the aspect term “dinner”. And finally, the model predicts the sentiment polarity as “Negative”. However, in the second “Negative” sample, the proposed model grasps the keywords “even offer” in the review, but it ignores the privative word “n’t”, which results in the mistake prediction of sentiment polarity about the whole review. Besides, for the “Positive” samples, ITGCN highlights the “are all outstanding” for the third sample and “It is fast and i have” for the fourth sample, which are both clear that the model’s attention is attracted by the positive words. Lastly, this work selects a sample with the ground truth label “Neutral” for further sentiment analysis. And through the
Table 9 The performance of the model with different dependency graphs on five datasets in ABSA task
Graph type Twitter Laptop14 Restaurant14 Restaurant15 Restaurant16 Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%) Acc. (%) F1 (%)
ITGCN 76.45 75.41 82.76 79.37 88.21 82.35 87.64 75.53 93.51 79.75
Mi,i′ 76.01 74.79 82.76 79.66 87.59 82.30 87.45 72.48 93.83 80.60
̂Mi,i′ 75.43 74.58 83.86 80.33 87.41 81.06 88.01 73.26 93.83 79.01
Note: The experiments are conducted with the learning rate 1e-5 and the instruction “the expected target aspect is”, and the optimizer is Adam
123


6430 X. Shi et al.
Table 10 Cases of attention weights visualization
ID Aspect Text Prediction Label
1 Dinner the dinner was ok , nothing i would have again . Negative ✔ Negative
2 Dessert We did n’t get drink refills and she did
n’t even offer us the option of dessert . Neutral ✘ Negative
3 Ambiance The service , wine selection , ambiance are
all outstanding and deserve recognition . Positive ✔ Positive
4 Internet connection It is fast and i have not had a problem with internet connection or any other problems . Positive ✔ Positive
5 Food The waitress suggested glasses of wine that went very well with the food . Neutral ✔ Neutral
Note: These five samples are randomly extracted from Laptop14, Twitter, Restaurant14, Restaurant15 and Restaurant16, respectively
visualization, it is found that ITGCN gives higher attention scores on the neutral terms “of wine”, which are totally neutral words and indicates that ITGCN has a strong comprehension on the sample. Eventually, the findings from the visualized Table 10 imply that ITGCN is sensitive to recognizing the sentiment information with regard to the aspect terms.
5 Conclusion and future work
In this paper, it proposes an instruction tuning- and bi-layer sentiment representation-based RoBERTa model (ITGCN) for aspect-based sentiment analysis. First, the model leverages the manually designed instruction template to guide LLM RoBERTa to generate the aspect-aware sentiment representation. Then, the position relations among aspect terms and other words in a review are also taken into consideration to highlight the importance of the specific aspects. Subsequently, the introduction of the external affective knowledge SenticNet enhances the proposed model ITGCN’s capability of capturing more affective information while learning the benchmark datasets. Furthermore, a novel bi-layer sentiment representation module is designed to extract and collect the global context features to enhance the utilization of the structured features generated by the dependency tree. Lastly, the experimental results on five benchmark ABSA datasets demonstrate the effectiveness of the proposed method. The ablation study also has been conducted to analyze the influence of the different modules of the model. Likewise, detailed comparisons are presented to evaluate the proposed model more comprehensively. In the future, we will try to introduce more sources of external knowledge to enhance the sentiment feature representation and improve the model’s performance on ABSA, such as the sentiment lexicon. Additionally, the manually designed instruction template limits the improvement of the model’s performance. Thus, we will try to study whether auto-designed instruction can strengthen the learning ability of the model on the ABSA task.
Acknowledgements This research was funded by the National Natural Science Foundation of China under Grant 62176084 and Grant 62176083, and in part by the Fundamental Research Funds for the Central Universities of China under Grant PA2022GDSK0066 and Grant PA2022GDSK0068.
Author Contributions Xuefeng Shi and Fuji Ren prepared the whole plan and conducted the related experiments. Xuefeng Shi, Piao Shi and Min Hu wrote the main manuscript text, and Xuefeng Shi and Satoshi Nakagawa prepared figures, and Xuefeng Shi and Piao Shi prepared tables. All authors reviewed the manuscript.
Data availability and access The datasets analysed during the current study are available from the corresponding author on reasonable request (https://github.com/zhangzheng1997/SSEGCN-ABSA).
Declarations
Competing interests Not Applicable
Ethical and informed consent for data used Not Applicable
References
1. Alkatheiri MS (2022) Artificial intelligence assisted improved human-computer interactions for computer systems. Comput Electr Eng 101(107):950 2. An W, Tian F, Chen P et al (2023) Aspect-based sentiment analysis with heterogeneous graph neural network. IEEE Trans Comput Soc Syst 10(1):403–412. https://doi.org/10.1109/TCSS.2022.3148866 3. Cao Y, Tang Y, Du H et al (2023) Heterogeneous reinforcement learning network for aspect-based sentiment classification with external knowledge. IEEE Trans Affect Comput pp 1–14. https:// doi.org/10.1109/TAFFC.2022.3233020 4. Cheng LC, Chen YL, Liao YY (2022) Aspect-based sentiment analysis with component focusing multi-head co-attention networks. Neurocomputing 489:9–17 5. Cui L, Wu Y, Liu J et al (2021) Template-based named entity recognition using bart. In: Findings of the association for computational linguistics: ACL-IJCNLP 2021 pp 1835–1845 6. Dai J, Yan H, Sun T et al (2021) Does syntax matter? a strong baseline for aspect-based sentiment analysis with Roberta. In: Proceedings of the 2021 conference of the North American chapter of the association for computational linguistics: human language technologies, pp 1816–1829 7. Deng J, Ren F (2021) Hierarchical network with label embedding for contextual emotion recognition. Research 2021. https:// doi.org/10.34133/2021/3067943. https://spj.science.org/doi/abs/
123


Aspect based sentiment analysis with instruction tuning... 6431
10.34133/2021/3067943. https://arxiv.org/abs/https://spj.science .org/doi/pdf/10.34133/2021/3067943 8. Devlin J, Chang MW, Lee K et al (2019) Bert: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of the 2019 Conference of the North American chapter of the association for computational linguistics: human language technologies, vol 1 (Long and Short Papers), pp 4171–4186 9. Feng S, Wang B, Yang Z et al (2022) Aspect-based sentiment analysis with attention-assisted graph and variational sentence representation. Knowl-Based Syst 258(109):975 10. García-Díaz P, Sanchez-Berriel I, Pontiel-Martín D et al (2023) A novel flexible feature extraction algorithm for Spanish tweet sentiment analysis based on the context of words. Expert Syst Appl 212:118817. https://api.semanticscholar.org/CorpusID: 249303298 11. Gu T, Zhao H, Li M (2022) Effective inter-aspect words modeling for aspect-based sentiment analysis. Appl Intell 53:4366–4379. https://api.semanticscholar.org/CorpusID:249532759 12. Gu T, Zhao H, He Z et al (2023) Integrating external knowledge into aspect-based sentiment analysis using graph neural network. Knowl-Based Syst 259(110):025 13. Gu T, Zhao H, Li M (2023) Effective inter-aspect words modeling for aspect-based sentiment analysis. Appl Intell 53(4):4366–4379 14. Han X, Zhao W, Ding N et al (2022) Ptr: prompt tuning with rules for text classification. AI Open 3:182–192 15. Hannan MA, How DNT, Lipu MSH et al (2021) Soc estimation of li-ion batteries with learning rate-optimized deep fully convolutional network. IEEE Trans Power Electron 36:7349–7353. https:// api.semanticscholar.org/CorpusID:229646875 16. Hou X, Huang J, Wang G et al (2021) Selective attention based graph convolutional networks for aspect-level sentiment classification. In: Proceedings of the fifteenth workshop on graph-based methods for natural language processing (TextGraphs-15), pp 8393 17. Hu M, Peng Y, Huang Z et al (2019) Open-domain targeted sentiment analysis via span-based extraction and classification. In: Proceedings of the 57th annual meeting of the association for computational linguistics, pp 537–546 18. Li XL, Liang P (2021) Prefix-tuning: optimizing continuous prompts for generation. In: Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th International joint conference on natural language processing (vol 1: Long Papers), pp 4582–4597 19. Liang B, Su H, Gui L et al (2022) Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks. Knowl-Based Syst 235(107):643 20. Liu Y, Ott M, Goyal N et al (2019) Roberta: a robustly optimized bert pretraining approach. arXiv:1907.11692 Retrieved from https://arxivorg/abs/190711692 21. Lu Q, Zhu Z, Zhang G et al (2021) Aspect-gated graph convolutional networks for aspect-based sentiment analysis. Appl Intell 51:4408 – 4419. https://api.semanticscholar.org/CorpusID: 234181768 22. Ma Y, Peng H, Cambria E (2018) Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm. In: Proceedings of the AAAI conference on artificial intelligence 23. Ma Y, Peng H, Khan T et al (2018) Sentic lstm: a hybrid network for targeted aspect-based sentiment analysis. Comput Cogn 10:639650. https://api.semanticscholar.org/CorpusID:3876403 24. Ma Y, Song R, Gu X et al (2022) Multiple graph convolutional networks for aspect-based sentiment analysis. Appl Intell 53:12985 – 12998. https://api.semanticscholar.org/CorpusID:252751388 25. Mencarini E, Rapp A, Tirabeni L et al (2019) Designing wearable systems for sports: a review of trends and opportunities in human
computer interaction. IEEE Trans Human-Mach Syst 49(4):314325 26. Phan MH, Ogunbona PO (2020) Modelling context and syntactical features for aspect-based sentiment analysis. In: Proceedings of the 58th annual meeting of the association for computational linguistics, pp 3211–3220 27. Radford A, Wu J, Child R et al (2019) Language models are unsupervised multitask learners. OpenAI blog 1(8):9 28. Rani S, Kumar P (2021) Aspect-based sentiment analysis using dependency parsing. Transactions on Asian and LowResource Language Information Processing 21:1 – 19. https://api. semanticscholar.org/CorpusID:245153835 29. Song Y, Wang J, Jiang T, et al (2019) Attentional encoder network for targeted sentiment classification. International Conference on Artificial Neural Networks abs/1902.09314. https://api. semanticscholar.org/CorpusID:67855317 30. Sun C, Huang L, Qiu X (2019) Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence. In: Proceedings of NAACL-HLT, pp 380–385 31. Vaswani A, Shazeer N, Parmar N et al (2017) Attention is all you need. Adv Neural Inf Process Syst 30 32. Wang K, Shen W, Yang Y et al (2020) Relational graph attention network for aspect-based sentiment analysis. In: Proceedings of the 58th annual meeting of the association for computational linguistics, pp 3229–3238 33. Wang X, Li F, Zhang Z et al (2021) A unified position-aware convolutional neural network for aspect based sentiment analysis. Neurocomputing 450:91–103 34. Wang Y, Yang N, Miao D et al (2022) Dual-channel and multigranularity gated graph attention network for aspect-based sentiment analysis. Appl Intell pp 1–13 35. Wu H, Shi X (2022) Adversarial soft prompt tuning for crossdomain sentiment analysis. In: Proceedings of the 60th annual meeting of the association for computational linguistics (vol 1: Long Papers), pp 2438–2447 36. Wu H, Zhang Z, Shi S, et al (2021) Phrase dependency relational graph attention network for aspect-based sentiment analysis. Knowl Based Syst 236:107736. https://api.semanticscholar.org/ CorpusID:244482737 37. Wu H, Huang C, Deng S (2023) Improving aspect-based sentiment analysis with knowledge-aware dependency graph network. Inf Fusion 92:289–299 38. Xu L, Pang X, Wu J et al (2023) Learn from structural scope: improving aspect-level sentiment analysis with hybrid graph convolutional networks. Neurocomputing 518:373–383 39. Zhang B, Li X, Xu X et al (2020) Knowledge guided capsule attention network for aspect-based sentiment analysis. IIEEE/ACM Trans Audio Speech Lang Process 28:2538–2551. https://api. semanticscholar.org/CorpusID:221590623 40. Zhang B, Li X, Xu X et al (2020) Knowledge guided capsule attention network for aspect-based sentiment analysis. IEEE/ACM Trans Audio Speech Lang Process 28:2538–2551 41. Zhang C, Li Q, Song D (2019) Aspect-based sentiment classification with aspect-specific graph convolutional networks. In: Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pp 4568–4578 42. Zhao A, Yu Y (2021) Knowledge-enabled bert for aspect-based sentiment analysis. Knowl-Based Syst 227(107):220 43. Zhao G, Luo Y, Chen Q, et al (2023) Aspect-based sentiment analysis via multitask learning for online reviews. Knowl-Based Syst 110326 44. Zhao M, Yang J, Shang F (2023) Dependency-enhanced graph convolutional networks for aspect-based sentiment analysis. Neural Comput & Applic 35:14195 –14211. https://api.semanticscholar. org/CorpusID:257748316
123


6432 X. Shi et al.
45. Zhao Z, Tang M, Tang W et al (2022) Graph convolutional network with multiple weight mechanisms for aspect-based sentiment analysis. Neurocomputing 500:124–134 46. Zhao Z, Tang M, Zhao FR et al (2022) Incorporating semantics, syntax and knowledge for aspect based sentiment analysis. Appl Intell 53:16138 – 16150. https://api.semanticscholar.org/ CorpusID:254200878 47. Zhong Q, Ding L, Liu J et al (2023) Knowledge graph augmented network towards multiview representation learning for aspectbased sentiment analysis. IEEE Trans Knowl Data Eng pp 1–14. https://doi.org/10.1109/TKDE.2023.3250499 48. Zhou J, Chen Q, Huang JX et al (2020) Position-aware hierarchical transfer model for aspect-level sentiment classification. Inf Sci 513:1–16 49. Zhou J, Huang J, Hu Q et al (2020) Is position important? deep multi-task learning for aspect-based sentiment analysis. Appl Intell 50:3367–3378. https://api.semanticscholar.org/CorpusID: 219331102
50. Zhou Y, Kang X, Ren F (2023) Prompt consistency for multi-label textual emotion detection. IEEE Trans Affect Comput pp 1–10. https://doi.org/10.1109/TAFFC.2023.3254883 51. Zhu Z, Zhang D, Li L et al (2023) Knowledge-guided multigranularity gcn for absa. Inf Process Manag 60(2):103223
Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.
Authors and Affiliations
Xuefeng Shi1,2 · Min Hu1,2 · Fuji Ren3 · Piao Shi1,2 · Satoshi Nakagawa4
Min Hu jsjxhumin@hfut.edu.cn
Fuji Ren renfuji@uestc.edu.cn
Piao Shi shipiao@mail.hfut.edu.cn
Satoshi Nakagawa nakagawa@isi.imi.i.u-tokyo.ac.jp
1 School of Computer and Information, Hefei University of Technology, Danxia Road, Hefei 230601, Anhui, China
2 Anhui Province Key Laboratory of Affective, Hefei University of Technology, Danxia Road, Hefei 230601, Anhui, China
3 School of Computer Science and Engineering, University of Electronic Science and Technology of China, Xiyuan Avenue, Chengdu 611731, Sichuan, China
4 Graduate School of Information Science & Technology, The University of Tokyo, Kitamura, Tokyo 113-8654, Japan
123