A MULTI-TASK TEXT CLASSIFICATION PIPELINE WITH NATURAL
LANGUAGE EXPLANATIONS: A USER-CENTRIC EVALUATION IN
SENTIMENT ANALYSIS AND OFFENSIVE LANGUAGE
IDENTIFICATION IN GREEK TWEETS
A PREPRINT
Nikolaos Mylonas
Information Technologies Institute, 57001, Greece
myloniko@iti.gr
Nikolaos Stylianou
Information Technologies Institute, 57001, Greece nstylia@iti.gr
Theodora Tsikrika
Information Technologies Institute, 57001, Greece
theodora.tsikrika@iti.gr
Stefanos Vrochidis
Information Technologies Institute, 57001, Greece
stefanos@iti.gr
Ioannis Kompatsiaris
Information Technologies Institute, 57001, Greece
ikom@iti.gr@iti.gr
October 15, 2024
ABSTRACT
Interpretability is a topic that has been in the spotlight for the past few years. Most existing interpretability techniques produce interpretations in the form of rules or feature importance. These interpretations, while informative, may be harder to understand for non-expert users and therefore, cannot always be considered as adequate explanations. To that end, explanations in natural language are often preferred, as they are easier to comprehend and also more presentable to end-users. This work introduces an early concept for a novel pipeline that can be used in text classification tasks, offering predictions and explanations in natural language. It comprises of two models: a classifier for labelling the text and an explanation generator which provides the explanation. The proposed pipeline can be adopted by any text classification task, given that ground truth rationales are available to train the explanation generator. Our experiments are centred around the tasks of sentiment analysis and offensive language identification in Greek tweets, using a Greek Large Language Model (LLM) to obtain the necessary explanations that can act as rationales. The experimental evaluation was performed through a user study based on three different metrics and achieved promising results for both datasets.
Keywords Interpretable Machine Learning · Explainability · Sentiment Analysis · Offensive Language Detection · Textual Explanations · Large Language Models
1 Introduction
Machine Learning and more specifically Deep Learning models are used in a plethora of different domains and tasks. The predictions provided by such models can often determine the outcome of important processes in many different sectors and may thus impact human livelihood or incur economic costs. In these scenarios, justifications for these predictions are crucial, as they are used by humans to take important decisions, making interpretability essential, in these domains.
Interpretability comes in many different forms, with the most common being rule based and feature importance interpretations [1]. These kinds of interpretations are not always preferred by non-expert users, as they may lack information in case of the former, or not be as intuitive in case of the latter. To that end, explanations in natural language,
arXiv:2410.10290v1 [cs.CL] 14 Oct 2024


A PREPRINT - OCTOBER 15, 2024
are becoming increasingly popular, as they are more easily understood by end users, while also containing the necessary information to explain the outcomes of machine/deep learning models.
This work explores the concept of multi-task pipelines that can provide both predictions and also explanations in natural language. The concept of multi-task predictions, containing both the labels and the corresponding explanations, has been explored mostly for sequence to sequence models [2], in which a single sequence generation model performs both tasks. In our work, we propose a pipeline that can be used to first provide predictions for text classification problems and then combine the predictions with the input to provide explanations in natural language for the predicted label, through the use of a sequence to sequence model. Unlike a single multi-task model that generates both labels and explanations simultaneously, our proposed pipeline allows for greater versatility by enabling the use of distinct models for each task, with independent performance, facilitating easier optimisation.
We experiment with our pipeline in a low resource language, namely Greek, for performing sentiment analysis and offensive language detection, two text classification problems, on two datasets originating from X (formerly Twitter) posts [3, 4]. To obtain the rationales needed to train the sequence to sequence model, we make use of a Greek Large Language Model (LLM) to generate explanations for each instance of the training/validation sets through prompting. Due to the lack of gold standard explanations for both datasets, the performance of our pipeline is evaluated through a user study on the explanations produced by the sequence to sequence model on the test set using three different explainability metrics: Plausibility [5] and Coherence [6], in conjunction with a new metric introduced in this paper referred to as Perfidiousness.
Our experimental results show that this pipeline can produce adequate explanations when a sufficient amount of training data with accompanying explanations are available, without hampering the performance of the classifier, as it may happen when a single multi-task model is used. Additionally, the quality of explanations provided by our pipeline does not significantly deteriorate, even when trained on machine generated explanations, rather than ground truth rationales. We make our code and user study results publicly available 1.
Overall, the main contributions of this work are: (i) a pipeline that can provide both predictions and explanations, effectively performing two different types of downstream tasks, text classification and text generation, with the results of the former being used as input to the later, (ii) a scheme for combining the information of the textual input with the classification label in order to facilitate the learning process of the explanation generator, and (iii) experimental evaluation in a low-resource language where ground truth rationales are not available, and thus we propose a way of obtaining machine-generated explanations that can be used for training the explanation generator.
2 Related Work
We first discuss the problems of sentiment analysis and offensive language identification in the Greek language, which are the main focus of this study. A lexicon-based sentiment analysis of Greek tweets during the COVID-19 pandemic made use of a network between accounts to aid in extracting the relevant sentiment of the tweets [7]. Another work focused on sentiment analysis on a scale from 1 to 5 [8], where several Machine Learning models, including a Multi-Layer Perceptron and the RBFKernel, were used for the classification task. To deal with the lack of annotated datasets in the Greek language, a rich set of resources for the Greek language was introduced [3], ranging from a manually annotated lexicon, to semi-supervised word embedding vectors, and also annotated datasets for different tasks, including a sentiment analysis dataset. This dataset includes the three basic sentiments, namely Positive, Negative, and Neutral, and contains political tweets annotated with these sentiments. Lastly, an aspect-based sentiment analysis on Greek tweets [9], using though a private corpus, showed the importance of handling aspect extraction and sentiment analysis end-to-end. Concerning offensive language identification in the Greek language, a recent study [4] introduced a manually annotated dataset containing Greek tweets characterised as containing either offensive or non offensive language.
Regarding interpretability, most common techniques provide feature importance interpretations, by assigning a weight to each available feature, based on its contribution to the model’s output; such techniques include the well-known LIME [10], Integrated Gradients (IG) [11], and SHapley Additive exPlanations (SHAP) [12] techniques. Feature importance interpretations, while informative, can sometimes be harder to understand for non expert users, who usually prefer interpretations in natural language. Additionally, such interpretations fail to provide reasoning as to why the model reached a certain prediction, instead simply showcasing the most important parts of the input and their contribution to the decision
To that end, a few works on textual interpretation generation have been recently introduced. Specifically, an approach based on contrastive learning [13] makes use of multi-modal inputs of image and text to provide explanations about
1Released upon publication
2


A PREPRINT - OCTOBER 15, 2024
the meme depicted in the image and whether it constitutes cyberbullying or not. A framework for generating textual interpretations for visual tasks, called LangXAI [14], makes use of advanced vision models to provide explanations that are easily understood by human readers and consists of two steps. First, an interpretability technique is used to generate saliency maps for the predictions of a computer vision model; these saliency maps, which are a form of feature importance interpretation, are then given as input, along with the image and ground truth, to a generative model tasked with providing an adequate explanation through prompting.
To evaluate interpretations, human annotated rationales are required. Rationales constitute ground truth explanations, often provided by human experts. When such rationales are available, standard evaluation metrics, such as the F1-Score, can be applied. Additionally, for explanations provided in natural language, the Simulatability [15] metric can be used, which operates under the notion that a model is considered simulatable, if another model can predict its outputs. In particular, Simulatability evaluates the explanation based on how well a second model can predict the output of the original model, when given the explanation as input.
However, human annotated rationales are generally hard to find and, therefore, interpretability evaluation can also be performed with unsupervised metrics that usually operate by evaluating certain elements of the interpretation. In the context of feature-importance interpretations, such metrics include Robustness [16], which measures the stability of the interpretation, Comprehensibility [17], which calculates the percentage of informative features on the interpretation, and Faithfulness [18], which measures how faithful the most important feature, according to the interpretation, is to the prediction. Additionally, there is, the Plausibility [5] metric, that is used to evaluate how convincing the interpretation is to humans, regardless of whether the interpretation concerns the correct label or not
When possible, the most appropriate way to evaluate interpretations would be by conducting a user study, as the main purpose of an interpretation is to explain to the user why a certain prediction was made. The standard protocol in these kinds of scenarios is for the users to compare the interpretations provided by various techniques and asses their quality [19]. However, such human evaluations can be prone to bias, leading to skewed results in certain scenarios [20].
Models that are able to perform both the task of prediction and the task of explanation are called self-rationalising models [21]. Such models are most commonly sequence to sequence classification models trained on both tasks [2, 22] and are able to provide plausible free text explanations for different sequence to sequence tasks. Those tasks include natural language inference [23] and machine translation [24] among others, and for the self-rationalising models to be trained both ground truth labels and ground truth rationales are required.
The combination of different types of tasks (e.g. classification and free text explanation generation) is less explored. One way of dealing with such scenarios is the employment of pipelines where (i) the model employed first is used for making the prediction for the downstream task, and (ii) the following model is used to generate the rationales based on the input and the prediction. This approach has been used to provide free text explanations for different sequence to sequence tasks including summarization [21], while our work follows this paradigm in order to perform sentiment analysis on Greek tweets, which is a sequence labelling task, and to provide accompanying explanations in natural language.
3 Text Classification with Natural Language Explanations
This work introduces a pipeline that produces both predictions and accompanying explanations in natural language for text classification problems. Specifically, we propose a two step process that first classifies a text with a classification model and then combines the predicted label with the input text to generate textual explanations through a sequence to sequence model in a conditional generation approach. This pipeline can be used for any text classification problem, provided that ground truth rationales are available to fine tune the second model.
In particular, the proposed pipeline is comprised of two different models that are trained independently and are used sequentially during inference to provide label predictions along with the explanations. The first model, called the classifier, provides the predictions for the text classification task and can be any model capable of handling textual inputs. The second model, called explanation generator, explains in natural language the predictions of the previous model. The input of this model should contain information about the provided input text along with its corresponding label. To that end, we create a new composite text by integrating the information of the input along with its label in the following fashion: “{input text} has {label} label”. This was done to integrate information about the label during the training of the explanation generator, thus resulting in explanations that support the instance’s label. Therefore, we produce these composite texts for each instance of the training set and use them to train the explanation generator. Additionally, we require ground truth rationales about the instances of the train set in order for the explanation generator to be trained.
3


A PREPRINT - OCTOBER 15, 2024
After the two models comprising the pipeline are trained, the pipeline can be used to predict the label of any incoming text and provide the explanation behind that prediction. Thus, the pipeline not only serves as a tool for classification, but can also be a valuable resource in increasing the transparency of Machine/Deep Learning models. An overview of our our proposed pipeline can be seen in Figure 1.
Figure 1: Proposed pipeline for predictions and natural language explanations
4 Evaluation Protocol
This section describes the evaluation protocol followed in this work. Specifically, we describe the datasets used and how we obtain the explanations that act as rationales to build the explanation generator, along with our choices for the two models comprising our pipeline. Additionally, we describe the metrics and the process followed to evaluate the quality of the explanations generated by the proposed pipeline through a user-based evaluation.
4.1 Datasets and Rationale Creation
While interpretability datasets do exist [25, 26], few provide rationales in natural language and to the best of our knowledge, none of them are for text classification tasks. Therefore, we opt to create rationales for a dataset within a well researched domain, namely that of sentiment analysis, where rationales can add significant value to the predictions.
In particular, we perform our experiments on two datasets one concerning the sentiment analysis of Greek tweets [3], and the other offensive language identification on Greek tweets [4]. The former dataset contains tweets annotated with one of three sentiments Positive, Negative or Neutral, with the tweets having political content, while the latter contains tweets annotated as either Offensive or Not Offensive, covering a larger variety of topics. Both datasets were split into training/validation/test sets using the following scheme 70%/10%/20%. It is worth noting that the distribution of sentiments in the first dataset is heavily imbalanced. Specifically, out of the 1640 total instances in the dataset, only 79 (4.83%) of them have a positive sentiment, with 582 (35.48%) having negative, and 979 (59.69%) having neutral sentiments. Concerning the latter dataset, there is also an inherent imbalance as is common in offensive language identification problem with 2391 (71.48%) instances out of the 3345 total ones being characterised as containing non-offensive language, and 954 (28.52%) instances as containing offensive language.
To create the rationales needed to train the explanation generator, we exploit a generative LLM through a custom created prompts sequence. This approach, even though not ideal, can help us deal with scenarios where ground truth rationales are absent, which is the majority of cases.
In particular, we employ the Greek LLM Meltemi (https://www.ilsp.gr/en/news/meltemi-en/) which is built on top of Mistral-7B [27] and has been trained on a very large corpus of high-quality Greek texts. We use its instruction-tuned variant Meltemi-7B-Instruct-v1, as we want to obtain the explanations through prompting.
We use two different prompts to obtain the explanations (see Table 1). The first one is used to condition the model so that it returns the output in the desirable format. The second one is the query made to the model, giving it information about the input text and the label, and asks it to return as output the explanation why the label is associated with the input text. Since the model is pre-trained on a Greek corpus, the queries used are constructed using the Greek language; Table 1, however, provides both the original prompt in Greek along with the translated version in English, for ease of understanding. The prompts showcased in Table 1, concern the sentiment analysis dataset, however the ones for
4


A PREPRINT - OCTOBER 15, 2024
Table 1: Original and Translated Prompts used for the Greek LLM Meltemi
Conditioning Prompt
Θα σου δώσω ένα κείμενο το οποίο έχει χαρακτηριστεί με ένα sentiment. Θέλω να μου επιστρέψει μια πρόταση μόνο που να επεξηγεί τον λόγο για τον οποίο το κείμενο αυτό να χαρακτηριστεί με το sentiment αυτό. Μην γράψεις τίποτα άλλο πέρα από την πρόταση που να επεξηγεί το sentiment. Translation I will give you a text that has been labelled with a sentiment. I want you to return only one sentence explaining why this text has been labelled with this sentiment. Do not write anything other than the sentence explaining the sentiment.
Query Prompt
Το κείμενο: {input text} έχει χαρακτηριστεί με το ακόλουθο sentiment {label}.
Γράψε μου μια πρόταση που να εξηγεί γιατι το κείμενο χαρακτηρίστηκε με το sentiment. Translation The text: {input text} is labelled with the following sentiment {label}. Write a sentence explaining why the text is labelled with the sentiment.
the offensive language identification are similar only being slightly different to account for the different type of text classification task.
Using those prompts, we obtain for each instance of the training and validation set the explanations required to train the explanation generator model. Table 2 provides an example explanation produced by Meltemi for an instance of the sentiment analysis dataset, along with the input text and the label for which the explanation was produced. Yet again, as the input text and the explanation are in Greek, we also provide the translation in English for both; additionally, we remove any mentions of names or political parties, in the showcased samples.
Table 2: Input Text, label and Meltemi generated explanation for an instance
Original
Input Text: Εκλογές 2015: Επίσημη παρουσίαση του
ONOMA από KOMMA και ΚΟΜΜΑ
Sentiment: Neutral
Explanation: Το κείμενο παρουσιάζει τον ONOMA ως υποψήφιο για τις εκλογές του 2015, χωρίς να εκφράζει καμία θετική ή αρνητική γνώμη για αυτόν. Επομένως, χαρακτηρίζεται ως ουδέτερο. Translation Input Text: 2015 Elections: official presentation of NAME by POLITICAL PARTY and the POLITICAL PARTY Sentiment: Neutral
Explanation: The text presents NAME as a candidate for the 2015 elections, without expressing any positive or negative opinion about them. It is therefore described as neutral.
4.2 Model Selection
As our experimental evaluation focuses on two different text classification tasks in Greek tweets, we decided to use Greek-BERT [28] for the first model (classifier). Greek-BERT is a BERT [29] model, pre-trained on a large corpus of
5


A PREPRINT - OCTOBER 15, 2024
Greek data, including texts from the Greek part of Wikipedia and European Parliament records in Greek. Once the model is fine-tuned on a task-specific dataset, it can then be used to provide predictions for incoming text instances. It should be noted that the classifier was fine-tuned for 15 epochs for the sentiment analysis dataset, and for 10 epochs for the offensive language identification dataset.
For the second model (explanation generator), we chose BART [30], a model that has been widely used for different sequence to sequence tasks, including summarisation and machine translation. BART is easy to implement and fine-tune, in addition to its adequate performance in different datasets, thus making it a solid option. Our explanation generator was fine-tuned for 15 epochs for both datasets.
4.3 User-Centred Evaluation
As the proposed pipeline consists of two models, each accomplishing a different task, we evaluate each model separately. Additionally, since the tasks of sentiment analysis and offensive language identification have gold standard annotations in the dataset, our user-centred study is focused only on evaluating the second part of the pipeline, namely the generated explanations, for which no ground truth rationales are available. To that end, we used the F1-Score and Balanced Accuracy metrics to evaluate the performance of the classifier part of the pipeline, while for the explanation generator, we conducted a user-centred evaluation through a user study.
As there are no ground truth rationales for the datasets we used, we cannot evaluate our explanations using metrics such as Simulatability [15]. Additionally, since we have explanations in natural language, we cannot use unsupervised metrics such as faithfulness-based metrics. Based on these facts, the most appropriate way to evaluate our explanations is through a user study.
We evaluate the quality of the created explanations through a user study using three different metrics. Specifically, we employ the Plausibility metric, designed for natural language explanations. This metric quantifies how plausible the provided explanation is to the predicted sentiment according to the input text. A plausible explanation should explain to the user the reason why the sentiment was selected for that input text, even if that sentiment is incorrect. Based on that, this metric tries to quantify how convincing the explanation is to the end user.
The second metric is called Coherence and quantifies how coherent the explanation is. A coherent explanation should be very close to a human written text, being devoid of syntactical or grammatical errors. On the other hand, an explanation with very low coherence would be akin to a text with randomly placed words, without any context or meaning.
For the final metric, we propose Perfidisouness which aims to quantify how good a generated explanation is for each instance, concerning a label other than the predicted one. Specifically, an explanation with high Perfidiousness should be faithful to any label other the predicted one, while achieving low values when the explanation is faithful to the predicted label. As a result, Perfidisouness measures the explanation generator‘s ability to capture label information from the textual context, ignoring the provided predicted label.
For example, let us suppose that we have a textual input that was predicted by our pipeline to have a Neutral sentiment. In case the explanation accompanying that prediction describes the example as Neutral and the reasons why it was predicted as such, Perfidiousness would be low. On the other hand, in case the explanation interprets the instance as Positive or Negative, giving also a reasoning as to why that is, Perfidiousness would be high. We use Perfidiousness to gauge the effectiveness of the proposed label conditional prompting approach and simultaneously as an indicator for the necessity of the classification task in the pipeline; in particular, if our explanations exhibit high Perfidisouness, there would be no need to integrate label information during the explanation generation process.
Two user studies were performed one for each dataset, with the same number of participants, specifically 15 native speakers all of whom had a technical background with some knowledge of machine/deep learning, but not necessarily explainability. We planned for the user study to be performed on 10 random instances per label on the corresponding dataset, resulting in 30 instances for the sentiment analysis one and 20 for the offensive language identification. However, the test set for the sentiment analysis dataset, contained only 8 examples for the positive sentiment. Based on that, to keep a balanced distribution of the selected instances, we randomly selected 11 instances with neutral and 11 instances with negative as the predicted sentiments, along with the 8 ones that had the positive sentiment. Since the number of instances per label is larger in the offensive language identification dataset, we did not face any such issue in that case.
The same instances were presented to all users for both user studies, giving them details about the input text, the label, and the explanation. The users rated the generated explanation according to each metric, using a score from 1 to 10 (with higher values denoting better performance). The scores of the metrics were then computed as the average score each user gave for each instance and then the average across all the instances was computed.
6


A PREPRINT - OCTOBER 15, 2024
Table 3: Average performance of explainability metrics per examined sentiment
Sentiment Plausibility Coherence Perfidiousness Neutral 9.00 7.68 1.41 Negative 6.34 5.06 2.41 Positive 5.46 4.29 2.96 Overall 7.08 5.82 2.19
Table 4: Average performance of explainability metrics for offensive and not offensive labels
Sentiment Plausibility Coherence Perfidiousness Offensive 7.93 7.50 1.83 Not Offensive 8.45 7.37 1.67 Overall 8.19 7.44 1.75
5 Experimental Results
It is worth noting, that our experimental results do not focus as much in the classification task, as both text classification problems studied in this work are generally considered as ’solved’ with the right tuning. Instead, this work mostly focuses on the evaluation of the generated textual explanations. Having said that, we will first showcase the results for the sentiment analysis dataset, based on the Balanced Accuracy and macro averaged F1-Score metrics. We additionally, showcase the F1-Score for each specific sentiment. The performance is the following: Balanced Accuracy (0.929), F1 (0.798), F 1P os (0.583), F 1Neg (0.875), F 1Neu (0.936). We can see that, as expected, the model performs much better for the Neutral (Neu) and Negative (Neg) sentiments, as there are a lot more available training data for these two sentiments. On the contrary, the performance shows a significant drop for the Positive (Pos) sentiment, where only a small amount of training data are available.
Regarding the evaluation of the explanations the average value for each metric according to the user study is presented at Table 3, for each different sentiment and overall.We can see that the provided explanations seem to have pretty high Plausibility and relatively average Coherence, according to the different users. Concerning the Perfidiousness metric, the value reported by the users seems to be pretty low, leading us to believe that in most cases our explanations are faithful to the predicted sentiment and not on different ones.
Due to the imbalance of the dataset for certain sentiments, there are not enough training data to train the explanation generator adequately, and therefore it is expected that the explanations provided for those sentiments are of inferior quality. The provided explanations concerning the Neutral sentiment, which is the one with the most examples in the training set, seem to be of higher quality for both Plausibility and Coherence, when compared to the other two sentiments. This is expected as the sequence to sequence model has encountered more examples with that sentiment during training, and therefore was able to better understand how to provide explanations for it. Similarly, the performance for the Negative Sentiment seems to be better when compared to the Positive sentiment, due to the very low number of instances labelled with the latter in the training set.
For the offensive language identification dataset, the results of the classifier are the following: Balanced Accuracy (0.866), F1 0.855, F 1NotOff (0.91), F 1Off (0.8). Similarly to before, the performance of the classifier is high even without any sophisticated tuning procedure. This time however, unlike the previous dataset the model performs similarly for both labels, as there is not a noticeable imbalance in the dataset. For the evaluation of the interpretations, the average value for each metric can be found in Table 4. We can see that the explanations provided for the language identification dataset, seem to have higher overall values for Plausibility and Coherence metrics. This is expected, as both those labels had an adequate amount of train examples for the explanation generator, to learn how to provide suitable explanations. Additionally, the Perfidiousness metric has a lower value for this dataset according to the users, meaning that more training data result in explanations that are more faithful to the predicted label, as the generator is able to better distinguish between the different labels.
All in all, we can see that the explanations provided by our pipeline seem to have high Plausibility, especially when concerning a label with an adequate amound of training data, meaning that they are able to capture and explain the predicted label of each text. Additionally, they have relatively high Coherence, meaning they provide a text that is understood by humans, but may not always be as clear as a human written text. The Coherence of the explanations seems to yet again increase as a higher amount of training data is provided to the generative model. Finally, the Perfidiousness of the generated explanations is low in both datasets, meaning that the provided explanations do not represent any other label besides the predicted one. An exception appears in the case of the sentiment analysis dataset, and specifically for the Positive sentiment, in which a low relative F1 score is achieved by the classifier
7


A PREPRINT - OCTOBER 15, 2024
6 Conclusions
In this paper, we presented a pipeline that can provide both predictions and the corresponding explanations for text classification problems. This pipeline consists of two separate models, one classifier for providing the predictions and one sequence to sequence model which produces the explanations, and can be used to provide transparency in domains where text classification is used.
Through experimentation, we found that the explanations provided by the pipeline contain sufficient information for users, while being coherent for the most part, meaning that they resemble human written text. Additionally, as the classifier model is trained independently of the explanation generator, the performance of the model in the downstream task of text classification does not deteriorate. Additionally, it was found that the quality of explanations increases, when more training data are available. This is expected, as the explanation generator, has more available data and rationales to learn from, and therefore is able to produce not only more plausible explanations, but more coherent ones as well.
As future work, we aim to extend our experimental procedure to include a larger number of datasets from the English language, ideally ones where human annotated rationales are present. Based on our initial findings, we hope that with more available data, the quality of the produced explanations will increase both in Plausibility, as well as Coherence, as was indicated by the higher quality explanations produced for the Neutral sentiment and the offensive language identification dataset, where more data were available.
A larger user study that includes more human users from different backgrounds will also be conducted to have a more representative sample of real users for our evaluation. Furthermore, we will also experiment with using a single self-rationalising model that can provide the predictions and explanations at the same time to compare how the quality of predictions and explanations changes. Finally, testing different models for the classifier and explanation generator is another future research direction we plan to tackle.
References
[1] Mirka Saarela and Susanne Jauhiainen. Comparison of feature importance measures as explanations for classification models. SN Applied Sciences, 3(2):272, Feb 2021.
[2] Hui Liu, Qingyu Yin, and William Yang Wang. Towards explainable nlp: A generative explanation framework for text classification, 2019.
[3] Adam Tsakalidis, Symeon Papadopoulos, Rania Voskaki, Kyriaki Ioannidou, Christina Boididou, Alexandra I Cristea, Maria Liakata, and Yiannis Kompatsiaris. Building and evaluating resources for sentiment analysis in the greek language. Language Resources and Evaluation, 52(4):1021–1044, December 2018.
[4] Zesis Pitenis, Marcos Zampieri, and Tharindu Ranasinghe. Offensive language identification in Greek. In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 5113–5119, Marseille, France, May 2020. European Language Resources Association.
[5] Bernease Herman. The promise and peril of human evaluation for model interpretability, 2019.
[6] Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jörg Schlötterer, Maurice van Keulen, and Christin Seifert. From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai. ACM Comput. Surv., 55(13s), jul 2023.
[7] Dimitrios Kydros, Maria Argyropoulou, and Vasiliki Vrana. A content and sentiment analysis of greek tweets during the pandemic. Sustainability, 13(11), 2021.
[8] Nikolaos Spatiotis, Iosif Mporas, Michael Paraskevas, and Isidoros Perikos. Sentiment analysis for the greek language. In Proceedings of the 20th Pan-Hellenic Conference on Informatics, PCI ’16, New York, NY, USA, 2016. Association for Computing Machinery.
[9] Georgios Aivatoglou, Alexia Fytili, Georgios Arampatzis, Dimitrios Zaikis, Nikolaos Stylianou, and Ioannis Vlahavas. End-to-end aspect extraction and aspect-based sentiment analysis framework for low-resource languages. In Kohei Arai, editor, Intelligent Systems and Applications, pages 841–858, Cham, 2024. Springer Nature Switzerland.
[10] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’16, 2 2016.
8


A PREPRINT - OCTOBER 15, 2024
[11] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks, 2017.
[12] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 4765–4774. Curran Associates, Inc., 2017.
[13] Prince Jha, Krishanu Maity, Raghav Jain, Apoorv Verma, Sriparna Saha, and Pushpak Bhattacharyya. Meme-ingful analysis: Enhanced understanding of cyberbullying in memes through multimodal explanations, 2024.
[14] Truong Thanh Hung Nguyen, Tobias Clement, Phuc Truong Loc Nguyen, Nils Kemmerzell, Van Binh Truong, Vo Thanh Khang Nguyen, Mohamed Abdelaal, and Hung Cao. Langxai: Integrating large vision models for generating textual explanations to enhance explainability in visual perception tasks, 2024.
[15] Finale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning, 2017.
[16] David Alvarez Melis and Tommi Jaakkola. Towards robust interpretability with self-explaining neural networks. In Advances in Neural Information Processing Systems, pages 7775–7784, Montreal, Canada, 2018.
[17] Marko Robnik-Sikonja and Marko Bohanec. Perturbation-based explanations of prediction models. In Human and Machine Learning - Visible, Explainable, Trustworthy and Transparent, pages 159–175. Springer, International, 2018.
[18] Chun Sik Chan, Huanqi Kong, and Liang Guanqing. A comparative study of faithfulness metrics for model interpretability methods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5029–5038, Dublin, Ireland, May 2022. Association for Computational Linguistics.
[19] Piyawat Lertvittayakumjorn and Francesca Toni. Human-grounded evaluations of explanation methods for text classification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP, November 3-7, pages 5194–5204, Hong Kong, China, 2019. ACL.
[20] Bernease Herman. The promise and peril of human evaluation for model interpretability. ArXiv, abs/1711.07414, 2017.
[21] Sarah Wiegreffe, Ana Marasovi ́c, and Noah A. Smith. Measuring association between labels and free-text rationales, 2022.
[22] Zheng Tang, Gus Hahn-Powell, and Mihai Surdeanu. Exploring interpretability in event extraction: Multitask learning of a neural event classifier and an explanation decoder. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 169–175, Online, July 2020. Association for Computational Linguistics.
[23] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
[24] Upol Ehsan, Brent Harrison, Larry Chan, and Mark O. Riedl. Rationalization: A neural machine translation approach to generating natural language explanations, 2017.
[25] Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. Hatexplain: A benchmark dataset for explainable hate speech detection, 2022.
[26] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. ERASER: A benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the ACL, pages 4443–4458, Online, July 2020. ACL.
[27] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023.
[28] John Koutsikakis, Ilias Chalkidis, Prodromos Malakasiotis, and Ion Androutsopoulos. Greek-bert: The greeks visiting sesame street. In 11th Hellenic Conference on Artificial Intelligence, SETN 2020, page 110–117, New York, NY, USA, 2020. Association for Computing Machinery.
[29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.
[30] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019.
9