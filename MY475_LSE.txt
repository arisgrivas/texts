Lecture 1:

Yeah.

I Yeah.

OK.

Is the Oh Oh.

Oh.

Yeah I.

Yeah.

It's just kind of a hard used to CBG.

be.

Yeah Yeah.

Yeah.

I didn't talk.

for this.

time.

Yes.

This is Yeah.

OK.

Yeah Yeah.

I I.

for 103.

Oh.

Yeah OK.

I I do.

I It was me.

Yeah I I So What did you say you know.

Yeah.

OK.

Yeah Yeah This is my I.

I'm Yeah Yeah.

New Year.

OK You Yeah.

I.

Uh-huh.

It's like.

Alright, let's, uh, let's get going.

Uh, welcome to, uh, the first, uh, ever lecture of

MY 475.

Uh this is a brand new course this year, uh,

that we've put on, uh, basically from the demand from

you guys in previous, uh, years more advanced, uh, tuition

in your networks, e-learning, the kind of things that in

the last couple of years I think have become incredibly

saging both in academia and outside of it.

today basically is to kind of get you up to

scratch with uh some of the mathematical apparatus we'll need

uh for this course and also just to kind of

motivate exactly how it's gonna run and as ever, there's

gonna be lots of time for you guys to ask

the questions you might have either about how we're gonna

proceed to uh, or if these kinds of questions have

anything, uh, we raise, um, today.

Does anyone have any like questions they just want to

get off their chest right now?

OK, OK, cool, right, so, uh, let's do some kind

of very high level kind of, uh, motivation, uh, and

then, uh, we can, uh, on some maths.

So the question here is why are you here and

maybe not in MY 474, the machine learning course, or

maybe you're doing both of them.

Uh, and so let's kind of just try and think

about exactly why we need this.

So Victoria's someone you've probably all heard of by now.

Uh, he has gone out and has this kind of,

um, um, goalpost where he's kind of trying to espouse

and raise the stock price of Open AI, uh, when

it lists, and he says in the next couple of

decades there's few things that would have seemed like magic

to our grandparents, and there's about 14 paragraphs where he

talks about things like.

Human prosperity, super intelligence, it's all very sickening.

However, at the very end, he just says this.

He said, how do we get to the step of

the next of prosperity, right, that ability that these models,

these deep models that we keep hearing about, how are

they allowing us to do things that our grandparents was

magic?

Well, he said deep learning works, right?

And kind of this could be construed as just more

marketing, uh, but actually I think this is kind of.

A really important paradigm, uh, that wasn't true, say 5

years ago, right?

We don't have neural networks for decades, but actually neural

networks for a long period of time were kind of

a theoretical type of model that we didn't really.

that much, but they were never as performed as around

for us.

They were never as easy to run, uh, as a

lasso regression or anything like that, right?

And so we've had these kind of like cold periods

of neural network science where there was lots of interesting

science going on but little kind of application.

And what we've discovered in the last, say, 5 years

is that actually with enough compute with big enough models

that these networks that we're going to learn about today

just work so well and much better than anything else.

OK.

So beyond any of the hyperbole you read from any

of the tech pros, right, uh, these models are so

good because they're thought to be highly flexible, and we'll

talk about exactly what I mean.

there they're thought to be highly predictive, right?

If you get a good model, it should serve you

exactly what you want.

You want a picture of a dog lying on its

back being scratched by your mom, right?

It can do that pretty well because it's so predictive.

And also they're kind of flex in a kind of

engineering sense in that unlike a random forest which always

looks like a cluster of decision trees going off in

different directions, you can really.

Change exactly what a neural network structure is in order

for it to do kind of very different types of

tasks.

You can have neural networks that just work on tabular

data, but you can also have neural networks that try

to boil down images or do data dimension reduction on

kind of quantitative data or produce text.

All of these things are possible using basically the same

foundational concepts or network concepts, but which A bit of

engineering skills have to do very different things and this

is the purpose of this course is to give you

not just the language of deep learning, but actually some

of those engineering skills so that you realise that the

reason it's taken so long for us to get here,

even though we've had some maths for decades, is actually

because you need to be a little bit of an

engineer as well as a scientist.

And it's not remembering that the news coverage we've had,

this is not only altering society right in the sense

that.

We have political advertisements, advertisements generated by these models.

We have deep faith for those sorts of things, but

it's also changing how we study it, right, how we

can use these tools to actually, um, uh, study even

the effects of, uh, LLMs, uh, and deep models, uh,

in itself.

OK, so what on earth is this thing called deep

learning, OK.

In a very broad sense, it's a type of machine

learning that uses neural networks.

That's its distinguishing feature.

A deep model has to be a neural network.

It's not a rain forest.

It's not an SBM.

It is a neural network and most of the time

is a supervised learning strategy.

So what I mean by that is there is some

kind of target that we want to predict or classify

where even if Text, I want to predict what is

the most likely next word or token, right?

And I'm going to have some examples that I'm going

to feed through this model to get it very good

at doing that prediction.

learning is so great because as I said before, we're

going to use it on kind of conventional social science

data, i tabular data, but also image and text data,

and we're going to do that all in this course

as well.

More specifically.

Deep learning is a neural network model that has at

least two what we call hidden layers to the input

and the output.

So actually in any textbook it's a very niche type

of thing.

It's a specific structure of the neural network where we

have.

Basically lots of parameters that we don't see as researchers,

right?

So we see the input going, then the philtre through

at least two hidden lays and I'll, we'll go through

exactly what happens later and then we just see the

output, uh, uh, at the end, OK.

But what I want to show you today at least

is that the very basics of these neural networks, the

things that we need to actually get this running in

a deep sent is just linear regression, OK?

And this is something so familiar, uh, to all of

you, uh, that is not.

Step change from understanding of how statistics and machine learning

works it's just thinking about how we can structure these

things in a much more complicated kind of interwoven ways

in order to kind of get the fundamental flexibility we

want out of a neural network in a deep model.

OK, and as I said, these are kind of very

flexible models, right?

We kind of think of them as if anything is

going to be able to correctly predict or classify, uh,

the, the kind of in the context that I'm, I'm

working on.

Uh, it's going to be a neural network, and this

isn't some kind of wishy-washy claim.

It is actually a theoretical claim that has various proofs,

OK, and the kind of the general kind of idea

here is such a universal approximation theorem.

OK, so the universal approximation theorem is not in fact

one theorem, but a kind of collection of proofs in

different contexts that makes you say some set of kind

of restrictions right with enough parameters.

OK, I should be able to approximate kind of the

actual true data generating process that functioning right to an

arbitrary level of specificity, right?

So if I want to get the function my model

to be even closer to the true wiggly line, right,

I just need to have more parameters and with sufficient

training, theoretically we should get there.

OK, so there is.

If we found the kind of space a little bit

and that's well beyond kind of the scope of this

course, but there is this idea that it can learn

anything precisely because it is a universal approximate.

In practise you're going to be limited because you simply

can't run a model the size that open AI can,

right?

You're also going to be limited amount of data that

you have.

You can't download these.

like an AI did and therefore it's unlikely that you

are actually going to approximate to a very specific level

true data generating process, but it at least has that

kind of potential capacity to do so and as we

kind of train towards it, we would hope that we

get some way towards a reasonable approximation.

So that's the kind of like big skill, right?

This is everywhere, everyone's talking about it.

Long gone on the daily, just knowing that the linear

ressions can get you a job right.

You need to be able to kind of like converse

and understand these models.

It has these amazing kind of theoretical properties like the

universal approximation theorem, but just remember this is kind of

like an engineering course in a sense we're going to

be talking about different ways of structuring these networks to

get to different, uh, outputs.

Anyone have any questions at this point?

OK, let's kind of whiz through the logistics of the

course, uh, and then we can do some maths.

So, uh, 10 lectures, 5 seminars in this course.

The seminars are each 2 hours long, same as the

lectures.

The 1st 5 weeks are going to be with me.

So today we're going to think about what is deep

learning and do the kind of mathematical background.

Next week we're going to do a really deep.

Dive into the kind of fundamental training algorithm of deep

networks which is called propagation.

Here we're going to do this by hand from scratch

on the board with numbers.

This is not going to be a high level introduction

to back propagation.

It's going to be very gritty.

In week 3, how we kind of settled all of

the kind of theoretical content, so we're going to then

start building deep networks and thinking about the different ways

we can structure them, and we're going to think about

all of the kind of like training restrictions we have.

How do we stop that model from overfitting, what are

the kind of like tricks of the trade in terms

of kind of more of our training algorithms, but also

the kind of things that we can add in things

like dropout, a normalisation, etc.

And in week 4 we're gonna start doing really cool

stuff, OK, so we're going to go from like a

classic prediction like predicting the probability of civil war occurring

in the next year you thinking about things like, well,

how can I generate synthetic data or how can I

do latent dimension analysis using a neural network, how do

we start thinking about images and Working and predicting images

and then in week 5 where I will leave you,

we will kind of do that image classification so we'll

learn how to take literary pixels and predict like a

label right of of what they're doing and then Friedrich

is going to take over.

And Friedrich is going to start you off with 2

weeks on text and the kind of underlying foundations of

large language models we can have a whole week again,

a bit like that propagation week of diving into the

attention mechanism which is this kind of step change.

Structural, um, tool that we kind of learned about, uh,

in the kind of late 2010s which has basically allowed

us to kind of generate chat to ET and everything

like that and then we're going to generalise that to

the level of transformers and you can think about encoder

and decoded text networks all of those things.

Then we're going to do a little bit of a

pivot and you're going to have 2 weeks on reinforcement

learning, and basically these weeks are going to be teaching

you a little bit of kind of the algorithmic content

of reinforcement learning, so kind of deep networks, etc.

etc.

but also to kind of highlight this, sometimes there are

problems where A learning doesn't really work or it doesn't

work as well as you might want to think it

does.

So an example of that would be, and this is

not an example, but something like chess, right?

If you try to build a chess engine, this is

the next best move any data could be just every

grandmaster paper, right?

We can record all those games and we have all

of those games, and we can kind of do it

as as supervised methods.

But that's really what you want to know is kind

of all of the possible moves of which there are

more than patterns in the universe, right, which is the

best one that requires a score.

The marks turn you a score, right?

You don't actually know for any given move how good

it was.

And even if you try to do kind of backend

things like using existing algorithm, who's to say that that

algorithm is the best.

of them actually sometimes we need more to learn just

by doing so reinforcement learning is essentially that's how we

would set a chess against a chess pot and see

who wins.

If we win, well surely this one is better.

So let's learn something from that.

Then let's repeat this millions and millions of times and

hopefully even though we can't get the true veridical score

of best moves from.

We should at least be able to get somewhere close

to it and reinforcement learning allows us to do that,

particularly with, uh, deep networks.

And then finally to kind of cut things off in

week 11, which is your 10 and just walk you

through things like alignment, how do we get these models

to kind of work in specific areas and also how

do we get models to work with multiple types of

data at the same time.

So I've been in an image and some text and

I wanted to kind of return, uh, something else.

Maybe I wanted to return text and an image and

give them some kind of user product.

Um, Cool.

Yes, um, cool.

So that's the pictures.

The seconds are going to be very hands on.

So the 1st 3 are designed by me and then

3 people, uh, the last 2.1, we're gonna, uh, build

our own computation blocks, OK, so we're gonna build from

scratch in Python using just calculus, OK, uh.

And the reason we're doing this is because it forces

you to really understand what's going on under the hood,

and if you can do that, you're already gonna be

learning and notice that there's to draw on that.

OK, that's the time I think we're gonna try.

And then I'm gonna show you, given that you now

understand the foundations, uh, how you actually would do that

in an industrial setting, so we're using Pytorch, which is

one of the, uh, two major APIs right Pytorch was,

um, originally founded by research scientists that matter.

Um, it's, it's quite nice in that it's reasonably high

level, right?

Like you kind of define these networks in a way

that makes sense in the English language, uh, but it's

incredibly powerful, um, and really you don't really need to

know anything else.

We need to now we're gonna do some, uh, so

we're gonna build our own revolutional, uh, neural networks.

Any questions on the lecture topics in 10 minutes?

I'm very sorry that people who are on the floor.

Um, how many are on the floor?

It's 5.

6.

OK, OK, I'll read that back.

I always get rubbish rooms.

It's just like uh.

Um Awesome.

OK, so the next thing to know is this course

is actually called Applied deep learning, right?

Is it applied learning, whichever, but it has applied thing.

OK.

Applied in our department does not mean no maths.

OK, this is not the easier version of the statistics

course because I've stripped out all of the maths.

Applied here just means that we're always going to be

thinking about, well, how could I use the things that

I'm teaching you in a social science setting.

OK, so apart from the first couple of weeks where

we're really just kind of getting to the grips with

the maths.

This course will always have the target of, OK, I've

got missing data.

How do I predict it and how does that help

me get an unbiased regression coefficient or why am I

using image recognition in the social sciences?

What are the possible use cases here?

So we're always going to have that in mind, but

that does not mean that we're going to skip all

of the methods in the meantime to spend longer on

that, OK.

It is going to be quite hard and theoretical, especially

in these first two weeks.

There are probably gonna be some bits of maths that

you haven't done since you were 18 years old, right?

That's totally fine.

Uh, it's the same for me, um, but just stick

at it if you can, right?

Do also remember that if you just feel like this

is gonna be too much work for you, right, that

you still want to do some humans or 74, that's

also got maths in it, but not quite to the

level of this course, right.

The reason I'm doing this, OK, because I could just

give you a course in our within our package you

type build me a neural network and it does it

all for you, right?

The reason I'm not doing that is because you don't

need to do that anymore, right?

I can ask Chat BT right here right now to

build a deep neural network with 16 layers and give

it the example data so it knows exactly what to

do, and jobs are good, right?

It's gonna run, right?

That's just not going to get you employed.

It's not going to be useful for you as research

scientists or to understand the kind of social world around

us.

The other thing is with neural networks, it may run.

You may get an output, and it could be wrong,

right?

That's not like CGPT giving you the wrong code per

se.

It's just that because these are such complicated networks, if

you really don't understand the kind of underlyings in the

algebra that creates them, you.

And into situations where Pytorch, for example, is kind of

trying to be helpful and changes the dimensions of your

data explicitly, you don't realise that you keep getting really

rubbish output, and it's simply because you just kind of

don't really understand what's going on under the, OK, I'm

trying to futureproof your knowledge so that even though the

kind of rate of development here is kind of rapid,

right, it's one of the fastest moving areas of science

at the moment.

You should be able to keep up, OK?

And if you can take these technical skills and apply

them social scientific, uh, areas, kind of, I hope you

guys can kind of be leaders, uh, in kind of

advancing our research and not kind of followers, OK.

Alright.

The next piece of advice, just use Moodle.

Everything is gonna be on Moodle, right, it slides for

the lecture, the link to the recording, uh, the, the

assignments when they come out, etc.

all of that will be on Moogle.

If you have questions about the course.

Ask it on Moodle, send me an email, but if

you have a kind of private question that you'd like

to ask about kind of your participation, of course, by

all means, contact me by email.

But if it's in week one you mentioned the universalation

theorem, do you have a better reference to that, put

that on the forum on the Moodle page so that

everyone can see.

I will respond quicker to the forum than I will

to email.

So just to encourage this kind of sharing of information

and by all means if there's a question on the

forum that you've seen you know the answer to, give

it a shot.

I will still read it.

I will check that I agree with it and I

will post my own comments, OK, but do try and

kind of like help each other by keeping these questions

public rather than just sending them to me by email.

OK, this is the slightly controversial slide which I dislike.

Um, there is no textbook for this module, right?

We're not going to be like this week's chapter one

of um Goodfellow's Deep Learning book, right?

This is.

A kind of pretty hard area because realistically 5 years

ago it was pure research in social sciences on deep

learning.

We're only getting now to the point where actually it

makes sense to build a full half unit course on

deep learning that is appropriate to us as social scientists

here at the LSE, and I just don't think there

is a particularly good textbook for us to use.

This is kind of my and Friedrich's approach to deep

learning and what I think it is important for you

and for us.

So instead, I think there's kind of specific resources that

are helpful, I would just have them, OK, and as

we go along, I will upload specific components of the

deep learning book or maybe other books where I think

they're particularly good examples of kind of discussions on the

concepts that we're talking about.

I don't think that week 2 is going to follow

chapter 3 of going to be.

Uh, it's just not going to help us, OK, uh,

over these, uh, 11 weeks.

If you have any questions, if anything is unclear, use

that for tell me, right?

This is a brand new course.

You are the pioneers of learning deep learning at the

LSC and kind of the applied setting that we're going

to be considering.

Um, and so you should feed that back to me.

I won't be offended.

I won't be angry, but in fact, I'll be very

grateful if you tell me what is unclear or where

you think we need more resources, OK.

If you do want some standard reference point, use the

Goodfellow textbook.

OK, that's free, just click the link in the slides

that will take you exactly where you need to go.

It's pretty fancy.

It's more designed for SATs people than it is for

kind of applied social scientists, uh, but right in conjunction

with, uh, attending these lectures and doing the seminars is

probably, um, sufficient.

What I would recommend.

two YouTube channels, right?

These are kind of phenomenally good, OK.

was a research scientist at OpenAI in the early days,

right?

He is kind of one of the kind of modern

day heroes of e-learning with large language models.

OK.

He has this incredibly detailed set of videos called Neural

Network here to Hero where he literally walks you through

all of the first principles of neural networks.

I think there's now like 7 or 8 of these

lectures, each of the lecture.

The videos are like 4 hours long, right?

They are grim in their length, but actually like every

second of them counts, and they are so full of

information.

And the first one in a couple of weeks that

we're going to cover here are really inspired by those

lectures.

It was eye opening for me when I went back

and taught myself this stuff again, right in the level

of detail I wanted for this course, and it's all

thanks to those lectures, so.

If next Christmas you're kind of at a loss with

what to do for yourself, put yourself into those lectures

and just follow along because it's just incredible.

If you want slightly more reduced on content, right, that's

not going to take you 4 hours per lecture.

You use the 3 blue 1 brown channel.

You've probably seen this already, right, but they have lots

of mass content, lots of slash content, but he also

has some very good kind of explanatory, um, videos on

deep learning.

And the nice thing.

reboot one brown is that they're very visual, where he

has these amazing kind of animation software things which help

show how these kind of linear algebraic concepts actually play

out.

So if you like visual explanations, use 3 blue one

brand.

OK.

And then evaluation.

There will be one practise problem set which I will

set later on this term.

OK, I will try to get this out and I

will try to time it so that it's not clashing

with MY 474 if you're taking that as well, OK?

And that of course is just a practise problem set.

It's not going to turn anything towards your grade.

It is your chance to come up and take risks,

rightly get feedback in a nonjudgmental way ahead.

So do kind of really try to kind of take

that seriously.

We will, you know, put a lot of effort to

giving you good feedback so that you can feed that

through into your actualative assessments.

And before you make assessments, unlike NY47 before, we do

have an exam here, OK, so there's gonna be one

take home assignment at the end of the term.

This is going to be some form of prediction challenge

maybe with a little bit of writing around it to

justify your choices, OK, but notice it's a very small

component, right?

This is partly because I don't prediction challenges and prediction

challenges they're good for me to assess that you can

actually build a model, but they're not kind of, I

don't want 100% of your grade if you have that

you've got an RMSE of 0.78 and you've got an

RMSE of 0.77, that's different, right?

So this is, this is.

It's just going to be about kind of making sure

that you can actually do this in practise, make sure

there's some kind of practical element of the assessment.

The rest of it is going to be an exam,

OK, that's going to be a pen and paper exam.

It's going to require you to do maths.

It's going to require you to link code, right, so

to read the code in front of you without a

computer and to see if you can spot errors or

ways of improving it.

It's going to be about discussing applications and implications of.

Of these deep models.

OK, so please don't be surprised about that exam.

We will of course upload example questions later on in

the term, to give you some familiarity with the style,

right?

And of course over the spring period you can book

office hours, email us, ask questions on the forum if

you would like any information.

Any material I cover in this lecture or which I

signpost explicitly to in a reading, OK, is examinable, right?

Um, there is not going to be a cheat sheet.

You will not be allowed to bring in, um, like

few pages of notes to the exam, because really this

should be about you being able to do this stuff

on the fly without these, uh, references.

It's gonna make it hard.

I'm not gonna beat around the bush.

This is gonna be a really tough course, but I

hope as a result, it will be really useful to

you and something that actually you can kind of carry

on with, uh, after the term and in the exam

finishes.

And any questions on evaluation?

It's gonna be scary.

It's gonna be scary for me as well, don't worry,

uh, but we'll get through it, uh, together.

OK, and then one last thing, um, GPT in this

course, right, we know that these models can make us

more efficient, and we also know that they can help

us learn kind of in the classes over the last

two years.

It's really nice to see people asking GBT stuff because

often it's presenting factual information in very digestible ways, and

I'll have no problem with that, right, particularly for this

course we're basically learning how to build them, right?

They are the current pinnacle of deep learning, so it'd

be absolutely bizarre if I said no, you cannot use

BT.

It just doesn't make.

OK, so from my perspective, and this will be the

same in MY 474, you can use chat GPT however

you want.

If you want to try to get the top prediction

mark in this course by asking Chat GPT to build

your deep learning model, be my guest.

You won't, but like try if you need chat GBT

to pass your code and check if it errors when

you're doing that deep learning application challenge, right?

That's good.

This is all helpful stuff.

Just.

That it's not inflammable, right?

It can make mistakes as we're going to see throughout

this course.

You can have a functioning neural network model, i.e. something

that takes its data in and spits something out and

you complete garbage, right?

It's not like you're going to get an error if

it's wrong necessarily.

So just be mindful that you should really be checking

exactly what it says and what it's doing, often by

cross referencing or kind of like building up some test

cases in your mind of well this should look like

this.

Does that model do that, etc.

etc.

The other benefit of this course is that you can't

use GPT in the exam.

You're not gonna have your computers, so at some point

you're gonna have to start converting, uh, whatever you glean

from that GPT into kind of, um, functional states in

your, uh, way.

Cool.

Any questions?

Yeah, I was just wondering, are there problems to prepare

for the seminars or do you have?

Uh, so I will upload the seminar material like probably

about a week in advance and have a chance to

go through it.

If you want to start it before the seminar, I'll

probably be better because it means that you know more

time in the seminar can be spent discussing solutions, um,

but you know, there's no expectation that you don't have

to the seminar having done anything before.

You'll you'll work through those, uh, worksheets with me in

the, in the, yeah.

Are we going to use Python on the, yeah, yeah,

we're definitely gonna be using Python.

So, um, not, not in this course, um, so you

will need, uh, if you turn up next week having

never used Python.

It's gonna be a rocky start.

Uh, so, uh, if you are kind of only just

getting scripts with Python, uh, just spend a little bit

of time over the next week making yourself familiar with

how to kind of activate an environment, right?

We're not gonna need any packages in the first seminar,

but for seminar too, you will need to have Ty

Torch.

It's a reasonably large download.

You might have spent 30 minutes making sure everyone's laptops

have Pior working properly right, we're never going to get

anything done.

So just try and get your, um, PC skills up

to scratch before you turn up to the seminars so

that we can just jump in and do stuff.

Remember, like I'm gonna assume that I could.

You know what a class is and you know how

to define a class.

I don't need you to like do all the magic

of classes and inheritance and all these things, but we

will in the first seminar define a class, define methods,

instantiate objects from those classes, all of these things.

If the words I'm saying now are a little fuzzy

to you, please just kind of like brush up on

that before you turn up, because otherwise it's just gonna

be, um, a steep steep learning.

So to what extent like these parts and machine learning,

yeah, uh.

Oh they're not here.

OK, so, um, they will obviously overlaps the most routine

learning courses, right?

Um, the benefit is that I'm teaching both of them,

so I know what is in both of them, and

I'm not going to duplicate content wherever possible, OK?

The difference is going to be that the MY 474

machine learning is about the language of machine learning in

general.

OK, so we're gonna cover the big topics like what

is lost, uh, what is cross validation, right, how do

we make sure that our models are overfit.

Etc.

etc.

etc.

we're going to look at a big broad church of

different methods from the Russian-based methods through tree-based methods through,

um, other ties and things like that, right?

So it's much broader and it's much more about the

kind of fundamentals of machine learning or supervised learning, uh,

in general.

This issue is both specific.

We're never ever going to leave the world of neural

networks.

I'm also going to take it as a given that

you kind of understand.

What you know, cross validation is, so we're not going

to go through cross-validation algorithm in this course.

You would get that from MY 474 and I've sequenced

it so that you'll get there roughly the same time

as you can learn it simultaneously.

But I if when I do these lecs, I will

assume that you kind of, uh, know the kind of

fundamental language of machine learning, uh.

Does that does that because I'm going to take.

I think it makes sense.

I think if you unless you've done lots of machine

learning, uh, and you're like fluent in it, then, uh,

I think taking both makes sense as long as you've

got the timetable space, um, to do that.

These are not going to be kind of overlapping courses

in the sense that we're assessing the same content, yeah.

Yeah OK, cool.

So I've left myself about 1 hour and 20 minutes

to get through, uh, maths, which I think is, uh,

roughly about.

I'll give you a break on food, don't worry, uh,

this is not gonna be 2 hours of talking, um,

uh, so let's kind of, uh, do some easy stuff

first.

Um, this slide should, uh, be really easy.

Let's just remember how to add things and multiply them,

right?

So 1 + 1 equals 2.

OK, no problem so far.

OK, 2 times 2 is 4.

There is something slightly, uh, geeky and no idea about

what I've just written there.

What's like slightly weird about how I've written 2 times

2 equals 4.

One's an integer and one's a float.

Yeah, exactly.

One's in one's afloat.

This is an integer right?

You can only a whole number essentially we know that

because it doesn't have a decimal point here.

You can cross it against 2.0, which is exactly 2,

but it could be 0.001 above all the kind of

idea here.

So this is continuous, this is discrete, right.

As you know, any Python, if you bus by an

in by a float, you get a quote.

OK, just bear this in mind, we're going to be

talking about types an awful lot in seminars where we're

trying to make sure that everything has its model as

we want it to.

OK, easy peasy tying and multiplying, tying and adding is.

is basically all we're going to do this entire course,

OK, it's just a question of what are we targeting

and when are we adding.

OK, so then we get to stuff like this.

You're just using those two operators, we can change together

more complicated expressions.

OK, so gracket bracket 2 times 3 close bracket plus

5 close bracket x 7 close bracket bracket, right, is

a way of saying try and find the innermost element,

2 times 3, or is that the innermost element.

I've got too many brackets there.

123, yes I have.

I told you you were pioneers in this course.

OK, well, what would we do?

Right, 2 times 3 is the innermost bit, then we're

gonna add 5 and then we're gonna turn the whole

thing, uh, 57.

I've got 12, And what's quite nice about this is,

even though that's just a statement, if I convert this

to something like a function, right, I say that our

output is now a function of some input X, right,

this now starts to look a bit more like a

model, right?

It's like saying no matter what the value of X

is, if I punt it through, I know to times

it by 2, add 5, and then times it by

7.

The other thing that's going to be really useful to

note is that when you have these kind of complex.

Um Statements or functions sometimes it's easy to break easier

to break them up into like tested functions.

So here what you can think we're doing is, well,

we've got this kind of first bit where I times

the input by 2 and add 5, and then we've

got the second bit where we're times the whole thing,

the result of that by 7.

So let's H X equals 2 x + 5 and

that G of X equals 7X.

These are just two separate functions.

Well now F of X is just applying geovX to

H of X.

OK, so the output of H of X becomes the

input to GovX.

That whole thing is why.

We're kind of happy with this.

Your network is just this nesting because this is why

I want you to get a little bit familiar with

with kind of like nested functions.

OK, it's only that simple, right?

We now need to start thinking about algebra.

So what we were talking about before right here are

kind of scalar.

OK, so scalar are just numbers, they're just a value,

OK, but normally when we think about data.

We need to think about kind of collections of those

values and the kind of simplest collection we can think

of here is a vector.

OK, so a vector is an ordered one dimensional array

of values or scalars.

OK.

So here we've got a, I've set that equal to

the vector 123.

OK, so 2 comes after 1 and 3 comes after

2.

That order matters.

This vector is not the same as 321 or 132,

there's different vectors.

And notice just from a notational perspective I'm telling that

as a lower case letter that's emboldened.

OK, so lower case is going to suggest that this

is one dimensional.

And emboldened is going to suggest that it's an array,

OK, so.

A equals 11 is a scalar, so I would just

represent that as uh a lowercase a that is uh

uh unemboldened or not emboldened.

In my emboldenment we know that we're talking about the

vector essentially.

OK.

And the scales so too with vectors when it comes

to addition.

OK, so if I've got two vectors here, 123, and

then 456, them like that.

If I sum those two vectors, what we do is

we sum the corresponding elements of those vectors.

OK, so the new vector is also of length 3

as a result of doing 1 + 42 + 53

+ 6 and assigning each of those calculations into the

corresponding elements of the new vector.

OK, are we kind of happy with that?

This is the kind of simplest operation that we can

do.

You cannot sum a vector 1234 with a vector 123.

They have to be the same length.

What would we do when we got to 4, right?

But there's nothing left in the other vector.

OK, so if you're summing two vectors, you have to

make sure they're of the same length.

OK, we can also kind of think about matrices as

well.

So a matrix is going to be a two dimensional

array of numbers.

It now has both rows and columns.

OK.

We're going to kind of denote these differently by making

them uppercase.

So emboldened again means.

Multiple elements it's an array of some sort.

The fact that it's uppercase is conventionally going to denote

this as a matrix as opposed to a vector.

OK, lowercase vector, uppercase matrixm means there's an array inside

it, OK.

Now the first thing to note here is that kind

of adding or attracting mass is exactly the same as

with vectors and with scalars.

You take the corresponding elements and you just do the

element wise operation.

OK, so here we're going to do 1 minus 5

and put that in the first.

OK, if we shift along to the first row second

column, then we're gonna do 2 minus 6 shift that

into the second column, etc.

etc.

And now things get a little bit more complicated, OK,

because we're going to have to think about different types

of multiplication.

OK.

So the first thing that we can think of with

multiplication is kind of like multiplying some object, be it

a vector or a matrix, by a scalar value.

The lambda here notice is not emboldened, although probably not

that clear on the whiteboard.

This is just a value.

It could be 2, it could be 3.7.

It could be 197.

That's.

After it's just a value, right?

And when we do lambda multiplied by a vector, well,

a bit like with addition, what we're going to do

is we're going to do lambda times by each element

separately and again our vector will be of the same

length.

So we'll have lambda A1 through lambda.

Each of those are separate multiplication operations.

And the same thing happens with a matrix.

So if we multiply a matrix by just a number,

a scalar, right, then each element of that matrix gets

multiplied by the same number, OK.

What you're gonna find, however, is that in deep learning

we're gonna be doing what's called the dot product, OK?

This is where we're going to multiply two vectors or

matrices, uh, together, which is a little bit different because.

We are not going to do what you might intuitively

think you're going to do, which is multiplied the corresponding

elements of each.

OK.

So let's start off with the vector and then I'll

show you how this generalises to a matrix.

Suppose we've got 22 length vectors here.

A is A1 and A2 and B is P1 and

P.

If I do I, sorry, yes, horizon vectors to indicate

that it was.

Yes, technically I am it's not in this sorry, yes.

This is the transposed version.

I'm just kind of representing it visually rather than uh

having the transposed, but you're absolutely right, uh, that that

that is transposed and I'll show you that probably on

the next slide I think.

So what are we going to do here?

Well, the way it works is we take the first

row of the vector on the left and we times

it by the first column of the second vector.

So in this case where we've, we've only got one

row here, we've only got one column here, OK.

The dot product is A1 times by B1 plus A2

times by B2, OK.

So even though these vectors are both of length 2.

Right, the output of A.B A times by B, this

is the conventional multiplication in linear algebra, is just a

scalar value, one value, right?

It's a 1 times B1 plus A2 times B2.

A technically is a scalar value in a vector, right?

We keep it within a vector shape, and now it

has a length of 1.

It's just that number, OK.

So you take the first row of the first vector,

and you multiply it by the, the first row of

the second vector, and you add the kind of terms

together.

OK.

If you really wanted to do them kind of more

intuitive or just times one by the other element wise

and leave it at that maintaining the same dimensions, we

would call that the Hadamard product, OK, and you would

represent it with this kind of circle with a dot,

uh, inside it.

So the Hadaard product.

Is A1B1, A2B2 a separate.

Again, to do the Hadamard product.

The two vectors would need to be of the same

length.

It's not true of the vector modification.

You get a general licence.

Any questions on this kind of very basic is very

into that we're gonna build on it, it, it's not

necessarily a basic concept.

OK.

So how does this work with matrices then?

So let's say we, again, we've got two matrices of

the same size, A and B.

What happens if we dot product those two things together?

We're gonna work through this slowly, so we're gonna do

a work example in a minute, but let me give

you the algorithm that you can always follow to do

this.

So we're gonna have two kind of position indexes, that's

called that I and J, OK.

Uh, and what we're gonna do is we're gonna take

the ice row of the left matrix and the J

column of the right matrix, and we're gonna do the

dot product of those two vectors, and then we're gonna

store that in a new matrix at position I J.

And then we're going to repeat this iterating over both

I and J on every row and every column of

the two matrices.

That is the general algorithm.

If you were to do any algebra as a stats

class at any point, you would just know those steps

and you would just do it very slowly, but hopefully

you didn't make any mistakes.

So in this case, our matrix multiplication AB is still

a kind of two row, two column matrix, but notice

that the terms are quite weird and quite intuitive on

first glance.

Let's kind of put this together from scratch.

So I'm gonna colour code it so it's slightly easier

to see what we're taking out from where, OK, so

the colour code, the rows in a in the columns,

uh, in B.

So we're going to start off in the top left,

OK, so we'll take I 1 J 1, so the

first row of the first matrix and the first column

of the second matrix.

We do the dot product of those two vectors, OK,

so A1A2.p product with B1B3 is just A1B1, A1b1 plus

a 2.

A 3.

OK, now we're on I equals 1 J equals 1,

so in our new matrix, we're gonna put this into

the first.

Row and first column.

OK, now let's increase J by 1 but keep I

equal to 1.

So we're still in the first row, right, but now

we're in the second column.

So now I've got A1A2, and I've got B2, B4.

Take the dot products for those two things.

So A1 times by B2, A2 times by B4 on

those elements together.

We're now at I equals 1, J equals 2, so

that's going to be the first row, second column.

We're going to pop that down here into that position.

OK, we can do the same thing for now setting

I equals to 2 and J equals to 1.

So the orange row and the blue column, and then

finally we're both equal to that's the orange row times

by of the product with the purple column.

So what we get out of this is this the

unwieldy.

But actually no more complex than either of the two

matrices, just a 2 by 2 array of numbers, OK.

This is just like like this is why I've never

liked it because this is just such a nuts way

of being like this is the modification of a matrix,

right?

It just like surely it should just be A1B1, A

2 B 28, right?

And why is that not the starting point?

I promise you by the end to today you should

be like, OK, that's why we want to do this,

right?

This is a way of kind of basically like manipulating.

Matrices through a pipeline or process.

OK, and that can be really useful to go big

amounts of x data at the beginning to a single

number at the end, which is our prediction or probability

or whatever we're trying to get on this model.

So this kind of way of doing a product in

a matrix is actually a way of kind of cajoling

the sizes of these arrays until we get to the

size that we want.

Now the really cool thing about matrix multiplication, if you

can say that about any form of modification, is that

those don't have to be the same size.

OK, so you can have a matrix that is say

3 rows by 1 column and a second matrix that

is just one row by 2 columns, right, and you

can do those two things together and you cannot have

a product, those two things together.

OK, it's just the same process.

We're going to take the first row of our first

matrix and the first column of our second matrix and

do the dot product.

Now in this case that is just a 1x B1,

and then we can take the first row of our

first matrix and the second column of our second matrix,

and that's now A1B2, that goes I equals 1 B

equals 2, the J equals 2.

OK, we can do the same thing over and over

again.

So here we go from a 3 by 1 times

by a 1 by 2 to get.

Right, 3 rows, 1 column, 1 row 2 columns, 3

by 11 by 2, to get a 3 row by

2 column matrix.

OK.

So crucially, when you're doing this matrix multiplication, you have

to have this kind of transitive relationship, right?

The number of columns in the first matrix must equal

the number of rows in the second matrix.

You can have a 70 by 34 1st matrix and

a 34 by 3 2nd matrix, and those two things

will do product, right?

That would be a horrific exam question, but it's possible,

right?

You cannot have a I can't remember the numbers now

37 by 17 matrix and an 18 by 37 matrix

that will not work because those two inner numbers are

not the same, right?

17 does not equal 18.

Notice also therefore that whereas 1 times 2 is the

same as 2 times 1, right?

BA is not necessarily the same as a B and

BA may be undefined if by inverting the dimensions of

that matrix you no longer have that transitivity.

So here we have 3 by 1 and 1 by

2, so we have 3 by 2.

But if I did 3 by 1 and if this

was 2 by 1, so this was now 2 rows

of one column, this would not multiply or indeed if

I just invert them, so 1 by 23 by 1,

we cannot.

multiply that, OK, because like those inner kind of, um,

uh, dimensions are not equal.

It took me 10 years to remember rose columns, which

way they have to uh uh map right but just

always remember that in a 13 by 11 by 2,

these two numbers here have to be the same, OK?

Everything else is fair game.

Happy happy as you can be.

Now we do.

Yes.

Um, OK, we're, we're about halfway, so, and it's by

2.

So what I suggest we do is let's have a

kind of So 5 minute break.

We're back here at 11 on the dot, uh, and,

uh, we'll start differentiation, uh, fresh rather than doing nice

stuff.

I'll hang around here and we get going again after

that.

And you.

I Yeah.

Yes.

I.

you.

Yeah.

I.

Yeah.

I.

Yeah.

Oh.

No.

you I know.

Yeah.

The.

What would you like.

you want to pick out Very well seems like.

I Yeah I like I I Yeah Yeah I think.

Really Oh.

This Oh I.

I I Please.

It.

I a few times I.

I.

I.

She actually.

I.

Yeah.

The.

Yeah.

Yeah.

I.

I Yeah I.

to be.

I.

Yeah.

Now.

She.

I I.

I.

What you I say.

you go.

I I you Yeah question.

Yeah I.

All right, that's, uh, that's OK.

Oh.

OK, any questions on that?

If you, if it's not kind of that's totally fine,

uh, but just maybe go back over these sites and,

um, there are tonnes of like exercises online and you

wanna have you, um, uh, so maybe have a bit

of that if you want to.

We're now going to try to think about kind of

um changes in functions.

So how does a function of change for a given

change in the input dimensions?

OK.

So let's just take a really simple function like number

by 2 and then add 3 on top.

OK, so FX equals 2 X plus 3.

Actually sorry, I didn't realise you.

I isn't it.

If you want to know how, although you can probably

just do this in the head by looking at it,

but if you wanted to know, how would, uh, why

the output change or a change to X, i.

I increase X by 1.

Uh, what we would do, and this should hark back

to your kind of high school education, is take the

derivative with respect to x.

So we're going to say how does Y change for

a change in X, OK.

And for something as simple as this, actually the rules

to get derivative are pretty easy.

We just take the non-existent power above.

The x right, multiply that at the front of the

equation and then reduce that number by 1.

OK, so this is what this middle equation here is

saying.

It's we have a B up here, sorry, a b

up here.

We're going to bring that down.

I'm going to do B times by any constant that's

in front of X.

Reduce the power by 1.

Um, and then, uh, this is actually gonna drop out

because, uh, there's, uh, no excess that's, uh, equal to

0.

So, or 2 x + 3, well that's technically 2X

1 + 3.

So we do 2 times 1, they bring the 1

down, that's still 2 X power of 0 is.

0 is 1, so we've just got 2 times 1

now, which is equal to 23 has no X in

it.

OK, so you can think of that as being like

3 x 0.

If I bring the 0 down times it by the

front, well, then that whole thing goes to 0, right?

Yeah, because 0 times by is 0, so our final

derivative is equal to 2.

OK, so as I increase X by 1, we would

expect Y to increase by 2.

OK, easy peasy lemon squeezy, right.

Things of course get more complicated when it's not just

kind of a linear combination of terms, OK, so now

if we have 2 x + 3 and we square

the whole thing, well, this becomes a little bit harder

because it's not just like, well, can I not just

take the kind of derivative of 2 X + 3

and then bring the 2 down?

It doesn't come back.

OK, if you were to do that, you would find

it yeah be wrong answer.

But instead, what you need to use is something called

the chain rule.

OK, the chain rule is going to be crucial for

everything we do this term, but the basic idea is

to simplify that complex expression into a set of kind

of simpler terms and then work on those terms separately.

OK.

In particular, we're going to say that our final derivative

D1 over D, X is just equal to D by

DU where we're going to define what you is that's

what we can decide to do, and we're going to

multiply that by DU, that same thing right by DX.

OK, so we now need to find two simpler derivatives.

And if you haven't seen the chain rule before, the

kind of basic intuition here is that notice that if

you multiply those two things, these DU terms.

out and you just get the ID X.

OK, so we have 23 squared.

Everything is squared.

So the kind of obvious simpler unit is probably 2

x + 3 really right that we can work that

easily.

So I'm going to say we'll let you equal 2

x + 3.

OK, so now I just need to implement the chain

rule.

So the first thing we can ask is like how

does why.

Uh, vary with you rather than 2 x + 3.

OK.

Well, it varies U squares, right?

This is just you so we've now got U squared,

OK, and this is really easy to work out.

The derivative of U2 is just 2 U.

We did that in the previous 5, right?

Bring that number down.

Reduce that number by 1.

OK, so we get 2 to 1 or 2 U.

How about the second derivative we need, which is DU

by DX?

Well you here is 2 x + 3.

We did that side as well, right?

The derivative of 2 x 3 to x is just

equal to 2.

OK, so now we've got our two separate terms.

So D by DU is 2 U.

D by DX is equal to 2, and 2 rule

just says multiify those two things together.

So we get 2 u times by 2, which is

equal to 4u.

OK.

And then we can start back in instead of the

original term.

So wherever you see you, you're going to put in

2 x + 32 times 2 x + 32, which

equals 8 x plus 12.

So our derivative of this.

More complex expression is 8 X + 12, but we

did that by doing too much simpler calculations and then

just multiplying them together.

OK.

So any time you can basically simplify it by replacing

some bracketed element with just a a character you, right,

then you're thinking how I could probably do the chain

rule here, uh, and get to an answer, uh, more

simply.

I'm gonna be happy with the chamber.

Right, chain rules allow us to kind of pass the

bar.

It's like that's that's too complicated to work out in

one go and split it up, do it separately, and

then we'll just merge it back together by multiplying the

two values together.

OK, so let me give you a more kind of

complicated example where we can actually kind of chain chain

rule applications together.

So imagine I've got some output Y and some input

X.

Let's deduct those two things from each other Y minus

X.

Let's square that whole thing.

Let's divide it through by N and then take the

square root.

Now I know what that is?

What would we call that in machine learning or indeed

statistics?

Nope.

Not far off though, is the movie's winner, right?

The root mean.

Squared that is roughly what we're doing here.

Now this is really complicated, right, because what Even this

bit, well, this bit here is basically what we did

in the previous slide.

So we could do a chain rule here.

We could say that this is equal to U squared,

but then we've got all of this orange stuff wrapping

around it as well, right?

So the easiest thing to do here is to just

take it in chunks.

So let's first think about everything that's not orange as

being a you.

We're going to sub that out.

So rather than having 1 over M1 x2 all the

5, let's just have 1 over n.

Some simplification to the power of 0.5.

OK, so let's call this new 1.

Right, so now we know that the derivative of why

we expect X is Dy over DU1 times DU 1

over D X.

We've still got this Y minus X2.

That's pretty annoying.

So I don't like that.

So we can just do the chainwa on this component,

right?

So this is just U2 squared, right?

Uh, so we know that the derivative of this.

It can be expressed as kind of like this with

respect to DU2 and the DU2 with respect to X,

right?

And then given that we would then know the derivative

of that, we can then put that back into the

main thing so we can expand this whole thing.

So if DU by DXU1 by DX is still too

complicated, we'll express that as a chain rule itself.

Right, and then push that back into the original chain

rule.

So notice here the DU2s cancel out, so we get

DU1 by DX, and then we've got the 2 DU1s

cancelling out, so we get DY by DX.

OK, so we can basically chain as a sequence these

operations, OK, in order to make them more tractable, not

only for us but for our computers too, OK.

It all makes some sense.

The basic idea here is.

Oh, sorry, see there the basic idea here is that

you want to kind of um bring the parties calculations

as much as possible, basically so that your computer processing

units can do the simplest forms of operations which are

normally the most efficient and quickest.

OK.

OK, so if we have all of this in, right,

that u1 equals Y minus X squad.

So now we're just doing, uh, differentiate 1 over NU1

to the power of 0.5 with respect to U1.

And then this little bit here, which is the kind

of uh U1 term, OK, and again we can break

this down so that if U2 equals Y minus X,

well then we just have to do DU2 uh by

D X here, OK, we're happy with that.

Awesome.

OK, uh, and because I'm not mean in case you

ever need to kind of like refer back to something,

uh, whilst we're doing right here's just kind of the

derivatives.

So we've already seen, uh, linear differentiation, right?

You just bring the power down and then, uh, lower

the power by one, integer, OK.

When you have two things multiplied together, you can use

the product rule, which if you have a times B,

well then you can do A times the derivative of

B with respect to X and then add to it

B times the derivative of A with respect to X.

OK, we're not really going to use that.

This is here more for reference.

More simply, if you have two terms that are added

together, right, you just take the the derivatives of each

separately and add them together.

We will use this if you're trying to take the

derivative of an exponent, OK, so e to the power

of anything where the e to the power of X

is just e to the power of X.

That's the quirky thing about it.

And indeed if you have anything in the power of

E that's more complicated, well then what you do is

you do the derivative of that same function, right, and

then you multiply it by e to the FX, the

original thing, OK.

So if for example, uh let me try doing my

math in my headline that says E to the power

of 2 X.

OK, so our F of X is 2 X.

derivative of 2 X is 2.

OK, so it's going to be 2 times bye to

the power of 2 X.

There's no decrementing the um um the the polynomial uh

term there.

And then finally.

I probably never use it.

The derivative of the natural log of X is 1

over X.

Don't ask me why.

Abby, this is just a Tsheet.

You need the chain rule for the exam and indeed

all of the seminars, uh, you're gonna probably need the

derivative and an exponent.

The other ones are just bonus ones in case you

come up with something quirky, uh, along the way.

OK, let's try to generalise this then.

So everything we've been working on so far has been

um kind of like a one dimensional Y is just

a function of a single thing X.

OK.

So what if we have multiple dimensions?

So what if we have a function that takes two

inputs now, so X and Z, and let's say that

our output is 3 x2 + 2 Z plus 1,

OK, and we want to know how Y changes in

general or changes in these inputs, OK.

So what you're going to do essentially is work out

the partial derivative for each turn.

It's partial because we're only explaining in each one of

these equations what happens as I change just one of

those inputs.

OK, so Dy by DX.

Well here it's just 2 times 3 is 6 load

that by 1.

That's 6 X.

There's no X here, so that gets set to 0.

There's no X here, so that gets set to 0,

hence Dy by DX.

The partial derivative is 6 X.

And by the same logic we get two when we

uh differentiate with respect to uh Z yeah for the

for the delta.

Yeah, so, uh, delta normally refers to like a partial

derivative.

Yeah, good question.

And that's all well and good.

You already know how to do all of that.

The kind of crucial bit from this slide, OK, is

that once we have, uh, calculated those partial derivatives.

We're going to set them up in a vector, OK,

and we're going to call this vector the gradient.

OK, this is a multivariate or kind of like um

array of partial derivatives that collectively explain like how this

function is varying, OK, and we normally represent gradient as

this upside down triangle.

number I think is what it's officially referred to us.

OK, so in this case if we wanted to know

what the gradient of our function is, OK, we're going

to do the postscript of X and Z and stack

those two numbers into a vector.

So the gradient here is 6 x 2.

This is just a generalisation of that very basic derivative

that we've looked at in the previous slides.

OK, that's it.

That's all the kind of like me that we need.

Uh, are we all happy?

Yeah, OK, you say you're not happy to go over

anything again.

Let's give us like ourselves a few slides on graphs,

computational graphs, which we're going to use extensively next week,

and then we're going to get to building in your

network today, right.

So let's go back and look at a very simple

function like F2 X + 3 minus 5, OK.

If we think kind of really kind of hard about

what this expression contains, well, there's essentially three things.

There's a kind of set of scalar values or placeholders

for scalar values.

OK, so there's 4235, and there's also X, right, which

is going to take a number at some point whenever

we calculate it.

And there's a set of operations, OK, so there's an

implicit times 4 times 2 x + 3, right?

Uh, uh, there's an addition there and there's a subtraction.

Uh, so that's our set of operations, what we're doing

to those scales, and then there's an order, right?

All of you would know that you would do 2

x + 3 1st, then you would times it by

4, and then you would minus 5.

That's because we do crackets, indices, division, multiplication.

Addition subtraction.

It has for other nasties, right?

That's how I was taught it at school.

OK, but there's there's an order there.

You know you can't just minus 5 1st before you

do, uh, or rather you actually can't times by 4

1st before you do the other operations, and that's guided

by the kind of hierarchy of operations.

That we have.

The really nice thing about this is that we can

start to represent this visually because of those three features,

OK, the scales, the operations, and the order.

The first thing we would do is do 2 times

by x.

OK, so we've got these square inputs here X and

2.

Those are part of our set of scalar.

We perform an operation.

And then nicely that operation, the result of it, becomes

an input into the next operation.

So 2 times by x gets fed into an addition

with 3, then that gets fed into a multiplication with

4, and then that gets fed into a subtraction with

5.

OK.

Or not, when you have your kind of calculator, this

is basically what it's about envisioning, right?

This is it takes the expression that you've given it

and converts it into a graph, a computational graph, which

allows it to do very similar operations sequentially combining the

outputs until you get your final result, OK.

So that's kind of become a bit nerdy and um

define it a little bit more clearly.

You'll notice that I use squares and circles in that

graph diagram, and those are supposed to represent different things.

So we're going to refer to the squares as value

posts, OK, they're actually called records, these um squares for

reasons that will become apparent in, uh, two points of

time.

You can think of these nodes as being like little

containers, right?

They're just there to hold some data.

That could be a scalar, it could be a vector,

it can be a matrix, but the slightly novel thing

about this value node is that it can also have

properties.

By that it can contain other things about it at

the same time.

So X, a kind of container or value node for

X, can have the data which will be the state

value that it's assigned.

So say X equals 3, but it can also hold

something like the grey.

What is the kind of um.

Kind of um set of partial derivatives for the value

node itself.

So notice here, although we just saw X, we can

think of it more.

Expand it as being well x is something that has

a data value and a gradient value as well.

OK these little containers that just hold things together, those

are the squares.

Our circles are going to be function nodes.

These are just going to apply some function to the

inputs that it receives.

This is, these are the computational units of our computational

graph.

I'll always represent them using circles.

Arrows that flow into the circles, OK, are inputs, right?

They are the arguments to that computation.

And arrows that flow out of the circle are going

to be the outputs.

Simple enough.

OK.

Now this could be something like multiplication where you're going

to have two inputs to one output, same with addition,

but it could be something like to the power of

2, where you have one input 1 output by X

to the power of 2 is just sparing it so

that it doesn't have any other arguments, right, but they

are essentially just very small calculations taking place on a

set of inputs.

And then we can think about like there's different ways

that we can structure these graphs.

So the one that you just saw with the calculation

of 4 x 2 x plus 3 minus 5, that's

what we would call an acyclical graph, OK?

It's acyclical because nothing further on in the network gets

fed back to an earlier point, right?

So even though this is much simpler, A goes to

B goes to C goes to D.

Here, although there are things feeding in at multiple points,

you know, 5's never been routed back here or like

the result of this addition isn't being routed back here,

that would make it much more complicated, and we would

call that type of graph if there were those connections,

a cyclical graph, right?

So a determines B, which determines C, but then C

determines a, but at the same time we.

Now a cyclical graphs can actually be really helpful.

They kind of basically add memory to a network, right,

because it's like, well, I'm gonna do this first thing

we're gonna do a to get to C, but then

remember C and then store that back in A, and

then we can do something again and we match it.

So if you think of things like, well, if you

come across things like current neural networks, which were kind

of very fashion before.

Uh, we got to transformers, that was something that basically

had this kind of like memory aspect, um, because it

had this cyclicality, uh, in the computation, uh, graphs, OK.

This is merely for your reference.

Um, we are almost exclusively going to work with asyical

graphs.

We might do some really cool things like bypass layers

and things, but it's still going to be asynical, um,

uh, and that's kind of important for the maths that

we're going to use, uh, when doing your.

OK, that's what the graphs are, right?

It's a set of value nodes and functions.

It's a way of simplifying the representation of a more

complex composite function into little mini parts that will connect

together.

Which now means that with any algebra differentiation and our

graph language, we can do neural networks, OK, uh, and

this is where we're gonna end up in the last

10 slots of today.

You been around since the 1960s, maybe even the 1950s,

and they kind of started off in biology and psychology

as a way of kind of characterising what a brain

neuron does, and this is why they neural networks.

Now these neurons are kind of cells in our nervous

system, so not just in our brain, but also down

our spine and the nervous system nerves that branch out

from it.

And the idea is that these neurons.

regulate how we send electricity and electrical impulses of our

brains.

If I want to pinch my fingers together that requires

a series of electrical impulses that trigger the muscles in

my finger to then do that.

OK, neurons regulate what fires and when, right?

If I want to do this, I don't need the

stuff over here with firing, OK, so you need these

kind of like little computational units, and our kind of

exploration of these um.

Uh, brain neurons in biology, right, show that they kind

of basically receive an input signal which in the biological

sense is an electrical pulse, right, there's a little shock

of electricity.

And then that neuron as a computational unit decides whether

or not to fire, OK?

It's like, well, should I pass that signal on or

should I just like block it?

OK.

So these are actually really, really simple computational units.

It's like fire or no fire.

It's an all or nothing action.

And when I say decide that it's too much agency,

basically what it's gonna have is some threshold where we

kind of.

Strength of the signal, let's call it, I can't remember

if it's like the voltage or what, but the current,

right, but there's some kind of like um strength of

the signal.

If the signal exceeds that threshold, it will fire, right?

I can't hold that back.

I'm gonna have to let it go.

If the signal doesn't meet that threshold, then it won't

fire because it stops at that point, and your brain

can learn and adapt those thresholds, right, so that when

I want to squeeze my fingers together, right, it knows

which neurons to send it through essentially and whether they'll

fire or not.

The really cool thing.

I let me just skip through the introduction here, but

we can actually visualise this, right?

So if we just very quickly look at this video,

what you'll see here is kind of like a microscopic

vision of neuron cells, and what you see is those

flashes are the electrical impulses, and you'll see that large

areas light up because all of those neurons are allowing.

The electrical energy to pass through, but you'll notice that

at certain points there are little kind of pockets of

that activity that you go on the top left there

is a little bit more localised, right.

What it's saying to me is that.

Like just a do I fire or do I not

fire, right, binary action.

Multiplied by billions, billions of neurons in your brain, right,

leads to our ability to have a high level discussion

about neural networks, computation graphs, and differentiation, right?

This is all just because of that activity.

This goes back towards.

Yeah, it's just ashamed of saying it, but what he

was saying, which is that like really worked, right, that

having these kind of just like massive connections or very

similar operations leads to very, very impressive and high fidelity

behaviour and context.

OK, so that's where neural networks kind of originate from.

And given that we observe these things happening in physical

matter, what we try to do then is represent that

in kind of a mathematical form.

So we're going to say that an electrical impulse coming

into the neuron is X, OK?

And then the very simple ceptron model of the brain,

OK, is we're going to take that input signal, we're

going to multiply it by some number w, we're gonna

add some number 2.

And now we're going to pass it through some other

function.

OK.

So what are all those terms?

Well, W here is what we're going to call the

weight, OK.

It's basically gonna either enlarge or shrink the size of

that input signal X.

And then B is the bias.

OK, so we're going to then shift it up or

down by a fixed amount, and then the sigma here

is not the standard deviation.

It is an activation function.

OK, so it's some transformation of that kind of weighted

and biassed signal.

If x is 1, W is 2, that becomes 22

times 1 is 2, and minus 3.

Now the question is.

Is that enough?

In order for me to transmit this information on that's

the kind of very simple perceptron, uh, model, OK.

So this sigma here is supposed to act like that

all or nothing quality of the neuron.

Do I fire or do I just hold it back

at this point, OK?

But the kind of subtle difference between our use of

this perceptron model and the kind of biological use of

the perceptron model is that we're going to allow sigma

to be a whole different like host of functions.

It doesn't have to be like fire or not fire.

It could just be a number, right?

It could transform that number so that it's strictly positive.

It could transform it so that it follows a nice.

of sigmoid curve.

All of these things are possible just by changing what

that activation function is doing.

That's be really handy when we try to cajole the

numbers into the ranges that we want both for training

and for prediction.

We're going to come back to this time and time

again, don't worry.

This isn't your only side on activation functions by any

means.

So this is what it would look like kind of

graphically how we might kind of represent that percept model,

how you would see it in a textbook, right?

You have some input data X that gets passed to

a slightly more complicated computational mode which weights, biases and

activates the inputs and and the output thing you get

is Y.

very nice thing.

Here is the computation graph is like our original one,

right?

We have a scale of values X, W, and B.

First we multiply X, then we feed that into an

addition with B.

We pass it through our application function, which is a

1 to 1 function, and then we get our output

Y.

All righty.

We're going to talk about computational graphs of the accounts

at home next week.

Now we've defined that computation graph in abstract.

X doesn't have any value at the moment.

It's, it's undefined, but what we're going to do then

is say, OK, we'll give this computation graph and we

actually get a number out of this.

So let's say that W is equal to 2, our

51 is equal to 1, and.

And this week and probably next week as well, let's

treat that activation function sigma as the identity function.

That is, it's just going to return the same thing

that you put in.

OK, so the identity function of 1 is 1, and

minus 7 is -7.

It just returns the input, OK.

Now let's say that X is equal to 3.

We are going to of course do 3 times by

2, which is equal to 6.

Then we're gonna add in our bias, which is equal

to 1, so we now have 7 and the identity

function signal of 7 is just 7.

Yes, just the sigma thing um why isn't it like

As a container.

Like, so it's a, should it be a container in

the multiplication or like I, I feel like I'm not

checking what the sigma is supposed to be.

Yeah, so yeah, that's a good question.

You could read this as being like multiply sigma by

that thing, but actually this is, this is a function,

and I've never really thought about it like that, but

you're absolutely right that that does look like a multiplication.

But in textbooks you will almost always see this referred

to as signal like this.

The idea here is that that's a functional unit.

Um, it's something like the identity function, um.

Uh, but yes, no, it's, it's not a, it's not

a scale of value, um, uh, but a very good

question actually.

And we call this whole process, so going from X

ending up with Y, we call that forward propagation.

We're going from left to right in our network, pushing

the data until we get to the final output.

OK, we're going to distinguish forward propagation from back propagation

where we go the opposite way, uh, next week.

All right, OK, so that that's like.

That's 2 X +1, right?

That that's kind of a very, very boring land, uh,

function and we need to kind of like start making

things more complicated if anything that even approximates like a

teeny tiny patch of our brain matter.

Uh, so the first thing we can think about is,

well, in that video I showed you, you had multiple

signals coming into the same neuron.

So let's try and uh kind of model that with

our sector model.

So here now we've got X1 through X3.

These are all separate inputs squares, so these are records,

right?

They are kind of values.

We've still got the same, uh.

So in basic setup, this is still multiply that input

by a weight and add a bias.

But now notice that both W and X are emboldened,

so these are vectors, OK, so we now need to

do some form of multiplication and you know when I

said that's such a nuts way to think about multiplication

and um.

It with kind of like arrays that actually makes total

sense here, right, because what we would probably want is

to weight the separate inputs by different amounts.

OK, so what we want to do actually is like

W1 times X1, W2 times X2, W3 times X3.

That's what our old W is representing those three separate

ways, and then we can sum them all together still,

right?

We still need a single output and that's our product.

Of a 1 by 3 times by 3 by 1,

it's just a 1 by 1.

It's a scar, right, because it's summing all of those

things together.

This is why the dot product in linear algebra is

so kind of central to deep learning.

It's because it's a way of very quickly getting out

a single number by multiplying two vectors together and to

your point earlier about where the hell is that team

that I'd expect.

To transpose the two matrices, right?

I'm gonna use a little, but yes, it's the same

thing.

That just means the transposition of those two things, so

that 1 by 3 and 3 by 1 become 1

by 1.

We're just going to control these things until we get

what we want out or the shape that we want.

Monitoring your shape of your outputs in your networks is

like 70% of the job.

OK?

You want to make sure that you're.

we're all happy with this.

I mean we've got a vector, so we need some

way of multiplying two vectors together.

We can do that with a dot product to very

efficiently get that number.

sorry, before we go on, isn't emboldened.

This is still a single number.

It just gets tacked on at the end.

OK, it is not a vector.

It's just a scale that gets added at the very

end of the calculation before you apply the acccation.

Does this just remind you of anything?

I.

Yeah, it does.

Yeah, look, I just changed W to beta, we've I've

just got a linear regression, OK, this multi input.

The single computational mode of the model is just a

linear aggression, right?

It's a linear combination of coefficient multiplied by the respective

variables 1 through 3 and in order to be able

to line up and down in.

OK.

Now this is cool.

I mean it's like this is we've taken a very

convoluted path to get to something that you learn about,

uh, in your kind of uh introductory physics courses at

university.

That is really nice because it's just yet another way

of showing how kind of like.

Um, universal in regression is as a way of estimating

the relationships between variables in data.

So even though you're taking this to the open AI

LM level high fidelity predictions, at the end of the

day, any one of those parameter, uh, or kind of

nodes in your network is just a basic regression.

OK, that's that's what you should be taking because this

is not novel.

This is not new science.

This is just that 101, OK.

Now some alarm bells might be ringing here because you

might be thinking, oh, but like if we say the

line looks like this, but all I'm drawing is a

linear aggression play, how on earth can it be a

universal approximator?

And you are absolutely right.

OK, so unless we said the only things in the

world here are kind of linear relationships which is unifestly

untrue.

This can't be a universal approximator, and that's absolutely right.

OK, that the universal approximation theorem rests on some kind

of like, um, um, requirement that our, um, model is

deep enough and has some kind of effect in nonlinearity.

OK, in order to actually be able to approximate anything.

So what we've defined here is a neural network.

It's not deep and it's not a universal approximate.

This is just a way of getting a linear regression

model or rather to be able to pass data through

a linear regression model.

I just want to talk to you about one other

thing with this equation.

So notice that this is the WX plus the component,

right?

Often we're gonna be working with like binary classification, like

is there a civil war or not, things like that,

right?

And if you were doing that with regression, you would

have a linear.

modelling space and then you would transform it to the

01 space using the logic, right?

You would use logistic repression.

You're going to model it as linear but in this

log on space and then use those kind of transitions

to make it range between 0 and 1.

We can do that with a neural network right if

Sigma, our activation function isn't the identity function, if you

said it's the sigmoid function, which is just 1/1 e

to the minus X, right?

Then we have a way of doing just regression.

OK, so this is not just a linear regssion model.

This is a generalised linear model in the sense that

by changing the activation function Sigma X, I can actually

do the range matching that I would do with a

regression model.

OK.

This is going to be super handy, right, really, really

handy.

OK, we're going to come back to this, uh, several

times over the next few weeks.

Any questions on this day?

OK.

Right, let's kind of keep making it complicated.

So we've now had this instance where we can have

multiple inputs into a single node.

But what if we wanted to make it more complicated

by adding multiple nodes?

So this is a much more complicated graph now, right?

We've got 3 inputs still, but they're going all into

this top computational node which is performing a set of

weight pointed multiplications and then adding a biassed term and

then activating it.

We've got another one for the same set of inputs.

OK, so these two things can vary.

Both of those become inputs into a final, hence the

superscript F.

A computational node, right, where some kind of combination or

aggregation of these two individual calculations is being made before

we get to 1 here.

OK, start off with 3 inputs.

Each of those 3 inputs goes into each of 2

nodes, OK, and then they both feed into one final

node before we get our outputs, OK.

So now that we're getting towards this more kind of

complicated space, we need some kind of terminology in order

to kind of make sense of it.

So those circles here when we're talking about neural networks

are going to be nodes.

These are the computation of units a bit like the

kind of operational function nodes and the computation graph more

generally our neural network nodes are going to weight bias

and then activate the input data, OK.

Next step is layers, OK, so these are a set

of notes that exist at a specific point in the

network, OK?

So it's where they all receive basically the same inputs

or inputs from uh the previous uh layer.

And so here, here's our input data.

This goes into this first layer, right?

The output of this is not being fed into this

node and vice versa.

These are separate calculations that occur at basically at the

same time in the network, and then this is a

separate layer because it's receiving inputs from both of these

nodes in the previous.

This layer.

OK, so when I talk about layers, I'm thinking about

this vertical stacking, which are the nodes that are basically

being kind of churned through at the same time as

I forward propagate my data.

Yeah, so here is the final layer, the inputs are

the the twos and the X is the.

Two ways.

So yes, you're absolutely right, yeah, so X here is

actually changing every time.

It's X here is like, I mean the inputs that

node.

It's not it's, yeah, yeah, you're absolutely right.

Great question.

So that's the layers.

Then we have these hidden layers, right?

We needed this definition in order to define a deep

network.

A hidden layer is just a group of nodes that

exists between the input and output layers.

OK, so in a very simple single node section model,

there is no hidden layer because basically the output of

that node is the output layer, right?

So it's not in between, OK.

So here this is a hidden layer because it exists

between the input and the outputs rate.

This is not a hidden layer because it is the

output node, essentially.

When we talk about the depth of, uh, neural networks,

we're calculating or counting the number of hidden layers.

OK so a deep network, a very deep network is

one that just has more of these hidden layers of

computations that are occurring either on.

Direct inputs nor outputting the final classification or a normal

layer is anything that's like connected to either the inputs

or the outputs.

A hidden layer is any layer that uh more than

2 is deep or 2 or more.

to do.

Then you have a deep neural network.

A single hidden layer like this one would not count

as a deep network.

To be clear, the computation.

Function and bias.

OK.

W1 W2 WF here are all completely separate factors.

Yeah, go for it.

Uh, is it random?

The first how do you set the weights?

Yeah, fantastic.

You need to come back next week.

So this is, you have a single player, so this

definitely isn't 2.

The reason I'm like slightly hesitant is it's like do

I count this as a layer even though it's just

the numbers, right?

Or do I count, uh, and therefore this is hidden

because it's not the input layer, right?

Or do I count this as the input layer like

I've always found that very kind of confusing.

Let me see if I can actually like get you

a proper, um.

Let me see if I can get you a concrete

answer and I'll put it on the Moodle page, um,

whether this even works as a hidden, yeah, that one

is a hidden there.

Why isn't that one a hidden there?

Yeah, exactly.

This is why I don't like it because essentially like

well Y is connected to this and F is connected

to this.

Normally you can normally you just draw off these squares

when drawing the networks, right?

Uh, it's because it's just like there's inputs coming in

and outputs, but.

Normally you treat that different layer it's not a weighted

layer.

It, it doesn't have weights and biases.

So this is, and my confusion, this is my confusion,

which I will clarify, comes from the fact that although

that certainly is the output layer.

Whether this is the input layer or not is unclear

because we have these kind of um records.

Yeah, like I just think it's like enough way of,

of like, it doesn't feel like a very concrete definition

to me.

Um, this is a kind of like methodologist, um.

I will have your questions, but essentially this isn't really

gonna matter because like when we talk about the networks

we talk about lots and lots of layers, so, but

technically it's one that's two, yeah, OK, so what would

you just call this?

This is a very simple multi-layer model.

If you describe this as deep, you, I mean, it's

certainly not deep, but if you were to describe this

as like a complicated hidden moralism, I mean, it's just

not, um, uh, the kind of problem we have here

is I'm trying to show you very simple parsimonious fotal

structures that we can see the inside of them, and

in reality we're going to be talking about like 64

to kind of 1000 nodes in a single layer and

maybe something like 5 layers, and then you can just

be like that is different, right, and it's much easier

to see.

Yeah, I think I got a bit confused now, so

both of them are hidden or they're not.

They are hidden in between that would definitely be.

OK.

We're gonna put this all together now, so we're gonna

have the next thing we're gonna have the uh uh

linear algebra, uh, and then we're gonna have this kind

of like discussion.

So, um, you can basically group all of the weights

from a single layer together.

OK, so W1 and W2 here.

Of both letters, right, they can be of length 3

because they have 3 inputs, so you need 3 separate

weights for each sort of 11 weight for each input.

OK, so we've got 23 length vectors which we can

just represent as.

A matrix.

Each row is a node and each column is a

weight within that node.

OK.

I notice we can also do with this with bias

tabs.

So even though the bias tab in each node is

just a scale of value, when we snack then we

get a vector, OK.

We do that per layer.

So W1 here has this um 2 by 3 matrix

structure for the weights and a 2 by 1 matrix

or like a 2 vector for the biases.

And then again we can think about this kind of

thing.

So at first, uh.

The first layer there receives the input beta X.

X is now vector, it contains 1 through x3.

and actually the weights times by X.

I put it all together.

I'm going to get this out of it, which is

going to be W1X1 plus W2X2 plus W3X3.

That is the dot product of that with that, oh

sorry, no, that with the Xer not with the biases,

and I can do the same thing for the second

row.

OK, which is gonna be those multiplied by that 3

by 1 vector for the inputs to get another scale

of value, OK, so this is gonna be a 2

by 1 vector, OK?

You're gonna have the basically the output of the weighted

component of that function, right?

For each of the two nodes and then we just

need to add the bias.

So we've got a 2 by 1 and a 2

by 1, you know, you just add to the corresponding

elements when you're doing the addition of matrices, OK, and

then that's going to get activated.

So the way we can express a neural network in

terms of nested functions is take your input data X,

the matrix multiplier with the weight matrix of the layer,

OK, Ad your biassed terms, activate it.

Then that's going to get fed through to the next

layer.

So an example here, which is there are only 2

inputs now, right, there's the input from the first node

and the input from the second node.

So now WF is going to be uh a weight

matrix uh it's gonna have two columns and two rows.

It's a 2 by 2 matrix, OK.

We can add our bar terms and then finally do

one file activation in order to get our output.

So our model, asking the question of uh from input

X to output Y is.

That first layer, which we're going to represent using linear

algebra, that then becomes the input into the next layer

again and that final bi can activate it and so

there's your output yeah go for it.

Um, so now for the weights we included both weight

1 and weight 2.

but for the buyers, we only included buyers 1.

What happened to buyers.

So, so in the, in the line that is the

last we looked at, we only have um vector B

one of the buyers, but don't use B2.

What.

So sorry, B superscript in brackets one is a vector

that has the buyer from uh the first uh, yeah,

that's a great one.

Let me change that as well, that's gonna be uh

E sub one, I think is the better way of

representing that that's like the buyer term in the first.

as well, just referring to each of those two things,

but yes, you're clear that is that is um you're

right that is slightly unclear notation.

Thank you.

That makes sense, makes sense.

I was just wondering yeah, no, that's, uh, the moulding

is supposed to differentiate it but that's not sufficient.

I agree.

Yeah, good.

OK.

How much time do we have?

A few minutes.

OK, wait, OK, so let's just kind of think this

through very quickly as a work example.

Let's just say X input data is just 12, and

3, OK?

And, um, you will show you how to set those

weights, uh, properly next week, right?

Uh, but for, say for example, let's just say that

the model is trained and they are 123456.

OK, that's 5 times the weight terms from the final

layer as well.

So we start off with that nested bit.

We're going to do the matrix transportation of W1 with

the input data.

So we're going to do the first row of W1,

so 123, not producted with the first column.

So it's going to be 1 times 12 times 23

times 3.

Add all those things together.

That's the product, OK?

I'm going to go down to the second row of

W1.

4 times 15 times 26 times 3.

Add those two things together, OK, so we're going to

get a 2 by 1 matrix out of just the

dot product of the matrix multiplication of W and X.

We have our biassed terms 1 and 2.

OK, so we add them just elementwise.

This all comes out as a 2 by 1 vector

1534.

Just trust them out there, uh, and then that becomes

the inputs right into the next final layer of our

uh network and we said that those weights are 1

and 2.

Yeah, I'm gonna transpose them so that, uh, we can

just get a single number out at the end of

it.

So we're going to do 1 times plus 2 times

34 because these are just two kind of vectors essentially

and that gives us our final output of 83, but

we have to remember to add that last biest term

of 1, and so it's 83 plus 1 equals 84

and that's the full propagation of that network just using

the linear algebra.

OK, yeah, you're just like transposing how are you allowed

to just do that.

Uh, you are just allowed to do so.

I mean, the, but the point is, right, we have

a target, right?

We want a one by one better at the end

of it, OK.

I'm being slightly facetious.

When, when you come to do it with Pytorch, right,

this is all handled by something called broadcasting, and so

it has a series of kind of rigid rules that's

like.

If I can, I can kind of slightly change the

dimensions of this matrix so that it fits, connects to

the bit that came before, the layer that came before,

and the layer that came after, I will, then it

has like an order in which it will kind of

like try and do that so we can be aware

of what it's doing.

And in these instances it's just a case of, OK,

so I get a 2 by 1 out.

Uh, but what I really want now is the next

layer is a 1 by 1, so I need to

transpose it to 1 by 2, so then when I

multiply it by a 2 by 1 vector of weights,

that's WF, uh, is 1 by 1.

Why can't you just start with it as you want

it?

I guess if you're like transposing it, why don't you

just like start with it, or why is it that

would that would be a totally acceptable alternative way of

doing it.

I think you'll find that at some point you have

to transfer something in.

about results when you should be absolutely concerned because what

if you've got a 2 by 2 and we're transposing

it makes it 2 by 2, yes, you should be

absolutely concerned about that, right, so you have to be

really hot on is this, is the input going to

the right weight, right?

That's absolutely what you should be doing, yes.

This is why we're taking 475, right, because, and this

is why it's structured like this so that you're aware

that there are going to be situations like you're saying

where you're like, what can I not just re-specify this

in a different way?

You absolutely can.

You'll get a number out of it at the end,

especially if you're dealing with like multiples of 2 and.

It's not necessarily correct, or, you know, the other problem

with neural networks is sometimes that they can train and

actually look like they're doing very well, and that's just

because you've got lots of parameters.

And so even though you've hobbled the model by not

putting it together quite correctly, it's just got sufficient.

So so it can kind of work around that mistake.

Um, it will just do it less efficiently and so

you do really have to be very hot on, uh,

are these things going in the right order to the

right places in order to make sure that you've got

an official, efficient and correct model.

Yeah, good.

You should always be concerned, right?

This is kind of big scary stuff.

We're going to start dealing with numbers practise that you

just simply couldn't hand check, right?

So you should really, really, uh, uh, pay attention.

OK, that's it.

You've just built your first neural network, right, this is

great.

You now have everything.

We can go away and we can kind of forward

and propagate data, um, till the cows come home.

That's great.

There's just one problem that's already been raised.

You just got to it too soon, right, which is

what on earth are W and B?

How do I find W and B so they actually

give me sensible predictions the moment we're just 123456.

That is not going to be like the answer.

Like, is it right?

So for supervised learning problems, right, those ones where we

go from X and we want to predict or classify

some i using example data, right?

You're going to want to find the values of W

and B that max toy as closely as possible, right?

We're going to need an algorithm and a training kind

of routine in order to do that.

We're going to do it really mistily using something called

back propagation and we're just going to adjust those parameters

based on how good the model is currently performing and

we are going to do that all by hand next

week in week 2, and at that point you will

be rural networks.

OK, so you come back next week and we'll do

that last questions.

I was wondering if you need more knowledge about mathematics.

I can deter answers.

All right, have a lovely week and I'll see you,

uh.

I.

I.

You Maybe.

So meals I Yeah You know Yeah.

you.

Oh yeah.

I, I didn't so much.

I And I was like, Yeah.

Yeah I I.

I Yeah.

Oh my God.

I I was Everybody.

But I I.

So was I.

Yeah.

OK Yes And I OK Mhm Mm.

So.

Yes.

I.

No.

---

Lecture 2:

The I'm not sure.

I just wanna.

Yeah, but I asked, I asked, uh, near my place

that that's like a, a dry or something I guess.

I do yeah.

I'm like, I'm not interesting, but this one is just

it's to the factory and they show.

Yeah.

it was, um, so for instance in the night, you

know, the main character, a 400 version of his head

so he pretend to, you know one because it was

like more flexible.

like, uh, it was like it has really changed my

life.

Oh my God, I know, but that that is.

that's my angle but like really I don't know but

I truly.

Oh yeah, you know what, yeah, I was like I,

I really need to make sure that I know what

I'm doing.

I watched the whole videos but not all of it,

it was the video.

I watched a lot of the 3.

Yeah, I wasn't I I I like it's the same.

I feel like it's.

Yeah it's a lot in the same thing but it's

just I I.

And it makes more sense and it would be like.

I mean, I know the area.

Thank you for understanding like so like I have an

iPod.

That.

I'm gonna make this so well done getting here an

hour early.

Sorry that we are hung out an hour earlier than

we were before.

That's as painful for me as it is for you,

I'm sure, um, but at least everyone has a seat.

This time, which uh ruins the Glastonbury vibe, but hey,

uh, I think you'd rather have a seat.

Cool, so, uh, where are we after week one?

Well, hopefully I have introduced to you the essential maths

that we'll need for this course.

Um, oh, this is a chance we'll run out of

seats.

OK, um.

This, uh, last week and this week are kind of

the most intense maths you'll do.

We'll do further operators like convolutions, uh, in week 5,

but realistically this kind of, if you can get through

these two weeks, you have done the nitty gritty and

that's where like the complexity of the maths will stop.

Everything after that point will be.

How do we structure networks in certain ways to get

certain um results.

So you should by now have the kind of core

linear algebra you need in your formative assessment, I'll give

you some like basic practise examples so you can like

actually check that you know how to do uh those

kind of um dot products and things like that.

Um, we've also discussed how even very basic calculations like

2x X and 2 X + 3, things like that

can be kind of represented as these directed graphs, like

we're gonna shunt some data from one side of a

set of nodes, uh, to another, and we define kind

of two types of nodes that we can use.

One of these kind of value nodes that store information

about our data at various points in that graph, right,

so we can have these kind of like intermediary checkpoints.

That's gonna be very handy today.

And then we have these compute nodes that actually apply

some functions to the data at that point, uh, in

the graphage.

And then putting that all together, kind of the reason

I'm showing you these graphs is because essentially they're a

way of representing very simple, um, neural networks, and we're

gonna use properties of kind of of graphs today in

order to actually uh make these models as good as

possible.

So we're still.

I'm afraid not going to be venturing into the world

of deep learning just yet.

We're gonna be focusing today literally on single inputs, single

outputs, single node models, and I'm gonna show you how

you actually update that model.

So we're gonna be building like a single neuron in

our brain, essentially.

And then, on the basis of that, we should be

able to change these things together at least implicitly, uh,

and get to the level of, um, uh, deep learning,

uh, next week.

OK, so before I kind of get into uh the

computation of these graphs, I just wanted to, uh, well,

make an admission and then try and correct for that

admission.

So I had an amazing question last week which was,

um, when, like, why have you, why are you using

the transposes operating?

There, why is it WX in these slides, and I

kind of waved it off as like, well, I need

to do that to get the right dimensions, right?

And then I went back and I looked at the

slides and I updated them so it was consistent and

everything was great, but I still wasn't happy.

And the reason I'm not happy is because even if

you have the right dimensions, you could still have the

wrong calculation.

So I really want to kind of impress on you

the importance of making sure that you are connecting your

graphs up correctly so that not only are the dimensions

correct, but the calculation you get out of the end

of it is the calculation you want.

So I've got a very simple example here which is

gonna kind of show you exactly uh what I mean.

And I think this is.

I mean partly my fault cos I should have anticipated

it um last week, but I think also it's just

partly a result of the fact that the way we

would intuitively visualise um these matrices, right, the data and

our weights, actually is a little bit incompatible with how

the maths works.

And so there's a tension between how we actually um

want to arrange them intuitively and how we need to

arrange them in order to get the right calculation.

So, uh, let me add uh some meat to the

bones there, um.

Typically, when you think of data, X, that matrix, right,

your rows are your observations and your columns are your

features or variables, right, that is the the most typical

way of thinking about arranging data.

In other words, it's an N by K vector.

And then what I introduced last week was this kind

of like, well you can have these multiple weights within

a node, right, and normally we would represent that as

a vector.

And if I have say multiple um.

Nodes in a layer of a neural network, then what

we're gonna do is stack them on top of them,

each other like vertically.

So now I've got an X matrix that I want

to intuitively represent as N byK and that weight matrix,

which is gonna be J byK, right, there are gonna

be J.

Nodes in that layer and each of those um nodes

is gonna take in K inputs, the K features from

your data, right?

The problem is if I have N by K and

J by K then WX is undefined, right, because you

need.

The, the inner components of those two dimensions when you

multiply them together, to be the same, right?

So I can only really either here what really only

I can do here is invert that X matrix to

make it K by N and then I could do

J by K and K by N.

Um, however, uh, we want to do it.

OK, so I think this is the fundamental tension.

We want our data to be observations, variables.

We also want our weights to be nodes, weights, interior

weights, and that's like, you can't do that, right?

So something has to be flipped here.

And I said to you, uh, last week, well you

just kind of flip it so that like you can

do N times K and then K times J or

something like that, right?

But the problem is like, what, which, which one do

I flip and why?

OK.

So let's just take a really simple example where we

have two rows of data, each with two variables, OK?

And let's say we have 2 nodes in a layer,

each with 2 weights, right?

So we're gonna stick with our intuitions here.

X is.

Observation, observation, variable, variable, right?

This is node node, weight in node, weight in node.

So the superscript here is indexing where you are in

the network or in the data and the subscript is

indicating uh the feature, essentially, right?

What we actually want is for each uh row of

our data.

To pass it in to the various nodes and get

the corresponding sum of that data point.

So notice this is the first feature of the first

observation, this is the second feature of the first observation,

right, and then we pass that same uh data through

the second set of nodes.

So I have a, we have X1 and X2 from

the first observation still but now I pass through that

second node W2 and if I move over a column,

well now I have the second data going through the

1st and 2nd 2 nodes, uh, respectively.

The problem with that is that, uh, the problem with

this example is actually worse than what I said in

the previous slide.

Not only is that not equal to WX, right, but

WX is defined, right?

So if you were to actually run this through your

kind of, um, computational graph builder, you would get a

result.

And the problem is, is if you do W or

by X in this case, because they're both 2 by

2, so any arrangement is defined.

Notice that in this first element of the output matrix,

you've got uh the first variable of the first observation

being added to the first variable of the second observation.

So we're doing this weird like contamination.

In these outputs blending the observations.

That is definitely not what we want to do, right?

We want to just see, well, how does that node

to transform this input row of data and how does

it transform that input row of data?

We don't want some blend, weighted blend, right, of two

different observations and then 22 different features.

So you have to be really, really careful, and that

was lazy advice on my part to say we'll just

make the dimensions work, OK, because the dimensions work there,

but that's not what we want, OK?

So what we have to do instead is just make

sure that the way we're multiplying them together.

Actually, uh makes uh sense, OK, so.

If you were to take the data that we saw

before, one way to make that work is to say

times W by X transpose, which is essentially going to

rotate that matrix round such that now notice the columns

of X denote an observation.

So that's an observation, that's an observation, and the rows

denote features.

Right, of that observation.

Now if I matrix multiply those two things together, I

get the right result.

I get this uh middle matrix here, which doesn't have

this contamination across, uh, observations.

If you know anything about um.

Linear algebra, you know, you can also kind of basically

um take the mirror image of the multiplication and invert

the matrices as you go and you get exactly the

same results.

So WX is the same as XW here, it's just

kind of moving X over here that is already transposed,

so it isn't transposed anymore and that becomes transposed.

Those are both exactly the same way of writing uh

the same thing.

Don't worry too much about that.

But the idea here is that you have to make

sure that when I do this first row times by

this first column, I'm not mixing up my data, OK,

which is what's going on out here.

So.

This is a very breathless, uh walk through matrix multiplication

again.

The, the, the main point here, right, is that although

you want to think of X as being N by

K, it's gonna be so much easier if we treat

it as K by N.

And I don't have to do this weird transpose operator

and then all.

My matrix multiplication works out well.

So if you think about X as big X, so

kind of uh a matrix, as stacking the data like

this rather than like this, then this whole thing works

out fine and we can just use uh WX.

If you want a kind of substantive justification, you can

think about it as like you're queuing up the data

to push through the network left to right.

So every column is like a position in that queue

to be pushed through.

That's maybe one way of thinking about it.

Or alternatively, if you really want X to be N

by K then you just have to remember that you

need to do something like XW or WX OK, in

your own calculations.

As with NY 474.

Your choice of notation is up to you so long

as it's clear and consistent and it's correct, right?

So you just have to use what uh uh you

think is, is justifiable and most useful to you.

I kind of checked various sources online, some use column

vectors for weights, right, but others don't.

Uh, and I think this is consistent with how I

will draw the diagram, so let's just stick with this

in the lectures, but that doesn't constrain you personally when

you come to revise this stuff, and it wouldn't prevent

you from doing how you want to do it in

the exam, right?

What matters is you get the correct answer, uh, not

how you get there.

OK, any questions on this?

So slight apology from me that I didn't kind of

anticipate that last week, but hopefully these slides can serve

as a reference as to what you need to do.

OK, enough about matrix, let's uh do some uh neural

network stuff.

Last week, we didn't even talk about an objective, right,

it was just kind of like build me a graph,

shove me some data through it, see what comes out

at the other end, right?

That's not what we do in computer science, let alone

what we do in social science, right?

In both of those instances and when we think about

machine learning.

What we're trying to do is often some kind of

prediction, right?

So, given I know your age, given I know your

ideology, given I know uh where you live in the

country, how likely is it that you're going to vote

for the Labour Party or how likely is it, uh,

that you're gonna turn out to vote at all.

Those are the kind of.

challenges that we might face, uh, in our day to

day lives as, uh, social scientists.

In other words, we're adding an objective to that model.

It's not just give me some numbers, it's give me

some numbers that tell me something about the world, right?

These are kind of simple, supervised problems.

For some subset of the population, we observe a set

of features about them, age, region, ideology, and we also

observe uh some kind of outcome which will here be

kind of why, right?

So um the question is can we use our kind

of observations of those individuals to predict the corresponding outcomes

that we observe for a subset, right?

Now remember for this to be useful.

There has to be some wider population that we want

to make a prediction over.

If I see why for everyone, this entire challenge is

moot because I know what Y is, right?

It's only gonna be useful when we want to um

kind of generalise over a population for whom we may

observe X, but we don't observe.

Why?

Think about a political party, right?

They will go out and run a survey, they will

know roughly what types of people will vote for them.

They're doing that so they can make a prediction over

the other 68 million people in this country, uh, for

whom they might want to, uh, target.

OK.

So that objective in more formal terms is can we

learn some function, let's keep it very arbitrary F right

that is going to map our input kind of features

X onto some kind of um outcome uh Y, OK.

For today, we are going to focus just on continuous

outcomes, right, we're gonna assume that this can vary from

negative infinity to positive infinity.

There's no restriction on the value it can take.

And that's just to simplify some of the maths early

on, but it's only a very basic simplification.

Um, and it also allows us to use that very

standard, uh, measure of like how good we're doing, which

is just the squared error, right?

So take the difference between Y, what you actually observe,

and what your model F of X is predicting, and

square it, right?

And we're squaring it so that if you underpredict or

overpredict.

It's still, you're still doing something wrong, right?

You're not gonna cancel each other out when you start

summing up, uh, those numbers, right?

And the reason you're in here and not asleep in

bed, right, is because we're gonna build FF X using

a neural network, right, so we're gonna try and model

Y from X using a neural network.

Today that's gonna be very simple.

Next week we are gonna start talking thousands of parameters,

OK?

All right, so here's a very, uh, it's actually more

complicated than what we're gonna focus on today, but here's

that example graph, uh, uh, from last week, right?

We have 3 input features in our X vector.

They go to, uh, 2 nodes.

Initially, the outputs of those two nodes go into one

final node, that gets collated, squished down into a single

outcome, uh, prediction.

Uh, why?

So we already kind of have the architecture, right, to

build this graph, um, but the problem is that we

need to kind of basically learn these parameters, so the

weights and the biases, OK.

Uh, we need some way of um.

Uh, kind of tuning them up so that they go

from a random guess, essentially, to the best possible combination

they can be, right?

This is the kind of best hypothesis searching that you

might find in, um, elements of statistical learning or introduction

to statistical, um, learning.

Now the real challenge with neural networks as opposed to

say a linear aggression, right, is that if you think

about some W1 in this weight vector here involves W1,

right, as I kind of wiggle that value, right, as

I adjust it slightly up or slightly down.

It's then affecting what comes into this note, which then

affects what comes out at the end.

And if you think about me stacking tonnes of these

layers, me wiggling W1 over here has this kind of

like knock-on effect to everything else, and it might be

that as I wiggle W1, I want to adjust the

value of WF.

Right, to accommodate it, it may be that I will

amplify it or kind of dampen it uh in some

uh respects.

So the, the inherent problem of trying to fit neural

network models to data is that how any kind of

weight parameter early in the network, i.e. ones close to

the input layer.

Like how that impacts the outcome is going to depend

on how it's filtered through every other node uh that

it it enters.

And most of the time we're gonna be talking about

fully connected feedforward neural networks, which means that every input,

sorry, every output of a node becomes an input into

every node in the next layer so there is gonna

be.

Lots and lots and lots of these connections, right?

In the kind of deep networks that we'll build next

week, you know, we're talking about kind of tens of

thousands of connections, right?

So there's lots of ways that if I just adjust

W one a little bit, that's gonna cascade through all

of these different nodes, uh, in.

Uh, the network, OK, so we need some way of

taking into account that downstream impact of an early parameter,

on a, uh, later, uh, parameter.

There's one seat here, I believe, yeah, sorry.

OK, so how are we gonna do this?

We're gonna do it using er er computation graphs.

OK, and this is the kind of um um piece

of resistance of perhaps this whole course.

um we're gonna do this really slowly, OK?

So we're gonna just remind ourselves about calculus and then

I'm gonna kind of show you the magic of that

propagation literally step by step through a network.

So, Let's forget about um anything complicated like a neural

network for a second and just think about a very

simple linear equation, which is something like 4 times 2X

+ 3 and then minus 5, right?

That is a compound set of operations performed on just

a single input X.

uh and if I was to map it out visually,

it would look something like this, using bid mass, right,

we first times X by 2, then to the result

of that we add 3, then based on all of.

That we times it by 4 and then we deduct

5 from the output and that's uh why.

Yeah.

A B C and D.

Is that just the storage for the intermediary results?

Yeah, great question.

Yes, I'm gonna use these ABCs just to kind of

like talk about like intermediary steps in that graph, right?

So A is storing the result of 2 times X.

You'll see exactly why that's useful uh in a couple

of seconds.

Um.

But yeah, good question.

OK, so this is a, this is an easy case.

So, um, I can think about how Y changes for

a unit change in X quite easily, right?

If I, uh, even though there's kind of lots of

numbers here, essentially, as X increases, this whole thing is

gonna increase, uh, by 8 because expand it out, 4

times 2 X is 8 X plus 12 minus 5

means it's 8 X plus 7, right?

This is a very simple thing to differentiate 7 drops

away, that's 8X the power of 1, so we do

1 times 8.

And then X minus 1, right, so that's X the

0, which is just 1, so we've got 8 times

1, the derivative is 8.

A unit increase in X leads to an 8 point

increase in Y in other respects.

OK, so as I wiggle X, right, I would expect

Y in this case to increase by 8.

That's easy peasy.

You would never ever draw a computation graph for something

like that, right?

It's just a linear equation.

Uh, but what if we had a more complex, uh,

calculation, right?

The whole idea of these computation graphs, right, is that

from something that's compound and complex, uh, we can break

it into simpler parts and then just focus on kind

of, um, uh, essentially differentiating, uh, those simpler parts sequentially,

OK, and we're gonna use that trusty tool, the chain

rule, right, in order to kind of cascade that derivative,

uh, through the graph.

Now.

In uh computer science machine learning type areas, this process

is called auto differentiation.

OK, so the idea is that we're gonna start at

one end of the graph and then calculate the local

derivative, right?

So if you've got DY by DU times DU by

DX, right, the local derivative there is DU by DX.

It's the little bit.

Uh, we calculate that local derivative and then we're gonna

propagate that forwards or backwards depending on where we start,

uh, in order to, until we get the, uh, quantity

of interest, uh, that, uh, we're ultimately interested.

So in that very simple example, right, the quantity that

we're interested in is DY by DX, right?

Uh, how does Y change as I wiggle X, uh,

in other words.

OK, I'm gonna do this exhaustively so that no one

can leave this lecture saying that I didn't show you

how this worked, right?

A differentiation can work forwards or backwards.

There is a benefit to going backwards, but I'll get

to that uh in a little bit, OK?

So.

We want to know DY by DX.

OK, there's these kind of set of intermediary steps in

between.

I'm gonna start on the left hand side, OK, this

is called forward auto differentiation.

OK.

Now, I could really start here and say well what's

DX by DX, but what is DXYDX?

1, right, if I increase X by 1, it increases

by 1, right, it's just one.

so I'm gonna start here instead, this kind of first

intermediary box which is A.

I'm gonna say, well, how does A change for a

unit change in X?

OK, so A equals 2 times by X.

So DA by DX is 2.

Also, DA by DX is 2.

OK, good.

Now let's think about, well, how does B change for

a unit increase in X?

Now we could just say, well that's 2 X +

3 and 2 X + 3 is just the derivative

of that is 2, right?

But we could also say, well, what, how does the,

how does B change for changing in A?

So as I wiggle A, how much does B change

by?

And the answer there is.

None of those 1, it's 1.

This is just A plus 3.

So if I increase A by 1, B increases by

1.

Right, there's no multiplication here, this is B equals A

plus 3.

If I were to take DB by DA, I would

just get 1, right?

So now I've got DB by DA which is equal

to 1, OK, but what I want is DB by

DX.

Well handling, I just calculated DA by DX.

Those two bits when you multiply them together will cancel

out, so you get DB by DX.

So just 1 times our previous local derivative, 2, so

we're now at 2, OK.

How about C?

So first thing we're gonna do is DC by DB,

OK.

So how does C change as I wiggle be?

4 times, right, yeah, it's just C equals 4 B.

DC by DB 4.

Great.

OK, we also now know DB by DX which we

said was 2.

So DC by DX is just 4 times by 2

equals 8.

All right, and then let's just finish it off.

So DY by DX is gonna be DY by DC

OK.

As I increase C by 1, what happens to Y?

Increases by how much?

By 1, right, yeah, good.

So that's 1.

We already knew that DC by DX is 8, so

the final derivative DY by DX is 8, OK.

That was an incredibly torturous way of getting what we

could, could have got in like a single calculation uh

for a linear equation, right?

We can also do the same thing backwards and notice

that we get the same result, right, but uh the

um uh the route to that result, how the derivative

is changing across uh the graph, the the path will

look slightly different, right?

So we can first say, well how does DY by

DC change?

Well, that's just equal to one, you told me that

before, right?

OK, so if I want DY by DB well I

can do DY by DC times DC by DB.

DC by DV is equal to 4, so that's 1

x 4 equals 4.

Are we happy with that?

Do you want me to complete that or we're all

good, right, it's the same thing just in um reverse,

OK?

We're gonna do some kind of um alternative versions of

this as well, don't worry.

OK, so we can either do this left or right.

OK, so how does A change by X and how

does B change by X all the way until we

get to Y?

Well we can start at Y and say, well, how

does Y change for the, the, uh, penultimate step and

then back propagate that way.

So come back and say, OK, well then how does

it change for the second penultimate step all the way

back to the beginning of the network, DA by DX

at the end there, OK.

Here is a slightly more complicated example.

Uh, where, again, you could actually, uh, expand this out

and solve it pretty quickly using, uh, linear calculus, but

like this is just.

These are just the motivating steps.

So here instead of 4 times 2X + 3 minus

5, we now have 4 X 2X + 3 minus

5, right?

The crucial difference here is in the computation graph, notice

that X goes two ways, right, there are 22 paths

through which X uh affects the outcome.

There's this interior bit, so 2 X, right, then add

the 3, but then you've also got 4 X and

then multiply that with the result of that 2 X

plus 3, OK, and then again we're gonna minus 5

at the end uh to get our final um value.

Now here hopefully you can see that the steps start

to become slightly different as you go forwards er and

backwards.

So, I'm gonna do this one exhaustively as well, I'm

sorry, this is just so that you're absolutely clear what's

going on, this could appear in your exam.

Um Let's start here and let's, let's do it in

the order I've labelled the nodes.

OK, so A, B, C, D, Y.

So DA by DX here.

Well, that's just 2 again, right, cos you've got A

equals 2 X here, OK.

DB by DX here is DB times uh sorry, DB

by DA multiplied by DA by DX.

OK, so DB by DA is just equal to 1,

right?

And then we said we know what uh DA by

DX is, that's 2, so 1 times 2 equals 2,

yeah, go for it.

I think you got a bit lost.

Could you explain again why the one is.

Hm, yeah, of course.

So we're only looking at A here.

So we're saying, as I wiggle X, how much does

A change by?

OK, so A is just equal to 2 times by

X, right, the derivative of 2 X is 2, which

that X, right?

So that's just 2, the DA by DX is 2.

You don't need to come down here.

Because uh this never fits into A.

So if I just look at how A is changing,

the only bits of that now I I need to

account is this little subgraph.

OK, so that's 2, then DB by DA, well that's

B is just equal to A plus 3, so that's

just +11 times 2 gives you DP by DX.

OK.

Now the nifty thing here, if we go to C,

right, which is on this like lower branch of the

graph, right, we can just say well DC by DX

cos there's nothing in between, right, so C equals 4X,

so that's gonna be equal to 4, right?

And then here's the cool bit.

So now we get to DD, OK, right, so D

is equal to B times by C.

So we need to know what uh DC by DX

is and what DB by DX is.

And then using uh the product rule here, which is

in the slides last week, we know that DD by

DX is equal to B.

Times by the derivative along the lower branch, DC by

the X.

And D times by a derivative along the top branch,

OK, that is the product rule in action here and

notice, right, if we just kind of sub in all

of these various things, well we've got 4B plus 2.

D, right, we know what B and D are, right?

B is just equal to 2 X + 3.

And D is equal to 4X, so sub that in,

expand it all out.

You get 16 X + 12, right?

So that is now the local derivative DD by DX.

And then the last step is to say, well, what

is the Y by DX?

Uh well, the Y by DD is just equal to

1, so 1 times 16 X plus 12 is equal

to 16 X plus 12.

Yeah.

Wouldn't it be C to DB by X or would

it be?

Um, oh, you're absolutely right.

Is that right?

Yes, sorry, that's a yo, yeah, that is, that should

be C.

Let me write that down right now so I can

correct it.

Um, yes, that should be seen.

Uh, change it to C 40.

Thank you.

OK.

We can do the same thing going backwards.

DY by DD is just equal to 1.

OK, DY by DC is gonna be that 1 times

by how D changes as I wiggle C.

as I wiggle C, that's uh gonna be equal to.

Uh Where am I, D I D D I DC

so DC.

Is a thing, right?

If I D equals B times C, so if I

do uh how does D change 10 C I get

B, right?

Assume B is a constant there, and I can sub

B in and I can do this all the way

back and you will find uh that you get to

exactly the same route.

Well, result just by a slightly different route.

Let me just check there's nothing.

Crocky that that.

We need to that Oh yes, OK.

So we can get back to DY by DA.

OK.

The weird thing.

Is that when we then say, OK, so I've got

here and I've already got DY uh by uh DC

up here.

So what is DY by DX?

Well I actually have two parts now, right, this is

the slightly quirky thing.

So the way you get PY by DX when you've

got these two paths is just to basically say well

why is changing in two ways it's changing, uh.

By the A route and via the C route.

So we do the uh.

Chain rule on both of those partial derivatives, right, so

notice that these actually both counts out.

This is why differentiation is slightly weak because it's not

counting out truly in the sense of like multiplicative counting

out, but you've got the change in Y for a

change in X along the A route, the change in

Y for you changing X along the C route.

So the total change is just for some of those

two changes, so you add them together.

That is slightly quirky, right?

But it works.

Do all of that together, you'll find you get 16

X plus 12 again.

OK.

Are we all happy with that last step?

Actually I had a question backwards like I didn't understand

the second part.

How is it you by DD into DT.

So divided by DD is equal to 1, so that's

just one.

We're just looking at this DD by DC that cancels

there.

Uh, if you think about it.

D equals B times C.

Now if we do uh D by DC we just

assume everything out for a partial derivative, we assume that

everything else is like a constant, right, so you treat

uh B like 5 or 10, like a scale of

value.

So if that was D uh equals 10C, we would

say the derivative is 10.

So when D equals BC, the partial derivative is B.

Yeah, yeah.

Does that make sense?

Yeah.

Yes.

So if you hadn't indicated like ABCD, which gives us

the order, we should do it, is it just best

to go on like the easiest order, or could we

have like gone down the 4 times the route first

and then included in the other?

Oh yeah, that wouldn't make any difference really.

I mean, ultimately they.

collide at D, so you can't do the, I mean

if you go forwards, you can't do the D set

without having worked out DC by DX and TB by

DX, the order in which you do those two things.

Well, it certainly is mathematically matter, whether it matters from

a computation perspective, I would doubt, but maybe I'm wrong.

Um.

I don't think it would matter from a computation perspective

either.

No.

I can check.

Yeah, uh, in the backwards one, if we were to

the equation, D minus 5 is gonna be different from

5 minus 2.

Yeah, absolutely.

So do I always start with the at the top.

Uh, yes, so, so, canonically, the, the way that, uh,

I will draw these sizes, and this is, this is

like a package called graph is, so I'm just using

their default drawing, right, but yes, it's always top minus

bottom, uh, is the way that's gonna be represented, but

you are absolutely right, and we will see it later,

that if this was actually like a parameter, right, if

you were to do DY by DD, well that's one,

but let's call this Z, right?

DY by DZ is what.

-1, right, so you have to be very careful because

that's going to -1 times anything flips its sign, right?

That's gonna have some serious consequences later on.

So yeah, I mean, great question, yeah, um, but yes,

grass, top minus bottom.

Any other questions?

Yeah.

Um, for the back or the 2nd and 3rd we

use a partial.

Yeah, I, um, yes, um.

Doesn't really matter.

Um, I, I wouldn't, I'd never mark you down for

it in an exam, um, and I think the reason

I did it like this was there's just quite a

lot of writing and then latech is quite convoluted, but

yes, I mean it should really take, um, backwards did

not take it when we did it we took the

actual derivative and the product, but then we took the

partial derivative.

And it gets on.

Uh, yeah, I think it's just um.

OK, so why do we do that by DX product

rule.

That's a really good question, actually.

Are you saying, sorry, So the second step where you

see uh in the back divide by DC is equal

to divide by DT and into D by DC and

we treat the other as a constant and doing partial

differentiation we are doing uh differentiation.

Um Sure, I don't know.

Let me find out, um.

So the question I'm asking myself in my head is,

could I have done it differently, could I have done

this there for partial rather than full?

Um, yeah, I don't, I don't know.

So let me find out and then I'll put that

on the, the updates, um.

Ultimately you want to get this quantity, right?

Uh, and, and any means to get that quantity is

a good means to get that quantity of graphs like

this, um, but let me, let me find out if

there's a, there's a proper reason, not just a reason,

and then I'll, uh, let you guys know.

Yeah, yeah, from that, is it going to be some

sort of weight or is it going to be the

or is it both?

So what is the purpose of doing?

It's, it's, it's like you're anticipating those slides, we're gonna

get to that in like 3 slides, maybe, maybe even

less than 3 slides.

No, no, you're all good, you're all good, this is,

this is fantastic.

OK, right, that's all differentiation.

So hopefully you can see that this is a nuts

case to use auto differentiation, you would just solve that

by hand, right, even this one, you could probably do

it by hand without too much trouble, so long as

you can expand out of bracket, right?

But you can kind of see that as the computation

graph gets more complicated, breaking it down into smaller parts

and doing these individual calculations just means that computationally every

step is very simple, right, that that's the goal here,

OK?

Now, uh, the real benefit therefore is that as I

make this massive, right, where basically you could never write

out the equation cos it's just too big.

There's still necessarily some underlying computation graph where all I'm

doing at each step is going, is that a multiply

or is that a plus?

Is that like a a partial derivative of 1 or

is it a partial derivative of B or whatever, right,

these are the kind of questions, uh, that you can

ask.

And the kind of calculations we're doing here are literally

adding and multiplying, right?

When we get to activation functions next week, the derivative

is slightly more complicated, but we'll, we'll know what they

are ahead of time, right?

So actually, when you build this graph, uh, you can

pretty much pre-define the calculations you're gonna need to make,

right, and it's just a case of asking the computer

to do it for the numbers that you pass through,

uh, the network, OK?

Forward order differentiation is probably the, the simpler one to

think about.

Start from the left, work out how it's changing here,

and given that that data's been passed forward, to work

out how it's changing there, etc.

etc.

etc.

uh but.

Uh, it does have one downside, which is that if

instead of this being a function of X, it's a

function of X1, X2, X3, Z, all of these things,

then actually you have to recalculate that entire forward pass

for each uh variable.

OK.

So that's gonna be really, uh, well, it's comparatively less

efficient cos if you do it backwards, you can calculate

all of those gradient terms in in one go, right?

So the backward order differentiation, although slightly um.

Weird to start at the end, actually way more computationally

efficient.

OK.

So in big input parameter models like a neural network,

auto differentiation is almost always done backwards because uh it's

just way simpler for your computer to churn through those

numbers, right?

Uh, and time is money, as they say.

OK.

So, you can do it either way.

insight to me when I build these slides, but normally

we do it backwards for this efficiency reason and not

for a mathematical reason.

All right, OK, now I can start to answer your

question, like, what on earth are we doing this differentiation

for?

So we want to fit a neural network to data,

right?

We have some built function like that, and the question

is, given I have X, what should the parameter values

be, right, before we were assuming them, 4 minus whatever

it was, 3 whatever, right?

Here, what if I want to tune it so that

4 could be 4.7 or 3.2, uh, how would I

work out whether that's the case?

So in a neural network, uh, or indeed any kind

of machine learning model, right, um, any parametric, um, machine

learning model at least, there are 3 repeatable steps that

you essentially do to fit that model to the data,

that is to get the best model possible given the

examples that you have to hand.

Right?

The first step is basically to say, well, how well

is it currently doing?

So if I just gave you a random model, right,

you could forward pass your data through it, right?

And then you can say this is terrible, right, like

this little like why it's supposed to be 7 and

you've given me minus 3,0082, right, like that, that doesn't

look right to me, OK?

What you're basically doing is kind of asking whether or

not it's generating these kind of plausible predictions.

Is this actually kind of useful to us, given that

ultimately we want to kind of predict the weather or

predict the, the chance of civil war or or whatever.

Once you know how well it's doing, right, you want

to then calculate how, if I wiggle those parameters, change

them slightly, will that make the model better or worse,

right?

So before we actually adjust them, which is the third

step, we need to say, well, if I adjusted it,

would that improve the model or not?

And as we've already discussed, right, the.

The inherent challenge of neural network modelling is that as

I change any one parameter that cascades through a set

of of kind of subsequent uh computations, uh, which means

that it's, it's not clear how adjusting that parameter by

say 1 would lead would change the outcome.

Contrast that against a linear regression, right?

Y equals B 1 X1 plus B 2 X2.

I know a priori that if I increase B to

1 by.

One, that's gonna kind of increase my output um by

um well the multiplication of, of whatever that is, right,

so um.

Uh, it's just much more complicated, uh, in a neural

network.

So the whole point of what I just showed you

is that this is kind of the, the starting basis

for kind of doing these three steps in a principled

and repeatable manner, right?

We're going to do it by hand and then we're

going to pass it off to the computer next week,

uh, so that, uh, it can do it, OK?

So we actually don't want to know the derivative with

respect to X, right, because our X data is fixed.

We observe X, we can't change X in a, in

a conventional um machine learning setting, right?

So knowing how our model is performing with respect to

X, right, where we're thinking about adjusting it is, is,

uh, um, pointless really, right?

We just, we're stuck with X, right?

Uh, what we want to know instead is if I

were to change the parameters that are, are kind of

applied to X, right, these weights and these biassed terms,

uh, how does that affect the, uh, predictive performance of

our model, right?

Uh, and so we're gonna do that using kind of

calculus, right, and the goal here is to find the

best parameters.

Anyone want to offer me what is missing here, what

is the square box?

Right.

OK, good, so I've got two answers here.

I've got Y and I've got the loss function.

So let's, let's take them both.

So uh D Y Y D W.

So what we're saying here is we could, and we

can definitely work that out, how does Y change as

I change either my weights or my biassed term.

So let's say I find that Y changes by 3

as I change W.

Is that inherently useful?

Maybe the functional.

The output Sigma output.

Ah, OK, so, so do you mean the activation function,

or do you mean just like literally like how does

the whole, yeah, OK, so that's also a good point,

but let's just pretend that the activation function is the

identity function.

So there's, there is no kind of change there.

Why, why is, I heard some grumbling when I said,

um, is it inherently useful for to look at why.

Why might that not be inherently useful?

Yeah, you look at the difference between Y and hat

like the predicted in actual value.

Yeah, good, right, so OK, so I can definitely find

out what is Y changing as W changes.

That's kind of almost obvious, right, like, yes, if I

change the weight parameters, why will change.

And I can probably tell you which direction it will

change, um, but you're absolutely right.

Ultimately we want to get as close to the labelled

examples as we can, right?

So actually what we want to do is minimise the

difference between why and as you said, why have the

predictions uh from this model.

And I think that takes you to the correct answer,

which was the.

Second one I heard, which is the loss function, right?

What we want to know is how is the loss

changing for a change in these weights, right?

Because what we want to do is get it so

that the loss is as small as possible, right?

So if I know how the loss changes for a

change in the weights, hopefully I can kind of tweak

those rates to make it so that the loss goes

down, right, that is the overarching goal here.

So that uh that little square box is the loss,

OK, and as I said, in our very simple case,

right, the loss here is just gonna be the um

squared error.

So Y minus what our model predicts, F of X

and then square the whole thing.

OK.

So Here's our very simple perceptual model, OK, takes a

single input X, we weight it, we add a bias,

and I'm gonna call the output here Z, OK?

Here's the corresponding graph, right, so WX plus B, the

applying activation function, and that gives us our kind of

output from the model, which is Z, OK.

This isn't quite useful yet cos this would be like

doing DYZ by DW, right, or DB.

What we actually want is the loss.

So we just tack it on at the end.

Right, so.

This rock ends here.

OK, that's the same thing.

But then we say, OK, well, now what I wanna

do is I want to take the error, so I'm

gonna do Z minus Y, OK?

And then that gives me a new, um uh intermediary

value D, right, and then I'm gonna swear that to

get me to L.

OK, so I've just tacked on that last bit of

computation.

Minus the deserved outcome and then square the whole thing,

right, so that what I get out at the end

of this is the loss rather than the outcome.

Alrighty, now.

One of the major advantages of representing these kind of

like intermediary stages, right, is that this is a full

computation graph, but Z is still storing our predictions, right?

So at the end of all of this, once we've,

we've got our best model, we can lop off that

loss calculation and just extract Z and that is our

predicted outcomes for this model.

Are we happy with this?

Right, so our target here is to minimise the loss.

We're gonna ask how does the loss change for a

change in W and B.

We do that by adding the loss calculation to the

end of that computation graph, and then it's probably gonna

come as no surprise to you that what we're gonna

do is uh try and um uh differentiate the loss

with respect to the things that we can change, which

in this case is W and B.

OK.

All right.

I've already said this but I'm gonna say it just

just for completeness and slides right this week we're just

gonna assume that sigma, our activation function, is the identity

function, right?

So it just is equal to what you put into

it, OK.

Therefore, it has a derivative of one, it's like doing

DL by DL or DX by DX, right?

And that's just gonna like simplify it.

But it is just worth noting that um we are

not going to do that from next week, so we

will use different activation functions and we will use them

in two ways.

The first way is uh to kind of um improve

training.

So it turns out in these hidden layers, if you

add nonlinearities, and if you constrain the outcome to be,

say, strictly positive, then, um, or actually weakly positive, it

can be equal to zero, then um.

Uh the model trains a lot better and it can

learn nonlinearities, OK, so that's one reason to have different

activation functions within the network.

The other is, we're going to think about, I don't

know, predict house price or or change in GDP, these

kind of continuous values, but if.

I was going to predict the probability of a civil

war occurring next week, right, it either occurs or it

doesn't, so I would not want a predicted value of

3.9, right?

I want essentially a number between 0 and 1, and

we can do that by changing the activation function to

a a sigmoid activation function, right?

Crucially, those activation functions we'll see next week have known

derivatives.

There's no like, like how do I differentiate that exponent,

right, we just know it, it's stored, it gets calculated,

um, but we'll come to that next week, yeah.

Mhm, yeah.

Uh yeah, uh, buckle up, neural networks are like.

Infinite choice, right, it's why people get paid the big

bucks, right, and that's why there's a difference between a

research scientist and a research engineer, because the research engineers

spend their entire lives thinking about like, should that be

a sigmoid or should that be like a relu function?

What happens if I had an extra node here, you

know, what if I kind of use this type of

activation versus this?

It, it's, there are so many researcher degrees of freedom,

right, that you can build these really complicated, really amazing

things that can serve you like the optimal Netflix title

in 0.1 of a second, right?

But also it means that you can create these like

crazy gargantuan beasts that make absolutely no sense at all.

Everything is a choice in your networks, everything, right?

Yeah.

Uh, when we say that we are adjusting the values

like in the previous, so is it that we are

taking to be constant and just changing the bias and

just then keeping bias the con constant and changing for

W, or do we change it together?

We can change it.

No, no, no, these are not marginal effects, they can't

be marginal effects because of the.

This is still linear W+B, right, so the it is

the marginal effect in this case because B and W

are not interacting, but in a DE network where the

WX plus B becomes the inputs to one of many

nodes, then, uh, it's not the marginal effect of changing

W anymore because it's cascading through, yeah, yeah, yeah, but

we're gonna do it simultaneously.

Good questions.

OK, cool, right, here's that uh loss appended computation graph,

OK, I'm gonna give you some data for the first

time, right?

Let's say that X is equal to 1 and Y

equals to 2, right, and let's just say we start

off with values of W and B equal to 0,

right?

We have no idea what they should be, OK?

So I'm just gonna set them to 0.

We'll discuss whether that's a good idea at the end

of today's um lecture.

Now W equals 0, B equals 0, X equals 1.

OK, so 1 times 0 is equals 2.

0 A 0 is equal to 0.

The identity function factor 0 is 0.

Excellent.

All good so far.

Y is equal to 2, so 0 minus 2 is

what?

Minus 2, minus 2 squared.

4.

Good.

OK, so our loss is equal to 4.

You've just done the forward pass, very simple perceptron model

on neural network given a single instance of data, right?

Now 4, I'm gonna tell you, is pretty bad loss,

right, that, that's not good.

We can get that to 0.

OK, essentially we can, we can learn the exact function

I have in mind, given the example I've just, uh,

given you.

And so the idea here is we're saying, how on

earth can I change W and indeed B as well,

right, so that my loss approaches 04 is just too

high, OK?

So let's do the auto differentiation, so we know the

the generic um er changes and then we'll put some

values in and I'll show you how you then adjust

your parameters.

So we'll start at the end of the network now,

OK, we're gonna just work with backwards uh auto differentiation.

So DL by DL is just one, OK, so we're

just gonna ignore it for the time being.

But DL by DD, OK, so L equals D squared.

So DL by DD is equal to 2D, right?

OK, so that's our first, uh, little mini step.

Now let's think about DL by DZ, OK, so we've

got DL by DD already, that's equal to uh 2D.

Notice I'm using the partial ones here, so I am

definitely gonna update these slides um um.

And DD by DZ, OK, this is the next, this

is just the chain rule, so D equals Z minus

Y.

So what is DD by DZ?

One, good.

OK, so we've got 2D times by 1, so we're

still at 2D.

OK, we're gonna come back further, we're gonna ignore the

um uh identity uh function, right, because these 8 by

DC going through the identity functions one, right, so uh

that stays the same.

Now let's think about DL by DA here.

OK, so we've got DL by DC which is equal

to 2D.

C equals A plus B, this is another plus operation.

So hopefully by now you're just realising every time you

see a plus, right, it's just one, right, so we're

gonna time the whole thing by 1.

Again, OK, and then this is where it gets complicated

and this is where it's a partial, right?

How does uh DA change by DW?

We don't care about X, X is fixed, we're never

gonna adjust that, right?

How does it change by W when it changes by

X?

OK.

So 2D times by X is 2DX.

So DL by DW, OK, is equal to 2D X.

That's our generic.

Formula, OK.

Notice also that if I wanted to change uh uh

B, I wouldn't have had to do this bit, right?

So I know D LYDC, which is just 2D.

So DC by DB is one, it's a plus operation,

right?

So DL by D B is just 2D.

OK, so that's the difference.

The change in the loss for a change in B

is equal to 2D.

The change in the loss for a change in W

is 2D times by X, right?

It's reliant on where you are in the data, in

other words.

Are we happy?

Are we totally frazzled by all differentiation yet?

OK.

It's reliant, oh yeah, sorry, go for it.

The idea that we uh derive to direct that through

W is basically that we try to sort of extract

that effect.

Why would we do that for?

Because we're also going to adjust.

Yeah, yeah, yeah, we will adjust at some point, yeah.

Um OK, scared him up pretty quick, uh.

OK, so as I said, DL by DW is 2DX.

OK, so it's totally reliant on what X is.

So if we want to know the gradient at that

point, if we have like a bell curve, for example,

right, the gradient at the bottom is 0, the gradient

up here is, I don't know, 10, right?

So depending on where I am on X, OK, forgive

me for the hand gestures, right, my actual value for

the gradient changes, OK.

So let's actually work it out for um.

Uh, our value of X, which we said was 1,

right?

X equals 1.

So we have 2 D X and we've got 2

times I'm gonna DDX versus 1, so we're at 2.

Now why on earth is this minus 2?

Well, D is just the error.

No, we predicted 0, we observed 2, so the error

D is equal to minus 2, right?

So the gradient.

At this point is 2x minus 2 times by 1,

which is equal to -4.

OK, are you happy with this?

Right, so you have that generic formula that you worked

out just by chaining these derivatives together.

Now for any point X, I can give you more

data and indeed I will, you can say, well, what

is the gradient at that point, OK.

And now we can start thinking about adjusting them.

OK, so we now know well how is the loss

changing?

That's step 2, now we can get on to step

3, which is adjusting the parameter.

So Quick recap and then uh I'm gonna let you

have a 5 minute break.

So.

Actually, no.

Have a 5 minute break now, we'll recap after the

break and then we'll go on and do that last

step, OK, 5 minutes, come back here at 5 past

the hour, uh, and then we'll get to the uh

magic step.

OK Yeah.

I uh and I and then I'm.

I think.

I think like, yeah, here we are.

I've never done one.

I think it just helps like more for understanding.

I think it was, but it was right like like

I'm very comfortable with that so it was a bit

weird to be like.

Yeah.

It's, but I feel like I wanna come back but

I I like.

Um.

Yeah.

So I.

I was like.

I.

the questions.

Alrighty, I'm gonna uh let's start again.

OK, so it's a very brief, uh, recap just whilst

the last few of us joined.

We've done 2 steps so far, right?

The first step is always to push your data through

the graph.

You need to push it through the graph because you

need to know what your loss value is at the

end, right?

At every node through that graph we can store its

current value, so we can say A is equal to

that, and then B is equal to that, C is

equal that, etc.

etc.

all the way through to L, uh, at the very

end, OK.

That bit is called forward propagation, so you forward propagate

your data.

OK, you always send it through to the end of

the graph.

Then what we did was back propagation, right?

We took that loss and we back propagated it all

the way through to the things that we ultimately want

to adjust, namely the weights and the bias terms of

our network, OK?

That propagation is like the founding principle of neural network

training on data, right?

This is what takes it from a biological model of

your brain to something that we actually use to fit

predictive models, right, and uh predictive models of incredible sizes

too, right?

So this is truly is the fundamental algorithm for fitting

neural networks uh to data.

So I'm gonna show you the whole thing again, but

now we're gonna think about it slightly more kind of

um computationally, that is we're gonna think about these as

being real objects that have like properties, OK?

And the reason I'm gonna show you this is because

you're going to implement this from scratch in like what,

3 hours' time or whenever your class is, OK, so

uh this is basically the visualisation of what you're going

to be doing with code.

Here's the same, uh.

Comutation graph all the way down to L at the

end, uh, but now what we have is for each

of these kind of um value nodes, they have two

properties, they have a value, and they have a gradient

or what I'm calling grad here, OK.

And so when we first build our neural network, this

is what it looks like, OK?

The um uh values any any kind of parameter is

going to be roughly equal to zero, that hand wave

is going to be resolved later on.

And we're gonna initialise all the gradients to have a

value of 0.

It hasn't seen any data, so how much do you

know what that gradient is, OK, so keep it at

0.

We set the value of our input parameters, OK, so

in this case that's just X equal to what they

are in the data, right?

But notice we also set Y down here equal to

2.

That was the observed outcome for when X equals 1,

right?

And then what we start to do is push these

values through.

This is the forward propagation.

So first of all, we do 1 times by 0

and we update the value, still 0.

We add 0, still 0.

You get a final output Z of 0, right?

But then 0 minus 2 is gonna be equal to

-2.

There's our value of D.

OK, and we square that to get our value uh

for L, which is 4, right, the loss we said

was 4.

That propagation then, so you push through the data, right,

and we'll go backwards.

We're now gonna start thinking, OK, well, um, what's the

gradient at these various value nodes?

We're always gonna set the gradient of L, the loss,

equal to 1.

OK, DL by DL.

Is equal to one, right?

But then we're just gonna do exactly the calculations we

did using algebra, now just kind of visually, the value

of uh the gradient here, right?

It's just equal to -4.

And then we have that being back propagated through the

kind of subtraction and notice to, to get to your

point earlier about top and bottom, right?

The gradient for Z is -4, right?

But the gradient for Y is 4 because of that

sign for, right, so you have to remember that DL

by DY right is going to be minus 1 times

um um.

Deal like D deal I've got that wrong slightly, but

you see what I mean like we're flipping flipping the

sign there, OK.

Then that gets that propagated through there.

This is all very simple because it's just the identity

function.

And again, those values go to A and B.

Now notice we now have a gradient value for B,

right, that was just 2D, right, the error was minus

2.

OK, so 2D is just minus 4.

OK.

And then X So sorry, W first gets minus 4

as well because X is equal to 1, so minus

4 times by 1 minus 4, OK, and the gradient

of X is still equal to 0 because to WD

or W is equal to 0, the value of W

is 0, so that's just gonna upset the whole thing

equal to 0, OK.

So let me just show you that little sped up,

right?

We start here with our initial values.

This is our naive neural network, no training, no data,

right?

We add the values for X and Y into this

graph, we update the values of just those nodes, then

we do the calculation by punting those values through the

various computation nodes all the way up to the loss.

And then as we back propagate and work out those

derivatives, and spoilers, you're gonna be able to do that

kind of automatically as part of building the network, you

then just update the values given your X data, right?

And that gets put to this point here.

So now for every value node in our network, we

know the gradient at the point X.

And so now we want to know how to kind

of change WMB.

Yeah.

So we have all this we have this the the

steps.

So do we do the for propagation then we differentiate

everything, get the grating, and then we do the back

propagation.

So the back propagation is getting corrected.

OK, but do we have to redo the calculating integrating

every back propagation in every epoch, uh, you mean for,

for each row of data in X?

Yeah, OK, so what do you think?

Mm.

No.

OK.

Like Because we're, we did the differentiation with the fact

that you.

Um, be so.

So what, OK, so if we didn't redo it every

time, so pass X through.

I bring in spark here, get the gradients, right now

I've got the 2nd observation in X.

What, what would I then do to these and put

that through so I can get the new value of

the loss for X to the second observation.

I can bring that back.

What do I do to the gradient there at that

point?

Can you think of like a an obvious how would

I date it?

What kind of calculation could I do?

with 2X.

You could, yeah, but do I want to overwrite that?

I mean, I haven't actually done any adjusting yet.

So W and and B haven't changed at all based

on what we just described.

OK, so we could just store the gradient, so we

could just add more to the gradient terms, right, so

we pass another observation through and we just increase this

number by however much it changes by, right?

So rather than equals RAD plus equals minus 4 or

whatever, right, that's, that's kind of there.

That's one option.

The other option is we do back gate, we just

balance and we take the second one, and therefore, that's

the difference between uh stochastic radient descent, which I'll get

to in a second, and mini batch gradient descent.

Or or batch radiate the same, um, and I'll, I'll

show you exactly how we do that in a second,

but you're absolutely right to be asking that question because

it's not clear how this generalises when you have more

than one observation, yeah.

So on the way forward, is the gradient equal to

0 or is that more of like a placeholder because

you've it's a placeholder and crucially, you're always gonna set

it to 0 when you forward past the data except

when you're doing batch gradient to set, and then you

need to do something slightly different.

But yes, yes.

OK, so we now know that those, the, the grad,

right, uh W and B is equal to -4, right?

So when X equals to 1, if I wiggle, right,

uh, the value of W or B, it's gonna increase

the loss by -4.

OK.

Now what we said is we want the loss to

go down.

So if increasing W leads to a change in way

of minus 4.

Right, should we increase or decrease W?

We should increase it, right?

Loss is going down as I uh increase W.

What if the gradient was 4, not minus 4?

Should I increase it or decrease W?

Decrease, right?

We're just going in the opposite side of the gradient,

right?

So when the gradient is positive, we want to decrease

W because increasing W would increase the loss.

When the gradient is negative, increasing W has that lovely

effect of decreasing the loss, right, and therefore that's what

we.

should do.

This is just called gradient descent, right, and you're gonna

come across this in MY 474 if you're doing that

this week as well.

But the idea is you're going to adjust those parameters,

W and B in the opposite direction to the gradient.

So if the gradient is negative, you increase the parameter.

If the gradient is positive, you decrease the parameter.

The idea is that you're hoping to converge on zero,

right, you're trying to get this as small as possible,

right?

This is not a neural network specific thing, right?

Loads and loads and loads of machine learning networks, networks,

machine learning models use gradient descent.

Some versions of your generic linear or logistic regression use

gradient descent.

It's the fastest, most convenient way of converging on uh

the optimal.

Parameter values.

If you are doing MY 474, 1 nice thing if

I say so myself, this week, right, is that you're

gonna learn gradient descent from your network and what that

looks like, and then you're gonna see exactly the same

concept but applied to a parametric model like a linear

or logistic regression, and you'll see the like same intuition,

but just different model types, but we still converge on

the parameters we want.

OK, this is what gradient descent looks like.

So theta I here is just gonna stand in for

any parameter, right?

So theta can be some part of W or it

can be some part of the Bs, right, these biassing

terms, OK.

So for every parameter in our model, theta I, we're

going to update.

Right, it's value by setting it equal to what it

already was, and then deducting from it.

The gradient at that point multiplied by some scalar value,

lambda.

OK, so maybe that looks a teeny tiny little bit

scary, right, but it's actually um pretty intuitive.

You take what the value is currently, you use the

minus to go in the opposite direction, right, so positive

gradient, you want to go down, right, so -1.

If it's uh um negative, you want to go up,

right, positive, so -1 times -1 is 1, OK, so

this is just moving us in the opposite direction.

Lambda just means that we don't like go full pallet,

right?

We've got currently a gradient of 4, right?

I'm gonna tell you now that is not the correct

amount that we need to move.

And so we probably want to take like a little

step, right?

So rather than going, all right, let's just move 4.

Right, let's move maybe 0.4 or 0.04.

Just see where that gets us.

Are we any closer?

Are we still in like a negative area of brain?

Do I need to go a bit more, or have

I actually gone over the other end and now I

need to come back a bit?

Right?

So this land is just a I mean it's literally

a scaling term, right?

It's typically about 0.001 to 0.001 right in a um

neural network setting, and we call it the learning rate.

It's like how much, what teeny tiny amount of the

gradient are we gonna use, right, in order to adjust

our values, right, and this nala here, right, is just

our gradient.

So in the case that we've been thinking of, both

W and B have a gradient here of 4, minus

4, whatever it is, OK?

Scaling it because if we did that big change we

might like overshoot it and then it's gonna take us

ages to get back, right?

And in fact, if you have too big a learning

rate, you're just gonna shuffle and you're never actually gonna

converge where you want to converge on how many observations

observations it could be lower if you have observations it

could be.

Yeah, fantastic.

So yes, you absolutely can do that.

Um, there's like, um, OK, so there there's.

Uh, two ways that you could do that.

So one is you, the researcher, when designing the network

makes that choice.

It's another researcher degree of freedom, which makes this just

like an incredibly challenging area to like train good models,

right?

The other one, and I will have a couple of

slides on this later, um, is you can use like

a dynamic learning.

Right, so gradient descent is very, very basic.

I mean, it's amazing, but it's very basic.

There is just a lambda, right?

But what if you wanted to like a slightly different

lambda based on how much training you've done?

Or what if you kind of wanted to like carry

forward a bit of what you've already done into the

next update, right?

A bit like a ball rolling with momentum down a

hill.

These are things that we can incorporate.

These are very standard and I will show you how

to do them in the labs, um, as well, because

I mean, you would never normally use just pure gradient

descent and like a production level.

Yeah, but great question.

Any other questions, yeah.

Pardon to the right.

Uh, uh, I'm just gonna tell you that the rule

of thumb is 0.01.

Uh, in general, if you, if you are doing some

production, right, you would use cross validation and search across

different values, right?

Does it change if I make it a lot smaller?

Does it, um, am I just losing speed but actually

ending up at the same kind of level of predicted

performance?

It's something that you have to tune, um, using something

like cross validation, OK?

This is why those kind of open AI models, right,

are so expensive because not only are they gargantuan, right,

but in order to get the one that they serve

to you, they've had to hypertune not only like the

learning rate, but all of these other weird mucky things

that they put in these models, right, and you're so

you're running like that gargantuan model thousands of times just

to get the right parameters before you even like get

to being like, OK, this is 40.

You know, 24th of June 2025 or whatever it is.

Um, it's really costly, right?

Unless it's deep sea, in which case it's not very

costly at all.

So the goal is to optimise.

Absolutely, and all of this has to be optimised.

OK.

Right, let me show you how this works.

So here's the graph.

But you're WB second.

Here's the graph that we've got to do after we

back propagated, right?

Now we do the update set, OK, so we take

the current value of W and B, now remember they

were both 0, and we minus from them.

The scaled amount of the gradient, which is -4 in

each case.

OK, so -4 is gonna be + 4, and then

I'm gonna say here, lambda is equal to 0.1, just

so that this isn't awfully slow and painful for all

of us, right?

So 0.1 times 4 is 0.4, OK, so we're gonna

get 0 plus 0.4, and now our new value.

Of uh W and B is equal to 0.4, alright,

OK.

Are we all happy with that?

That's that last magic step, that's the gradient descent step

of this whole process.

Alright.

So now we've got 0.4 and 0.4 for the two

parameter values.

Let me zero my gradients, OK?

We're gonna do this physically, the, the line of code

is zero gradient, right, in Pytorch, right, you have to

do this every time you pass uh data through the

network once you've done an update.

OK, so we're gonna zero those gradients, apart from the

one at the end, that has to be equal to

1, otherwise 0 times anything is always gonna be 0,

and that's gonna be annoying.

Um, and we're gonna pass our data through again.

OK, so X is equal to 1, right?

Oh, go for it.

Well, why do we need the gradient at the end

at 1?

And you also choose a different one.

Um, no, because, like, uh.

Um I actually don't know the answer to that question

off the top of my head.

My intuition is no, you can't set it to anything,

but maybe just because it's a scalar, it will just

fall out anyway, so, um, let me have a look.

I it doesn't really matter, but, um, but yeah, yeah,

it's an interesting question, yeah.

Yeah, so DLW DL is definitely equal to one.

There there's no way around that.

I guess the question, OK, so DL by DD is

just 2D.

Yes, OK, that's why.

So technically.

DL by DD is the same as DL by DL

times IDL by DD.

So if DL by DL is not equal to 1,

say it's equal to 4, then you're gonna be 4

times DL by DD, which is gonna be 4 times

what it should be.

That's, that's what you can't do it.

Yeah, thank you very much, great.

Look at that, collaborative.

All right, OK, so X is still equal to 1.

We've still got this single observation, but we've slightly updated

the parameters.

So I'm gonna punt it all the way through.

Now I notice that these values start to change.

1 times 0.4 is 0.4, add 0.4 is 0.8.

Nothing happens, identity function, 0.8, 0.8 now, our error is

minus 1.2.

Now that's smaller, that's a good sign, isn't it, OK?

And then we swear that's get our loss, which is

1.44, which is definitely less than 4, right?

Good job.

OK, so we can just keep doing this.

So I've just punted my data through again, that's the

forward propagate step.

Now I calculate the gradients going back.

We already know the calculations, right?

We know that this is 2DW, right, 2 times the

error times by W.

So we don't have to recalculate that derivative, we just

need to substitute in D and W.

OK.

So now our gradient value is -0.96. Notice, if you

think about this as a curve, but that's on.

A point to -4.

So we were like up here, steep bit of a

parameter.

We've now moved down, still on the, still on the

wrong side of the slope really, but we're at -4.96,

so much shallower tangent, right?

So we're getting close to that zero point where basically

there's no more updating we can do.

So gradient isn't beaten to -0.96, we just do that

gradient descent step, right?

We're gonna update by a scaled amount of that gradient,

OK?

So now W is equal to 0.64 and B is

equal to 0.64.

And if I keep punting this through, you're gonna find

that W and B are gonna converge on one.

If I just focus on this single example.

Right, if you were to repeat this process, these numbers

would eventually, sorry, these two numbers, the values of our

weight and bias terms would eventually just equal 1, right,

and you would have a perfect fit.

1 times 1 is 1, plus 1 is equal to

2.

Bob's your uncle, right, that's, that's the data we observed.

X equals 1, Y equals 2, loss equals 0, so

there'll be, all of those gradients would be set to

0, ultimately, right, model would stall, that's it.

It's kind of like I'm happy, I'm saturated, like that's

it, OK?

Hey, good.

Right, let's go here, here, and then here.

So just to clarify, the YMB converging on one is

just for these particular data points, or would they converge

in, in, you know, on some other on like the

loss maximising Good hold that thought.

Combinations of WN which give us, absolutely.

So we got there because our initial values were both

equal to zero right?

That's, that's a 1 to 1 mapping there.

There's no way that you can avoid that.

But uh, if I had slightly changed Airbnb, we might

have got slightly different, uh, solutions.

And so that's the same amount like for each iteration.

Or I change, so they, they are changed by the

size of the gradient.

These gradients in this example, OK, and only this example

are identical, so they're being identified the same amount.

I'm gonna show you an example in a second.

Yeah, yeah, and essentially if you really want to think

about what's going on here, right, we've got X equals

1, Y equals 2, so it could be 2 X

or it could be X plus 1, right, that explains

that.

There those are two of the possibilities, OK?

But we set both W and B equal to 0,

OK, so the model is kind of like, well.

You know, I can increase in both and I, I

get it and that's just the quickest route to converging

on zero loss in this case.

Now if I'd set B to slightly less than 0

maybe and W to slightly more, maybe I would converge

on 2 X, right?

And in, in general, that's roughly what we want to

do.

This is a process called initialization.

I will, I will talk about it next week in

much more detail.

I'm gonna go here and then I'm gonna go here.

Uh, sorry, 0.

Yeah, OK, so.

Here's our initial um.

Here's our graph after we've updated it once.

OK, so this is 0 plus 10th of the gradient

here minus 4, right, so, uh, that's 0.4.

I'm keeping my X data the same, so X is

still equal to 1 here, Y is still equal to

2, but I push it through again, giving my new

values of W and B.

OK, so when I do that, notice now that one

time by 0.4 is 0.4, B is equal to 0.4,

so adding it I get 0.8.

That goes through to the end, my loss has changed.

Then when I backpropagate that now.

Right, I have to do um.

This is the, we said that the partial derivative DL

by DD for example is equal to 2D.

Right, cause it's like D squared is the equation.

So 2D now is 2 times by minus 1.2, so

the gradient is now -2.4. That's gonna cascade back all

the way through here.

And then at this point, the reason you get minus

0.96 here and not minus 2.4 is because DX sorry,

DL by DX is 2 times WD, that's what we

said in our derivatives.

So now we've got 2D, which is minus 2.4 times

by W, which is 0.4, so 40% of minus 2.4

is is 0.96.

That's why it's doing that.

Then I'm punting it through again, right, so I update

my values of W and beta.

Based on that gradient minus 2.4, whereas before it was

-4.

OK, and as I punt it through again, you'll notice

that um it all changes and ultimately you'll find if

I did this a couple more times you would get

to W equals B equals 1.

And then I had one more question over here I

think.

Are you sure?

OK, cool.

OK, here is that step, OK, I'm gonna keep saying

this because I really, really, really want you to remember

this, you forward past the data, you shuttle it through,

you calculate the loss, right?

Then you back propagate that loss using the known derivatives

of each of those inter intermediary value nodes.

Right, then you use gradient descent to update those parameter

values, you're just gonna shift them slightly, that learning rate

amount just to see if that's a better place to

be.

Given that, you then pass the data through again and

repeat this process until hopefully you converge on the optimal

solution.

Um, in the navigating the loss, it's basically trying to

understand how much loss comes from each part.

Precisely, exactly, yeah, absolutely.

It's like, OK, so there are all these different bits

that I can change, like what are the ones that

I should turn, like these, think of them like um

like knobs, right, and what you want to do is

say how much should I turn it and in which

direction so that I kind of fit that line so

it better matches the data distribution.

That's what's going on here and we have lots and

lots of them, right.

When we get to kind of LLM size, we're talking

about billions.

Will we always just be adjusting like the WB or

do you adjust the function itself like?

Yeah, no, she.

They're one and the same in the kind of neural

network philosophy, right?

The way you would adjust the function.

Is to set some of those values to 0.

Right, cos it's like turning off data passing through that

bit, it's just gonna return 0 when you do that.

So that's roughly how you would do it, but you

can think of these as being like.

With enough parameters, you have an infinite number of knobs

you can turn.

And so technically you should be able to configure it

like Wallace and Gromit style, into something that resembles exactly

the function that you want, right?

It's not going to look like Y equals 4 X

times 2X + 3, it's gonna look like some massive

weight matrix multiplied by some massive input data, right, but

ultimately that's gonna be an approximation.

Of the true underlying function.

So we're never gonna get 4 X 2X + 3,

right?

What we're gonna get is some configuration that gives me

something like 4 X 2X + 3, and the universal

approximation theory that I briefly mentioned last week says that

if I have, As many nodes as possible, right, like

an infinite number of nodes, I should be able to

get that approximation as close to the true function as

I want, right, arbitrarily close.

I just need to add more nodes or have more

layers.

Yeah, it comes to neural networks or we choose that

as well.

I am afraid you're gonna have to choose that yourself

as well, right?

This is just a number, right, just a calculation.

You could tack on another term there before you get

to L.

You could have anything you want, right, and different things

and different configurations do different types of loss.

Um, we will roughly update it so that when we've

got continuous.

Outcomes, we use some form of like squared residual, right?

If we've got a categorical predictive um classification type outcome,

we we use some form of like entropy calculation in

general, right?

Um, but there's nothing to stop you tacking extra stuff

on.

So regularisation in a neural network often means like like

add another term so that, that none of the weights

are too big, right?

Um, or you can do things like add um.

The, the, um, Wasserstein difference is a very common regularisation

loss function for when you're trying to generate synthetic data,

because it's like trying to minimise how much work you

have to do to make one distribution look like another

distribution essentially.

It can.

Anything, right, you can go absolutely like mad and and

change it to whatever you want.

Again, this is why people get paid the big bucks

to be engineers, right, because there are so many things

you can change.

Um, I just want to ask if this has any

Asian elements.

Ah yeah, great.

Uh, is it Beijing?

At this step, I'm gonna say no.

Although you're kind of right in the sense it's like

I have some kind of prior belief, which is my

current weight distribution.

I pass some data through, I look at how it

performs, and then I update it, and in that sense

I agree that it has a Bayesian flavour, uh, but

I don't think it's Beijing yet because I, we're not

deliberately trying to model some, um, posterior distribution, right?

We're still trying to learn like Y equals F of

X and F of X here is.

Uh, non-stochastic, uh, I will show you how to make

a basin, uh, next week, right?

There's some super cool stuff that we can do to

make it kind of basin, um, but yeah, it's good

kind of insight there.

Yes.

So do we start with the gradient of W and

B and then we change the values or Yeah, great.

OK, so, OK, thank you, that's a lovely segue, right?

Um, uh.

Uh, I'm gonna get it after this slide if that's

right, but you you're absolutely right.

Yeah, good, I, I'll talk about that.

Um, OK, so forecast the data, right, that says how

well is it performing, what is my loss value?

Given that loss value, you're going to back propagate it,

right, remembering that those derivative rules can be pre-calculated in

advance.

It's just a chain of local derivatives, OK.

Then we use gradient descent to update those parameter pipes,

we're just gonna shift them a tiny amount.

In reality.

Right, we don't have X, we have matrix X, right,

which means that there are multiple observations here, OK, and

the question I've already been asked is like, how on

earth?

Like what, OK, so what do I do?

Do I, do I send them all through first and

then make an update, but what then happens to the

gradient?

You said zero of the gradient, you know, there's, there's

multiple ways that we can do that.

The two that I'm gonna focus on here are stochastic

gradient descent and mini batch gradient descent, OK?

So stochastic gradient descent.

Works as follows.

Take one observation, forward propagate it, back propagate it, update.

Take the next observation, forward, back, update, next observation.

It's stochastic cos technically, you should randomly sample the order

in which you take those observations, right?

Because you don't want it to be dependent on any

order in your data set.

But the idea is that you make one update per

observation.

OK, question.

Precisely why you then move on to mini batch gradient

descent.

Which is to say, well, hang on a second, that's

nuts.

Like, you, you, like, what happens if I just so

happen to find the only, um, like, um, oh, I

was gonna think of a silly, like the only like

like communist grammar in Hobo, actually that's probably not that

unlikely, is it, like communist grammar, I don't know in

like the south of England, right, uh, and so like

you have this crazy person, I shouldn't say it like

that like this like.

Very abnormal description of a person, right, who has some

very unusual voting behaviour, and when I try and update

the parameters, it just sends it haywire, cos the loss

for that poor old grammar is like massive, right?

Um, so we probably want to like smooth it out

a bit, right?

We don't want to just like, we don't wanna like

lurch the model over here and to be like, OK,

sorry, it's just that one crazy grammar, let's bring it

back to here, right?

So in that case we use mini batch gradient percent.

Which is that we pass through a small batch of

observations, OK, and we basically add just one over M

where M defines the number of observations in your batch

to the gradient and then we perform the update.

This is more stable because if you select the communist

rama, right, she's gonna be balanced out by um um

more kind of like centrist people or whatever, OK.

If you use stochastic gradient descent, what you find is

that you have very fast convergence because you're making lots

and lots of updates to this model, OK?

But it's really noisy cos every so often you get

a crazy grammar, right?

Mini batch gradient descent is a good compromise because the

idea is probably to take all of your data, pass

it through and then make one update.

But that's kind of very intensive, you're gonna be making

very few updates per kind of like run through your

data, so it's gonna take a very long time to

actually get where you want it to go.

By randomly chunking up your data into small bits, you

can kind of have a little bit of the smoothing

that comes from kind of gradient descent without casing, but

still have the speed that comes from like making multiple

updates for one pass through your entire data set.

Yeah the batches?

No, yeah, great question, but no, they're normally chosen stochastically,

and I'm afraid the number that you choose is determined

by you.

Right, so it could be 32, it could be 64,

it could be 128, right?

As you increase the N relative to your data size,

right, you're getting less of the mini batch and more

of the batch, right?

As you decrease it, you're gonna have more updates, but

it's gonna get noisier.

So that's what you're tuning for there.

Let's go to the back first.

So for mini batch, you're still like sending them through

11 version of time you're just not updating it until

after end times.

Exactly, yeah, yeah, yeah.

So basically send it through.

Add that value to the gradient, send the next one

through, add that value to the gradient, divide the gradient

through by the number of things that you've sent through,

right?

And then make the update.

Then 0 the gradient, yeah.

Um, for the computational efficiency of those two processes, I

could imagine that the first one is quicker.

Yes, yes, it is that that's the balance you need

to make, uh.

If you can do it, if, if you own lots

of GPUs, lots of compute, right, you might want to

increase the batch size because you, you, you've got the

power to run it quickly anyway, right, you know, there's

the, there's the compute time, but you can always augment

the compute time by buying more compute, right?

That's the kind of trade-off that you need to make

here.

Did I have another question over here?

You're looking very confused by that, not to single you

out.

I'm just thinking like gas is like less needs less

computing power, whereas with like each, uh, you know, data

to go through this process and in mini batch.

Yeah, so, so the.

And So there's one extra computation, right, which is the

averaging, OK, and then also you're kind of like waiting

for the batch to complete for it all to fall

and propagate through before you make the update.

It's just the case that like if you have a

single observation, you're doing this very simple dot products so

that will be quicker anyway, they're less.

Think about the the WX, right, the dre WX, uh,

in that case you're like the bigger the size of

the matrices are, right, and so it could be bigger

because we're adding more observations in, right, the more summations

you have in each element of the output vector, that's

just more calculations.

Very simple dot products, there will be fewer additions, right,

and multiplications, and there will be no averaging at the

end, so it's just quicker on, on like teeny tiny

examples of like 100 data points, you are not gonna

notice the difference in speed, but when we're talking about

a million on stations, then it starts to become a

little bit more relevant.

Yeah.

I can see the.

Well, um, yeah, so, I mean, so practically it could

be, it's 4 o'clock on a Friday and your boss

says I need a neural network by 4:30, uh, and

I need you to use all of this consumer data,

right, so you can, you will just finish quicker if

you do that, and, and, but substantively, um.

If you're, OK, yes, so if your data is well

behaved, right, so let's assume your data is sampled from

a random normal distribution, right?

So it's just there's, there, there are gonna be no

crazy commons grammars, right?

Then you might as well use the catsegra in descent

because there's not.

Really gonna be much impact of outliers.

If you have very diverse data, where like the order

and the type of observations in that data are gonna

lurch the model left and right, right, then mini bats

trading descent is probably gonna be better.

So if you think your data is well behaved, say

it's like um.

Like physical process data, say we have some kind of

chemistry or physics experiment or repeat observations of these things,

right, and we think that there's a pretty simple law-like

process going on underneath, then you probably use the cat

grade in descent because it's fast and the amount of

noise you're gonna find is going to be more limited.

Yeah, that would be.

Yeah, that's why we make it stochastic.

So is there some way that we the first part

of the, I don't know actually.

I, I haven't come across that.

My gut is telling me like why would you want

to do them first, because in a sense if you

did.

You did all of the outliers, right, and let's suppose

there's some bias towards communist grandmas over like authoritarian grandmars,

then what you can do is you're gonna really shift

the model over here, right, and then you're gonna spend

the next 1900 observations clawing it back.

It might be better to like shift it here and

do a little bit, right, and then, OK, it goes

over here, but no, no, no, you're coming back, right?

That's the kind of kind of tussle I think that

you would want to do.

I don't think you would want to like bias the

model.

Initially, and then I spent the rest of your time,

cos it may be that you want to actually be

slightly further this side and it's gonna just take you

longer to get there.

Yeah.

But yeah, I, I, I can look and see if

there's anything on that, but I don't think there is.

Yeah.

Um, so for mini batch, can the same observation go

and choose different groups, so you choose both the number

of groups and the number of observations.

So it's sampling without replacement.

So if you are in batch 1, you cannot be

in batch 2 through K, right?

Um, and vice versa.

So it's, it's something without replacement, not something with replacement.

We call one run through the entire data set an

epoch.

OK, uh, and so you will only ever, the model

will only forward propagate an observation once per epoch.

And this is because we have just one variable X.

So if you have more than one variable it will

be absolutely yes, so you're always taking the observation as

a whole, right, so all of X, the first X's

features go into the same batch.

We don't split it by uh the features as well.

But if you have an X and another variable, for

example, the same batch would go through for both variables

for each variable you have it's a different batch.

First one, same batch variables.

OK, some quick terminology, right?

What we're doing here, this whole, this 3 step process

is training the model, OK, show it some data.

That probably get lost, update the parameter values, this is

training.

The training data, right, this, this, uh, should not be

that unfamiliar to you, right, that's the data that we

use to adjust the parameters and the reason we can

do that is because we also observe why, right?

We need to observe why because what we do is

we back propagate the loss, right, and the loss requires

us to know how poor or good that prediction is.

The test data is the data we're gonna use at

the very end to validate our model, right, so um.

This is like, you've done all this type of proter

tuning, you've worked out the batch size, you've worked out

everything, OK, you've done that with consolidation, you're gonna hold

back a little bit of data, the test data, and

before you send it to your boss at 4:45 p.m.

on a Friday, you're gonna say, and this is how

well it performed on data it didn't see.

OK, and this is gonna be important because our neural

networks are gonna have a uh an annoying habit of

overfitting to the training data, OK, you already kind of

saw it in action, um, here.

This epoch, right, or step, OK, is just one run

through all of the examples in your training data.

OK.

So with stochastic gradient descent, that means you're gonna, you're

gonna do N updates, one update per observation.

With mini batch gradient descent, you're gonna do N divided

by M updates where M is the batch size, right,

the number of observations in each batch.

Yeah, go for it.

Uh, what's convergence like in a couple of slides up,

like, how do you define?

Ah right, so do you mean like how would we

actually decide that in practise, or like what is the

concept?

Oh right, OK, so, so that would be like, so

hopefully, right, we want our model to basically.

Say Y equals X + 1, right, so W equals

1, B equals 1.

Hopefully we want it to kind of, kind of, it's

gonna start off weird and it's gonna get towards like

0.64, then 0.96, and then 0.99, 1.01, and we, we

want it to kind of that that plateau to converge

on the optimal parameter value.

Right, and that's what I mean, er, by convergence.

In practise, that won't happen most of the time.

What you'll find, and I'll show you this next week,

is that your loss does this, it goes down, right?

And this is loss on like out of sample data,

so test data.

The loss is gonna go down, down, converge, get like

a bit stable, and then come back up again.

And that's because your training loss continues to decline and

it's overfit.

It's connecting the dots in the training data and it's

forgot.

You know, and more amorphizing it, but like it's forgotten

that there's like some general data that may not fit

the exact pattern you see in the data.

We're gonna talk about this next week and in fact

we're gonna like implement it in in the second um

lab, um, so you can see what this looks like

in practise.

I guess my question to that is like, is it

more about how long the model takes to run through

all of the in samples or does it stop earlier?

Like when you're comparing the convergence rates, does the model

stop?

No, never stops.

between the two.

It, it would matter in like a um production sense,

right?

It costs compute to train a network, so fast convergence

is better because you don't have to run your computer

for as long.

If they're both converging on the same point, then, then

I, I agree, ultimately if all you're interested in predictive

accuracy, it doesn't really matter, right, but we're, we're, we're

gonna assume that we're cost constrained here.

That makes sense.

No, because it still seems like you're comparing like how

long each model takes to just fully run through all

the data, not comparing how fast it takes to because

it's convergences here and here, Convergence isn't the speed to

do an epoch, right?

Convergence is like the number of epochs required.

To get to the minimum.

Right?

Assuming there is some true minimum that you can be

getting to.

OK, so I'm not talking about um.

You're never gonna do a partial epoch, right?

So we're always gonna think about a full run through

the training data.

OK, the question is how many runs do I have

to go through before I get to the best that

this particular model in santiation can do.

Questions, yeah, um, so at least intuitively given that we

do more updates, um, That should Do less overfitting with

respect to mini badge gradient, right?

Because it doesn't identify patterns as well, given that you

changed tweak the gradient at every observation.

So does that also play a role in what gradient

descent are you actually choosing because there might be some

cases where overfitting is really bad.

You can't afford to have it.

Yeah, that's a great question.

I don't know the answer to that.

I mean, um, because maybe by trading percent is also

a category like the composition of the batches is random

over epochs, you might expect that any kind of, um,

biassing as a result of the ad is um.

Also average out itself.

And also you may also prevent it from.

You know, it's very and authorities this example is getting

very tired now, but that communist grandma, right, like she

is.

Um She has full say over the next set of

parameters, the next set of the parameters, right?

So in that sense the model is overfitting on, on,

uh, Grandma, right?

In the, the mini batch, she's always going to be

tempered by those around them, and so any idiosyncrasies to

that specific data point should be averaged out.

So I wonder whether that trade-off actually balances out, but

I'm not sure, so let me go away and have

a look and I'll, I'll get back to you.

Yeah, um, we set the lambda to 0.1 is the

is there a way to derive the value that would

like, um, it would take the the fewest number we

wants to converge to.

Um, OK, yes, so yes, you can, so one.

The, the simplest thing that you can do, and if

you watch the Andre Carpathy neural network zero to hero,

um, YouTube videos, so the thing that he shows is

like, um, basically you start with a reasonably coarse lambda,

so set lambda quite high, 0.1, right, which is quite

high for your network, and do that for uh 50

epochs, OK, and then you're basically good and go from

like this area of high loss to a shallower area,

so hopefully you're great is the thing.

And at this point.

You might want to like tail off on that lamer.

So you reduce it down to 0.01, right?

So now you're making much smaller updates and do that

for another 50 epochs and then hopefully you won't like

skip over anything and then shoot off into the kind

of ether.

Um, there are more kind of um uh principled ways

of doing this, uh, and I'll talk about them in

a couple of slides, um, time.

Yeah, the example you gave, could be the uh learning

it on the nose like.

Instead of saying that for 50, could we say that

if the loss gets to a certain amount, then yes,

you absolutely can do that.

That would just rely on you knowing when to set

those thresholds, right?

And depending on your output, whether it's house prices, which

might be measured in the hundreds of thousands or, um,

I don't know, the number of pets measured 0 to

10, right, then um you'd have to have different thresholds.

So you'd either have to think about scaling.

or knowing what a sufficient loss value would look like

for the specific context you're doing.

But yes, that's totally possible.

Right, let me, um, I love your questions, but, um,

I have a few, few last things to show you,

OK?

I want to show you overfitting very quickly.

So suppose, remember that our network was converging on W

equals beta equals 1, i.e., the function that it was

approximating is X + 1, right?

But suppose actually the true data generating process is Y

equals 2X.

OK, the problem is with just X equals 1, Y

equals 2, those two functions intersect at that point, right,

so the model has absolutely no way of distinguishing between

the two of them, and indeed, given the initial values

of W and B to be set, uh, it's, it's

gonna converge on, on W equals 1, V equals 1.

So, um, let's just quickly suppose that we observe that

second observation.

So this is the stochastic gradient descent, uh, element, OK.

Notice that as it converges to 1.

That our loss is still non-zero when we consider that

new data point, right, because uh X equals 3, Y

equals 6 does not conform to Y equals X +

1, right?

That's the idea here.

OK, so let's just go back a few steps.

Let's just say that we've done one update to the

parameter value, so we're back at 0.4 for W and

B.

Now we're gonna pass through X equals 3 and Y

equals 6.

OK.

I noticed that when we do this, this is the

stochastic gradient descent element, and I back propagate it.

My gradient for W and B are now different, OK,

and notice that the gradient is much larger for W

than it is for B.

So what we can expect when we do 0.1, 2

times these two numbers and update the parameter values, is

that this is gonna change faster than this, and hopefully

we should get to a point where we're gonna have

W equals 2 and B equals 0, OK?

So this is the importance of seeing more data because

you might have functional equivalences just focusing on, say, a

single example, right, or even a small subset of examples

within a larger population.

So I update those values, so now W is equal

to 3, roughly, and B is equal to 1.

I then pass back through my original data points.

This is like the second epoch, so X equals 1,

Y equals 2, make sure to zero my gradients, that

gets me a new loss, but clearly uh W and

B are too high for this example.

So what we're gonna find is that as we keep

repeating this process, notice now that my gradients are, well,

um, at least with W.

Smaller than they were after I passed through that second

example, uh, and my values are being updated close to

2 and 0.

OK, that's just what I wanted to show you in

these uh slides.

Are we all happy with that?

I know I'm, I'm rattling through it, um, I just

wanna make sure we get out of here on time.

OK, we've talked about this um uh through multiple questions

already, so I'm not gonna say too much here, right?

Gradient descent is what we call an optimizer.

When we use Pytorch, you are going to define an

optimizer, and the starting point is going to be SGD

stochastic gradient descent, OK?

There's no reason you have to use that one, and

indeed in the last 10 years we've moved away from

it, OK.

So things like adding momentum.

So that if I'm already rolling down the gradient, I

should carry some of that previous update into my next

one, OK, that's gonna speed up our convergence in terms

of the number of epochs because it's gonna be able

to keep rolling more than it otherwise would be able

to.

You can also have different learning rates for different parameters,

right, and this is called uh Adam, adaptive momentum, right?

So it's gonna change the amount of momentum per um

uh weight parameter based on what's already going on during

the training, looking at the loss and the partial derivatives,

OK?

And there's an even.

More new one called uh Adam W weighted um adaptive

momentum uh training, OK.

I'm not going to cover these in detail because they're,

they're not necessary for us to know the ins and

outs of for this course, but I will show you

how to implement them and indeed, when you're thinking about

how do I get the best um uh trained neural

network in your assignments, you might want to think about

subbing out stochastic gradient descent for Adam.

It's just changing one function for another, OK, and I'll

show you how to do that.

We did that for a single perceptron, right?

One input variable, one weight, 1 bias parameter, a teeny

tiny deep neural network, right, 5 inputs, 2 layers, 3

nodes per layer, looks like this, and I drew that

by hand, OK, so I appreciate that just for a

couple of seconds, right?

Notice how complicated, like how many uh computations there are

just for this teeny tiny that would not be able

to predict anything, right?

Super tiny, massive number of computations, right?

Your assignment is about to propagate this whole thing.

It's not, but like it's gonna be really, really convenient

that we can store all of those partial derivatives as

part of the model building and then basically we just

say, OK, go and compute it and come back to

me, right?

And then we just do that little update.

This is all numerical, it's lovely, right?

These complex graphs are not a problem.

The derivatives can be pre-calculated.

If I make the uh activation function, the identity function,

it knows the derivative is one, right?

It also knows the derivative when we use the sigmoid

activation function, these are just stores, you load it in,

right, then it can just do the little calculation, that's

all it needs to do, right?

And the back publication itself is very simple.

We're not taking like the tan H or like the

sec or anything like that.

It is literally do some matrix summation and a little

bit of matrix multiplication.

That's it.

Like what you need to back propagate fast is not

a simple network, you just need lots of simple computers

that can work in tandem to get this uh working,

OK?

And so that's why all you see at the moment

is talk about GPUs, right?

And they're kind of like, you know, one, model gets

released, uh, in China and the S&P drops by multiple

percentage points, right, because Nvidia, the major producer of GPUs,

right, has become so dominant in recent years, like literally

look, look.

Up until here it's just like nerdy people playing video

games, right?

And, and now it's every kind of data scientist at

every top university uh in the world, right, all because

these GPUs are just thousands upon thousands of minicomputers doing

linear algebra, right?

Making sure that we can compute complex graphs like that

almost instantaneously, right, this is why it's so important.

OK.

Almost there, last slide.

What I want to tell you is that this is

great, but there's still lots of work to be done.

Next week is gonna be an engineering week.

We're gonna think, how do we make these deep and

how do we actually implement them so we don't run

into problems like W equals B equals 1, like these

symmetry problems, OK?

So we're gonna talk about initialization.

We're gonna talk about how many nodes and what arrangement

they should look like and how many times we should

run through our data so we don't get this problem

of the loss starting to.

---

Lecture 3:

That's I it was like it was like the most

like thrilling thing of like, but it was like fun

like I've never been to a wedding was a shock

and I think it was it wasn't a huge wedding,

but like.

the events yeah yeah um and the last day in

the morning and at 6:30 in the morning I was

like I need to go.

I'm having so much fun.

I was like, I don't want to leave like I

had to come back and stuff and they're like it's

gonna happen.

Yeah, it was too much, but I guess it was

yeah.

It was so nice.

Oh no, that's why I was like I was like.

Yeah Yeah this is like my mom's wedding was a

scared wedding and I was sitting there.

She re oh there's so many 1 5 kg.

Yeah, we were like yeah.

I, I, I don't know how she she lives here,

she would have brought the 5 kg like on the

flight probably had its own seat and 2024 Porsche for

like like that he was out with like the.

Probably not, but like everything else.

No, I don't I was so like mesmerised by everything

else I was like, really good.

Ros and I have been working and I was like,

is that?

And like Jerry I was like, I that that Oh,

you did.

Oh, you actually did something and I know I just.

and my eyes.

OK.

And I'm like, oh, you must be 15.

And yeah, it's, it's it's like but yeah it's like

that's not like.

Yeah it's not it's going to be a bit, but

I'm.

I it's like yeah, yeah, yeah, yeah, yeah, like next

to.

It looks like.

Alright, shall we get going?

Awesome.

Welcome back.

We've thinned the room a bit.

Hopefully that's a 9 a.m. effect, not 701 effect.

Um, uh, what are we doing today?

Uh, well, uh, those first two pretty tough weeks have

given you the kind of fundamental and mathematical building blocks,

um, for building basically any neural network, right?

So what you now have are the skills you need

to do anything from what we'll do today, which is

your conventional fully connected neural network.

All the way through to more complex things, uh, like

concurrent neural networks or or convolutional neural networks, which we'll

do in week 5, Transformer models which we'll do later

on in term, all the way up to kind of

the big bad monsters of the kind of modern age,

including things like chat GPT and uh LLMs of that

scale.

So.

that when uh building uh any neural network, right, we're

just going to send our data from left to right.

We're going to multiply the inputs by a set of

weights, right?

They're going to be a set of matrices, typically, we

add some bias and then we activate the results of

those things, although up until now, that up until now,

that activation has just been just return yourself, right?

We've been using the identity function.

We're gonna swap that out today.

So we can push the data forward through our network,

right, but the problem is that we're typically gonna start

with weights that are near zero, that's not gonna be

an informative model.

The whole point of doing any form of machine learning,

including deep learning, right, is that we somehow induct from

the data, uh, weights and biassed values that get our

model performing as well, uh, or as uh approximately well

to the true data generating process as, um, possible.

And so we do that using this.

An algorithm called back propagation, which starts from the end

of the network, calculates the loss, and then works out

how the loss changes for the values of the weight

to buy turn in the kind of penultimate layer.

And then once you've got that, you can push it

back to the the uh next, or the previous hidden

layer all the way back through to the kind of

inputs, and at that point, uh, you can stop, right?

The actual adjustment is made using gradient descent, uh, or

some kind of more modern variant like Adam, which does

some kind of like.

Additional tuning, uh, that we're not gonna worry too much

about this course, although feel free to use that.

You'll see it's just a single line of code.

OK, so what are we gonna do today?

Well, given that I've kind of, um, smashed you through

uh the kind of hard maths of all of this,

we're gonna think more generally today about how would you

actually build a deep, uh, network, uh, and think about

the various kind of choices that you as the research

engineer have to make when constructing that network.

So what is the size and scale and number of

hidden layers that you include, um, how do you set

those weights initially?

Why might we not want to set them exactly to

zero?

Given that there are many different activation functions, sigma that

we could have, which one should we choose, uh, and

then we're gonna kind of like think about this, uh,

in terms of uh a real applied example today, so

kind of predicting or forecasting, um, violence, right, this is

a, I think, excuse me, a um.

This is an area of the literature where kind of

predictive tasks have become part of like the norm, right?

This and maybe kind of bits of sophology and predicting

elections.

These are cases where, uh, actually there is a kind

of scholarly literature on can we predict violence better?

Can we forecast when civil wars are gonna exist?

They are kind of like benchmark data sets and competitions

that academics submit their models to, etc.

etc.

Now outside of those, I think there's actually relatively few

areas where like a pure predictive focus is um.

Kind of prevalent, um, so I think this is a

nice kind of place for us to start.

Yeah.

Yeah, that's a great question.

I will, I mean, I'll define it um more in

a bit.

No, I'm, I'm thinking like violence as a result of

civil, uh, basically, um.

And this leads me on to there's no point here

which is that we're gonna assume that we've still got

like yes, sorry, not, not kind of like um domestic

violence, this will be um um.

I don't know school or school, yeah, yeah, yeah, um,

no, no, I'm thinking here like political violence, um, broadly

construed.

OK, any questions before we begin?

Awesome.

OK, so this is what we're basically going to be

building up to today.

This is uh the preview of a deep neural network.

So we have our input layer there.

We're gonna just for the sake of example, assume we

have 5 inputs.

Uh, these are gonna be fed through the kind of

core or heart of the, the neural network, which is

those hidden layers, right, and it's the hidden aspect of

them that makes these models so kind of.

Uh, performance or flexible because we can basically make that

middle box look however we want it to look, and

then that's gonna output to some final, uh, layer where

we actually get the predictions, uh, that we want.

Now, we're gonna define all these things uh in more

detail in a second.

Uh, but this is perhaps a more common representation of

exactly the same network.

We normally drop both the kind of input layer, which

is just our data, right, and the kind of value

representation at the end, which is the square box for

Y.

OK, so these two things are basically synonymous.

Here we're just looking at the nodes in which there's

any kind of form of um computation, right?

Multiplying by weight matrix, activating, etc.

etc.

Yes, go for it.

What is it, uh, on the one before, what is

the final arrow, like, the difference between how do you

further transform your out?

Right, so what's coming in, so this is actually kind

of like, this is a uh this has a weight

matrix, right?

And it has a, it does also have a biassed

term and it will have an activation function, right?

So it is taking all five inputs from the last

hidden layer.

Weighting them, adding them up, adding the bias, and then

activating them still.

And typically, uh it's a good question because typically here

this activation function, uh, in the output layer is going

to be very different from the activation functions we use

in the hidden layers.

Yeah.

And also like the input layer in this picture, it's

um it's one observation with 5 features or 5 observations

with unknown number.

This is one observation with 5 features.

So you can think of like the data.

As being like stacked this way, right, so you've got

a queue of observations and they're being shunted through one

by one.

OK.

So this is the first of the first.

OK, so why like one feature goes to one like

I not all of them should go to every single

um.

Because uh this is just kind of like what gets

fed into just this first.

Um, I do know exactly what you mean.

Why is it not fully connected from the input to

the hidden layer, um.

Hm.

Let me come back to you on why like I

just did it like this.

I'm, yeah, let me come back to you on that

rather than kind of like 5 pictures should go to

every single neuron in the first hidden layer.

Yeah.

It could well be that Let me come back to

you.

I'll, I'll write you a proper answer rather than answer

on the spot, um, but yeah.

OK, cool.

Here's our.

Thing, the next thing I wanted to say was this,

OK, so if you look at some other textbooks.

Um, or indeed just online blog posts.

Um, sometimes what you'll see is, is a slightly different

representation, which is you have these, uh, nodes still, so

we've got our input node, we've got our hidden node,

we've got our output node.

But rather than kind of think of those as being

like little computational units that do something to data, those

are just values, right?

And that instead what happens is that you have uh

your weights as the uh quote edge weights, right, of

this network.

Now if you're doing, I forget the code, but the

social network analysis module, right, this is probably how you're

familiar with looking at a network.

Uh, the data flows through and gets weighted as it

hits the next uh value uh node.

I don't like this notation because I think like you

should think of those nodes as being little computations, right?

I think you should think about the kind of like,

um.

Uh, them as being like little mini kind of processes

in and of themselves.

Uh, and this I think is also just harder to

kind of, um, write in any great kind of complexity.

So, um.

Just to note that if you see other resources where

they're weighting the edges, this is the same thing as

what we're doing, we're just assuming that W1 is like

in H1 because it's the weight.

In that computational uh node.

OK, I like to think of data flowing in and

out, uh rather than that being affected on the way.

OK, so, um.

Here, here's just an article I found from kind of

the last few years, uh, specifically in this kind of

prediction space, um, and their motivation for kind of like

generating a highly predictive model, and this is what this

um paper forecasting fatalities does, um, it is that, like

they say, and rather intuitively, knowing the dire consequences of

armed conflict, preventing and containing future conflicts are high on

policymakers' agendas, right?

Early action.

However, requires early warning.

So this is your kind of classic ML task, right,

because it has, I mean, basically there's two features of

it.

One is it's predictive, right, we want to predict, um,

uh, or have these early warnings, quote, uh, of, uh,

conflict, armed conflict.

But I think the other kind of aspect of this

which is implicit, is it's about future conflicts, right?

This is very different from your conventional uh take data

from the last election and analyse why people voted Labour

or Conservatives, right?

This is thinking, I don't know what's gonna happen in

the future, like we can't see into the future.

What we want to do is kind of make a

shortcut to waiting, right, by like having some features that

we think might be predictive of it and therefore being

act, be able to act, uh, earlier.

OK, so there's kind of two features.

One is it's predictive, but also it's predictive on future

events, things that we can't actually measure or observe, um,

yet.

So the basic research question here is, uh, can we

forecast or I predict uh conflict?

And uh what they're using in this paper is a

uh a data set called the Cross National Time series

data on Violence, um.

And I think, I, I think it goes even before

1945, but I think I lopped it off at 1945

as kind of, kind of classic post World Wars, uh

era of uh violence.

And in this data set there is this kind of

aggregate measure of conflicts.

Now, I'm not a conflict scholar, so uh I'm not

gonna go too much into the detail of what exactly

that is.

The crucial thing for us is that it's continuous, right?

This is not a binary measure of was there conflict

in this country in this year.

This is a measure of how much uh conflict.

Uh, and I believe it's based on the, basically the

number of deaths, uh, in any given year, in any

given country.

So, Here I have a um.

Uh, slightly more convoluted modelling structure.

I'm not even sure it's a deep, uh, neural network

type structure.

I think it's another form of machine learning.

Um, but we can, we can definitely try to do

this with a, a conventional deep neural network.

Um, and just to kind of give you a kind

of why might, how might someone kind of like write

the, uh, 2025 version of this paper trying to implement

deep neural networks to predict conflict, they might say something

like, well, we might think that deep neural nets are

gonna be kind of performance, i.e., they're gonna be highly

predictive, uh.

If we think that the relationships between these variables are

complex, right, this is your classic motivation for doing anything

uh non-parametric in the social sciences, right?

We think that like it's just not gonna be possible

for this relationship to be described using a straight line,

essentially.

OK, so what does that really mean in practise?

Well, I think for most of the time it's gonna

be one of two things.

We're either gonna think that the kind of um the

way in which variables impact the outcome are non-linear, right,

that there is some kind of U shaped or kind

of inverted parabola, or maybe it's kind of a wiggly

line or or whatnot, right, but there's just not a

kind of standard straight line.

That maps X to Y here, right?

And although in certain situations you may be able to

kind of like modify your linear aggression to include a

quadratic term or even a cubic term if you're feeling

really adventurous, right?

If it's more squiggly, right, or what if it's a

kind of a sign curve or something like that, uh,

that's going to be much harder for you to kind

of a put into a regression model and be justify

the inclusion of kind of a pro, right?

So for example, and here is my kind of comparative

politics 101 knowledge, right, we might think that, uh, conflict

is kind of, uh, I say influential here, what I

mean is kind of, uh, at its peak, um, at

middling levels of GDP per capita, right, because of very,

very low levels of um.

At very low levels of GDP people are simply subsisting

and don't have the opportunity to.

Engage in violence.

And oh it's my laptop, that's what it.

And at very high levels of GDP people are satisfied

and therefore not engaging in conflict, right, that's the kind

of.

Very, very limited, uh, theoretical justification for kind of this

kind of like non-linear, there's gonna be this peak of

violence when your GDP is, is, is neither too low

uh nor uh too high.

Uh, and the other thing is, not just kind of

these nonlinearities, but, um, kind of interactions between features, right?

So it may not just be that you have rugged

terrain in a country which promotes violence because it's easy

for kind of easier for guerrilla warfare, but it may

be the combination of, I don't know, rugged terrain and

GDP.

It may be rugged terrain and GDP and average rainfall

or something like this, right?

There's, there's, there's many ways in which features may interact

with each other.

In order to um uh cause uh the outcome in

the real world, and it may just be impossible uh

to kind of think through all of those interactions uh

in a standard parametric model, right?

So wiggly lines, but also lines that are kind of

impacted or um kind of mediated by potentially many different

variables interacting, uh, together.

So this is the quote unquote simple prediction problem.

Uh, that we are gonna kind of focus on today.

Can we build a model that, um, kind of gets

us some way to predicting the amount of, uh, conflict,

continuous quantity, um, in, uh, the, in the world, and

ideally in the future.

Anyone have any questions on the context?

No?

OK, so these are the steps that we need to

take today.

So we're going to do all of these things, um,

uh, in turn.

So when defining a neural network, especially one capable of

predicting violence.

We have to take these 5 steps.

The first is to define the network structure, OK, so

what exactly is that model gonna look like, how many

neurons are there, where are they, how are they connected?

For each layer in that structure, we're gonna need to

define uh what it's allowed to output, right?

So what is the activation function that transforms the kind

of um weighted calculation inside the neurons into the kind

of uh inputs to the next layer.

How we actually initialise this network, which is really, really

important.

How we uh would measure whether the model is doing

well and precisely how, once we know how well the

model is doing, we're gonna update the parameters, right?

And then finally, um, the kind of intricacies of training

the model.

Effectively.

So even though you've defined the structure, even though you

know your activations, even though you've initialised your model, and

you know you know how you're going to judge how

good it is, there are still a whole load of

tips and tricks that you need to actually kind of

extract any reasonable performance from a neural network.

So we're going to discuss that in the last kind

of third of uh today's lecture.

The really important thing about this slide, right, is that

almost all of these steps say define, right?

This is uh one of the kind of almost contradictions

of uh deep learning in particular.

Which is that whereas something like a random forest, you

have a uh have a limited number of hyperparametters, right,

you might say I'm gonna have 1000 trees, I'm gonna

have a maximum depth of 5 or something like that,

and then it defines the network structure, right, that's the

kind of classic inductive, I'm gonna let ML do all

the work for me type approach.

In a neural network, actually there's a lot of kind

of engineering that you have to do before it even

sees the data.

Right, these are all fixed things once you've decided.

So the number of nodes, what the initial values are,

uh, the activation.

These are not things, this, this is not a kind

of form of modelling where it's gonna say, well, I

don't need that 15th hidden layer, I'll drop it off

or I'll set all of the values to one.

That's not what, uh, neural network training, uh, is like.

It really does involve you kind of being pretty, um,

uh, proactive in setting up a network that will work.

OK.

All that gets updated during this kind of training the

model stage, remember, are the weights and bias terms.

Nothing else.

The structure is constant once it is defined.

OK, so we are essentially.

Uh, and is this gonna be true throughout the rest

of the course?

Almost always in this course, gonna be focusing on what

we call fully connected feed forward networks.

OK.

So what does this mean?

What it, what it essentially boils down to the fully

connected aspect is that for any layer, OK, so layer,

layer, layer, every node in that layer is connected to

uh every node in the the subsequent layer.

OK, so contrast the first and the last graphs here.

This is fully connected because.

All of the outputs from, sorry, the output from this

first node in the first layer sees both uh nodes

in the second layer and vice versa for this.

Second node and similarly both of these nodes output into

the only node in the last layer.

In the full, the final graph here, right, H11 goes

to H21, but it doesn't serve as an input into

H22.

OK, so this is not fully connected.

There are far fewer um.

Cross kind of connections here, uh, the weight matrice is

gonna be much simpler.

For this specific network, um, and it's not fully connected.

Normally we want this because we want every opportunity possible

to kind of tweak the kind of um prediction function,

right, uh, to get it as close to zero as

possible.

Yeah.

So what are the cases when other uh structures are

literature?

Yeah, fantastic.

So if we come to this.

Middle graph here, for example, notice that it's got these

uh recurrencies.

Right, so basically what this is saying is that the

output of this node here becomes an input to that

very same node.

Can you think of an example where that may be

useful or a type of data?

like stock market prediction.

Yeah, and what's particularly about stock market prediction might make

this useful.

What's a characteristic of that type of data?

Yeah, time series data, right?

The, the value at time equals 1 influences the value

at time equals 2.

So you might actually want to model that basically that

recurrency, right?

Uh, so it just gets fed back into each other.

Yeah, go for it.

I mean, if, if the goal here is to not

like as kind of complex as relationships as we can

that are possible, why would we not want the recurrence,

so sometimes it might be redundant, um.

For example, um, I, I mean this is, this is

not, you know, a perfect analogy, right, but the, the

conflict data we have kind of, um, uh, let me

think of part of, we have.

Uh, OK, gee, we have like, uh, the level or

amount of violence in, um, uh, Ghana in 1945, right?

And then.

We go to Greece, right, is the next one, right,

in 1945.

Now, of course, OK, 1945 is a bad example because

it's just post-war, but like it'd be, there's not much

of a connection between those two places.

Right, so having that recurrency, feeding the value for Greece

to impact the value for Ghana or vice versa, right,

is.

Probably gonna be redundant.

That's slightly different from when it's like uh the value

of uh Nvidia stock at 12 p.m. versus the value

of Nvidia stock at 12:01 p.m. right?

They're gonna be highly related, so that recurrent link is

going to be um pretty, pretty important.

but that's the same entity versus different entities.

Uh, yeah, exactly, right, but, but I think that's the

point of the structure of the data is, is that

you would train it so that the recurrencies only occur

within an entity, right?

Um, you're right though if we thought about Greece 1945

versus Greece 1946, this recurrency could be helpful and in

lots of kind of panel data, situations like this, we

might have a recurrent neural network, um.

But actually, like, I mean, I'll show you some slides

next week where a fully connected feed forward without any

recurrencies can actually predict time series data pretty well um

in precisely these types of contexts.

OK, the interesting thing is that if you have this.

Recurrent neural network, the way you actually solve that is

by making it.

A not fully connected, but feed forward network.

So basically what happens is.

These rates down here.

A tasked with mirroring these weights here.

But your first time series point goes in here, and

your 2nd time series point goes in on this like

2 branch.

So you basically unwrap this recurrency into a, a feed

forward, uh, network.

Um, we're not gonna get to recurrent unless Friedrich does

it, we're not gonna get to recurrent networks in this

course.

Um yeah, no, of course you can.

Like for 2 years, it will be like um 2

different features or two different data points points.

OK.

Right, so we would observe, say.

Because sometimes you can like like the structure the data,

the time series is like one entry.

So in, in that case, you would basically be able

to approximate recurrency just with a fully connected feed forward

network, right?

Because if GDP in 1945 is related to GDP in

1946, right, then, uh, modelling or approximating the interactions between

those two features.

would essentially be the same as like punting just the

1945 data through and then just the 1946 data, but

having this recurrency that would allow those two things to

be connected together.

Yeah, go for it.

If we do have limited data, for example, we have

data for let's say 10 years, then would the network

be able to not losing data points as against if

we bring say or fully connected network, then we might

need to like include labs so it would reduce data

points.

Is that the case?

I, I don't quite understand what you mean by reducing

data points.

So for example, if we have 10 data points for

10 years and I'm including two lads in the model,

right, so I'm technically having 8 data points left because

I so if we have, would that lead to having

10 data points instead of 8?

Uh, it would.

Whether or not that would give you better performance, I

don't know.

Um, because there's more, essentially more information in those 8

data points, right?

So the amount of entropy in information theory terms is

probably pretty similar, um.

I also think that, and this is a little bit

of a facetious response, but like with 10 data points,

it's just not gonna train, right?

Uh, so it's a bit immaterial, like you'd be more

thinking about like 1000 versus 998, at which point the

loss of observations is probably limited, right, and it's more

about.

Yeah, yeah, of course uh correlation matrix or something like

that.

So for these feature engineering, should we have some standing

set of like Connection or changing.

Um Not really.

I mean, I'm sure they exist in industry.

I'm sure there are packages that kind of like automate

this process.

I don't think you really need to worry about kind

of um um multicolinearity, right?

It it's, it's not going to stall the model in

the same way that it would like, like perfect multicolinearity

would wreck your OS model, um.

Uh, for one thing, for a second thing, because this

is inductive, you might just be like, well, look, like,

because I can include it, I might as well, because

the model will hopefully sift out what's important, it will

learn that these two things covariant and, and whatnot, um.

And then I think the kind of third thing is

you're always gonna have to do some form of feature

engineering in terms of like scaling and and and um.

Yeah, well, mainly scaling actually, um.

And maybe some resampling of the data and in those

instances.

Um, Yeah, I think that's basically it.

We'll talk about some of these kind of like data

sets that you need to take um er later on

er today.

Yeah.

Cool.

OK, so fully connected just means every node in a

layer inputs into every node in the subsequent layer, right?

Fully connected but non-feed forward here is because this data

is returning to itself, and this is clearly not fully

connected because H12 isn't connected to H21 and H11 isn't

connected to H22.

Right, now let's think about uh these layers in particular.

So we assume that we're just working with a fully

connected feed forward, right?

Uh, the input layer is gonna have uh K features,

right, these are the uh explanatory or predictive variables.

If I come all the way back to here.

Right, these are the uh X1 through X5, and you've

got to think about the data being rotated so that

the features are now rows and the number of observations

of columns, and it's like a queue.

You're gonna pump the first one through, then the second

one's gonna come through, then the third data point is

gonna come through.

Etc.

etc.

etc.

So this is basically fixed by your data, OK, the

size of your input layer has to be K, the

number of features or predictable, uh, variables you have, uh,

to hand, OK.

So each node here is going to correspond to a

single feature, and it is technically a layer, but crucially,

because it's just the inputs right, there are gonna be

no weight biases or activations associated with that input layer.

This is why, and to clarify what I said in

the first lecture, that first fully connected layer.

Here, right, it's technically a hidden layer, because technically there's

the input layer behind it, um.

That is why uh it is technically a hidden layer,

OK.

Um, so that first input layer is just passing the

data through, right?

It's just a way of getting the data uh into

the model and hence, because it doesn't really do anything

other than pass it through, it's come to drop it

from the kind of visual representations of, uh, your neural

network.

OK, so the key thing here is that your input

layer must have K input features.

That's gonna be very important when you're defining your models,

as you'll see uh in next week's seminar.

Now, here's the exciting bit, right, these are the hidden

layers.

So essentially, your hidden layers, right, the things that are

neither the input nor the output layer, right, uh, can

have any dimension you want, right?

We can, we can have 64, 128, 2 million nodes.

In a um uh hidden layer, even if K were

equal to 5, right, nothing stops that uh kind of

explosion of parameters within the hidden layers of your model,

and that is a hyperparameter that you have to choose,

right?

What should the network structure actually look like?

So your first hidden layer, the first kind of set

of grey dots I had on the visualisation before, has

a um dimension of K by K.

OK, so K is your number of input features, each,

um, and this is why I'm thinking maybe I should

adjust that diagram.

Each uh.

Node in the hidden layer receives K inputs, right?

Uh, but then it can expand out to essentially kind

of K.

There can be K of those nodes, right, because we

can send the input to every single node in that

layer, uh, separately.

So K is what you're choosing here.

So if K equals 5, we could set K equal

to 32 or 64 or whatever, OK.

Then in your second hidden layer.

You have to take K inputs, right, cause it's fully

connected feed forward, right?

But then again, you can change K here to be

whatever you want.

So you could have an expanding number of nodes, so

you could go from 32 to 64, OK.

Or what we'll talk about next week is the opposite,

where you have a shrinking number of nodes, right?

So you might go from 32 down to 16.

Uh, for example, right, or perhaps the most conventional kind

of fully connected feed forward model, you just keep it

the same.

So you've got 5 going to 32 in that first

hidden layer, then maybe 32 going to 32, uh, in

the second hidden layer, and so forth and so forth.

And notice that not only is the number of nodes

in each hidden layer essentially arbitrary, right, other than the

ends, uh, but also the number of hidden layers you

have.

So long as I can keep multiplying these matrices together.

Right?

Then I can make this as as large uh or

as long or as deep a network as I want,

right?

What matters is that they just chain, uh, together, right?

So 5 times 4 could go into a 4 by

3, that would be a 5 x 3 matrix, and

then 5 times 3 times.

By 3 times 4 it's gonna be 5 times by

4.

So this, this is all computable, right, and so it's

all possible.

Just have to make sure that the number of outputs

from the previous layer is equal to the number of

inputs in the uh current uh hidden layer that you're

defining.

Are we happy with this?

OK, so arbitrary number of nodes, just to make sure

that the number of input connections matches what's coming from

the left in the network.

You can have an arbitrary number of hidden layers, uh,

as well.

Uh, and in very large models, you know, that can

be kind of like, you know, lots, lots and lots

and lots of hidden layers, not just 2 or 3.

Then once you get to the end, right, you're gonna

have this output layer.

Now here, this is, this is also fixed a little

bit by what you want to do, OK?

So, uh, essentially the size of the output layer is

going to be determined by what we're trying to predict.

So in this case, this is a single number, right?

It's the amount of, of conflict, uh, that we have.

Uh, and so for problems like this, which we would

call a regression problem, right, it's just a continuous prediction.

Or indeed for binary classification problems, we basically collapse all

of those kind of hidden outputs into a single uh

output node, which is just gonna per observation output a

single scale of value, right?

So do all this complicated uh matrix multiplication in the

centre, but at the end of it, we want to

collapse that down to a number, which is the amount

of violence, for example, in a given year.

It doesn't have to be a single node, actually, right,

and next week we're gonna think about cases when we

have multiple nodes at the end of our network and

the reasons we might want them.

Um, so the first is if you have a multi-class

classification problem, right, so, um, maybe we didn't have the

amount of violence, but maybe we had kind of like.

Uh, no violence, local violence, national violence, international violence, something

like that.

That would be 4 different type classes, right?

We could have, um, 4 nodes at the end of

our network where each node is trying to, uh, give

you the probability.

Uh, of each class, right?

So ultimately what we'd want at the end of it

is something like 0.45, this is local violence, 0.3, there's

no violence in the other two categories, something, uh, to

the remainder 0.25.

OK.

Or indeed a more perhaps obvious example for political scientists,

it's kind of like predicting party affiliation, where there are

multiple parties.

So, uh, it could be C nodes when you have

a multi-class classification problem, there are C classes.

Uh, but it could also be the same size as

the input data if what you've got is a generative

model, right?

So for example, I take photo, uh, photos as my

input data, and what I want to return is some

kind of like cartoonization of them, right?

You want to look like, um, uh, caricatures.

Uh, in these instances.

Uh, I'd actually want K inputs out at the end,

right?

I would want the same size image.

I just want it to look like a cartoon rather

than, um, like a photo, in which case we can

have lots and lots of, uh, nodes at the end

of our network, each one predicting basically the pixel value,

uh, of an image where K is the number of

pixels, uh, you have, uh, in your image.

We will talk about how exactly to do that, what.

But working with kind of like, uh, generative models, uh,

next week, but also in, um, week 5 when we,

when we look at images.

OK?

Veronica, was that a question?

But here K also relates to the number of features

in the input layer or it's just.

Yeah, yeah, yeah, absolutely.

And normally that's because what your input is is uh

a value for each.

I'm thinking about images here, so each feature is the

pixel value, right?

And so if you have a 64 by 64 pixel

image going in.

You'd probably want a 64 by 64 pixel image coming

out, uh, so K would be the same in both

cases.

It doesn't have to be, right, you could downsize it.

You could have a neural network that does some form

of like compression, makes 64 by 64, a 32 by

32, uh, that would be totally fine.

So, input layer must have the number of nodes that

you have features, right?

Hidden layers, arbitrary but it must matrix multiply together, right?

And then the last one must take all of the

kind of outputs from that final hidden layer and squish

it down to the size you want, the size of

your target, OK, so that's gonna be context dependent.

For us, that could be.

A single node, right, for regressional binary classification, but it

could be a multi node output as well.

Yeah, go for a ring.

Um, I guess I'm just finding it hard to understanding

the process of uh having a node and an input

layer connected to both nodes.

Mm.

And in the case of the equation that is the

last class, it may make sense, but here, um, doesn't

that uh Lead to multilinearity, like, are you considering that

these two variables or features?

Or not.

I think you need to think about it as.

So the big kind of conceptual difference between this and

a regression model is not just that one is a

straight line and one is a wiggly line.

It's more that like the parameters are just kind of.

Ways of adjusting the line.

In a, in a deep neural network.

Let me just put my um.

This on so I can stop it from going off.

Yeah, here we go, cool.

Um, so in your.

In your regression line, like multilinearity is a problem, because

if you have two variables that are um um just

a a linear combination of one another, right, then the,

the model can't distinguish like if I hold one constant,

how does the other change?

Because they only change together, right?

They move like this.

So they're basically one variable.

Um, that just doesn't happen in neural networks because you're

not, you don't really care what those parameters are, you're

not trying to estimate, well how does feature X3.

Input impact Y.

So there's no need to hold them constant per se.

What you just have is a big like dashboard of

um toggles that you can rotate independently, and those toggles

are just kind of in like they have data flowing

into them and you either amplify or you dampen what

comes out of them, and you've just got so many

of these things that you can make that line do

this rather than just like a straight line.

And the idea is that you can kind of.

Basically kind of um exploit any correlation between variables because

that correlation may be suggestive of kind of like mediating

factors or or.

Or even if they're not, even if it's just like

I've measured GDP in terms of like actual output, and

I've measured GDP in terms of how people are spending,

essentially, right, that um it just learns that it only

needs to focus on one of them, right, or it

takes half of each or or something like that, right,

these are just kind of things that it does implicitly.

Yeah.

The more connections, the reason you have a fully connected

one, right, everything going to everything, is it just gives

you more toggles.

So you can make more kind of finer adjustments, right?

That, that is the reason you want fully connected.

It stops you from having to say, well, I think

GDP needs to interact with X2 and therefore I need

this specific connection.

If it doesn't need to do that to learn the

relationship, it's just gonna set that weight to zero.

I say the choice of weights is or deter like

between you and here or do I choose uh one

variable to have more weight than them?

Uh, yeah, I mean, it's gonna be, it's not gonna

know what those features are, right?

And we're not going to, um, we're gonna try and

make it so that the model just looks at all

of them and considers what it needs to do.

If you don't scale your data, for example, you can

implicitly guide the model towards doing some things because if

you give GDP in the it's raw dollar form, right,

there's a massive number, so you can really change things

by just even small weights.

So we might scale it down to it as the

same range as, I don't know, like your binary indicator

of terrain ruggedness or whatever.

Yeah.

OK, so Turns out in this data set, we actually,

this is not, not the biggest dataset, it only has

22 features, OK, so that our input layer size is

fixed, uh by that uh data.

OK, and uh what I'm gonna do in this model

is um set K, which is going to be the

number of hidden nodes uh 264.

OK, so we're actually going to expand this data, so

we're gonna have quite a few parameters that we can

toggle uh for a relatively low number of features in

a deep learning, uh, context.

OK.

Uh, so we've got 22 coming in, our hidden layer

has 64, so that first kind of wetrix weight matrix

is gonna be 22 by 64.

Then we're gonna have 64 passing 64, so we've got

64 by 64, and we're just kind of, this is

a regression problem, we are just trying to predict the

quantity uh of conflict in a given area.

So this last one is gonna be 64 by 1.

So we're gonna take the outputs of the uh penultimate

hidden layer of which there are 64.

Do one final weighted sum of all of those outputs,

right, do one final activation and that's gonna give us

our singular output, uh, for any given observation passed through

our data set, OK?

So.

22 features, two hidden layers of 64 nodes, each one

output layer, that actually gives you lots and lots of

panels on that toggles on that dashboard, right?

5,568 weight parameters and 129 biassed terms.

So, you know, this is clearly much more complex than

a linear regression that has 22 terms, right, even if

you were to kind of do some additional kind of

like modelling here, you might increase that to 45 or

44 or whatever, still way less than the number of

kind of.

Effective beat parameters that you have in this neural network,

uh, deep, deep implementation.

OK.

Awesome.

Any questions before we go on to activation functions?

So now what we have is the number of nodes.

Uh, but remember, kind of, uh, we want to kind

of allow this model to learn kind of non-linear or

complex relationships.

Uh, and for the sake of kind of like the

ease of the maths, the last two weeks we've just

been assuming that that activation function is the identity function,

it basically does, uh, nothing.

And, but there's a problem with this, which is that

an arbitrarily deep or wide neural network, if it just

uses the identity function, will never ever return you anything

but a straight line.

OK, uh, you need nonlinear uh activation functions in order

for your model to be able to predict, uh, or

make nonlinear, uh, predictions.

Now normally it's just kind of left there, you just

kind of say that, right, you need nonlinear activation functions,

uh, and then we move on to describe what they

can be.

But I think it's actually worth just kind of very

briefly looking at the maths since we spent some time

learning uh what goes on in these things.

So let's just suppose we have um two features of

an input, right, X1 and X2, uh, and suppose that

we have, uh, just only two, nodes.

Right, although this is an arbitrary dimension here, 2 nodes,

uh.

In our kind of hidden layers, so WH, and then

our final layer is going to collapse this down to

a single node, right?

It's got one row here, so it's just gonna be

a single number.

Let's assume that there are no bias terms.

The bias isn't going to do anything that just shifts

it up and down, right?

It's just a scale of value.

And let's assume that our activation function across all of

these is just the identity function so we can drop

it.

So what does wine actually look like?

Well, we're gonna take our X data, right, and multiply

it first by the values of the weights in our

first hidden layer, then multiply the result of that by

the values of our weights in our second hidden layer,

and then finally collapse that down to a single node

in this nth layer here.

This is our output node at the end.

If we actually do the maths, first looks like this,

right?

This is the dot product, remember, so we're going to

do the first row of WH OK, multiplied by the

first column of X.

So we're gonna get W11, X1 plus W12, X2, then

the second row here, so this is W21, X1, W22,

X2.

OK, here's that bit.

We can repeat that whole process, right?

So now we've got W11 for the second hidden layer

times W11 for the first hidden layer times by X1.

So on, so forth, do it for the last one,

you get this thing here, if you simplify this down

and just note that uh.

These are all scalar values, right?

W1 in the nth dimension, uh, W11 in the second

in the 1st, etc.

etc.

right?

If we just squish them down and say they're numbers,

A, B, C and D.

Notice that actually this whole thing simplifies to just A

plus C, X1 + B + D, X2.

This is just a linear regression, right?

So you've just done a really complicated linear regression that

you could have solved with just your Y equals beta

1 X1 plus B22X2 model, right?

So this is not non-linear.

OK?

And notice also that in this output, although this is

fully connected and feed forward, it's not modelling any interactions,

at least not explicitly, right?

X1 is not being multiplied by X2 here.

So you need to kind of.

Have these nonlinearities, in other words, uh, to, uh, get

this to run?

How much time have we got?

OK, let me show you this very quickly.

So we're gonna come back to this.

I put this together, um, very quickly this morning.

You will learn how to do exactly this, uh, next,

uh, week.

Um, this is just a very simple neural network, which

basically just does what we have on the board.

So, uh, this is gonna have a very slightly different

structure.

It's gonna have, uh, two hidden layers.

Uh, each with 5 notes.

OK, and then that's gonna output to a uh final

output node with just a single uh node, so this

is gonna be your kind of classic prediction.

OK, this is how you technically define it um er

feeding forward, right X just gets passed through these layers.

Uh, and then we instantiate it.

Now we're gonna learn the sine wave, OK, or we're

gonna try to learn the sine wave.

The sine wave looks like, well for you guys, this,

right, at, uh, uh, towards infinity.

OK.

Um, so I'm just gonna create some random, uh, let

me run all of this for you.

I'm gonna uh just take 100 random values for X

and then make them have corresponding sign.

Functions right by just calling the sign function on that

X.

I'm gonna train my model, don't worry about any of

this, we're gonna do this in the class, right?

This is just a 100 epochs of training, let me

generate some test data.

Print the loss, but then most importantly, this is what

I want to show you.

If we plot the true data against our predictions, right,

this is your very complicated neural network model that's just

out to you in your aggression line, OK?

So this is with the identity function, i.e., nothing's happening,

OK.

So even with a really cool little deep learning, and

this genuinely is deep learning, you have just generated this,

um.

Uh, straight line, these are the orange points here.

What we want is something that looks like the blue

line.

OK, so we're gonna come back to this in a

second and we're gonna change it to see how adding

non-linearity allows us to, uh, predict the outcome, uh, better.

OK, so Long story short, Even though this is kind

of like high variance, it has lots of parameters, right,

it's not nonlinear, and as it's not nonlinear, uh, it

it's not going to do a great job in those

instances where we said it was going to do a

better job than than any other modelling strategy.

OK.

So To wrap this bit up then I'll let you

have a 5 minute break.

We're never ever gonna like pre-specify interactions or important variables

or anything like this, right, this is still an inductive

approach, the model is supposed to learn when it should

like approximate these things and when it shouldn't, like this

is the whole kind of like statistical learning.

Uh, approach, uh, that we're gonna rely on.

Um, and so I want to bring you your attention

back to this kind of theorem, right from that we

discussed in week one, which is kind of the universal

approximation theorem that says, given an arbitrarily deep or wide

neural network and some restriction over the data generating processes,

um, a neural network should be able to learn that

function, irrespective of what it looks like.

To an arbitrary degree of uh specificity, right, like it

should be able to get as close as you want

it to get to the actual uh data generating process

if you just give it enough kind of nodes, either

in terms of depth uh or width.

But crucially, because what we actually have here, right, is

just a set of like weighting.

In combined values, it doesn't have a way to actually

kind of output you Y equals sin of X, right?

That's just not possible.

There there there's nothing in this.

We're not gonna add any other peculiarities like give it

trigonometry, right?

It's only ever going to be weighting things, adding them

together, weighting things, adding things together, right?

Uh, so it's only ever going to approximate these values

and it's only ever going to approximate the functional forms,

right?

So when you ask a deep learning network to learn

the sign function.

It is not going to learn trigonometry per se.

What it's gonna learn is a way of waiting and

adding terms together such that it looks like a sine

function, right?

This is the kind of approximation aspect of the universal

approximation, uh, theorem, right?

It's gonna try and get us a representation of white

with sin X just given a kind of a set

of weight matrices that's gonna get us as close as

we, we want to looking like.

Assign function, uh, uh, or whatever.

OK, if we want it to look even more like

that function, we just need to, uh, give it more

training data, more neurons or more hidden layers, right, that

is the UAT universal approximation theorem, at its, um.

Cool, OK.

Alrighty, so I think that's a good place to stop.

So why don't you go have 5 minutes now, we'll

come back and start at exactly 10 o'clock, and I'll

walk you through ways that we can add nonlinearity to

the model.

Oh, it's all right, don't worry.

You wanna go on.

I gotta get my, um, um, if you like to

do one I have, if you go.

It's like 70s that's just, yeah, yeah.

I I I I I don't.

I feel like this section where like for like.

That's my.

Oh and it actually like.

Yeah.

Ellie Ellie.

So someone died.

OK, yeah, that's that's that's yeah, but he's right, yeah,

if you have with your friends.

I'm like no I don't think I'll try so I

just think it is.

It's gonna be my challenge.

Is it unreasonable to buy climbing shoes from um.

Yeah, but like here like.

And I was like, Oh wow, actually it's very important

that's uh.

So we saw that just using the identity function essentially

means that we're not going to get what we want,

which is a uh network capable of learning nonlinearities, OK.

So the way that you introduce nonlinearity into your network

is simply by changing that activation function to one that

is itself uh nonlinear, OK, and typically these are applied

to the hidden layers of the network.

So you can think about guaranteeing some ability to learn

something nonlinear by changing activation functions of the hidden layers,

OK?

You'll change the activation function of the.

Output layer differently and that's to kind of range match

your outcome which we'll talk about in a minute, OK?

But specifically so that we can get nonlinearity in our

model, uh, we are going to change the activation layers

of the, or the functions, sorry, of the hidden layers.

So there are basically two very common non nonlinear activation

functions to use, OK, in your hidden layers.

The first is the hyperbolic tangent function or tan H,

uh, which is this uh blue graph here.

So notice that X spans from, well, technically minus infinity

to positive infinity, right?

Uh, and basically there's this, uh, changing gradient, right, that's

why it's nonlinear, uh, that, that limits it in the

Y action, so this is tan H of X on

the Y, between -1 and 1, OK?

When you have very large numbers.

Very large numbers, you're gonna have um.

A tan H of X, which is basically one, very

small numbers, i large negative numbers, you have something that's

very close to -1, but in this kind of range

of, say, -2 to 2, you have this kind of

varying kind of bit where it's gonna be somewhere between

the two.

Notice that at 0, tanH is of X's at 0.

OK, so we preserve that function.

The alternative is something called uh the rectified linear unit

or relo relo relou, all of those are fine, right?

The idea here is that if X is greater than

0, just return X.

So for positive numbers, relu is just the identity function,

but for negative numbers, you set it to zero.

OK, so you have this flat region.

OK, and then once X is greater than 0, Relo

of X is just equal to X.

Both of these serve to add non-linearity to your uh

model.

So let's just very briefly look at what these look

like, OK, so, uh, here is the definition of the

hyperbolic tangent function, tan H of X, OK.

It's uh E to the power of 2X minus 1

divided by E to the 2 to the power of

2 X.

plus 1, OK, this guarantees that range between -1 and

1.

It has this nice smooth transition between the two.

TanH was the kind of like.

One of the original activation functions for hidden layers, right,

this was very popular in the kind of like 1980s,

1990s, early 2000s, um, but it has this problem of

vanishing gradients that we're gonna come back to.

But basically, notice that actually when X is not even

that small or not even that big, you're basically always

at 1 or minus 1, right, it has these flat

regions.

Um, and what that means is that your gradients are

gonna be really small at those points, right?

Which means that when you perform gradient descent, you're gonna

be making really tiny adjustments.

Uh, and actually what you can find is that these

adjustments are so small that your model basically doesn't train.

If it gets caught in those regions of basically zero

gradients, it's not performing any updates and so the model

just kind of like doesn't really uh do anything, OK?

We'll come back to that in a bit.

The really important thing about the tanH function, right, is

it has a known derivative.

So you know that you can backpropagate anything where you

know the partial derivative of that function, right?

Uh, and in this case it's just 1 minus uh

tanH2d, right?

That is the derivative of the tanH function.

So sigma here is just tannage uh of.

Given that we know that derivative, we can always calculate

it and therefore we can always uh propagate the loss.

So jobs are good and this is something that we

can include in our model.

OK.

Right, let me just very quickly code on the fly

and show you.

How that affects our models.

So remember this was really bad, this was just drawing

a straight line.

Now what I'm gonna do is just activate all of

my um uh outputs from my layers, right?

FC1, FC2 and FC3 here are just layers uh with

that tan H function.

Well, I could have done that way more efficiently, there

we go.

Torch.tanH OK, come over this side and close them off,

and actually, I'm not gonna activate, hm, I'm not gonna

activate the last one because this is the output layer,

right?

So I'm only gonna activate the two.

Hidden layers, FC1 and FC2.

OK.

Now if I call this whole thing again, let me

just close out of.

Pipe up before it freaks out, so if I redefine

my model.

Instantiate it, again, take a random draw.

We'll do all of this stuff in the seminar, run

it, and then just plot the results.

OK, so now all I'm doing is saying that like

in those hidden layers, as it's shuttling through this very

small deep network, right, you must, if that number is

really small or really big, you must set it either

to -1 or 1, and then it has this kind

of curvy bit in the middle.

The last layer, however, can just still vary however you

like, right?

I'm hoping is, right, that we're getting predictions now that

are non-linear, OK?

So it's not perfect, right, sin comes back up here

towards zero, right, and it's, it's kind of not learned

that bit of it, and again it's kind of missed

the inflexion, uh, point here.

But you can clearly see that that's a much better

representation for the vast majority of the data, and that's

simply because we added that nonlinearity into the capability of

the network, right?

So I have not constrained the outcome.

The outcome can be any number between negative infinity and

positive infinity, but just by changing that activation function in

the hidden layers, OK, we're getting much better predictions, um,

and non-linear predictions, more specifically, uh, as a result.

OK?

So that is the benefit of um this nonlinear function.

OK.

But I said, Tanics was kind of old school, a

bit old hat, it has this Spanish ingredients problem.

It's, it's not, not the best thing to use these

days.

Most people, when training deep neural networks, uh, would use

the Re function.

OK, so here is a function definition, if X is

greater than 0, then just assign X.

If X is less than or equal to 0, then

assign 0.

OK, so this is just a modified identity function, but

it's nonlinear because of this kind of capping of values

uh at or or putting a bottom threshold that the

value of the output cannot be less than zero, it

makes it all non-positive.

OK.

This is empirically just proven to work a lot better

than tanH most of the time, OK, so you get

better problems here and you don't have this problem of

vanishing gradients.

And now, if you think about what the derivative of

this function looks like, you basically have two regions.

You have the bit where X is, is at 0

or greater, or greater than 0 and then anything that's

less than 0.

So, uh, at X is greater than 0, because you've

just got Y equals X, the derivative is just equal

to 1, simple enough.

Uh, if it's less than 0, right, the derivative is

0, it's a flat line, OK.

The only tricky thing about this is that at 0

exactly, there's a plausibly infinite number of derivatives you can

take because that's a sharp point, right, so I can

pivot along that point.

All of those are acceptable, uh, derivatives, so you have

to make some kind of shorthand choice.

It turns out that, um, Pytorch, uh, for example, just

sets the gradient equal to exactly 0, at that point.

Um, you could take a more fancy approach like calculating

the subgradient, but it's just not worth it, OK?

Um, this is also a problem with lasso models, uh,

which we'll come to if you're doing MY 474.

But here, just assume that if you did just so

happen to have an output that was exactly equal to

or an X that was exactly equal to zero, right,

that it's going to assume the gradient there is is

zero, OK, even though plausibly it could be a whole

set of um values.

Any questions on Reddit?

Different way of doing the same thing, which is just

introducing nonlinearity.

Why is the absolute value thing that we're doing here?

Why, why does that work for non-linearity?

I'm a bit confused.

It's just adding this kink.

That, I mean, that's why it's nonlinear, right, because, I

mean that's, that's all it is.

I mean, it's kind of mad that just by preventing

values from not being less than zero, right, that that's

sufficient, but it actually is, um.

And you just don't have this kind of, um, creeping

kind of, you get very high and, and, and therefore

it just gets very close to one, yeah.

Um, comparing the models, I'd say the, uh, function has

that benefit of being nonlinear.

So, and we sort of lose that with the relo.

So why would we want to combine those two, to

say that everything that is negative is zero and everything

else is nonlinear.

Because, I mean, assuming the identity function is quite a

strict assumption.

I, so I, um, yeah, so, so absolutely nothing stops

you from doing that, right?

We could define the reptan H function, right, which would

be like if X is greater than 0, we'll then

apply tan H, otherwise, um, assign zero, right, that would

give you both.

That would certainly stop vanishing gradients in the lower part

of the thing.

It would still be nonlinear, so that's absolutely fine.

I think you'd still find that because as X increases,

your gradient is, is dropping off to near zero, that

you would still find gra uh vanishing gradients in, in

that region and so I'm not sure you'll get much

more benefit.

The nice thing about Relo is that after this harsh,

harsh cap at zero, right, you allow X to continue

and the gradient is constant at that point.

So there's, there's no kind of um.

Uh There's no diminishing in the power of the update

based on your value of X, essentially.

That is the the power of the update during um

that propagation.

So hidden, hidden activation, hidden activation functions typically use Tan

H or Relu.

In practise use Relu, right, that this is, that would

just be the kind of general advice.

Don't pivot away from Relu unless uh you really, really

want it.

If we were to very quickly just change this.

Uh, to Relu.

I mean, it might not work as well in this

case because we do have such a parametric, um, data

generating process.

Uh, sorry, you can see my, uh, podcast there.

Uh, don't judge me.

Um, let's rerun it now with, uh, Relu and said,

I reckon this won't work as well, only because Sinex

is so continuous and also it's bound between -1 and

1, so tanH is just like the natural one to

use.

But if I train it for exactly the same time,

oh.

What is it?

Is it in neural network?

Yes, there you go.

OK, let me run this again.

Oh, OK, I'm not gonna be able to do this

now, I don't want to waste your time.

Um, I, let me run it and then I'll put

it in a, a blog post, uh, on the, the

Moodle page, but I reckon that won't work as well

as TanH in this case.

But I think that would be a rare case where

it doesn't work.

And it's specifically because we've got such a nice mathematical

function that's related to Tan.

OK, so hidden layers.

No activation functions on the input.

Use value for the hidden.

Now we get to the outcome, and this is a

bit like do you do logistic or OLS regression, right?

It depends on what your uh target or your output

is.

So we just want to make sure that the model

is basically outputting in the same range as the outcome.

Now with enough data and enough time, you might hope

that the neural network learns to constrain itself, right, and

it's, that's certainly possible.

It should.

Learn, you know, like if you have a, a non,

uh, negative outcome, never to really spit out negative numbers

or at least those negative numbers should be really close

to zero.

but you can just kind of help it along by

just like limiting it, right?

So if you know that, uh, it can only be

between 0 and 1, we'll give it an activation function

that means it will only ever be between 0 and

1.

So if you're predicting a continuous variable, and a truly

continuous variable that that ranges between negative infinity and uh

infinity, that final activation function.

Should probably be the identity function, right?

Just, just let it, let it be.

If you have a binary outcome, then we want the

output to lie between 0 and 1.

We want it to output the probability that you belong

to the positive class.

And indeed if you have a multi-class outcome.

Where you have C notes at the end, what you

want is for the outputs to sum to 1, that

is that the probability of getting one of those classes

is one, and it's broken up into individual probabilities of

each class, right?

So we change the output function accordingly.

For regression, you'll use the identity function.

For binary classification, you'll use the sigmoid function, which we've

come across before, right, 10 1 + E to minus

X.

Uh, and then for multi class classification, you're gonna use

something called the softmax function, which is actually.

It's very easy.

It just makes the output of any node proportional to

the sum of outputs of all of the nodes, right?

So it's always going to constrain it to be uh

equal to one across the nodes of the output.

Don't worry about multi-class classification, we'll do that in some

depth, uh, next week.

Again, these um output activation functions have um known derivatives

and very simple forms, right, the derivative of the identity

function is one.

OK.

The sigmoid function.

Again, it is a little bit more uh involved, but

actually it has a known derivative that we can just

kind of like save as a property of sigmoid activation

functions in our kind of um er neural network class

in Python, right?

So here, uh, it's just 1/1 + E to the

minus X, and the derivative is the sigmoid function multiplied

by 1 minus the sigmoid function.

Actually pretty handy cos we're gonna get most of that

information from the forward part, right, we're gonna know what

that is and just be able to like multiply it

back.

And that's it.

I won't show you the soft max until um next

week, OK?

So crucially, these are all things that are very simple

to calculate that completely integrate with everything that we did

in weeks 1 and 2 mathematically.

So where are we?

So what we've done now is we've defined a network

structure, right, 22 inputs, two hidden layers of 64 nodes,

and a final output node with just a single kind

of um.

Uh, set of weights.

Uh, we've made it deep, Rex, we've got multiple hidden

layers, and then we define these neuron activations.

So in our case, our outcome is continuous.

Um, it's, it's technically greater than 0, right, so we

could technically maybe use Relu and just stop it from

ever predicting like.

Like negative violence, uh, right, which probably in this case

would mean like birthing children as opposed to killing soldiers,

um, or civilians.

But, um, uh, let's just keep it as the identity

function so that we don't complicate things.

And we'll use value, uh, for our, um, activation functions.

We still can't train a network because uh for each

of those 64 nodes in our hidden layers, right, and

in the output node uh as well, we need to

set some initial parameter values so that we can actually

forward propagate the data and then kickstart this back propagation

process.

And we also need to define um both what a

good observation would look like and how we tell the

model that, and then how we optimise on the basis

of that, OK?

And then we actually need to do the hard bit

of training the model.

So Let's go through each of those three things in

turn.

As I said last week, initialization is this process for

determining what the initial values of W and B uh

should be, and for convenience, we've been setting those to

equal exactly um zero.

But once you start getting into deep networks with lots

of parameters.

Um, and especially if your kind of data generating process

is a little bit complex, um, setting all the weights

equal to zero can actually be really problematic because basically

what you have is a network where everything is equal

to 0.

You 4 propagate your data, now you can do that

because you can do 0 times anything, right?

Let's suppose we have some loss, which we'll define in

a little bit.

When I then start to back propagate and I'm thinking,

well, how is my, how is this specific weight value

in this specific node in this specific layer, kind of

like impacting the amount of loss we have, well, it's

the same as every other node across all of these

5200 and whatever nodes we have in the network.

And so the model gets a little bit kind of

freaked by it, and it's like, well.

Like, what do I change?

Like, because if I change it here to that, I'm

gonna have to change it here to that as well,

and that's gonna be cause it to be very rigid

and, and it will break down.

This is kind of like the neural network equivalent of

multicolinearity where like you can't disentangle the two variables from

varying and so it freaks out.

A neural network freaks out when it sees lots of

zeros and it's just like.

Which one should I be changing, right?

So what we want to do is basically stay around

0, OK, because that is our best initial guess, but

just have minor changes across these parameters at the beginning.

So that if it notices that W11, right, is, has

a, a, a slightly positive gradient, right, then it can

just reduce that one ever so slightly, and if the

other one has a slightly negative gradient, there's just some

distinction, right, between these nodes that is random, right, but

it exists and therefore it's able to start to tweak

things and as soon as you start to get it

on the right track, it will kind of cascade and

and move towards that um uh minimization of, of the

loss that we want.

If you watch the Carpathy lectures, he makes a mistake,

uh, which, as any good teacher does, turns it into

like a learning point, right?

Which is that he initialises it with zero, doesn't say

anything about it, and then, uh, he's like, and look,

we get to the right answer, and indeed he does.

And that's because he has a very specific and just

linear data generating process that that makes this all work.

So the problem is, is that in testing, if you're

not kind of a little bit rigorous and think about

kind of like testing a couple of data generating process

with a few peculiarities.

You can find that this actually doesn't matter and so

then you could scale this up, run this on your

kind of like.

Straight version of the internet and they'd be like why

is it not working now, is it the data that's

wrong when actually it's just like you miss the fact

that you created the symmetry in your network in the

first place, OK, um.

What actually happens is it it causes kind of like

exploding or vanishing gradients because everything blows up or everything

shrinks down um and er then the model just can't

recover, like you've pushed it too far away from um

the minimum uh for it to ever kind of like

get back uh towards it.

Uh, if you want kind of like more kind of

detail on this kind of problem, follow the link here.

I thought that was kind of like a really good,

uh, primer.

So, we need some principled way of making it roughly

equal to 0 but not zero, exactly.

The kind of, uh, initial proposal to do this is

something called Xavier, uh, initialization, right?

It, it doesn't say like we should set the, the

values to anything other than zero in expectation, right?

Like 0 is halfway along the number line.

That's as good a guess as any, but what it

wants to do is just introduce a little bit of

random noise, uh, to prevent symmetry.

And what they suggest is having the weights, uh, for

a layer being draws from a normal distribution.

Uh, with a mean of zero, so in expectation, the

weight's gonna be equal to zero, but it's gonna have

a standard, uh, well this is the variance.

Equal to 1 over the number of nodes in the

previous layer.

OK, so in our first hidden layer we have 64

nodes, but they are receiving inputs from 22 features, right?

So we are gonna set the weights in that hidden

layer initially to random draws from a normal distribution with

a mean of zero, but a variance of 1/22.

OK?

So notice that the more features you have, the uh

tighter that normal distribution is gonna be, right, in the

first hidden layer, or the more nodes you have in

the first hidden layer, the tighter the distribution of uh

initialization will be in the second hidden layer and and

so forth.

OK.

What this is?

Yeah, great, so the reason they say, and you should

look at this primer for like.

Even more detail, but the reason they say is that

this guarantees that the uh variance of the activations across

the layers remains the same, OK, so it's just kind

of like keeping the keeping the network orderly, right, is

how you should think about that, um.

And then I'm gonna finish the dot, then I'll come

to you.

Um, and Xavier initialization is like, like has theoretical properties,

i.e. it keeps the variances the same when you're using

either tanH or sigmoid activation functions.

OK, so if you have an old school network using

tanH, uh, hyperbolic tangent, uh, activation functions in your hidden

layers, then use.

your initialization because that will ensure that the variance of

these activations will be uh the same across uh layers.

Now I'll answer your question.

So that is a normal distribution that if I get

that correctly.

Yes, absolutely.

So basically all the ways we assume that the normally

we assume that the weights themselves are normally distributed, right?

So if I were to take the mean of all

of the weights in that layer, I would expect it

to equal 0.

OK, and I would expect the, the, the dispersion of

the weights to be equal to 1 over the number

of nodes in the previous layer.

I mean in linear regressions we just assume that these

are just, this is a calculation, right?

In our this is like our norm 0 and then

uh 1 over, you know, 22, right?

Um, this is just a calculation.

OK, so Xavier initialization, uh, this is for tanH or

sigmoid, but as I said, we're cool kids, so we

use Relu now.

Um, uh, and so this has been, uh, fundamentally altered,

right?

Uh, so we use totally different and absolutely not related

initialization function called the he or Kai Ming, uh, initialization.

Um, and I kid you not, the only difference is

that variance term is now 2 over the number of,

uh, um, uh, nodes in your previous layer, OK?

So this is apparently, theoretically guaranteed to ensure that the

variances of the activations remain the same across layers when

using Renu, OK, um.

In practise, Uh, um, you actually don't have to worry

about this anymore.

Like 55 years ago, when poor Tom was writing his

first deep learning software, you had to like specifically decide

how to initialise everything.

This is basically solved, OK, and PyTorch will just do

it for you, right?

So it will just be like, cool, you've given me

the network structure, I see you're using Ralu, um, I

will just use he initialization, OK, and you don't have

to specifically do that.

But you should remember that under the hood, what it's

doing is just randomly distributing those initial weight values so

that they have a mean of zero, but some dispersion

to account for the fact that uh you don't want

these vanishing or shrinking gradients.

Is there, do we always use the same activation function

in the middle layers, or can we switch?

I mean, there's nothing to stop you switching.

I don't think you'll find much, if any, of a

performance improvement, um, but there's absolutely nothing to stop you.

These are just derivatives.

We, we know the derivatives, it can be back propagated,

it doesn't really matter.

Um, I can't think of a a solution off the

top of my head where you would do that.

I think most people just default to Relu now, but,

but yeah, it's totally possible.

No, no, well.

I don't know, that's an empirical question I, I expect,

yeah, yeah.

So basically Pyearch will examine in some way the input

data and choose the right institution function or a default

it will take.

Uh, I think it just by default does kinding initialization.

I don't think it actually is as smart as looking

across it and setting the initialization based on the activation

functions you're using.

Uh, I could be wrong, but my, my instinct would

be like.

One is just like tiny bless you.

One is plus the dispersion of the other, so long

as it's not exactly equal to zero, things are gonna

be fine.

There are much bigger fish to fry when it comes

to tuning your network.

So it will just default to heat, but I can

check, I mean, I can see if it's searching.

I'm thinking that something is wrong with the final neural

network and if the reason could be.

I think it's so unlikely that the reason is uh

the choice of initialization process that I wouldn't even bother

checking it, right?

It's much more likely to be, and we'll talk about

this at the very end, much more likely to be

your data or some misconfigured set of parameters, um, or

the need to add some quirky things that we'll discuss

in a bit, right, the chance that this digitalization process

is so tiny that I just wouldn't worry about it.

I think the way you might check that is by

like maybe resampling your data and just rerunning the model

a couple of times and, and seeing if like it

works for some but not for others, that would suggest

that.

That maybe would help you suggest that this well or

actually even using the same data but just randomly varying

the seed and running for a very small number of

epochs, because then the only thing that would change uh

initialization would be the actual draw from the random normal

distributions.

So that might give some hint, but I don't, I

don't think anyone has ever done that.

Like uh it's never gonna be the initialization that's the

problem if you have a problem.

OK.

OK.

A quick function, a quick side on loss function.

So this is the next bit.

So we now know how to initialise the network.

Technically, you can punt the data forward at this point,

but we want to be able to also send the

loss backwards, right?

We want to do that propagation.

Uh, so remember that your loss function is just a

measure of how well the model is performing.

It's also determined by you.

OK.

Uh, typically we think of the loss as being the

difference between the model's prediction and the true value, but

depending on whether you're predicting.

Probabilities or continuous outcomes or multi-class outcomes, we're gonna have

to use slightly different uh loss uh functions, OK.

So typically, and you can vary this, right, then you

can try use binary cross-entropy to calculate regression loss, but

you'll see it goes totally wrong.

Um, we'll use mean square error, OK, our trusty mean

square error when doing regression.

We'll pair sigmoid and activation functions in the output layer

with a binary cross-entropy measure of loss.

OK.

And when we have multiclas classification, we're just going to

generalise that DCE binary cross-entropy, uh, to what's called kind

of a categorical cross-entropy or just cross-entropy, um, more, uh,

generally.

OK.

Crucially, and you can actually see that in my code.

I define my loss or my criterion, right?

Before I train my model, right?

It is just a function that I'm gonna call uh

all times, bless you.

OK.

So just consult this slide if you ever get confused,

but means spread error, continuous outcome, binary cross-entropy for uh

for like binary classification, categorical cross-entropy elsewhere, unless you're doing

something really fancy like generative modelling, but we'll talk about

that in the next couple of weeks.

OK?

We have to define it at the beginning because it

gets tacked onto that computation graph, right?

It is the first calculation we have to make.

So given we have our neural network plus the activations

plus the loss function, we have a complete computation graph.

Uh so now to actually train it, right, to do

the back propagation, we have to decide how we're going

to optimise our network.

OK, so we could just use stochastic gradient descent.

For simple networks, that's probably totally fine.

Um, but if you wanted to use something that kind

of has a little bit more oomph to it, um,

you would probably choose something like Adam, uh, or weighted

Adam.

I might see if I can add a slide or

two on these, uh, towards the end of the lecture,

seeing as I keep, uh, mentioning them.

Uh, we're also gonna have to determine something like the

learning rate, right, so how much of an update we

do, uh, at each, um, uh, step.

And indeed you, you still need a learning rate for

things like Adam or, um, Arop or any other kind

of optimisation algorithm you choose.

And you're also gonna have to decide how many times

to loop through the data, right?

The example I had here had 100 examples and I

looped through the data, I think 100 times, right?

And that was sufficient to get a pretty good uh

model fit.

Now these hyper parameters will be totally context dependent, right?

More complex data will probably need more epochs to learn

from it, more hyper, more nodes in your network probably

means you need to trainer for longer, etc.

Um, the only kind of rough rule of thumb I

can give you is that in terms of the learning

rate, that is normally set somewhere between 0.001 and 0.01.

Sometimes something even higher like 0.1 can be totally fine,

but often that's gonna be too coarse.

OK, that's just gonna cause too many jumps and you're

just gonna shuttle.

Um, uh, but, but normally, and if you watch people

like Andre Carpathy talk about it, they'll say the same

thing, which is between.

Uh, 1000 and 100, uh, of, of a full gradient

update is, is kind of, uh, what you should be

aiming for.

Yeah, great question.

I know you'd, you'd have to set the batch separately.

So the batch size is set separately to um the

uh optimisation algorithm.

Um, and indeed, we will have to do this in

the, um, seminar.

Uh, normally when you're batching data in Pytorch, you use

something called a data loader, which is a way that

is like a little wrapper over your data, which not

only, um, chunks it up randomly into the batches with

a batch size, um, but also converts them all to

tensers and, and, and makes it work, um, uh, properly.

So you do have to define it, but not in

the, not in the optimizer.

You'll see I don't define it in my optimizer here.

um, I just set the learning rate, um.

But you will have to define it in the data

loader.

Yeah.

OK, and then here's the, uh, here's the punchline.

You do all of that and it's still not gonna

work, right?

Uh, it's just, it's just not, I promise you.

On anything but Y equals sin of X where you

give it pure Y equals sin x data, it is,

um, uh, going to throw a wobbly and not be,

uh, particularly, uh, friendly towards you, right?

And this is the reason why ML engineers get paid

a little bit more money than they otherwise would.

So there's a few problems that we need to overcome,

OK?

The first problem is overfitting.

OK?

So overfitting technically is when the model learns the noise

in the training data.

What I mean is it starts to see that as

more than just noise and connects the dots, right, so

you actually just have dispersion, right, there's just measurement error

or whatever.

Uh, but it's like, oh no, there's structure to that,

and so it starts to build these kind of like

astrology maps, um, and it gets, becomes very good at

learning your training data and too good, so that when

you actually apply it to generalised data where that noise

actually doesn't, you know, have anything more than just kind

of random dispersion.

Uh, it freaks out and gives you like very, very

poor, uh, predictions.

OK.

Now the reason this is such a problem for neural

networks, right, is because even in our quite simple deep

learning model, we have 5200 and something parameters to turn,

right?

So it's very easy.

It's easy for it to start connecting the dots, right?

Uh, often you're gonna have maybe more parameters than you

have data points, right?

So it's gonna be actually incredibly easy for it to,

um, overfit, uh, and just, uh, connect, uh, the dots,

OK?

So the first thing you should always do when training

in a deep neural network, right, is hold out some

validation data, OK?

This isn't your final test data that you use to

kind of sell your model to uh a company wanting

your expertise, right?

And it's not your kind of future data, namely, tell

me if there's gonna be, or how much conflict there's

gonna be.

Uh, in the United Kingdom next year, right?

This is some of your training data that you have

the labels for, but which you don't actually use for

training, just so that you can assess, uh, whether or

not the model has started to overfit, and the way

that you do that is by looking at the loss

is declining across both your training set and your validation

set.

Um, approaches like cross validation already in the training?

Yeah, so you, you're probably gonna want to um use

cross validation to set lots of those hyperparametters.

Uh, for example, the learning rate is a crucial one,

maybe also the size of the network, the number of

hidden layers, uh, so on, uh, and so forth.

We can talk about cross validation in this week's MI

474 lecture if you're also taking that.

When we see that uh is the laws declining across

both training and validation, then what are we actually doing

like once we train them all on the training.

I'll be applying it once to the validation set or

retaining it.

Yeah, I know, so I think this, this, and I'll

show you a graph of this in a little bit,

but the, the, the basic.

You're gonna use cross validation for things like your number

of nodes, your um learning rate, etc.

because they're kind of like fixed at the beginning, um,

and once you've set them, you've set them, right?

Um, but for the number of epochs in particular, so

like how many times are we gonna repeat through this

data, um, that is something that you could basically say,

well, I've seen it do 200 and I think it's

overfit, but at 100 epochs it looked pretty good.

So what you can kind of basically do is plot

the loss across both training and validation sets.

Watch them decline, and they should both decline, and at

a certain point, the training loss will continue to decline,

but your validation loss will increase.

So what you could do is stop, look at that

point where they cross and retrain the model with that

number of epochs.

What you can also do is take checkpoints, right, so

save the model.

Um, at various stages and then just revert back to

the save before they overlap, right?

That's uh something that you can do.

Yeah.

Yes, that's early stopping, right?

Or you can, you can actually, um.

I mean you can do that in multiple ways, so

you can say uh look at the training loss, or

look at the validation loss rate, and uh when that

stops declining, stop.

um um.

Yeah, I mean, I think that's actually the, the basic

way of doing it, but yes, these are, these are

all kind of like early stopping criteria, um, or just

when the like the, the absolute value of the, the

of the change in, um, loss is, is like less

than 1 to the power of minus 5 or something.

So is it that after every epoch we take the

parameter values which have which have been.

Uh, from the training set and pass it on to

the valuation for every book we are like fitting it,

and then, yeah, exactly, but obviously that validation data would

not be used to update the parameters, right?

You wouldn't do a back propagation step having seen the

validation data you can hold it out, yeah.

So that question here, no, OK.

And is it, uh, when it comes to overfitting, can

we also use a big strategy to train a couple

of different models on uh parts of our data set

and then somehow combine weights.

Huh Um, I don't know.

That's a good question.

Um.

Like major walls or something.

Oh, like, uh, uh, so like, yes, um, so you

can have these deep ensembles that would work, so like

a a majority vote would work, uh, if you had

like a binary classification task, right, and you'd ask each

of these models.

Um, deep ensembles normally work by.

Wow.

The naive approach, naive, it's not naive, would be to

run separate, totally separate models, like bootstrap your data or

subset your data randomly or something, and then take a

majority vote.

I think most people there would argue like, why not

just chuck it into one big model, because then it

can learn there's more information.

There is a risk of overfitting for sure.

The slightly more, I think the way that most deep

ensembles work is using something called dropout.

Which basically implicitly trains multiple models within a model and

then rather than turning up, we're gonna talk about this

in like two slices of time, but rather than turning

it off at test time, uh, you keep it on,

you take multiple draws and then you take the vote,

you take a majority vote, um, either a hard or

soft vote on, on those multiple draws.

Um, yeah, I look at deep onboards would be the

kind of like, um, where you apply this.

OK, so this is what it looks like, this is

what I'm talking about.

Um, you'll see that we have um our both our

training and our validation loss here.

So training is blue, validation is orange.

Initially, in these first few epochs, and this is the

um um uh conflict data.

OK, what you see is that they both decline.

Now this is noisy because this is actually mini batch

grading percent that I've implemented here, right?

So it jumps, you know, it goes down and it

goes up and it goes down and it goes up

and down and goes, but you see it's converging.

Now at this point here, right, the um.

We've basically exhausted.

The performance of the model on the validation data.

So this is that little bit of data that I

hold out from training that is never used to update

its parameters, so it basically hasn't seen it.

OK.

And at this point it kind of bobbles along for

a bit, you know, this is the grace period, right,

where you've gone and made your cup of coffee, you

haven't stopped it yet, but then you start to see

that like.

It's rapidly increasing, and that's because here, this is technically

still sloping down, but the minor improvements that you're getting

in your training loss here is because it's starting to

connect the dots of the noise, right?

So it's learned the main signal by about this point,

and this is just learning noise.

Learning noise is a bad thing because that noise is

not gonna look the same as the validation or indeed

your test data, and so that starts to rapidly increase

your, um, uh, validation loss almost back to a kind

of early levels like after two weeks of training.

Yeah.

This is my question kind of about the universal approximation

theory where like if there if there's always this kind

of this irreducible error right that's the distance even at

the minimum point between the validation and training loss like

what.

This is a universal approximation.

Does that mean you can just get it to a

point theoretically where it's just like infinites and small that

irreducible error?

Um, no.

Well, yes, if you, if you start to say the

noise is part of the, the, the noise in the

data is actually part of the moral, yes, you could

arbitrarily reduce it to zero loss, right, but that wouldn't

be very good, but I think the kind of universe

approximation theorem.

Would, would really be about the systematic component of the

equation, right?

So technically you can get it arbitrarily good at learning

the bit that we should be able to predict, right?

So just so that what is left is the random

noise, right?

Um, that's, I think what you're aiming for.

Yes, it can overfit and and and thus treat your

noise as actually systematic.

So it's, it's referring to training plus in that case.

Uh, Yeah, exactly, yes, the training was, yeah, it's gonna,

it's gonna perform terribly on out of sample validation, um,

yeah, yeah, yeah, yeah.

Uh, you wouldn't be able to reduce the irreducible noise.

Yeah.

I guess I'm just struggling with what the difference between

validation and like the test data is like what do

you then do differently because it looks like it's.

Yeah.

So the way I, the way I think about it

is, so you should see the same thing on the

test data if you were to train so multiple box,

right?

But the um.

The difference, OK, is.

I'm actually using the validation data in my training, so

imagine I've done cross validation to set all my hyper

parameters, now I'm just thinking about how long should I

train this for.

I'm gonna look at this and I'm gonna say, well,

it stopped the validation loss.

Stops decreasing around the 50th epoch, OK.

So technically, even though the model has never actually like

updated its parameters based on that data, I have used

it to train my model because I've used it to

to determine when to stop.

OK.

So I really shouldn't use this validation loss.

As the loss I report to someone else about how

good my model is, because I've deliberately chosen the model

such that it minimises the validation loss.

I need another set of loss, my test loss.

Right, where that's like, I didn't use this for any

part of the training procedure, including determining when to stop.

OK, so if it's performing well here, it's simply because

my model is good, right?

It's the test loss that I'm basically gonna sell to

um my customer, right?

The kind of um big tech company or or whoever

who wants to buy my model.

Right, the validation loss is just useful for me as

a way of saying, kind of, um, how much should

I be training my model or kind of like what

do these hyper parameters look like, yeah.

So follow up to that is like I know a

lot of times like data limitations kind of dictate how

you do model building.

So like, how much data do you need in order

to be able to have like training, validation, and I,

I, I mean that's, that's the short answer.

The long answer is it depends on how complex the

underlying.

Data is, if you've got Y or sin of x,

you need 100 data points to get a pretty good

approximation, right?

Um, but realistically you need quite a lot of data,

you should not be running deep neural networks on 50

observations or even 500 observations.

I think you're more thinking 10,000.

Yeah.

Yeah.

And so just to make sure, so the validation loss

or the validation set is basically only used to determine

the optimal number of epochs that we're going to run

the algorithm on.

Essentially because everything else you can use cross validation for.

And then after we've determined that number of epochs, we

then run the algorithm on a completely new testing.

Exactly, yeah, yeah, yeah, yeah.

This is the kind of pristine isolated the validation is

the uh the test data, yes, for sure, if you

haven't done this step, right?

But now I've used it for the terminology in some

books is like a training test and validation for final

report.

Uh, OK, I don't like that.

I think test data is like the siloed.

Not seen until you're writing the final report.

The validation data I see is a little bit of

a holdout of the training data, as a subset of

the training data.

But yeah, yeah, yeah.

OK, right.

I do want to get through a few more slides

because er I think there is a lecture after us.

OK.

So the first thing we can do is just kind

of control some of those high parameters, right, tune the

learning rate.

Um, Uh, and, and, and do some kind of like

early shopping routine using some validation data.

Um, another thing you can do is just make the

job of the neural network a little bit harder, right?

Uh, and one particular way that you can do this

and this, this would eventually lead you to deep ensembles,

um, is to use something called dropout, OK?

So dropouts are essentially like very thin layers that exist

between your hidden layers.

Uh, and what they do is they're, they're like gates.

So they're either randomly open or shut, according to some

probability P, OK.

Uh, and they basically prevent some of the, um, outputs

from nodes being sent onwards, OK, or more mathematically they

just set them equal to zero.

Right, and what this basically means is, even if that

node is like learning and training, there's some probability P

that it's not gonna be able to pass that information

on to the next layer of um um nodes.

And so you kind of implicitly forcing the model to

be resilient to this kind of like random distortion, right?

So by saying there's no guarantee that the nodes in

H11 will actually be fed through.

You're gonna have to start to kind of think about

ways where the H12 through H164, right, can pick up

some of that slack, right, so that if H11 isn't

allowed to pass the data through because the dropout has

shut the gate, um, it can still do it, right?

So what happens is, um.

You can basically think of like your neural network with

dropouts, where nodes are randomly turned off during training, as

being like a, a big collection of different models.

All of those models are slightly thinner, they have fewer

nodes because some of them being dropped out.

And so the idea is that there's gonna be multiple

representations, right, of the same data generating process.

And that's just gonna make the job of the model

a bit harder.

And so it shouldn't overfit uh as quickly and in

fact it should be more resilient because it's having to

learn multiple different ways of describing the relationship between X

and Y.

OK?

Yes.

Does that not, don't you need the same like matrix

alignment?

If you are having some go, does that not?

You do, but just imagine that you just time some

of them by zero, so you have a weight, a

weight, you've got your weight matrix.

Now imagine a second dropout matrix of the same dimensions,

but the dropout matrix just has uh ones and zeros.

That's, that's how you can imagine it, yeah.

P is a probability of each node.

Yeah, and it's uniform across all of the nodes, right?

It's not like W11 has a P of 0.3 and

W12 has a P of 0.9.

It's the same probability for all nodes uh in the

layer.

Now, typically, and this gets us away from deep samples,

you just turn that off at test time.

So when you're actually evaluating your model, you, you turn

the dropout off so every node is firing again, OK?

So you've like let it like learn all these thin

networks and then you let the.

Entire network loose uh on um uh the network.

Now you actually have to remember to do this manually

when you code this up, OK, so you're gonna have

a um a method called .eval in your Pytorch class.

When you call that method, it turns Dropout off along

with a few other things.

Um, but then as I was writing these slides, I

was thinking, well, that makes absolutely no sense at all,

because like if each thins network, right, every possible permutation

of, of dropout in that network is trying to predict

the full outcome, right, cos we're still backpropagating the loss

against Y.

When you turn it all on, surely like it's just

gonna like, it's like fry, right, because it's just everything

is firing and that's gonna be too big.

Uh, and it turns out that is um actually a

problem.

So what happens is you kind of scale the uh

weights at test time to shrink their values.

So if W11 is equal to kind of 5 during

training, right, that's where you're randomly turning it and others

on and off, you're gonna make it like 4 at

test to account for the fact that some more nodes

are gonna be firing at the same time.

So basically, the way you get around this um issue

and again, this is not something you have to worry

about, this is just for interest if you're thinking like

how does that actually still give you the outcome, it's

because you basically scale down the size of these weights

so that the outcome should still match what you're measuring

it against, which are the labels in your training data.

So that's why the probability is the same, right, exactly,

yeah, yeah, yeah, yeah.

Right, we've already talked about this, uh, a little bit.

Uh, we have this problem of vanishing gradients.

Here's the same.

So you saw the tan H and the Relu graphs,

um, before I didn't show you sigmoid, uh, and we

just had like X against tan H of X.

Now I'm showing you uh X against the derivative of

tan H and indeed the derivative of the sigmoid function.

OK.

And notice just that these derivatives quite quickly.

Get to 0, right, and at 0 you're basically not

gonna be able to update your model, this is the

vanishing gradients problem.

And apologies for going quickly, but I do want to

get through these last few slides if I can.

So when X is actually not even that big, like,

around 5, the derivatives of these activation functions are near

zero, and that means that when you backpropagate those uh

derivative values, you're gonna be doing like 0 times anything

which is gonna be 0, which is basically gonna mean

your model isn't going to improve because you're not able

to make an adjustment, right, your step is, is essentially

gonna be um equal to 0.

So The way we get around this is not by

using Dropout, right, cos dropout isn't gonna stop the fact

that uh you have uh essentially kind of like um

these kind of zero regions in your derivative function.

um.

You're either going to have to change your activation function,

so use one that's less prone to vanishing gradients, right,

that's why we switched to Relu, uh, in the, the

2010s.

Although Relu is not perfect, right, you can, you can

have like dead Relu nodes where basically the outputs going

into it are always less than zero, and so it's

firing zero and then you actually have basically the same

problem, but it's much less likely to happen.

Um, or because we basically have good functions in this

region here, right, where the derivative is, is non-zero, what

if we were able to kind of like squish the

outputs so that when you kind of pass them through

the derivative, they're in that region and therefore we're getting

like.

Formative, um, values, uh, for the gradients.

OK, so if we change the distribution of the hidden

activated outputs such that they lie in that region of

this graph where the derivative is non-zero, well then we

can actually start to improve the model again, right, because

we're gonna be doing meaningful steps.

There are basically two ways of doing this, OK?

They're called batch normalisation and uh layer normalisation.

Now this looks very complicated, um, and I'm going to

leave most of the detail for you to just kind

of stare at in your own time, and I'll show

you a graph that or a figure that I think

makes more sense.

But the idea with batch normalisation is for every mini

batch in your data and for every feature, you um

normalise.

Those vectors such that they have mean zero and standard

deviation uh equal to 1, OK?

That kind of basically means that X is always roughly

gonna be in this area.

OK, so you're always gonna have non-zero gradients, OK, or

it's gonna be much harder for it to be outside

that area, uh, and so it's gonna be much harder

for there to be kind of this problem of, um,

vanishing gradients.

There is some um rough logic why you need this

kind of like little epsilon here.

I wouldn't worry too much about it.

There are also some more learnable parameters if you do

this.

OK, so these are just ways of scaling and biassing

the normalised bit itself.

You just need that so the model still has some

flexibility and it's gonna basically normalise it and then.

Denormalize it a little tiny bit, just, just to allow

it to kind of like move as it needs to

move.

OK, um, but that works in the kind of feature

dimension, so every feature in your batch gets normalised.

Uh, this dramatically improves the stability of training.

Um, it basically avoids Spanish ingredient problems in all but

the most complex of data.

Um, and it avoids kind of like the, um, batches

themselves, like.

Overly influencing the training, so even though they're random batches,

there's still gonna be like differences between each of those

batches in terms of the distributions.

That can mean that can like judder the model in

terms of its training, right, as it's kind of like

switching across these random shifts.

If they all have mean zero standard deviation equal to

1, well then all the batches look roughly the same,

so they should have roughly the same input and smoother

uh convergence towards the loss minimum.

Um, and, and apparently also it reduces the kind of

need for correct initialization, right?

This is just kind of like, um, um, even further

means that we shouldn't really worry about changing Pytorch defaults,

OK?

Because there's less risk of overfitting, cos there's less risk

of kind of um vanishing gradients, there's less need for

dropouts, um, and also quite nicely, uh it allows for

slightly higher learning rates.

OK, so you can make bigger adjustments after seeing each

mini batch of the data, therefore, hopefully you can train

for less time and use less compute.

You might be thinking, OK, if we're normalising the hidden

outputs, should we not also normalise our input data, and

absolutely you should, you should always really scale your input

data before it goes into the network, just so that

you start the model off, uh, in a good place.

Um, this is just a standard step, right?

Use something like SKLE scalers to squish your data down

before you um pass it through.

The alternative is layer normalisation.

Uh now instead of here going across the kind of

features of the batches, what you do is you take

the outputs of a layer of your neural network, so

for a single observation.

You pass it through the layer, you have that hidden

output, essentially, and then you normalise those values so that

they have mean zero standard deviation equal to 1.

OK, so for every observation and for every hidden layer,

if you use layer normalisation, you take those 64 outputs,

for example, OK?

And then you, you divide through by the uh mean,

right, and then you, you nor so you normalise um

and then you set the standard deviation equal to one.

Uh, uh, and so you now have kind of, um,

again normalised data that's, that's more stable, but this time

in the kind of like, um, layer dimension rather than

the batch, uh, dimension.

Everything else is quite similar.

Uh, you also have these learnable parameters, you have this

minor little perturbation here.

Um, the nice thing here is that layer normalisation removes

dependency on the batch, right, so the, the actual normalising,

um.

Uh, quantities, so that you get mean zero and standard

deviation equals equal to 1 are not gonna change as

you have different, uh, batches.

Layer normalisation is great when working with things like text.

I'm quite proud of this diagram, did it myself, um.

This is basically how it works.

You have data flowing in through your um uh your

network, rotate it 90 degrees to get your kind of

conventional thing, but this just helps you see what's going

on.

So here are the outputs from my notes, so.

This is a batch, right, there's 3 observations, OK, and

there are 4 features or 4 outputs, and mobile network.

If I take the entire kind of output of that

layer, right, and then calculate the mean and and variants

so that I can normalise it.

OK, then I'm doing layer normalisation.

If I take each kind of feature or output, right,

so I've just got that column, so the outputs for

that batch for this yellow node.

And then I calculate the same, so the mean and

the variants so that I can do normalisation, that's batch

normalisation.

So it's just the dimension along which you do the

normalisation.

The principle is the same, right?

Get it so that they have a mean of zero

and a standard deviation of 1.

That means that the model is, has a much more

predictable set of inputs, right, and should be much more

stable.

OK, I'm gonna get some real evils in a second,

but just a couple more slides.

It's really not easy to train your networks, OK?

So just remember to be good social scientists when you're

doing that, right?

We're, we're typically not just gonna be like maximising meta's

revenue.

We are going to be thinking about kind of like

having good models that help inform good decisions by policymakers,

etc.

etc.

So as you're doing these kind of trial and error

type things, you need to kind of document, uh, what

you're doing and use principles like cross validation to make

those choices robust.

OK, and then I'll leave this for you here, but

this is kind of the checklist of things I would

always ask when my model looks really bad, right?

I've got really high loss.

Uh, it could be that the neural network is finicky,

but it also could be you don't have enough data,

or maybe you're not measuring the right things, or maybe,

uh, you just don't need it, right?

Maybe a linear aggression is all you need.

OK?

So just bear those things in mind.

Next week we'll go on to like generative models, thinking

about things like encoders, autoencoders, missing data inputation, etc.

All right, lovely to see you, uh, see you in

a week's time.

Mm.

I.

probably like next week Friday or Thursday.

I can.

took a I I know that I yeah, I know

it's, yeah, I decided no, not more.

I'm I've done I've done this to myself.

and they came here for a week but then I

saw that you know I I I yeah I you

know I.

Yeah, of course, and it's like hard because we live

in London for a long time because I think that

we had this way this way of visits, especially in

the first year when we left here and now it's

like, OK, you've already been here yeah yeah.

Yeah Oh It.

I

---

Lecture 4:

In kind of major production settings, you would probably want

to.

Really kind of test the robustness of your model by

starting it from different points.

Um, but on the flip side, in production settings, if

the thing's making you money, even if it's not in

the global minima.

You'd probably, you're probably OK until your bosses get reliant,

right, so, um.

There isn't really much you can do in kind of

analytic settings, the, the, I think the place where this

comes up quite a lot is in some more advanced

um types of topic modelling.

And I think it's been shown that basically where you

start that optimisation process from can have quite significant effects

in terms of the, the topics topics it finds.

um, and so I think their advice is always, well,

change your random seat multiple times and see how robust

the topics are, because if you're finding that, you know,

they're not relatively consistent across those iterations, well then that

suggests that this.

It's a really kind of weird kind of shaped thing

that you're trying to minimise and you're just ending up

in different local minima.

um, but you know this is all, it's all very

nice when I show you parabolas, but the fact is

we never see the parabola, we don't really know what

that looks like.

When we're thinking about kind of.

And, and also the other major issue that we have

right is we've only really got an estimate of the

population assumption because we only have a finite amount of

data, um, so you're kind of, um, doubly handicapped, handicapped

because you don't really know what that looks like, um,

but also handicapped because there's no guarantee that the paraent

that you could draw from the date you've seen in

fact the ones that.

generalises other than tweaking your learning, right, so you have

more of a chance of diagnosis and less of a

chance.

Um, but it would be adaptive.

You would probably cross our learning rate or more generally

and then and see which one performs best, and you

have to assume that that's as close to the global

minimum you can get, yeah.

No stress.

OK, while my covers of my time like I did

not really like.

OK.

That's also great because you can go away with an

idea.

Of course I.

Yeah, this, this is gonna be painful.

Yeah, I have so many layers.

I went to the and I was like thinking something

cookie but like not too y.

I heard.

Yeah, I agree actually.

Yeah, I mean it could be worse.

what's here, um, I didn't know I'll just, I can.

Oh really, is it yeah.

you.

Mhm.

turned out.

I.

I.

you know, it's like a little away from them.

See this is what you do.

My girlfriend and so I'm just like I just.

let's uh let's get going then.

So we got last week before, this is your penultimate

lecture with me before I have this course open in

free group.

Um, I have called this lecture Generative deep models, um,

and we will do generative deep models, however, um, more

generally this is going to be a little bit of

a medley of topics today that's going to be linked

by the idea that you don't necessarily.

Need one output node at the end of your network.

OK, so everything that we've looked at so far has

been either kind of a regression task, i.e., predict a

continuous um outcome, uh, or a classification task, i.e., is

this A or B, did they vote for the Democrats

or Republicans, questions like that.

Um, but actually, kind of the real power of neural

networks is that this kind of structure that we started

talking about last week is incredibly flexible, uh, and the

one thing that we didn't consider is, well, what if

we allowed ourselves to have more than one output code

and what would that be, uh, useful for?

So essentially there are going to be kind of 4

kind of, um, quarters to to today's um uh lecture.

So we're gonna look at multi-class classification, which is the

most obvious, uh, generalisation of what we've done so far.

So what if we're not just predicting.

Uh, whether you're a Republican or a Democrat, 1 or

0, what if we were predicting, um, I don't know,

your occupational class or, uh, your favourite pets or something

like that, where there are plausibly many, um, categories, uh,

and indeed, obviously the kind of final extension of a

multi-class classification problem is text, right, where you have a

dictionary of words potentially or tokens, uh, potentially kind of

thousands of classes in length, uh, and you're trying to

identify which is the, uh, most likely next, uh, token.

Then, given that we kind of have the structure sorted,

i.e. multiple output nodes, uh, we'll think about what exactly

generative modelling is and why it differs from what we've

done before.

And rather than kind of dive straight into generative modelling,

I'm gonna try to take you on kind of like

a, a charted history of the development of this area

through essentially kind of three modelling types.

So we're gonna start with what's called an autoencoder, OK,

and we'll think about why this isn't generative.

And then I'll show you.

Kind of why autoencoders are not, um, particularly well suited

to empirical data, um, via kind of a a discussion

on denoising, and we'll eventually get to generative modelling, uh,

by one further extensions, uh, which takes us to VAEs

or variational or coders, which are really cool, and, uh,

are kind of incredibly useful for generative modelling.

And then lastly, and this is, this is the nice

bit for me, um, this is my area of research,

right, so um I have uh work in exactly this

area applying how we use generative modelling to, uh, things

like missing data, uh, and multiplication.

So I will walk you through, um, how I implemented

some of this stuff, uh, in a social science setting,

um, and kind of compare it against non-de approaches so

that you can kind of see the power, uh, of,

of what we're trying to learn, uh, to do in

this course.

Does anyone have any questions before we begin?

Logistically, I will try get out your formative uh this

week so that you have plenty of time to do

it.

It won't be particularly onerous and I know that lots

of you doing MY 474, so recognise that you're gonna

have to balance those two assignments slightly.

Um, but I will try to get it out this

week and then you'll have, I think, 3 or maybe

3 and a bit weeks to, to complete it.

So if you have any questions once I've done that,

just shoot me a message on uh the forum.

Alrighty then, let's start with multiclass er classification.

So, as I said, everything so far, and much like

what you would do in a machine learning course, right,

we've considered two types of problem either what what uh

computer scientists like to call a regression problem, i.e., you

have a single continuous outcome, or a binary classification problem.

Uh, but often, especially in the social sciences, we're gonna

have far more than two categories that we want to

predict over.

So for example, at the moment, I'm working on some

um research that tries to uh automate.

The coding of one's occupations.

Now this turns out to be surprising.

I didn't realise this when I started an incredibly difficult

thing to do, and humans are really bad at it.

Um, so you might say, uh, well, what's your job

title?

And I'd say, um, uh, I'm a, I'm an assistant

professor.

Actually, that's, that's a pretty easy example.

Let's, uh, let's say pizza chef.

OK, this is the one that we've been toying around

with at the moment.

So I say I'm a pizza chef.

Um, and then you ask a pretty expert PhD graduate

that's looking to earn a little bit of extra money

to, to choose from one of these categories.

So are they a manager, director, professional occupation, associate professional,

etc.

etc.

all the way down to elementary occupations?

Now the problem that we're finding with Pizza chef is

that there's basically two types of pizza chef, right?

There is, uh, the kind of artisan, Italian style pizza

chef who has probably lovingly raised this dough for the

last 3 or so days with natural yeast, blah blah

blah blah blah, the kind of thing that I would

eat.

And then, uh, you have the kind of Domino's conveyor

belt type pizza chef who receives a pre-rolled, uh, base.

Bastes a little bit of sauce on it and pop

some pre-cut vegetables on top.

Now those are two different types of occupation.

Uh, pizza chef is in, uh, category 5, and, uh,

uh, I think they're called like a culinary technician, uh,

tends to be in, uh, group 8.

Um, and so actually predicting which one you are based

on basically just your job title, uh, is pretty complicated,

uh, and something where we might need, uh, deep, uh,

neural networks.

And in particular there we're using these big generative large

language models to kind of basically, um, adaptively probe someone

so that when they say I'm a pizza chef, it

says, OK, do you work in, uh, in Domino's or,

or do you own your own kind of teeny tiny

sourdough pizza joint, uh, things like that.

There are 9 categories here, essentially, that was a long-winded

way of saying there are 9, not 2 categories, um,

and therefore our kind of conventional 10 approach isn't going

to, uh, work on this type of data.

OK.

I should also say that this is like from social

science perspective, really important, right?

If you don't know where people are working and the

distribution of workers that you have, you can't make effective

policy, and this is exactly what we're finding out at

the moment if you're interested.

Look at the uh what we call the labour force

survey, which is a hugely important survey that has just

died a death since the pandemic.

People don't respond, the data is really bad.

We don't know whether the GDP is going up or

down.

We don't know whether you're employed or not employed, uh,

and that means that every policy decision is a stab

in the dark, essentially, uh, which is quite scary.

OK, so we need to kind of convert our data,

right, there is no kind of natural way of uh

predicting professional from managerial uh using just kind of those

labels.

We have to encode that data, in other words, back

to numbers, right?

Um, and I'm sure that many of you in here

are familiar with this idea of one hot encoding, right?

This is essentially what we've been doing already with Democrats

and Republicans or kind of uh at civil war or

not at civil war, right, where we convert one class

to one and the other to zero.

But when we have more than 2 classes.

What we basically do is create a matrix where there's

um or a vector per observation, right, where there's a

uh an element for each possible category.

And then we say if you're in the first category,

well then that first element is a 1, everything else

is a zero.

If you're in the second category, the second element is

a 1, everything else is a 0.

Right, so your outcome, say I'm trying to predict that

you're a manager, right, is um 8.

Or 9, can't remember now, uh, elements, right, but only

the first element is a 1, everything else, uh, is

a, um, 0, OK.

So this is interesting from, uh, two perspectives, right?

One is that this kind of a nice way of

us being able to think about, well, um, well outside

of the binary class, but I think the other interesting

perspective here is right, we actually have like multiple output

elements now, right?

Uh, it's not just kind of give me one single

number, I need the model to return, essentially 8 or

9 numbers.

What is it, 89, there you go.

9 numbers, right, that give me the probabilities over each

of those classes.

OK, so we have some dependents still, but we have

this kind of um multi output, multi-element output, uh, for

each observation, right?

This here isn't 9 observations, this is one observation where

we have one hot encoded.

The um categories slightly different from.

Uh, like, um, multinomial logistic regression, right?

Or indeed any dummy variable in any regression, uh, there's,

we don't drop the reference category, right?

There's no need.

We don't care about the multilinearity issues here.

That's just not gonna cause us a problem.

So 9 categories, 9 elements.

We can't have a single node, right, a single node

is only gonna output a single number.

Uh, per observation, if you think about shunting all that

data through.

So instead, and intuitively what we're just gonna do is

instead of having uh one node, we're gonna have K

nodes where K is the number of uh classes.

OK, I'll show you a, a visualisation of this in

a little bit.

Um, and what I'm gonna do is I'm gonna refer

to those kind of pre-activated outputs.

So imagine you get all the way to the end

of your network.

But you haven't quite applied an activation function yet, right,

those numbers that you would get before applying that activation

function, I'm gonna call that the log jit.

OK, um, and they're low jits because they're, they're a

little bit like the kind of linear space, uh, in

a kind of logistic regression model before you've applied, uh,

the sigmoid function to transform them back to probabilities, OK?

But essentially, you know, big numbers, big positive numbers are

gonna mean high likelihood, big negative numbers are gonna mean

very low likelihood and things around zero and we're gonna

be unsure about.

OK?

So that model should, in this case, and ignoring activation

function for a second.

Give me big positive values at the elements of that

output vector where it thinks that they're kind of highly

likely to be in that category.

So if you are in fact an artisan pizza chef,

the number for the 5th node should be very high,

and everything else should be very negative, right?

It's highly unlikely to be anything else.

OK, so they're all gonna be uh large and negative,

but you're highly likely to be an artisan chef, uh,

and therefore, uh, that one should be higher.

Now, if you have a slightly less well performing model,

um, and this is gonna sound very weird for anyone

that turned up after my pizza chef story, uh, but

if you, uh, are unsure, if the model's unsure whether

you're an artisan chef, i.e. 5, or a, uh, technician,

i.e., 8, right, then what we should see is kind

of like middling values for those two elements, but it

can probably still say, well, you're definitely not like a

CEO, so that should be a large, uh, negative, uh,

value.

So we're kind of like following along so far, we've

just literally added K minus 1 extra node at the

end of our network and we want to search for

the biggest number.

OK, so this is kind of what it's gonna look

like, feed our data through our network.

Uh, that can have kind of hidden layers exactly like

here.

And then at the end if we have 9 classes,

well then we should have 9 output, uh, values.

Now, in a very small network like this, it's actually

very easy for us and machine to work out which

is the right class, right, we just kind of scroll

along.

Through these, uh, predicted numbers for a given observation and

and find the largest one.

So in this case it's 5.3.

Everything else is either small or indeed very negative.

Uh, and that is probably the most likely thing, uh,

for that person to be given what we've been able

to model.

And so we would say, well, you're in.

Class 4, whatever uh class 4 is.

Yeah, go for it.

Yeah, so what comes from the order of the class

in the like which not refer to which class?

Yeah, so, um.

I mean in my diagram you can just assume that

this is class 1 through to class K.

and when we um.

Let me show you the loss function in a couple

of slides time and that it will make sense.

Um, but you just need a distribution of numbers here

and you're going to assume some order so that you

can um.

It's essentially arbitrary because your loss function is basically gonna

force it to as human an order.

Um, but I'll, I'll show you that like, um, in

the equation in a second.

So we can see, we can see which is the

largest here, right, that's, that's 5.3, but this number is

a little bit um uninformative, right?

And indeed, um.

We can't really use just these logits as the final

output of our model because gradient descent really needs us

to be working in probabilities.

OK, if you don't kind of convert these back to

probabilities, um, you're gonna find that gradient descent is gonna

really struggle if not, if not fail, right?

Um, so we need to transform these.

Back to probability space.

OK, but we can't use sigmoid functions, right, we couldn't

just apply sigmoid function to each of these activation layers.

Because that would treat each of these separately, right?

And so what you would have is a sum of

probabilities that doesn't necessarily equal to one, which is a

problem, because if I am an artisan pizza pizza chef,

right, I am not a culinary assistant, nor am I

a CEO right?

Think about.

These one hot encodings for a second, right, these are

my probabilities.

When I observe that you're a CEO, you are definitely,

but you're definitely not any of the other categories, right,

so those vectors.

Sum to one, OK, we need exactly the same thing

here.

If we think there's a high likelihood that you're in

class 5, you, you should have a low likelihood of

being in, uh, class 9.

There's a dependency here, right?

Uh, and so the more certain we are about one

category, the more certain we are not, uh, in the

others.

The sigmoid activation function applied to each of these things

separately would not guarantee that.

So instead we use this powerbo here, which is called

the softmax activation function, OK?

So what you do is you take the logit for

a given uh.

Um, Class, OK, and then you divide it through by

the sum over all loadeds.

OK.

So this is just a proportion.

Right, this is a fancy proportion.

If you ignore the the exponents, you're just doing, well,

what is the low jit for a specific class and

divide it through by the sum of all of those

load jits.

OK?

The only weird thing that we do here is we

exponentiate it to introduce a little bit of nonlinearity, OK?

But that's quite nice, right, so here, it's not just

kind of.

This is the largest number, but it's this kind of

the probability is this divided by the sum of all

of these together, and the exponent there will obviously strip

out any of the um uh negatives as well, right?

Hence there's no need for like a square or an

absolute sign, uh, here, OK.

Really nicely because we're gonna iterate over each eye, each

output node, right?

That means that because this is summed over the same

number each time.

We are gonna get A vector that sums to 1.

OK, we're not gonna get 100,000, we're gonna get like

0.8, 0.001, 0.003, something like that, but if you sum

all of those, given that we've calculated the proportion, right,

we are going to have something that sums to one

in general, all of them will be between 0 and

1, and so we can treat each.

Activated node as the probability of belonging to that class,

yeah.

Uh sorry, because it's got a negative number.

Um, no, because I think if I do the exponent

of, say, -5, I get a very small number and

very small positive number, right, minus 100 is.

Pardon?

I, I didn't see the E.

Ah, OK, yes, yeah, yeah, yeah, that's it, so it's

um.

That's why we don't need to worry about the negative

numbers here, right, negative numbers just indicate very low probabilities,

right?

Uh minus 0.00 should be around, yeah, like that.

OK.

Cool.

As ever, to back propagate, we just need to know

the partial derivative, right?

We can just pass it along.

So anytime we have this in our, uh, computation graph,

right, we just need to know what the derivative is.

Here is said derivative, OK, so it is the softmaxx

by 1 minus soft max for the last class, otherwise

it's just minus the soft max times uh uh the

softmax for the others.

OK.

All happy here.

So that's the activation.

This just makes, like, adds that dependency across the output

nodes.

Here's what it looks like.

So we take our logits, pass them through that soft

max, and what we get is a set of numbers,

right, where again, this is still the largest number, this

is still the most probable class, but now it's within

the probability space.

That makes a lot more sense, right?

We, we expect that 4 out of 5 times, uh,

it would be in this class, uh, in other words,

everything else is uh very small.

OK.

Right, and now I can get to your kind of

question about like what, what about the order, does it,

does it matter?

Um.

The way that we're going to back propagate the loss

is, is to look at this cross-entropy uh loss function,

OK, uh, which is um very similar to the binary

cross-entropy, right?

Uh, but whereas in the binary cross entropy I basically

had just a literal sum, right?

It's like Yi times the log of Y hat, plus

1 minus Y1 times the log of 1 minus Y

hat, right?

If I've got K categories, I would have to chain

lots of those together, so it's easier just to write

it like this with this uh summation operator.

Uh, so it's just a negative.

Summing over all of the different categories K of the

one hot encoded value for that category, times by the

log of our predicted uh um uh probability uh for

that category.

Now notice that because this is one encoded, these true

values Y K are either gonna be 1 or 0.

So if you actually think about iterating through this, you're

just gonna, you're gonna kind of punish it, uh, only

when you get to the right category, cos otherwise this

is gonna be zero.

So if they are a manager, the first one, this

is 1 to 1 times then you take the log

of that, this is going to be bigger, uh, the

smaller the probability is, OK, um.

Otherwise this is serious, the whole thing cancels out and

basically um.

Your order of your outcome or your one coding will

just force the model to learn that the first node

in that output vector of the model, right, has to

correspond to the first class because otherwise it's gonna get

punished.

So it will just, it will sort it for you,

um, in the model, what just matters is the kind

of ordering in your one on one coding, right, that

has to be consistent.

Um, if you, if the model doesn't know that the

first node in the output vector is the first class,

but it gets punished as if it were the first.

So what if, for instance, we do not have in

the data set any um data points which are.

Which will have one for I don't know like 9

and 8th of February, but we are still interested in,

I don't know probability of other data points into this

category.

So from where we can we can you you simply

so if you haven't, OK, so what you're saying is.

Um, my, let me see if I understand what you're

saying.

Uh, uh, my occupational codings have 9 categories, but my

sample from the data only includes people from the 1st

8.

But we're interested in people that have elementary occupations.

That's category number 9, right?

Is that what you're saying?

So what happens then?

I do not have any and uh.

But still in the results, I will have for for

instance for the first uh data point, which is, I

know from the first category, I will, I will still

get probabilities for 8th and 9th category, but from where

I will get the knowledge which.

Uh yes, so I think a good.

Good model here, right, that's performing well would just assign

zero probabilities to both of those things, because there's just

no chance of seeing it in the data, so you

would never ever be rewarded for returning a large number.

Um, this is essentially, I mean, I think you could

do that, right?

So you could have a one hot invector which is.

One hot encoded vector which is longer than the actual

number of observed classes.

Actually there's there's the, the complication you had is you

said 2 not observed.

Yeah.

If you assume there's just one that's not observed, you

should just expect mode collapse, which is where it just

never ever predicts that category because according to the training

data, it doesn't exist, right, so there's nothing to stop

you including it.

But it wouldn't, it wouldn't know what to do.

Now in the case of two missing categories, you have

both that problem, namely you should never predict it, because

according to the training data, it never exists, but also

you wouldn't be able to distinguish between the two of

them.

So it would not be able to distinguish between the

two of them.

Yes, yes, there should be absolutely no reason why either

of them should have a non-zero probability, uh, for a

well-trained model.

OK, I see.

Um, but it certainly wouldn't be able to do and

it wouldn't make sense to have your data like that,

you know.

I I just don't, yes, particularly in the case we

have more than one missing category in your training data.

There is, there is absolutely nothing the model can learn.

Between them, right, so, so you would probably just collapse

it.

I mean you would probably strip both categories out if

I'm totally honest, um.

Because I think you know what the model should do

if it doesn't see the data, it predicts error.

Yes.

Can you just explain what these are?

Yeah, of course, can.

So, um, let, let's take this as pizza chef output,

right?

Um, so, and let's say they are Tom's favourite sourdough

pizza maker, right, so they're an artisan, um, pizza chef.

Uh, so this is the probability here.

So what happens is we start from the top and

we work our way down.

So our first, uh, K equal 1.

So the ZK here is 0.121.

So log of 0.121.

Is minus this number, OK, right, uh, but.

YK here is 0.

They're actually in our, so that becomes zero, so we

add nothing.

When we get down to um uh number 5, we

said that was 0.810 or something like that.

The log of that is only minus 0.09.

Right, so then we times that by one because the

corresponding.

Element in the output vector is one, and that gets,

that is basically the cross-entropy loss for that observation because

everything else will be zero, except we make that -1,

right?

The whole thing gets times by -1 at the end.

So the loss, cross-entropy loss for that observation is 0.009.

So it's kind of like rewarding with the higher probabilities.

Absolutely.

as this gets to one, right, it's never gonna be

exactly one, but as this gets to one, this gets

the cross-entropy loss gets very small, right?

And it's just looking at the category that is in

fact correct.

Like, it doesn't actually really like take any information about

like the distribution of probabilities for the incorrect categories, except

that.

They're related, so if the incorrect categories are higher in

sum, the correct category of probability has to be lower

for all to sum up, right?

So that information is encoded in the higher probability in

the correct class, essentially.

Any other questions before I keep going?

So to clarify, why is the predicted one binary value

or is the actual binary is the actual value is

the actual value at that position?

Are they, are they uh category 5 worker for example?

OK, so this is your cross entropy loss function, right,

this is how you're gonna assess how well the model

is doing when you have something like this, i.e., multiple

outputs at the end of your model, and they're linked,

right?

You can only be one of those occupation categories, you

cannot be in multiple of them.

OK, and once you have this, OK, and once you

have your softmax activation function with a known partial derivative.

We then you can just do graded descent just as

normal.

OK, so nothing else changes.

Unlikely to actually use gradient scent, you're more likely to

use something like Adam, and I put some slides in

at the end today to tell you exactly how Adam

works so we can get some intuition there.

All good.

Awesome.

OK, that's more class classification.

Really there's nothing more to it than that.

You just have to remember that you have a soft

maxed.

Activation function over K nodes, right?

Um, but you would, um, define that in a very

similar way in Pytorch, um, uh, which we're gonna do

in the seminars this afternoon.

So, uh, don't worry too much about it.

So the loss function changes, we need cross entropy rather

than binary cross-entropy or indeed we need squared error, um.

And we use Soft Max rather than say Sigmoid or

me or your identity function.

OK.

Now it's generalised further.

So I've, I've shown you that you can have an

arbitrary number of output nodes at the end, OK.

That arbitrary was kind of a little bit stricter with

multiclass classification because it equals K, right, the number of

categories you have.

Um, but there's really no need for, uh, uh, that

to be the case, right?

And actually there are many.

Kind of modelling tasks where actually we don't want those

output values to be uh dependent, right?

We might want one variable to kind of range between

0 and 100 and another to range between 0 and

1 because they're kind of multiple descriptors of the same

observation, we may be interested in them both, right?

Um, and the kind of beauty of the neural network.

Is that It is flexible enough to to kind of

do that very simply, like there's there's no kind of

like we have to relearn that propagation or uh anything

like that.

We just kind of set different activation functions and and

let it go wild, right, that that is essentially kind

of uh why neural networks are so kind of um

prevalent at the moment, is because they are simply so

flexible and can basically do whatever you want.

So.

Moving from multiclas classification to kind of more generative modelling,

we're gonna think about rather than like imposing dependence on

those output nodes, by having this softmax function, i.e. every,

every output is going to be, Some proportion of the

total output, right?

We're just gonna let them freely vary based on what

we would like, uh, to see.

Uh, and so what you should think of as our

outcome now is gonna be kind of a bold vector,

right?

Uh, and this is gonna be kind of the set

of, um, kind of ja descriptive elements that we want

and typically they're gonna have separate activation functions depending on

their independent ranges.

OK, so we're thinking that these kind of.

They may be correlated with each other, one's gender is

related to um one's propensity to go into higher education,

etc.

etc.

right?

But the kind of ranges of these variables are um

independent, uh, of each other.

And it's gonna be super useful when what we want

is something that's kind of multivariate in output.

So think of like an image.

Um, or multimodal, right, we may want some text and

an image, where we're gonna need nodes that can kind

of vary independently because your pixel values will be between

0 and 255.

Your kind of text values is going to be a

really massive soft max, uh, essentially.

And it's very useful when we want to generate data.

So maybe you want to kind of train a model

that takes in some real data, right, and then outputs

data that's like that real data but is in fact

fake, right?

That can be very useful if you're kind of like

benchmarking a method or you're working with sensitive data, and

we'll talk about kind of those types of applications uh

in a few slides of time.

So, um, let me kind of.

Give you a primer on kind of the difference between

what we've been doing so far, which we're gonna call

discriminative modelling, um, and, um, uh, generative modelling, the kind

of the thing that's very trendy at the moment, so.

In both cases we're gonna have a supervised task, right?

We're gonna have X, a matrix of descriptions about some

observation and Y, which is a set of labels or

targets, right?

Um, and what we've basically been doing so far is

modelling the probability of an outcome, Y, given the description

of that that instance X.

OK.

Now it's slightly kind of a little bit of a

pivot about modelling a probability distribution here.

But it's not too far fetched, right, when you did

kind of binary classification, for example, you know, you get

a predictive probability of 0.8, say, right, so there's a

0.8 probability that it's in one, and by virtue of

it being a binary problem, there's a 0.2 probability therefore

of it being in past zero.

So you have modelled the discrete probability distribution, right, it's

80% and 20%, we have.

The area under that curve is still one, right, it's

the sum of the probabilities of the individual, um, possible

outcomes, right, so it is a probability distribution, but crucially,

you notice here that Y is on the left hand

side of this conditional probability, right, so we're modelling Y.

Conditional on what X is.

So given a set of observational data, for example, predict

or classify those labels.

Why?

So an example of this is, you know, what are

the people's occupations, right, where these people's is the kind

of examples that we have, right, the descriptions of them

and the output or the kind of target that we're

looking for is the occupation itself.

OK.

If we flip X and Y over, we're not doing

discriminative modelling anymore, we're doing generative modelling.

So what is basically the probability.

Of that kind of description given some target, OK, so

um.

Before I get to this example, think about kind of

something with occupations, so, um.

What does a CEO look like, right, is a generative

task, because I've given it the label, CEO, so give

me some kind of probabilistic statement about what that would

look like, right, in terms of uh the kind of

descriptors that we have.

So probably high income, skews.

Male, probably, definitely, uh, uh, and, um, lives close to

the southeast of England, right, that's, that's kind of a

reasonably high probability that these things are gonna be the

case.

Given that you are a CEO, right, contrast that against

a, um, pizza chef in, in Class 8.

So what you're doing is the reverse, right, you're generating

the example of the description, give them some uh label

Y.

So this is often the case in kind of like

um non-tabular contexts, right?

I think this is where it's more obvious.

You log on to kind of chat GPT or Dali

and you say, uh, give me a a photo of

a lecture process, right, or um uh draw me a.

Disney cartoon version of my dog, right, those are the

kind of things that you can get these generative models

to do.

It's like, describe that.

Disney caricature X, given that I've told you it is

a Disney caricature.

Uh, so that's the difference.

So when you're modelling Y, it's discriminative, OK, when you're

modelling X, given Y, it's, uh, generative.

Uh, so, uh, why might we want to do generative

modelling?

Um, obviously there are the kind of fun cases, uh,

of kind of like having cute pictures of your dog,

but there's also, uh, kind of, I think some social

science applications too.

So, first one is kind of correcting errors in your

data, right, our measurements are noisy and sometimes error prone,

right?

Uh, the enumerator in a face to face survey might

may write down something wrong.

Um, or the data may be corrupted in a potentially

quite janky data science pipeline, right, where you think it's

going somewhere and it's not, or you're multiplying it by

the wrong number.

There may be missing data because people kind of don't

give responses or don't want to give responses, right?

And in those cases you may want to generate data

sets that are like the original data but with those

corrections removed.

That I think is a generative task.

Um.

The second one I've already kind of briefly mentioned, but

it's this idea of kind of creating synthetic data, i.e.

data that looks like it comes from the real world

but doesn't, um, to protect the privacy of people, right?

So, uh, this is very common in financial settings, right,

you may want to kind of train models, uh, or

get data scientists to train models that detect fraud, right,

from credit card activity, an obvious case.

um, but do I really want to be sending, or

do I, as the customer consent to have my.

Private credit card data sent to uh some um NSE

graduate, right, to work out whether or not there's there's

fraud in my account.

Uh that's not safe because you're going to see my

account details, uh you're going to see identifying information about

me, uh.

Uh, I'm probably not going to like that, right?

So lots of these companies are pursuing generative modelling techniques

to basically take in real credit card data and output

data that looks like credit card data and has exactly

the same kind of like distribution of features, this kind

of probability of X given Y.

But which is not identifying, right, it's differentially private.

There is no way of reversing it so that you

can say that that kind of um purchase of the

coffee was, was Tom's purchase of a, a coffee, right,

that's what we're kind of trying to avoid here.

So, uh, preserving privacy.

I've worked on data with um.

Um, school children, right, so the UK, uh, despite having

a terrible labour force survey, has very, kind of like

individual level data on children's progress through schools, that is

accessible, but I have to sit in this horrendous box

room in the LSE library to be able to access

it on like a Windows 98 computer.

Right?

It it's horrible.

Um, um.

Synthetic data is a way of kind of like allowing

me to not sit in that room.

If I can train models to learn what that looks

like, right, then I can go away and work on

data that has the same qualities but will not reveal.

Tom Junior's kind of like great and behavioural records and

things like this and where they live, which is the

worst bit.

Yeah.

If we're using such a highly complex model to generate

synthetic, is there a risk of overfitting to it?

Yeah, absolutely.

So this is, I think a real concern is that

what If your model gets so good that actually it

does tell you Tom's coffee, right?

And actually, we're gonna, when we look at autoencoders, the

model can be, models can be very lazy, and it

might just work out to tell you, Tom's coffee order,

right?

Uh, because that's easier than, than generating something that looks

like Tom's coffee order.

Uh, so yes, you do have to be very careful.

Uh, the kind of, um, the big concept here is

differential privacy.

Right, this idea that some ways of anonymizing data.

Do not allow you to reverse engineer the original data,

and the question is whether inductive complex or network methods

are always differentially private, excuse me, because they, they may,

for example, just spit out the training data.

What I'm referring to is like if you want to

generate kind of like.

I mean, I guess if I guess what I'm thinking

about more of is like the idea of unseen data

is not a lot based on.

Or if you wanna, if you want to generate like

a uh what I'm wondering is, I mean, what, what's

the advantage of this over like, you know, observing like

the distributions of different, you know, the ways that you

generate synthetic data for, for a class, for example.

Um, oh, you mean as in like I'm not sure

I fully understand, so do you mean something like.

I see that the kind of there is a, I

don't know, skewed left distribution for purchase price and there's

uh kind of on average 0.8 of these things are

non-fraud and so I just do like a conjunctive draw

from that.

Um, yeah, I think it's, I think that's a good.

I think that's a good strategy and I, I reckon

8 times out of 10, uh, that is probably as

good as you need.

Uh, and I think that, um, that is differentially private,

right?

There's, there's no way to reverse engineer, assuming that you

have a large enough scale, right, so like 10 or

more data points, the ONSence system on like 12, I

think is the minimum number for a mean to be

reported.

But you could do something like that and then you

wouldn't get to reverse engineer.

Um.

But if you thought that the relationships were complex.

Right, where it's not just like independent draws from these

separate dimensions, but like my draw from one really should.

Impact my draw from the other.

And maybe we don't think that they're nicely like distributed

in kind of that kind of tea distribution or, or

standard normal distribution.

And then these kind of more complex methods kind of

kick in, right?

And things like credit card fraud are, um, kind of

mercifully rare, right?

So they're unlikely to follow quite nice distributions because it's

actually reasonably unlikely that there is credit card fraud evident

in your kind of, um, account statement, essentially.

Yeah.

So in this example, you would like train a model

to get synthetic data, transport that out, build a model

to do predictive modelling on the synthetic data, and then

return back to your original.

Yeah.

OK.

Yeah, yeah, yeah, yeah.

So take the sensitive data and make it non-sensitive by

generating synthetic data, train a model on that.

And then, well, I, I guess there are two avenues

one could take.

So if you were confident in your firm's um.

The security of your fund's infrastructure, right, you could train

a model on the, or you could play around with

models on the synthetic data, so you allow lower level

employees.

With less strict security clearance, right, to, to, to play

and model and and come up with things and then

maybe your final model structure you train on the real

data, assuming that you can do that in a siloed

way.

Or alternatively you just train on synthetic data and that

is the production model, then as new test observations come

in from me buying copies, right, you, you test whether

or not you think um um they're they're fraud.

Yeah, synthetic data, are we making it for the variables

which would be included in our model as well, or

just like to fill in the gaps?

No, it, it wouldn't be just to fill in the

gaps.

You would, you would generate entirely new observations for all

features in, in conventional synthetic data generation, um, settings.

Like if you're seeing some other company, then how did,

how would the credit card fraud be detected like because

you're creating a synthetic data before even like quitting the

movie.

Yeah, so.

You, you're absolutely right, so I think there's, there's the

kind of unconditional synthetic data generation, which basically works like,

so I trained a model that can kind of output

your new observations.

Um, and you basically just pass in random noise, uh,

and then that will generate you a data set of

observations that should have structure that looks like the original

data, trading data.

Um, but of course, you know, if fraud is rare,

it's, it's.

Probably quite likely that you have no fraud cases uh

in that data.

Um, but what you can do is like conditional generation

of data.

So what you would do is pass in a partial

description of the observation that you want.

So you might compare 30 year old Tom buying coffees

to.

Uh, kind of 85 year old Tom's grandma, who is

constantly getting calls by scammers trying to get her to

kind of like check for viruses on the computer, right,

that's the kind of the difference, and, and in that

instance there, right, it's much more likely that my grandma

is gonna be a victim of fraud and so we're

gonna get more ones there.

So you kind of prime the, the generation, say, well,

here's some little bits of information, complete it with the

rest of the features.

Testing for two different things and generating synthetic data, for

example, credit card fraud versus predicting expenses.

So the data features which need to be retained will

be different.

Or you could say you could more simply say generate

the observations where there is credit card fraud.

Right, and it would learn probably just if that one's

a little I'm a grandma are not like me, right,

that's, that's the, that's the, the difference there, my poor

grandma.

Uh, OK, awesome.

Oh, I didn't do that.

AI models, right, this is the one you're all familiar

with.

Chat GBT is a generative model, right?

Given a label, i.e., write my MY 475 formative for

me, it generates you the, um, required code, right?

That is.

That is uh the X and you can do it

with images too, yeah.

This might be like my understanding, but I thought it

was more like what's the most likely next word.

So I was always kind of assumed it's like individual

neural nets for like each next word.

Um, yeah, I think you're absolutely right, so, um.

Yes, in practise, the LLM type architecture that we're currently

very.

Familiar with is next to prediction still, right?

So, but this, I'm being a little bit lazy by

saying that that sequence, the generation of the sequence is

generative, right, because you essentially have some label like draw

me an image of a cat or answer this question.

And it is the way it goes about describing that

by producing X is, is sequential, but it is essentially

doing a kind of supervised X token prediction task under

the hood.

Yeah, I think that's.

I think that's an important point.

Maybe let me, let me think if I can kind

of like adjust my description here, because I think you're

right.

I mean, that is actually just, um, multi-class classification, um,

at the token level.

At the sentence level, it is a form of generative

modelling.

Um, yeah.

Is child GPT also built using neural networks.

It's chat GPT a neural network, yeah, yeah, yeah, yeah,

so it has a whole host, I mean like it

is an engineered thing, right, uh, but yes, at its

heart it's a transformer model which you're covering, um.

Uh, I don't know, week 7 or week 8, Friedrich

will do that with you, and there's some reinforcement learning

there to kind of tweak the the weights and biases,

but yeah, it's just a one big old neural network.

Yeah, yeah, yeah.

OK.

Yeah.

Yeah, yeah, yeah, of course.

I'm I'm like there's a case with privacy.

I was wondering is there any case of using synthetic

data to like add to the real data and have

like a bigger data set and um, um, can anyone

think of like a slightly morbid example of where this

might be useful?

This is where you start out doing good things by

learning about deep learning, then you become a very evil

person.

Um, some things are deliberately hidden, like, for example, missile

silos or, um, uh, artillery batteries.

Um, you know, they're actually very hard to get pictures

of because the people that own them do not want

them to be seen.

So, uh, models trained by armies to predict whether or

not that's where they should bomb, right?

Often use synthetic data to boost the number of images

of silos and bunkers and battery rounds and things like

that.

Battery rounds, it's not the mediaeval times, um, um, um,

uh.

Yes, and that's a case where basically it's, this, this

comes under this kind of, um, again this issue of

mode collapse, right, your, your conventional model trained just a

lot of the observation data, they really struggle to predict

whether or not that's a missile silo because there are

just so few examples of labelled missile silos, right?

So what you can do is generate pictures that look

like missile silos that you have a greater probability of

them such that it would be really bad if the

model was lazy and just said no, that's not missile

silos.

Yeah.

Do you know just kind of amplify any noise?

Like, aren't you just taking that data and like, if

I copy and paste it and duplicate it, it, it

doesn't give the model any deeper insights that we're already

there.

Yeah, and I think this is really contentious at the

moment.

I, I think, so the way I've been trying to

think about this, um.

Is um using information theory, right, so information theory is

basically about um.

Well, originally it was about quantifying how much information you're

sending down the telegraph pole, right, so in the, in

the days of telegraphs, you know, like, dear sir, I

kindly request 2000 pounds, stop, you know, that kind of

message, right?

The question was how much of that can be carried

in a, in an amount of time along the electricity

cable.

Um, and, and therefore, how could you boost it?

Um, more generally, you can think about information as being

about like the amount of surprise.

Uh, contained in something, right, so, uh, something has, uh,

higher information if when you reveal it, you learn more,

right?

And so what you're saying is, but you, we're kind

of, you might start with 2000 observations, you generate all

of the synthetic data, sure you can generate a million

versions of it, but it's only based on 2000 observations,

right?

So that you think you have lots of surprise, a

million, right, but actually you really only have 2000.

Um, and I think that's a really genuinely good point.

I think it's probably somewhere in the middle, right?

It's not quite as bad as um bootstrapping, right, where

bootstrapping is just randomly resampling the data with replacement.

So you genuinely can have duplicates in your data set.

Um, but it's not like having a million new observations.

It's somewhere in the middle and you're kind of hoping

that the model has inductively learned some inherent relationship that

means that, and although there's some new surprise, right, because

it is working from basically a random distribution to start

with, um, it, it can't be fully new, like it

can't have learned anything that wasn't in the original training

data, um.

But actually, you know, like the, I mean the.

The current paradigm of.

Big beefy neural network models that are destroying the planet,

right, it is that you, you.

With enough parameters you can actually get like an amount

of information that is basically indistinguishable from, from real world

sampling, right?

I mean we, we just think these things are kind

of very, very good and non-synthetic, um.

Because the parameter space is so large.

Um, but yeah, I I think that's, I think it's

contentious.

I don't think we actually know yet what the limits

of this are, you know, um.

But I do think it's, it's better than bootstrapping, which

would, would be a, well I know it's better than

bootstrapping, I've simulated it, um.

There the surprise is much less.

OK.

So these generative models are what they're gonna call uh

are are typically stochastic, right?

That is that they're modelling this kind of joint probability

distribution of the data.

So given why, you know, your label, like what is

the probability that we see a an 80 year old

grandma versus a 30-year-old lecturer, etc.

etc.

right?

Um, and so there there there's, there has to be

some randomness to this, right, because we want to generate

novel data each time we say, give me a case

that that it looks like credit card fraud.

Um, so we take a random draw from this distribution,

right, so whereas before it was kind of like the

probabilities of distribution is 0.8 and 0.2, therefore it's 0.8,

right, there's no randomness to that, we just always assign

the positive class.

And that's true also in the multi-class case.

In these kind of generative cases, we probably are going

to want to realise some of that randomness, so that

we get kind of new looking uh novel observations each

time we ask for a, a, a separate um instance.

This is kind of a reasonably complicated process, right, um,

going from your standard multi output, your network to something

that is random uh and giving you novel, um, data.

So as I said at the beginning, I'm gonna kind

of give you a plotted history of the development of

this area, show you different model types, and, and hopefully

get us somewhere close to um understanding the full um

pipeline.

We are only gonna focus on type of the data,

so I'm not gonna show you anything to do with

images or text.

Um, we will do images next week, uh.

But basically the underlying principles, especially in the variation of

water encoder, leads you pretty quickly onto diffusion models and

things like Dali too.

OK, so these, these are the building blocks still of

the kind of class leading models that we're all interested

in um today.

So Start to for 10, and this is the last

thing we'll do before having a quick break.

Uh, suppose we have kind of, um, just 5 features

in our data, right?

Very, very simple, um, uh, X matrix.

Um, but I want to compress this data down to

2 dimensions, right?

It's just too much information, I don't want 5 variables

in my model, I only want 2.

Can anyone kind of think of a way that we

might want to structure our neural network to kind of

achieve this dimensionality reduction?

What might be like an intuitive shape for our network.

The notes in the layers are gonna decrease.

Yeah, to, to how many?

If we want to degrade it to two features, probably

no.

Yeah, great.

So we, we just want to kind of like boil

our data down like this, right?

So shrinking, uh, number, uh, of nodes.

This is kind of, you know, your output can be

any dimension you want.

You know, if you want two latent dimensions, set K

equal to 2.

If you want 15 latent dimensions, which would be weird,

given 5 input features, right?

Set it to 15.

Uh, but yeah.

And notice that not just kind of we're saying we'll

start from 5 and get to 2, but we're kind

of slowly boiling this down, right, so we start with

all of this kind of 5 kind of different ways

that the data can vary, that gets squished down to

4, that gets squished down to 3, and we're kind

of each time compressing it, quite literally, until we get

to just 2 numbers at the end here.

So this is how you do dimensionality reduction using your

own networks, right?

Just have uh the number of nodes in your network

that that correspond to the number of features that you

want to estimate, the dimensions that you want to estimate.

I Yeah.

Just curious about what the pros are of this compared

to other dimensionality reduction like PCA or something.

Um, well, that's a great question.

I don't actually know off the top of my head,

I think, um.

So, uh, so I don't, I, I don't want to

speak out to because I don't know enough about like

PCA and, and other such nasties, but my, my assumption

would be that they're like much more linear in their

kind of the way that they're separating out these things,

right, they're finding the principal axis of variation, netting that

out, they're looking for the next one, netting it out.

um.

This still allows for all of these like complicated um

nonlinearities because you know these are gonna be regular activation

functions or whatever and um you can kind of boil

it down and and ignore features if you don't really

need them, etc.

etc.

and you're gonna do that inductively um but I, I

don't actually know whether this is like.

Hugely better than uh PCA.

Uh, or similar such things.

So, this is called an encoder.

Right?

We're encoding the data because we're starting from something that

is kind of more uh high dimensional, and we're shrinking

it to something um uh less dimensional, right, this is,

this is a compression algorithm, essentially, right?

So we learn this by using kind of decreasing number

of nodes per layer to kind of squeeze that data

or compress that data down to a smaller number of

values.

We started with 5 dimensions along which we describe the

data, we ended up with 2 dimensions that describe that

data.

Let's call that number more generally B, and it's going

to be called B because it's going to refer to

the bottle neck.

Layer, OK, you're gonna see right at the bottleneck in

a second, right?

And this is just a form of kind of compression

or latent dimension estimation, right?

It kind of aims to remove noisy or irrelevant signals,

right?

We might not need both your age and your salary

uh in order to estimate your kind of underlying, um,

I don't know, ideology, right, because.

Older people tend to earn more money, right, up to

a point.

Working older people tend to earn more money.

So you kind of, you don't need both of those

dimensions to describe it.

So we kind of squish it down, we try and

merge those things into the actual underlying dimension, uh, which

is kind of like increasing salary, um, over time, right?

The other kind of thing that's worth kind of mentioning

here is that kind of.

It's like boiling down.

Something's suspended in water and removing all of that kind

of like.

Dumps that you don't need and just keeping the essence.

Right, so you're kind of really trying to strip out

anything that's really not necessary in order to kind of

describe the instance in questions.

These are kind of how compression algorithms work on your

computer, right, or like an audio signal or a or

a photo.

There is a certain amount of kind of extra um

uh information that you don't really need in order to

kind of keep that image of the cat being a

cat, right, so just find me that thing that's much

smaller, uh, but which, which maintains the essence, um, or

fundamental qualities of that instance.

So, um, I'm gonna define the output.

Of encoding some input data acts as that.

OK, and this is just gonna be our kind of

classic kind of nested neural network, right, where and feed

forward.

So we start with one layer, this is typically gonna

have K dimensions, right, the number of input features 5,

that gets passed to the next layer after being activated

all the way through to our bottleneck layer, but crucially

here, right, the dimensions of those weight matrices.

Are going to shrink, uh, so that by the end

of it, when you've done this kind of transitive matrix

multiplication, you're gonna come through with kind of uh far

fewer, uh, dimensions, in this case, uh, 22 dimensions.

OK.

Now what's the big problem with this setup?

Why can't we just go train that now?

So we start off with a model that looks like

this, this is great, this is exactly what we want.

It's shrinking our data down to two latent dimensions.

Why can't I actually train that model?

How do we choose our loss function?

Right, yeah.

Can you be slightly more specific?

Why can't we choose the loss function here?

Because there are there's information that's being squeezed together.

So the output is not wide.

Yeah, the apple is definitely not white, it's like it's

like.

I We never observe it, right?

These are latent.

The whole idea is that they are kind of, kind

of intrinsic qualities of the data, but things that we

don't see, right?

You know, we've squished together in some complex way, age

and um um uh salary or or whatever I uh

gave us the example as, right?

So there is no way to have a loss function,

not because like we couldn't say, well, it's a, it's

a real numbered value between negative infinity and positive infinity,

we probably could say that.

But we don't have anything to measure it against, right,

there is no, there is no Y here for the

latent dimensions, right?

So this won't train.

You can try, but it, it won't train.

Um, you, you will just not be able to define

it because you won't know what DIM1 and DIM2 should

look like, right, the actual values of, of one in

this case.

Before I show you, anyone got any idea how we

solve this?

Because No one at least the starts one from the

first layer and then uh and then.

And then we do something, yeah, um, almost, yeah, I

think you're right to be thinking about a normal model,

um, and what in particular about a a normal model

do we have that we don't have in this case?

Right.

Why, we don't have Y.

Right, well, sorry, we have Y in your normal model,

right, we have true labels that we can benchmark against,

so we need to bring that in, so you're absolutely

right to be thinking about.

Anyone want to kind of extend this layer after la

so that it gets.

Yeah, so you're, you're absolutely right.

I'm not gonna do exactly what you say here, but

I'm gonna do something very similar which I think blends

both of your points, uh, which is, uh.

Well, hang on Yeah, well, let me tell you, first

of all, what I'm gonna do is this, right, which

is boil it down to that bottleneck layer and then

bubble it back up to the original size.

OK, now the point here is that this original size,

right, we have the Y for, this is, this is

the same dimension as Y, right?

It's in fact X, uh, and that's what we're gonna

do.

So let me just kind of like jump back ever

so slightly and just um uh define the problem more

specifically.

So we said we didn't have Y for the latent

dimensions.

Obviously, right, they're latent.

Um, and we need to be able to assess the

loss at the end of our neural network, right, the

outputs of that, uh, in order to train it, right,

in order for that propagation to work.

So the end or the output of that model must

be something we observe so that we can calculate the

loss and that propagate it, OK?

So we can never just have an encoder and train

just the encoder because if we knew what those latent

dimensions were, they wouldn't be latent and we wouldn't need

to train the model in the first place, OK?

This is a very common trick in deep learning, right,

which is, you may be interested in something latent.

So in this case, it's gonna be uh kind of

the boiled down essence of our observations.

Uh, in a text case, it could be an embedding,

right?

A, a representation of words in a smaller than K

space, right, where, you know, maybe it's like 256 dimensions,

although there's a kind of a million tokens or whatever,

right?

What you do is you basically take that thing you're

interested in and smack it in the middle of your

pole, and then you put something else at the end.

Your idea, right, that we can measure against that is

is why, right, uh so that we can backpropagate the

loss, and then at the end, we're just going to

extract that hidden information rather than the um explicit outputs.

So here's that, that model again.

The really nifty thing about these models, and we're gonna

call them autoencoders, is that why, the thing that we're

gonna measure the loss against.

She's ex.

So X has 5 dimensions, 5 features, we shunned it

through this shrinking number of layers to 2.

This is our bottleneck layer, these are the latent values

that we think kind of really describe the data.

And then we're just gonna expand that back out to,

notably, 5 features.

And what we wanna do is say, well how good

or how closely do the decoded elements, i.e. the end

of this process, resemble what went in.

OK.

If it can learn how to squeeze the data down

to something essential and then expand it back out to

its original shape, well then that suggests that this kind

of bottleneck layer contains good solid information about uh uh

the essence of that data.

OK, so the nifty thing about an autoencoder is not

just the shape, right?

But the thing you're benchmarking against your Y is in

factual X, right, that's why it's an auto, uh, i.e.,

self, uh, encoder.

Yeah.

Is the mirroring always like the left hand side and

the right hand side have the same number of neurons

and the same structure.

I don't think it is necessary.

I think you could jump from 2 to 5.

I just think that this is the time.

Uh, and it's kind of mirroring, mirroring the process, right,

so it's kind of like, well, it took this number

of steps to boil it down, so let's give it

that number of steps, give it the opportunity to expand

it back out.

It's possible that if your data is simple enough that

maybe you can do it in one step.

I would have thought, although I can't guarantee it, that

that would basically mean that this is overparameterized and you

could probably get rid of the layer here as well.

Um, but, um, you could try it out.

You'll, you'll have the tools to be able to do

this, um, after today's summer.

OK, I just realised the time.

Go and have um a good 5 minute break.

Uh, come back here at like 1213 minutes past the

hour, uh, and we'll get going.

Oh Yeah, I have a walk.

yeah it's like a hard time like I'm an American.

I like it because I was talking to about the

NYP you got about the other guy.

and they will try and like.

Yeah, yeah, I used to be like the last two

weeks I came like I was like late I didn't

probably like I this is.

Yes, just using the bottles like that.

OK.

like clustering for like DSL I think it's just that.

a lot of people are like corporate machine learning.

I had an auto schedule, but I think it was

like.

I think so yeah yeah yeah.

That's what I'm talking about, but I'm.

I mean, obviously, yeah my friend who works.

Yeah, it's like I don't like.

It's not like it's not like you're in like a

mental yeah I was like yeah that's yeah that's my.

Uh, we still don't have quite enough information to train

that model because just like in a conventional network, uh,

we still need to kind of work out the loss,

and the loss is now slightly more complicated, right, because

we have these multiple outputs and we can't just do,

um, binary or sorry, uh cross-entropy, uh, because it's not

like a one hole can code a vector at the

end.

These things uh will vary and maybe even have, uh,

different ranges.

Uh, so in these, uh, cases where you're doing something

like autoencoding your data.

Uh, we normally just average the separate loss for each

of the output nodes.

OK, so this is just kind of, uh, for 1

to 1 over J where J indexes the number of

uh loss functions you need, right, sum up the loss

for the corresponding elements of that output vector.

Now the reason it's J not K, uh, is because,

um, imagine that one of your output, um, um.

Classes is is like multi-class.

Categorical, right?

So maybe what you have here are, um, uh, I

don't know, voters, and you ask which party you're most

likely to vote for at the next general election, right?

And in this country maybe you give them 5 or

6 options.

So actually there even more complicated than the kind of

case we just considered, not only do you have kind

of like um.

Uh, maybe I don't know, 5 features, but one of

those features itself has 5 outputs because you need one

for each of the classes of that, um, output.

So that J here would range over the kind of

feature which would be when J equals 1, so would

be passing shit, um, and that would be a uh

um a softmax over 5 of the nodes in the

output.

But it it's, it's pretty straightforward, right?

You're just like averaging how well it performs on each

of the dimensions.

Because it's just a straightforward average, this reconstruction loss, you

just have to be slightly mindful that if your data

has different ranges, right, and uh your losses from different

features may vary in size and magnitude, and as a

result.

Uh, the training of the model may be more impacted

by some features than others, right?

Because say salary is actually measured to the pound, right,

your loss in terms of squared error for say a

prediction of 30,000 when they actually earn 60,000 square that

is gonna be much bigger than say the um binary

cross-entropy loss of uh whether or not they intend to

vote at all.

OK.

So you just need to be mindful that particularly in

these cases where you have this um reconstruction loss.

Uh, that, uh, you need to scale your inputs, so

they're roughly in the same range so that no one

feature dominates the training, right?

So the models just kind of like, look, I can

really reduce the loss if I just focus on income

here, and it neglects to focus on, uh, the other

elements.

So try and normalise your inputs, we will do that

uh in a very simple way in today's seminar.

And that's it.

OK, so once you have that reconstruction loss and once

you've set it up with something like Pytorch, you're, you're,

you're good to go.

Now at test time, right, and test time here is

like extract the latent dimensions, uh, we don't want the

outputs of that model, right, cos that's just hopefully gonna

look pretty similar to the inputs, uh, which we already

have.

What we want to do is basically backpropagate.

The model, update all of those weights and then we

just extract the outputs from the um er bottleneck layer,

right, so you, you shunt your data.

Through through through through through at test time and you

stop.

Right at this bottleneck layer, you're gonna get two outputs,

they will be the two latent dimensions that you're trying

to estimate.

That is very similar to how a word embeddings model

works, right?

You extract the vector uh from your embeddings layer, not,

uh, the, uh, final layer.

We essentially ignore that decoder, right, the decoder is just

there so that we have a sensible loss function at

the end.

We don't need it in order to extract the latent

dimensions, right?

We're still in this dimensionality reduction at the moment.

But just notice very briefly, that that decoder is still

pretty useful.

It's like given a small set of inputs, I can

generate very descriptive 5 feature data sets, right, and you

might want to start thinking in your head like, well,

if I put in some random numbers into that um

decoder, might it generate me some sensible looking synthetic data,

right, the decoder is gonna be pretty useful for generative

tasks.

OK.

What is the ideal function for the autoencoder to look?

Like give me the exact function.

That would yield zero reconstruction loss.

We've already talked about it in this course.

The identity function, yes, exactly, right, the ideals, right, function

for an autoencoder to learn is, uh, the identity function

because then.

You know, auto encode X is just gonna be equal

to X, and so there's going to be zero loss.

Now that, that wouldn't necessarily be a problem if like

it had learned an informative identity function, but neural networks

are lazy, right?

And so it will just try and pass the data

through without changing it.

OK.

Now that's not very useful for us cos what we

want it to do is learn that essence and then

rescale it out.

But what you'll find is these conventional autoencoders just break,

uh, and they just learn to pass the data through

without perturbing it, right, they set some weight equal to

1, others to -1, so that you only basically get,

uh, uh, your, um, input data out exactly.

That satisfies your loss completely, right?

It's by design, the best thing to do, but it

doesn't get you what you actually want, which is an

informative representation in these hidden layers uh of your network.

Um, so we need uh some way of stopping the

model essentially from being lazy, uh and just uh.

Uh, learning the identity function in a way that is

not informative to us, in a way that hasn't basically

done this kind of boiling down that is essential for

this model type to actually be useful as a kind

of dimensionality reduction, uh, technique.

Right?

And the kind of the thing I want to impress

here.

So for the 3rd time, right, this is an inherent

problem.

Right, this isn't like you've trained your model wrong, right?

If you use an autoencoder, it is going to try

its damn hardest to find the identity function, because that's

what you're essentially telling it to do, right?

It's just that if it does that.

It's gone too far, right, so we need some way

of reining it in, uh, essentially.

So this then moves us on to denoising, right?

What we don't want it to do is just learn

that X equals X, OK?

And the because we have this kind of back propagation

style of fitting.

What we need to do is to kind of penalise

it such that if it learns X equals X, right,

it doesn't get 0 in the loss.

So denois in autoencoders are actually very similar to autoencoders,

except before it passes X through the forward part of

our model, it randomly corrupts it.

OK, so it will just permute those values ever so

slightly.

OK, so one way you could do this is set

some of those values equal to zero.

Right, or maybe it's like a little bit of random

noise, OK, it's, it's a noising process.

Let's call that data Xtilda, squiggly line on top of

it, OK.

So your forward pass is Xtilda, right, to to get

Y, right, this is a vector that is supposed to

look like our input data, right?

Now the really magic part is that when you back

propagate, you assess the loss not against X Kilda, but

against X.

So the model is Xtilda, this slightly noisy thing, and

if it were to learn the identity function, we'd have

non-zero loss because Xtilda does not equal X, right?

So when you backpropagate the loss against X, you are

telling the model, don't just learn.

Exactly the identity function, learn something that that kind of

basically strips off that noise, right?

This is like lovely, like, you like you, you add

some noise to basically kind of learn more from the

data, um, because if you were to learn the identity

function, if you were to get lazy, you wouldn't do

as good a job, right?

So by design now.

The identity function is not what you should learn, because

if you learn the identity function, you'd be predicting X

til the, not X, and we're gonna assess the loss

against X.

Super cool, yeah.

So we want the noise to account for the compressed

features.

No, so the noise should be random and it's applied

at the input, so when you still have K +

5 features, right?

And the idea is just that like because we're still

assessing.

The outputs against the uncorrupted data, right, it really makes

sense for that model to learn the essence at that

bottleneck layer, and it really shouldn't be doing anything to

take a shortcut and just learn Xil because Xtil doesn't

equal X, right, that's the crucial thing.

Those two things are not identical.

The identity function will no longer be the optimal loss

function.

Oh sorry, the, the model function.

OK, so you can kind of think about this a

bit like um er like restoring paintings.

OK, so here's kind of the original, what they think

the original fresco, uh, look like, right?

Over time that painting gets corrupted, right?

We can think of the kind of slodges here as

being kind of that added noise.

The idea is that you kind of wanna shunt your

uh noised image through your.

Auto encoder, right, but then compare it against what the

image actually looks like without the noise, OK, so that

you kind of like, you learn this representation.

Obviously you need a good one because if you ask

the poor nonna that tried to do it in that

chapel, right, you get something that's actually pretty terrible, but

you can kind of see what's happening here, right, this

is the process, you kind of take the original thing,

you corrupt it, and then you try.

Underline to reconstruct it, right?

Uh, to get something that looks like the original, not

like, uh, the noise, right?

Think about the denoising autoencoder as like scraping off these

glyphs, these kind of errors, the noise, to try and

get back to the, uh, original image.

I was quite proud of myself when I thought of

that, uh, analogy.

Uh, OK.

Uh, even with this kind of like random noise added

to the uh training of, uh, an autoencoder, denoising autoencoders,

we wouldn't really consider them generative because.

You know, that bottleneck layer is just a number for

any given X and it's, it's deterministic, um.

Even though there's a little bit of like uh corruption

noise going into it, that's really only affecting how we

backpropagate uh the loss.

In other words, there's like a 1 to 1 mapping

from input to output.

If I pass in exactly the same thing multiple times,

I'll get exactly the same output.

That's not going to be very good for credit card

data, right?

We want things that like vary, right, we want some

variance in this thing.

So we need uh the generative aspect of this to

add uh some uh randomness to our data.

We could think about just kind of doing noising, right,

rather than denoising and at the the end just kind

of, well, randomly increase that feature a little bit, randomly

decrease that one a little bit.

But You know, we've just spent all this time training

a model to really learn the connections between these features.

If I then go feature by feature and just like

randomly add a little bit of noise, just so that

they don't look exactly the same.

I'm not really doing anything.

I think this gets to your point about like, is

there really any extra information there, that really wouldn't add

anything, right?

In expectation, assuming I have had like a zero mean

noising function.

I just have the input data still, so that wouldn't,

that wouldn't be great.

Um, so what we do instead is we actually go

back to statistics and say, what if we can change

something in the model so that rather than simply kind

of learning this 1 to 1 mapping, I actually learn

like a stochastic mapping itself, and mainly I learn a

probability distribution, right, at the core, uh, of this, uh,

network.

So we're gonna move from denoising autoencoders now to variational

autoencoders.

Uh, and the, uh, difference between the two is that

notice that our latent space or our bottleneck layer here,

right, is now parameterized slightly differently, rather than just having

kind of, um, scalar outputs, right, which are the latent

dimension.

Representations, we now learn.

I mean and standard deviation for each parameter that we're

interested in, right, the number of dimensions, right, so here's

the, um, we're boiling it down to 2.

Uh, latent dimensions, so we have a mean and standard

deviation for one dimension, a mean and a standard deviation

for the second dimension, and this is the crucial bit

that makes it variational.

We then use those means and standard deviations to take

random draws from a normal distribution.

Right, so that truly is stochastic.

Given the same mean and given the same standard deviation,

2 draws should yield me two different values.

There is a high probability that they're going to be

centred around the mean, right, but it's not guaranteed, and

the, the standard deviation is gonna tell me by how

much I would expect it to spread, OK?

So now, the bottleneck parameters help us define basically a

multivariate normal distribution.

Rather than Uh, uh, defining the latent dimension explicitly, right,

just getting, uh, a 1 to 1 mapping to values.

Now, the mind blowing thing about this for me when

I was writing these slides, um, is that, like, notice

what is influencing the mean and the standard deviation, it's

like these input layers, right?

So the mean and standard deviations will change.

Dependent on your inputs, right, so you don't actually have

one multivariate normal distribution here, you have, Like any, right,

the number of observations, this is a gaussing process and

it's a, a set of multivariate normal distributions that vary

depending on the input data coming into it, right?

So you can get different draws based on the features

it learns from the data.

And so we can basically get this.

Kind of representation here, this is the kind of probability

of the kind of uh latent space given our input

data X, right?

And model that as a function of the inputs itself,

OK.

Where these kind of outputs, Z1 and Z2, now these

really are the kind of latent values, these are draws,

random draws from normal distributions where the mean is equal

to mui, where that is determined by the input data

and with a spread, right, that is also determined by

the input data.

Yeah.

So we're using MCMC simulation here.

Nope, nope, this is still, oh, no, we're not using

MCMC simulation because you'll still take one drawer every time

you want it, you won't take like 1000 drawers, um,

uh, to get it.

But it has a similar flavour and this is certainly

more basic I mean it's kind of, um, um, approach

to to modelling, um, but it's not an MCMC approach

directly, I don't think, yeah.

Bhibition, didn't they have one value for Z?

For one observation, yeah, absolutely, but they will be random

draws.

So if I were to take exactly that observation again,

pass it through and take another draw, although 1, sigma

12, sigma 2 are the same, that bit is 1

to 1.

The draw itself will be random, so I'll get slightly

different values of Z1 and Z2.

Yeah.

This is separate from the noisy or OK.

Yeah.

So you would probably also want to add noise uh

to this as well, yeah, yeah.

But I'll show you another way that we regularise this

in a second, yeah, like if we're using the same

input data and we're learning from the features to compress

them down to.

Yeah.

How was the, how's the output and distribution of values.

Because you could punt the same observation through 1000 times

and get the empirical distribution of Z1 and Z2.

OK, for observation.

OK, so we're doing that and then we're figuring out

the meat and the and it's always normal.

Ah yes, we, yeah, yeah, yeah, yeah.

Yeah.

Can I follow up with the question, so, OK, when

we're compressing down the features, how can we know that

the compression is actually a good compression?

Like, is there a loss that we can.

No, we can never miss the loss of the, uh,

we can never measure the loss of the compression directly,

right, because it's latent.

All we can say is, and this is why it

is inductive, right, which maybe is a kind of reason

to not favour over PCA in some instances, right?

All we can say is it's reconstructing the data very

well.

But that's all we can say, we can never say

that the compression works.

It's not like an audio file where you compress it

and you listen to it and you're like, well that

still sounds like the Beatles, right?

It's not like that.

It's um we just have to say, well, it looks

like an audio file.

It looks like the audio file that went in.

And where does the missing information in between go?

What do you mean the missing information?

Um, so let's say we're pressing down like.

Where does the question that we don't account for it

then go.

It just gets absorbed, right, this is, this is what

the shrinking layers are doing, they're kind of adding these

things together after a multiple.

So for example, if it thinks this feature is useless,

well just times it by 0.

Right, times it by 0 and times it by 0,

right?

Uh, and then that gets out of 0, gets added

and it just gets evaporated, or if you think it's

really important, times it by 10, and then add it

to the other features, right, it's just, it's all in

this consolidation, this kind of multiplying and then adding together,

which is at the heart of any neural network.

Yeah.

OK, I'm gonna have to speed up, uh, otherwise, uh,

uh, we are gonna be woefully out of time.

OK, now, I said the whole point of a variation

on autoencoder is to make that autoencoder generative, right, so

there's some stochastic elements, so it generates novel data points.

The problem that we therefore have is if we just

use the reconstruction loss, we would penalise exactly the thing

we're looking for.

If it generates me something that's slightly novel, right?

It would not look like the input data, and so

there should be some non-zero loss there, and that's a

little bit problematic, right, because actually if you were to

just train a variational autoencoder with um.

Just your reconstruction loss, it will probably just look like

a autoencoder because it will just set the mean equal

to the mean and the standard deviation equal to zero

or like as small as possible so that you're basically

just getting the the mean expectation uh each time.

This is the other kind of like.

Machine learning strategy right in these cases.

So we said already kind of like in deep learning,

you would often, you target like a hidden output, right?

The, the second thing is, if your loss isn't doing

what you want it to do, just change it, right?

Uh, so here, I'm gonna work from bottom to top.

Here is the kind of loss function that we're gonna

use for our variation autoencoder.

The first bit is the reconstruction loss, right, this is

just um how good does the output look compared to

the original.

The poster they are, the better, right?

This is the autoencoder bit.

But then, say aha, but don't do that entirely because

what I also want you to take into account is

this thing here called the um oh we'll call that

Li divergence, the KL divergence, right, which looks super scary,

but actually what this is doing in abstract is assessing

how far any given latent dimension indexed by I here

um is from a standard normal distribution.

Now in Bayesian terms, we would be basically be saying

that our prior over the distribution, uh, new and and

Sigma is, is, is a standard normal distribution.

So as you adjust.

Your values of Sigma and me.

So that they er reduce your reconstruction loss, you are

going to incur a penalty because those values will move

away from the standard normal distribution.

So this acts like a break.

It's like, OK, you're moving to get them as close

as possible, but if you start to move too far

from that very basic standard normal distribution, you'll be incurring

loss, and at some point, the reduction in loss by

having ever closer reconstructions of the, the data.

That will be less than the loss incurred from this

combatlas term.

OK, this looks really horrible.

I've got a footnote here which you should um read

in your own time.

Don't worry too much about it, it does just expand

out to this roughly, and that's what we would use

in our um loss function, and it's just a break,

right?

It's just a way of telling the model to slow

down as it approaches just learning the input data.

OK, it's just gonna kind of say, oh, don't go

too far, right?

I wanna be somewhere in between.

Learning just the input data, i.e. being an autoencoder.

And being centred on a standard normal distribution.

And basically all variational autoencoders.

Assume the prior of a standard normal distribution, OK.

Now there's one last um issue.

That we have when we think about back propagating the

loss.

So I've just given you the full loss, that works,

promise.

er but you won't be able to do it if

you set up the network just as I described.

Can anyone think what the problem is?

So we've got our decoder, right, it has K outputs

that and the and the loss at the end of

it, right, we can, we can definitely get to that

step, so I can back propagate it there.

I can probably back propagate a few more layers into

the decoder and then at some point I hit Z.

Right, these latent dimensions, where is that coming from?

Mhm The normal distribution, how do you back propagate through

a normal distribution?

You can't, it's impossible.

It just doesn't work.

It's, it's a random draw, how does, how does differentiation

work with a random draw?

It just doesn't.

Um, so we can't have a network that looks like

that.

Um, and so what we have to do is something

called a reparameterization trick.

Uh, and so what we do is we define those

latent dimensions, Zi as equal to a scalar value mui,

plus uh another scale of value sigmai multiplied by epsilon,

and we assume only that epsilon is random.

OK, so Epsilon is gonna be a standard normal distribution

and that is never gonna change.

OK, this is always just gonna be random noise, uh,

added to uh the uh network, right?

But crucially we can backpropagate through new 1 and m2

because these are now just scale of values, right, there's

there's like no other every other node in our uh

network.

And notice that say I took a random draw from

here uh and I get.

0.1, which wouldn't be too unlikely given a standard normal

distribution, right?

So what would that do?

Well, mu would be maybe centred on 5, right, and

then we'd do 0.1 times the standard deviation, right, which

would be essentially the amount of spread.

Right, if this is greater than 1, I would suggest

it's, it's got a more variance than the standard normal

distribution, so we just shift that number more.

We've still got randomness because this is totally random, but

we just don't backpropagate through the standard normal component, we

only backpropagate through these two parameters.

Uh, which are, um, uh, your, um.

Your mean and standard deviation uh parameters of a normal

distribution, right?

Because you can essentially write, if this was the normal

distribution of uh.

Me and Sigma here, right, you can take me out

and have it out over here, right, and similar uh

with uh you can divide through by sigma essentially.

OK.

So the reparameterization trick, uh.

Just allows us to separate out the random noise from

actually the parameters that really matter, things that are shifting

either, like how far along the dimension uh we would

expect the value to be new and how how spread

out, uh, the actual random draws should be.

OK, so, uh, let's briefly recap here.

So autoencoders work, right, because Y is X, right, they're

trying to predict themselves, uh, and that is thoroughly uninteresting

until we start to compress the middle layers of this

network, right, until we try to boil down the data

and then re expand it back to its original size.

And we're gonna use that bottleneck layer as our kind

of representation of the late latent dimensions.

We are never gonna know whether it's a good one

explicitly, but we're gonna implicitly judge it based on how

good it is at using that bottleneck layer to expand

back out to the original data.

OK.

These are very prone to the um uh identity function.

This isn't like a theoretical thing where they kind of

corrected it before anyone noticed it.

I find this all the time, if you train an

autoencoder, you'll be like, why is my loss 0.0000 and

oh God, OK, it hasn't worked, right?

You need the denoising essentially to limit the model's ability

uh and and and kind of like.

Um, Desire to learn the identity function.

Variation on autoencoders are different because whereas an autoencoder is

just about learning kind of like, you know, a 1

to 1 mapping from a complex description to a simpler

description, the variational autoencoder wants to kind of boil the

data down to a random process, a probability distribution over

the data that has fewer parameters, and from that we

can take random samples and then therefore get kind of

like a diversity of of different um data that maybe

shares a structure, right, maybe they're all cases of fraud,

for example.

And That has some difficulties, right, we've already discussed this,

we don't want just the reconstruction loss, that'd be a

terrible variation also encoder because it would always just like

want to predict the mean outcomes, right?

We don't want to punish it when it has that

little bit of kind of um er novelty.

And so we use the callbackli of divergence as a

way of um reining back this move towards er just

becoming a autoencoder.

Uh, and when you back propagate, you can't backpropagate through

a random sampling process, so you need to separate that

out from the parameters that define it, and conveniently, and

this is why we use a normal distribution, right, we

just separate out m and sigma, uh, treat them as

scalars which are then modified by.

A random draw from a standard normal, which we don't

that propagate through.

OK.

I'm sorry that this speeds up uh inexorably as we

get closer to the deadline.

um.

Let me kind of show you this all in practise.

OK, so there's, there's really not too much new here

at all.

I just want to show you how we used it,

um, in a specific kind of statistical context, which I

think typifies the kind of like incorporation of deep learning

into social science, which is mainly, it's rarely the final

model.

It's normally a tool that we use to then do

something else at the end, a bit like what you

were raised earlier about kind of like how we then

analyse the the credit card fraud data.

So Here's my 101 primer to missing data, right?

We don't always observe everything for a variety of reasons,

interviewers may forget to ask the question, they may, uh,

get subjects that don't want to tell them who they're

going to vote for, etc.

Uh, they may, uh, not really a missing data problem,

but you know, voters lie 9 times out of 10,

it's really annoying.

Um, and it may be that kind of like, uh,

what I find in my own research is that we're

not always allowed to ask every question in every country,

um, uh.

And in those instances, right, we may have missing data

that is kind of like correlated with um uh features

of the data itself, right?

Older people, for example, are less likely to tell you

who they're going to vote for.

They also skew conservative.

That's a problem because I therefore have fewer conservatives in

my data if I were to just delete those rows

than I should expect, right?

Um, and so what we do is, rather than kind

of delete those rows, oftentimes we want to impute them,

right?

We want to take a guess.

And the benefit of taking a guess is that, you

know, I can see that this person is from the

south and has some value 0.3, rather than losing all

of that useful information by deleting this row because I

don't observe X1, right?

Instead, I'm gonna try to guess that value and surely

that's better because, you know, I'm keeping in 2/3 of

the actual observed uh information.

So imputation is the process of going from a missing

data set like this to something like this.

And a very common strategy, right, is to actually learn

something about the data.

So if you can work out that older people are

more likely to vote Conservative, and if you observe that

that person is 70 years old, you should probably impute

conservative rather than.

Uh, labour, right, that that is the kind of gist

of a kind of predictive imputation strategy.

Alternatives would be to kind of put a zero in

there, right?

Like null imputation, um, less good, uh, predictive strategies might

be just to guess the mean or the mode, um.

They can be all right, but they're they're tempted to

not work quite as well.

Now, that's invitation, but I don't really want to talk

about imputation, I want to talk about multiple imputation.

And as I said, oftentimes we're gonna be taking our

deep learning model or or whatever, uh, and we're gonna

use it to generate some data that we then do

conventional, often linear analysis on, OK.

Now the problem with those imputations that I made here

is that they are not observed, right?

They are not actually what the subject said or did.

And so there's some uncertainty over them.

It's just a prediction, right, therefore.

you know, maybe it was a 4, but it could

have been a 4.5, it could have been a 2,

right?

There's some probability of those uh values being the case.

But if I were to just put my imputed data

sets through a linear regression model, the linear regression model

would make no difference between that.

Actually observed 10, and this predicted 4, OK, and so

what you find is that these kind of regression models

or train fit on um imputed data are overly confident,

their standard errors are basically too small.

Right, uh, because they're not taking into account the uncertainty

we have over the imputed values, uh, themselves.

So what you have is you, you know, it's a

problem because you draw a 95% confidence interval, right, which

means that under the assumption of repeated sampling, 19 out

of 20 of those confidence intervals contain the true parameter

value, but actually what you're gonna have is something that

is only contained.

16 or 7 times, right, so you're actually making the

wrong um judgement, taking the wrong action by rejecting the

null sometimes or we're more likely to do it than

the kind of base rate that you ideally set.

OK.

So multiple imputation.

It's a way of overcoming this kind of issue of

uncertainty.

So rather than just kind of imputing the data once

and fitting a linear regression model, you impute the data

multiple times.

To get variation in those predictions, you estimate multiple regression

models, one on each of the imputed data sets, and

then you combine those estimates.

And the idea is, because the uncertainty over the predictions

is going to yield different predictions, which will in turn

affect the individual parameter estimates, we can basically look at

how much those parameter estimates change to adjust the final

values.

So We're gonna have multiple data sets, let's index them

like this, X, superscript I is here gonna refer basically

to kind of like multiple versions of the same imputed

data sets where there's some kind of generative model filling

in those values.

OK.

It's generative, therefore it's gonna be stochastic, right, so as

I take repeated draws passing the same X data through,

I'm gonna get subtly different predicted values for the missing

data.

I take M draws, OK, so I equals 1 to

M, and I estimate exactly the same regression model on

each of those data sets.

And then it's really not that difficult.

Your final parameter estimate beta hat, which is the average

of the individual beta hats, right?

Nice and easy.

The more important thing here is the standard errors.

OK, so the standard error is basically the average standard

error, right, that is kind of basically how much uh

variation is within each of those models on average.

And then you look at how the beta parameters are

changing, right, this is the variance in the, the beta

parameters across the models.

Divided through by M11 because statistics, right, so you have

the average standard error and then you have this kind

of inflationary term, which takes into account that the beta.

Estimate is actually changing because of the uncertainty in the

predicted values, right?

The data you observe is held fixed, that is not

changing as well.

Therefore, it's just the change in the predictions that is

yielding these changes in these parameters.

So this is adding something onto the standard error, therefore

it's making it bigger, it's correcting for that overconfidence that

we get in the single invitation case.

This is the stances, right, the causal infancy stance.

Uh, the interesting thing for us in MY 475 is

how on earth do you get a good prediction?

Like how do you generate a, a stochastic model that's

actually good and efficient at filling in those, uh, missing

values, right?

And, um, to quote two learned scholars.

Uh, right, the kind of growing scale and complexity of

the data that we have in the real world means

that some of the kind of conventional, uh, ways of

doing multiple limitation that have been around since the early

2000s simply fail, right?

Um, in our slightly facetious simulations that we published, you

know, some of these existing, um.

Uh, modelling strategies for multi invitation took weeks to run,

like literal weeks, because we had kind of categorical columns

with say 50 categories.

Now in your network just says, OK, give me 50

nodes, right, but some of these kind of expectation maximisation

algorithms are just so.

Intensive that it, it, it grinds to a halt, right?

Um, I mean literally we spent hundreds of pounds just

to prove that these things take weeks to run, OK?

Even when they do run, if you try to simplify

the model space so that they do run even with

those things, they.

You have to make some kind of fairly simplistic assumptions

about what the data looks like, right?

It's multivariate normal distributed, maybe with a little bit of

skew.

It's probably not what your data actually looks like er

in practise.

Um, so if our data is big and complex, we

asked, like, how on earth do you impute it efficiently

and well.

Uh, so, We thought, a bit like kind of the

flaking image uh of the fresco, those missing values are

a bit like noise, you know.

Uh, they're kind of things that we want to scrape

off and uncover the original paint or the original value.

Um, so the, the natural thing to use here is

a denoising water encoder, where some of the noise present

is actually what we observe, it's the missing data itself,

but surely there's something underneath that that we can kind

of, um, predict out by scraping off, uh, that noise.

The slightly nifty thing that we do, right, is that

because it's missing, we actually can't measure the reconstruction loss

for those data points.

So what we have to do is rather than kind

of guess what that would be, which would be a

terrible strategy, we just add some extra missingness, right?

So we have in our data.

The kind of actually missing values and then a random

extra bit of missingness where we can measure the reconstruction

loss and if it does well, if it backpropagates through

on the bits that we can remove, right, and, and

judge, right, then hopefully it does pretty well at at

imputing the um uh originally missing values uh as well.

So our kind of um.

model here is just a denoising autoencoder.

Denoise encoders aren't generative, we said, right, they're not variational

autoencoders.

The way that we make it uh generative is not

to use variational autoencoders, although we do have that option.

Uh, we use this super.

Impressive, um, theoretical work by Yarn Gal, who's at Oxford,

who basically says, remember that thing called Dropout that we

discussed last week, right, it randomly turns nodes on and

off and it kind of forces the model to learn

multiple ways of, of describing the data.

But at test time you turn it off, right, so

that you've got the full whack of the model.

Why don't you just keep it turned on?

So now when you fall past data through at test

time, it's still randomly turning nodes on and off, and

so you're getting randomly different data each time.

So now my model with Dropout kept on at test

time, OK, is gonna be stochastic because maybe H1135 doesn't

fire this time, right, and that's gonna affect and cascade

down through the whole thing.

And this super impressive piece of work, which you should

definitely read, uh.

Basically shows that by doing this, by keeping Dropout turned

on at test time, by making it stochastic, you actually

learn these kind of deep Gaussian process models, which are

these kind of like distributions over the way in which

you can describe the data, not simply one kind of

mapping of uh data to outputs.

OK.

So we use denoising and auto.

Coders and we keep dropout turned on, which means that

if I shunt the same missing data through the model

multiple times, I'm gonna get different imputed values, different predictions,

and therefore I have exactly what I need for multiple

imputation, which is variance in the predictions so that I

can run multiple regression models.

This is Midas and in all its glory, right?

You can see it has this bottomic layer, OK, we

have more layers, you can change the number of layers,

but, but you know, you can see it's been squished

down.

Notice that we have originally missing data, but then the

data that we corrupt ourselves, and then what we do

is a benchmark against the bit that we corrupted.

Right, our loss is wide it said.

And really, it's just this value here against what we

predict there.

This originally missing, just so happens to be filled in,

right, because it's gonna try complete and correct all of

the data.

cos we boiled it down at that bottleneck and then

expanded it back out.

Does anyone have any questions on my whistle stop tour

through remind us?

OK, this is a very convenient point to just briefly

highlight the Adam optimizer.

Uh, because I decided when we, I was writing the

seminar for today.

That we are going to use it, so I might

as well give you a couple of slides.

Uh, when we were writing Midas back in 2019, 2020,

um, Adam was kind of like under threat from a

version of it called Adam W or weighted, uh, Adam.

So if you read our paper, you'll see me talk

about the weighted Adam optimizer.

It actually turns out that it really, it never really

won out.

Adam is, is just simply.

Fine, um, way better than gradient descent.

Adam W doesn't really do much.

It struggles to get support by, um, Tensorflow in particular,

and so I think people just kind of have largely

ignored it now and gone back to Adam.

But, um, the kind of underlying principle of both Adam

and Adam W.

Is that kind of, um, while gradient descent is absolutely

fantastic, right, as you have all asked me multiple times

in these lectures and seminars, how on earth do I

know what the right learning rate is?

And surely that this whole thing is kind of really

kind of contingent on whether I set that too big

or too small, and even if I cross validate it,

are we sure that's always going to be right?

Um, and so what some of you have already suggested

to me is why don't we kind of like slow

it down as it trains?

Right, so shrink the learning rate the longer you go.

Uh, the longer you train, so you're making ever more

fine adjustments to your thing, that's a great idea.

That's called creating a learning rate schedule.

But what Adam tries to do is basically bake that

in.

Uh, to the training itself so that you don't even

have to say after, say, 200 epochs, shrink the learning

rate, OK?

This is like automatically trying to work out how fast

or slow the update should be applied, given an initial

starting, uh, learning rate.

So it does it in two ways.

Uh, you're gonna have different learning rates for each of

the parameters in your model, OK.

So if things need to be trained faster versus others,

it can do that.

And it's also gonna try carry forward momentum.

So the question I had at the very beginning of

the lecture is like what happens if you're on like

a local minima, right, and it just so happens that

you bubble.

Well, one way you can get out of that is

like, like carry the momentum forward.

So yes, you might get to the bottom, and although

yes, now you might be on the other side of

the slope and really you should be going back, if

you've got enough momentum, you may dip down into that

global minima, right?

So all Adam does is try to incorporate those two

things.

These are based on other.

Extensions of stochastic gradient descent, so, um, RMS prop, which

is the adaptive learning rate, and momentum is the um

adding the momentum to gradient.

This is the algorithm, OK.

The crucial difference from stochastic gradient descent, right, notice it

still has parameters, notice it still has a step size

or learning rate, OK, but it has these two parameters

here, beta 1 and beta 2, which you have to

set.

And these determine the, the decay rate.

So given how fast I adjusted previously, how much of

that should I carry forward into my next update?

OK, so this is uh basically controlling kind of how

much momentum are we carrying forward with that ball as

it kind of like gets down to that global minimum.

So we start off by initialising two values, M and

V equal to 0, and T here is gonna index

our like steps, so our epochs.

And whilst we, we haven't got to where we want

to get yet, we increase T, right, so we're gonna

say now T is equal to 1.

We compute the gradient just like standard gradient descent, but

then we calculate this like momentum term, this is called

the first order momentum term, which is take the existing

term from the previous.

Uh, right, which for T equals 1 is gonna be

0, multiply that by beta1, beta 1 is normally set

at something like 0.9, so it's like take 9/10 of

the existing gradient.

OK, and then add on 0.1, 1 minus 0.9 of

the current gradient.

So you're only subtly adjusting the, the, the gradient with

this new information, right?

This is called an exponential decay because obviously as you

kind of proceed through one hour beta one is being

times by M1, right, and then so on and so

forth.

Increasing beta one here is going to increase the momentum

term, right, decreasing beta one will give more priority to

the current gradient.

And then this is the kind of bit where you,

you have to kind of just wave your hands a

bit.

This does exactly the same thing but for the squared

gradient.

OK.

And my understanding is that this term drops away pretty

quickly, but it's a pretty useful kind of um additional

impetus that you can add uh to the network.

Beta 2 is uh normally uh set to like 0.999.

Then because we initialised M and V at zero, that's

actually a form of biassing, uh, these parameters.

Um, and so you do this kind of like subtle

correction to the estimate of them by just taking the

value and dividing it through by 1 minus the um

the current um decay terms.

You do that for both.

And then you perform, look, you should recognise this very,

very quickly.

This is just your existing parameter value minus, right, it's

descent.

Your learning rate times by this term here, right, which

is just gonna be your kind of modified gradient, taking

into account your momentum and the squared um gradient.

And then this little epsilon here is just a teeny

tiny bit of random noise, like literally.

10 to 8 that just stops this being like divide

by 0, right?

It's just to to stop it from, from stalling, and

that's it.

So we're still doing exactly the same thing, this is

grading%, right?

But we just have a slightly modified update term which

is carrying forward the gradients from the previous epochs, which

should help us hop over local minima and arrive more

quickly at the global minima.

Notice that as you kind of go from a positive

to a negative slope, right?

Those, the, the momentum is gonna be carrying you this

way, your update is gonna be pushing back this way,

so what you should find still is it.

And then it comes back down, right, so it's still

gonna have this kind of oscillating property, um, eventually.

OK, we are nearly there.

Here's me just gloating.

Purple line is Midas, lower numbers better, OK?

M is way better, right, it's more accurate, right, these

models are just deeper, more complex, they can learn things

better.

It's more efficient, here's the time, right?

This was, it just ran for ages, OK?

Uh, and even though I didn't have any time series

components to this model, this is not a recurrent neural

network or anything like that, what we did here was

we kind of predicted um GDP over time, leaving out

an observation.

And it was really damn good at kind of imputing

that time series, right, you can learn it anyway, even

though I didn't say, by the way, this is pound

data.

Um, the fact that you can't really distinguish the um

triangles, which are our predictions from, sorry, which are the

actual values from the squares which are our invitations, it

suggested that this is working, like, uh, pretty well, OK,

this is just a voting.

Good, right, that's it.

So, uh, read this in your own time.

You can clearly see that it's gonna take you a

long time to actually get a good model, OK?

But what we've shown this week is that by just

changing the shape, by changing the kind of structure of

that model, you can get very different things from a

basic regression task all the way through to generative, uh,

kind of imagery and all of these things, merely by

changing the structure of, uh, your model, right?

I'll see you in seminars, uh, later today.

Yeah.

Mhm.

Yeah, I can't remember because I like a very big

class like.

---

Lecture 5:

Like the turmoil like mentally and emotionally, like things were

changing very rapidly as I was like making friends and

meeting everyone.

And now there hasn't been as much of a change

because like we're kind of setting our routine and so

the time just passes or something like that, yeah.

OK.

I'm going to Edinburgh see my brother.

Are you gonna?

Oh yeah.

I've known them for a while.

I like my friends from school.

That's my first time.

I'm just going to be better that time.

I didn't have my coffee.

Yes.

What about you guys?

Um, I'm going to Istanbul.

I'm excited.

Never been, so I'm having to do a lot of

Googling like the food is outstanding is just like like

street food vibes like restaurants like everywhere.

And I thought it also kind of didn't.

It was like $10.

but yeah, I thought it was gonna be a lot

more expensive the flights were not like the gorgeous like

13 pound flights.

You can sometimes find the flights were like a little

bit expensive, but my hostel was pretty cheap, and I

think once like 4 hours it's quite long.

It's a nonstop every time it's like everything is like,

oh yes, 1 hour everything's like um if you're probably

like just for like 3 full days.

So like we coming back.

Even having like the back to I don't know work

yeah no literally.

Which allegedly, yeah, I looked it up and it's supposed

to be like it's supposed to like snow on Saturday.

What?

I know I went to April and I, I, I

really expected to be over.

Celsius.

Yeah, Friday high of 2, low of -1.

I'm so I'm like I'm backpacking it and I'm like

I'm gonna have to strategize so hard.

I have to like pack all my slush up to

wear them all the oh yeah, just wear it all.

Um, but I think it'll be, it'll be fine.

I'll just be warm and we'll see what happens.

I'll try to be warm.

Yeah.

Oh no I'm like in the end.

Is it really?

I mean, you go as early as.

I think the overnight trainer and.

Yeah, it was an unexpected, but it's still nice, but

it's like it's because the diamonds yeah.

So what's your um how are you?

I'm good.

I feel like I'm so tired.

it's not like the last day you get used to

it like.

like yeah it's.

I don't know.

Oh yeah, I know that I, I didn't like I

I I I might.

So actually yes, how was the DNA.

Yeah I was laughing too and I was like yeah

well he he was like obsessed with.

I feel like.

I thought that was like.

and I was walking the street.

I, I have a friend uh in March, and so

I'm like interested to see how it's gonna go because

I'm like I can't like yeah yeah.

I'm sure he's lucky, but I was like, Oh no.

Oh my.

Yeah, yeah.

Yeah.

OK, then that's, uh, that's stop.

Uh, cool, so welcome back.

Uh, this is your last lecture with me, in fact.

Uh, you'll be moving on to.

Uh, pass it to me with, uh, Friedrich when you

come back from, uh, reading week.

Uh, so I thought I'd, uh, kind of give you,

uh, a bit of something different this week, uh, as

a, a fine treat.

So we're gonna forget about tapping the data for a

week, OK.

Uh, so no more columns and rows of numbers.

Uh, we are going to now think about working with

images, OK, and how to kind of.

How does the flexibility of the neural network kind of

schema allow us to handle things that would otherwise be

quite difficult to work with.

Uh, and in particular I'm going to talk to you

today about convolutional neural networks.

Um, so a little bit of today's lecture is gonna

be going through what a convolution is as separate from

how they're, uh, implemented, um.

In kind of predictive modelling, um, and I say application

handwriting recognition and image semantics.

We'll talk about image semantics more towards the end of

today's lecture.

The handwriting recognition we'll do in, uh, the seminar after

you come back, uh, from, uh, the reading week.

Um, handwriting is, of course.

Many of you have probably come across the Mist uh

data set, which is kind of that standard can you

predict the numbers 0 to 9, um, uh, from a

handwritten one actually that's quite useful in things like political

science, social science, often we're counting up vote tallies, comparing,

uh, handwritten reports against official numbers, etc.

to detect tampering and, uh, violations of electoral law, etc.

etc.

So, uh, as social scientists.

Before I begin, does anyone have any questions or, uh,

comments before we dig into images?

All good, all happy with kind of the formative.

What's expected, yeah.

If you have any questions, just shoot me an email

or reply.

Hey, on uh on the Moodle post.

Uh, OK, cool, so working, uh, with, uh, images, um.

I think the basic idea here and why we might

care as social scientists as opposed to say, uh kind

of more natural field, say you worked in a um

a kind of computer vision or something like that, is

that kind of we want to measure things about the

world, uh, but in fact, uh, and kind of your

training probably as a postgraduate student, uh may maybe even

as an undergraduate is to kind of try and quantify

as much as possible, to convert everything to a tabular

data set, uh, so that we can run a regression

or, or something more fancy, right?

Um, but actually that's quite hard.

Uh, our experience of the world is, uh, sensory, and

there may be things that we kind of see and

experience that it's very hard, uh, to put, uh, a

number on.

So, uh, anyone recognise the bloke on the right?

It is Guys, this was almost the Prime Minister of

the United Kingdom.

This is Ed Miliband from uh 2015, um, when I

was an undergraduate, and I, we were waiting for the

election results and thought, Labour's gonna win, it's gonna be

fantastic, and then Brexit wouldn't have happened and all things

like that.

Um, and then he ate this sandwich and the paparazzi

took a photo of it and it kind of derailed

the entire campaign, um, because this image really resonated with

people.

Um, it made him look, well, gawky and out of

touch, like he was trying to eat a bacon sandwich

and be a normal person, but, uh, struggling to do

so, right?

Um, and, uh, so he didn't win the election, but

also if you think about trying to like record this,

like what, what swings elections?

Well, apparently eating a bacon sandwich, um, and but that's

very hard to put into numbers, right?

Um, and I think when I was kind of putting

these sites together, that there's essentially kind of two principal

reasons why we as social scientists might be particularly interested

in images.

One is that there are dimensions of images or experiences

that we have in the social world, uh, which would

go unmeasured if, uh, uh, we try to convert it

to a number.

If I tried to kind of describe this image in

a, in a finite set of features, I think I

probably struggle to convey just how awful an image it

is.

um.

And the other thing is it's less reductive, right?

So even if I was to uh measure the cringe-worthiness

of this image, which is clearly 10 out of 10,

right?

uh.

I, I would be doing so much kind of compression

from the kind of that visceral kind of uh snapshot

to a single number that um we'd lose an awful

lot along the way and it might be that there's

important things, I mean it's just like, guys, it's so

horrible.

It's not only it's like how he's holding the sandwich,

but it's also dripping out of his mouth.

Yeah, right, like you need 1000 variables to describe just

how bad it is, right?

So two things, unmeasured, that some things might go unmeasured.

It's very hard to kind of theoretically think up all

of the features of this image that might matter, and

B, it's less reductive, right?

Uh, even if we do think about, like, we want

to measure how cringe worthy it is or how much

food is dripping out of his mouth, you know, it's

like putting that into a numeric form is, is difficult.

Um, so we might want to work with images, but

actually what I'm gonna try to show you today is

that kind of the particularly digital images that we're gonna

be um considering are actually remarkably similar to quantitative data

sets, right?

Even though there's so much emergent phenomena in that uh

image or captures uh many uh phenomena in that image,

right, actually, that is just a series of dots on

a.

Page and I can describe that series of dots, uh,

using, uh, numbers, right?

It's just that there's an awful lot of them uh

and they're arranged in very complex ways.

So we have lots of numbers and we have complex

relationships between them.

This is screaming, non-parametric neural neural network style, uh, modelling,

right?

Um, and the way that you, the kind of the,

the pivot that you need to make is to think

about these images as being kind of like just a

set of pixels, and those pixels have an ordering to

them, right?

So think, think of them like who painted this?

Well done.

Lots of Europeans in the room, right, this is, um,

this is kind of pointillism, right?

Lots of Ponteism, uh, lots of like individual paint dots

right in a row.

And each of those paint dots is essentially a point

of light that has a certain, um, shade, and in

this case, uh, colour as well, right?

Put lots of those points together, you get emergent things

like beautiful sailing yachts on lakes.

Much nicer than E Miliband's picture.

Right.

So that's all you need to know, right, we can

convert that kind of image, which is kind of so

descriptive uh into a quantitative dataset simply by kind of

thinking about uh assigning numbers to the individual pixels of

light, uh, that we um uh show.

So, uh, let's think, let's take an easy case so

that's like a grey scale image, right, so let's forget

about colour for a second, uh, and just think about

black and white.

Now typically.

A computerised or digital image that's in grayscale that is

truly grey scale.

Each individual point of light will just be a value

from zero, which means totally and utterly black, like the

pixel is turned off, to 255, which is absolutely white,

right?

So there are 256 uh values here if you count

0, and the kind of um intermediary values, so closer

to zero means It's darker and close to 255, uh

means uh lighter.

So, uh, maybe this is harder for you to see,

but I tried to zoom in on just a very

like teeny tiny point on this cool dog's head.

Uh, and you can see these individual blocks, right, which

are the colours ranging from literally 255, uh, probably to

about kind of a 100 there.

OK.

So this image as a whole.

It's comprised of a set of pixels in the X

direction, right, and a set of pixels in the Y

direction.

And this is a square image.

So let's say this is a 256 pixel wide to

256 pixel high image.

That means that there are 65,536 numbers here describing right

that image.

So even though we're kind of not thinking about tabular

data sets, you know, where columns are features and rows

of observations.

We still are really because this whole image can be

stretched out just to one long list of 655,000 numbers.

Happy with that?

Awesome.

OK, colour images uh are probably more uh interesting to

us, right?

It's naturally what we tend to take uh these days

when we take photographs.

Um, it's exactly the same, you're still gonna have numbers

from 0 to 255, but in order to kind of

represent the kind of, um, colour, right, we have 3

channels.

OK, so we're for every single kind of point.

There's going to be an amount of red, an amount

of green, and an amount of blue, and each of

those, right, is gonna be a separate value.

OK, so if you think about your kind of classic

RGB setting colours in R or or however you want

to do it, right, you set a value separately for

red, green, uh, and blue, and it's the combination of

those things that gives you, uh, the colour that you're

interested in, right, so orange or purple or um ochre

or whatever, OK?

This actually interestingly isn't the only way that you could

represent colours quantitatively, like this kind of three channel RGB

approach.

Um, but apparently the reason we do it this way

using RGB is because it's very similar to how our

eyes actually process, uh, colours.

We have, we're a trichromatic, uh, species, uh, which means,

uh, that we.

of detect.

Basically, we're, we're very good at detecting kind of three

separate frequencies of light which correspond roughly to kind of

primary red, uh, green, and, and primary, um, blue.

OK, so there's this, this kind of analogue.

Dogs are cool dogs like this are, um, biochromatic.

They're not actually colorblind, but they, uh.

They don't have the third receptor, so their, um, visual

field is more limited in terms of uh colour.

OK.

So colour images exactly the same as grayscale in the

sense that there's a number between 0 and 255, but

for each actual pixel of light to get that orange

or to get that purple or that um uh grey.

Or brown, right, you're gonna represent that with three values.

So take your kind of matrix of values for your

grayscale image and then stack 2 more layers behind it.

Each of those layers, right, is a um uh a

separate value uh for red, green, and blue.

So this is where it becomes a little bit more

complicated for us, right, because we're very used to working

with two dimensional data, where there's a certain number of

rows and columns or a height and uh uh width,

right, and grayscale images don't, don't, um, depart from that

kind of intuitive, um, structure.

But now we have this kind of stacking, right, this

depth, uh, to our images, uh, when they are colourized.

So if you see even a really small image like

an 8x8, um, bit map, right, that actually has, that's

actually a cube, right?

It has 8 by 8, that's the kind of actual

two dimensional size of the image and then you've got

to stack, uh, 3 layers, each one representing uh the

uh different colour channel.

And so notice that to do this we're gonna basically

have to use one extra position in our subsetting.

Right, so you're not used to seeing square bracket I

J co square bracket, right, that's the I row and

the Jth column.

But now we've also got the 1st, 2nd and 3rd

channel, which is a 3rd dimension within that kind of

um subsetting uh operator, OK.

Now this is just a generalisation, right, you can see

it going from vector where you just have I.

The Matrix where you have I and J to this

three dimensional thing, right?

All of these are tensor and this is what you

encountered in your first or your sorry, your second seminar

with me, right, this idea of converting things to tensors,

the nice thing about tensors is they can have an

arbitrary number of dimensions.

So you.

You'd have IJKL or IJKLM, right?

These are kind of 456, dimension tensers which are just

generalisations of uh the mathematical structures you're already very used

to, namely vectors and, uh, matrices.

Yeah.

Why not use like hex codes in this because then

you kind of just reduce it down to one.

Like, what's the benefit of RGB?

Um, yeah, that's a really good question.

Um.

I guess, I don't know, I don't know for sure.

My, my intuition would be that.

Hex codes are.

But more like categorical data and so.

Like measuring changes, you know, like going from dark, dark

grey to dark grey, right, which is just an increase

in, in, uh, you know, in a grey scale and

it's just the, the value, decrease in the value, um.

That's easier say than like working out what the uh

like the related hex values are, but maybe that's not

true.

I don't know actually.

I can look it up.

I mean this is, this is certainly the canonical way

of doing it, but yeah, I don't know why you

wouldn't use a a hex decimal system.

And I see your point, right, it's just a single,

you don't need the mixing.

Yeah, I don't know actually.

Oh, maybe, maybe, no, maybe it's because.

It must be this kind of idea of categorical stuff,

right?

Like you may be able to go from light grey

to a lighter grey quite easily by just shifting a

single value, but like, think about that, you know, the

colour pickers, right?

You can actually move in lots of different directions and

I imagine that that's not so easy to do varying

the hexadecimal.

Code, I don't know, ah, let me, yeah, yeah.

Any other questions?

We happy with tensor, right?

that would have more than 3 dimensions.

Yeah, movie.

4?

Yeah, right, you'd have, uh.

You'd have uh width and height, then you'd have your

colour channels, and then you'd have time.

Right, so you'd have a a a a frame, whatever

your frame rate is 30 per second or 60 per

second, that would be the 4th uh channel of your

um image, oh sorry, of your tensor there.

Right, let me just stop this from turning off all

the time, there you go.

Does that make sense?

Or like um.

I'm trying to think of something that isn't an image

that would have 4 dimensions.

Um, I can't think of anything off the top of

my head actually.

Um, maybe something like audio, cos that has, that has

a sequential element as well.

But then does that need, yeah, I don't know, I

don't know.

Um, I'm sure there are things.

Yeah.

Uh, so just to make sure 10 is basically something

like a a matrix and then 3 dimension.

No, no, actually tensor can be any number of dimensions.

But.

No, no, no, it, so it can be um a

a vector.

Is a one dimensional tensor.

A matrix is a two dimensional tensor.

These, these colour cubes are three dimensional tensor and and

so forth.

The tensor is a is a generalisation in the sense

that it, it can have any number of subsetted kind

of dimensions as it were.

Yeah.

OK, um.

So, uh, let me kind of give you kind of

a few, uh, kind of more concrete reasons why we

might want to um work with images in the social

sciences, given, given, um, what we've just discussed.

Um, so we're still going to work in the world

of classification here, right?

So there are lots of things that you, um, can

do, uh, with image data that we would need an

entire course to cover, right?

So like separating objects out of an image or kind

of, you know, you see the kind of CCTV and

then the green box and it follows the person around

the room and things like that.

Those are all things that you can definitely do, uh,

using convolution on your own now.

Works.

Um, but today we're going to focus on a more

general task of like given an image, what is its

label, right?

Um, so, uh, the obvious and canonical example is, um,

recognising handwritten digits, so chunk up a electoral tally, you

know, is there kind of, my colleague, uh, Johan Abach,

right, looks at whether there are kind of.

Manipulations on the final vote tallies versus what the individual

polling stations are returning, and they use handwriting recognition in

order to extract that information and compare, uh, the numbers.

Sorry, you had a question like that.

Yeah, I think that would be.

Um, Oh, probably 2 dimensional.

Yeah.

I think it's 2 dimensional because you've essentially got, you

know, your columns still.

Beaches.

And your.

Rose are time, right?

Or I mean you could represent it as three dimensional

by.

Having channels of stocks, right?

And then features of the mm, yeah, no, I think

it's two dimensional.

Yeah.

So like for that or is there like a better

method which is more competition.

Honestly, you could use a neural uh convolutional neural network.

I don't think you would.

I think you would use something that had some kind

of like um.

Like distinctly sequential aspect to it, so like a recurrent

neural network would be the obvious, I think, choice for

kind of time series stock data or something like that.

I mean, I'm sure there's a million1 RNN stock predicting

algorithms out there, um.

Yeah, I, I, I don't, I'm sure you can use

a competition.

I'm not sure you'd get the advantage necessarily, um, and

you'll see what you'll see why later on in these

slides, yeah.

OK, so, um, handwriting digits, that's, that's the canonical example,

that's what we'll do because it's a small and easy

to handle that like these are very small images, right?

Um, so we should be able to run it on

our own computers.

Other things include, uh, identifying violence, right?

Given a photo, um, maybe a satellite image, can you

kind of detect whether or not this is an area

where violence has recently um occurred and we've talked about

recognising um objects already.

Uh, you can think of like the kind of.

Um, global classification task of like, is there a bike

in this image rather than detect every single bike in

those images, right?

So, distinguish those two things, you might have an image

and ask it to find all of the objects and

label them in it.

Right, that's like a multi- object recognition system.

Or you might just have a question like, is there?

Is there a bike, any bike in this image?

Yes, uh, or no, that would be a more kind

of global classification problem.

Uh, that we are more familiar in the social sciences

with.

Um, here's a quote, follow the link through to the,

uh, article.

Uh, Michelle Torres, who is going to be visiting us

in April at, um, Palmouth, uh, which you should all

come to if you are around, uh, has lots of

work using kind of.

Image, uh, data for political science, um, uh, so yes,

come, come see her, give a keynote talk on probably,

uh, this stuff.

And as I said, you're gonna, you're gonna implement your

own handwritten digit model in in the next seminar with

me or Friedrich.

Alrighty, OK, so, um, actually, interestingly, what I want to

kind of contrast before I even show you how to

run a convolutional neural network or build one, is that

actually like it, it, although I said clearly this is

a task where a neural network should excel, you'll actually

find that even simpler or more kind of uh normal

um machine learning models can perform surprisingly well on images,

even though they have to make quite simplifying assumptions about

what's going on.

Um, so if you've got that kind of like 256

by 256 uh colour image, right, uh, you can kind

of like expand that out into this huge, uh, long

vector of numbers and you can basically imagine treating each

like pixel as a separate feature, right?

So there's the kind of value at 0.11 at 12,

1314 all the way through to 256, 256 and then

times that by 3 for all of the values.

Um, and it's totally possible to kind of use something

like a decision tree to kind of partition that data

based on those values, right?

Like it's not, uh, impossible to, to do that.

It might be quite slow because you've obviously got a

lot of features, um, to consider, but it's actually not

um infeasible, uh, it, it turns out, um, and.

Although you're really not kind of preserving anything about the

image when you do that, you're just extracting numbers quite

brutally from um the raw um data.

It can work very well.

So, um, random forest, for example, if you train it

on this MNIS data, which is the handwriting digits, can

have about 97% accuracy, which is, you know, pretty damn

good, probably as good as human, right?

Um, and it's learning nothing about kind of the spatial

correlations of the data.

It's got none of this kind of like feature extraction

logic that we're going to consider today.

So it is totally possible on um small images at

least to run regular methods and just rely on the

brute.

Force, there is some way that I can approximate the

kind of underlying relationships, even if it's really, really coarse,

right?

I just need enough compute or enough trees in my

forest, for example, um, to actually kind of get somewhere

close to approximating it.

But it, but it is possible.

Um, the downside comes when you're working with more complicated

images, so not handwritten numbers from 1 to 9, right,

but instead, um, images of conflicts or protests or, um,

faces, right, where actually the the the size and the

um detail in the image.

And probably the lack of contrast, right, things like this,

means it's gonna be very hard without any kind of

specific features to handle um.

Image relations, right?

Uh, it's just gonna really kind of struggle, right.

The nice thing about handwriting digits is, you know, you

write 9, there's this big bold black area and then

there's white, right?

There's just clear contrast, it's either 0 or it's 255.

And it's, it's easy to kind of partition those, uh,

that data.

But what if you've got lots of like muddy colours

or people wearing similar clothing or kind of this overcast

sky, those kind of things, it's gonna be much harder

to actually separate out the data between kind of like,

these are the bits of the image that are kind

of black, right, and these are the bits that are

white and, and for different numbers they're gonna have kind

of similar um.

Uh, representations.

OK, so that's why we might use deep learning is

not necessarily because you can't do it with other um

um models or model types, but because with complex uh

images, uh, it's just gonna be really computationally in feasible

to do so.

And indeed the really interesting thing, if you read about

kind of like the development of working with images and

neural networks, is that kind of the gold standard still

will be your fully connected multilayer perceptron, right?

Because of the, the, the things that we've discussed before,

namely the universal approximation theorem, right, we know that with

sufficient compute, with enough nodes and enough layers, we should

be able to approximate whatever function is driving the kind

of.

Um, systematic element of our observations, right?

So in this case, we should be able to learn

what makes the digit 1 look like a 1 or

2 look like a 2, etc.

just using exactly the architecture, uh, that we've learned in

the 1st 4 weeks of this course, right?

This is the gold standard.

The problem, of course, is that to get at that

kind of universal approximator, you would need so many nodes,

right, and so many um uh.

Uh, well, GPUs and um actual computers to calculate the,

um, weights and bias parameters, but it just, you'll never

get there.

You will never find the universal approximate because we just

don't have the resources, uh, on this planet, uh, to

do so.

So for example, just take that very small 256 by

256 image, right, that's, that's a very small component of

a website, right, that is not kind of like um

a very large image at all.

You kind of expand that out just to that input

vector, like we talked about in the last slide, so

you've got 655,000 input nodes.

And then even if you only connect that to like

a hidden layer of 128 nodes, because it's fully connected,

right, that's actually 25.2 million weight parameters.

Right, so just feeding it in to a hidden representation

that is very small, right, that is a massive amount

of compression.

It it takes a lot of, of compute, right?

Think about, um, Chat GBT does everything it does with,

you know, maybe 100 billion parameters, which I know is

a lot more than 25.2 million, but you can kind

of see that 25.2 million for just a single image

is, is getting like towards that kind of scale, right,

that magnitude.

Um, so it just, it is just kind of.

Too taxing, right?

And so lots of the kind of work on the

images uh within neural network science is how could you

get something that approximates what we would want a fully

connected multilayer perceptron to do, but given that there's just

absolutely no way that we have the compute power to

to train that um ideal model, right so.

So everything we're going to consider today is like making

simplifications uh to both the data and the network such

that we can kind of compute it in finite time

without losing too much information.

Yeah.

Is it a single pixel on the image or the

the image?

That's, that's the representation of one image.

So one image is one observation, right?

So.

How many pixels that image have?

65,536, so each input node is one pixel value for

that image.

Yeah.

So take, take the image, stretch it out so it's

one long line of numbers, rotate it, and that is

what you're feeding through the uh the network.

Any questions so far?

Yeah.

Uh, do 6, 5,536.

Times by 128.

Is that right?

8.3 million.

Uh, how did I get to that, is that?

Is it because Mm, I think I was thinking maybe

it's causes.

Times by Ah, there you go, OK.

Ah, that's a bit bad actually.

This is 65,536 input nodes per channel.

And we have 3 channels.

Uh, that's a very good question, yeah.

So 256 x 256, right, which is the size of

the image, gives me 65,536.

But actually there's 3 channels which gives me 196,000 and

then times that by 128 because there's one connection between

each of those channel values and the hidden layer, and

you get 25.1 million.

Yeah, it's the depth, right, the channels are the colours,

red, green and blue.

Um, yeah, good, um, I'll update that slide, thank you

for, for noticing that.

OK, so the two big questions are how do we

compress that model space, right, so that it's computationally feasible,

we simply cannot, you are not going to train a

25.2 million uh parameter model.

Remember that's not even a full model, that's just like

to get it into a hidden layer, you're then gonna

have want to cascade it through.

Um.

And the way that we can think about doing that

is kind of, can we share parameters?

So is there some way that kind of, we want

to do the same thing multiple times, can we kind

of reduce the number of actual weights we need in

this network, which is gonna be slightly novel for us.

Um, and then like we can think about the kind

of full.

Um, resolution of the image, right, which will tell us

some very specific things to go back to poor Ed

Miliband here, right?

You need a pretty precise, uh, high resolution image to

kind of like focus in on, uh, uh, the food

there, but there are other aspects of him that you

don't need that resolute poor Ed.

I really liked Adam Ed Miliband, I voted for him,

um, um, but, um, he's also now the Energy Secretary,

like he hasn't like gone away, he's quite an important

person, um.

But there are other aspects of this image of Ed

Miliband, right, that you don't need that resolution, like he's

hunched over like this.

You really only need like this outline and to recognise

like that, um, uh, my day to day posture as

it were, um, you don't need to kind of see

the kind of individual, uh, aspects of this, um, um,

oh, even that's horrible, isn't it?

And the flowers on the table and then stuff like

this.

So, um.

Is there, so basically we're going to try to share

parameters whenever we can, but we're also gonna try and

think about compressing that image so that at different levels

of resolution we can extract different features uh of the

image, uh, essentially.

So that's the first thing is it's literally about computation,

compressing it wherever we can.

The second thing is about kind of like, is there

some way that we can kind of force the model

to focus on like areas within the images like correlations

in terms of space.

Um, and so if you read, for example, this article

here which I link through.

And the bibliography at the end of these slides as

well, um, you know, you can train these neural networks

essentially to be kind of quote unquote translationally invariant, so

if I move things around in the image, right, the,

um, I don't know the, the, um.

A bit morbid, but like the bomb crater was in

the top left in the training image, right?

It should be able to recognise a similar type of

bomb crater, but it's now in the bottom right, right?

Those are the sorts of kind of um uh spatial

correlations that you might want to, uh, force the model

to learn.

That's very difficult for kind of this flattening approach of

a random forest, right, because it's, it's just a series

of numbers and you've got to really kind of hope

that it's able to kind of say, well, if I

partition on this area, then partition on it again.

But that might not work if, you know, in the

test image, the kind of exact same phenomena is just

in a different location, right?

You'd actually want to partition there now and, and so

that's very difficult for um non kind of spatial uh

spatially aware algorithms like convolutional neural networks, um, to, uh,

learn, right?

We want to be able to recognise objects regardless of

where they are in the image, uh, in other words.

And that leads us on to convolutions, yeah.

Would they looking for transitional the example of the images

dropping from his hand instead of his mouth.

We would, we would absolutely.

We want it to be kind of.

Able to kind of just notice things in the image

rather than to be like aha, in the lower third

of this image, this thing exists, right?

Uh.

We want it to be able to kind of.

We want the model to have the flexibility to light

up and kind of send a positive signal if it

detects, for example, right, predicting the cringeworthiness of an image,

right?

If it detects like food dropping, no matter whether it's

dropping from the mouth or the hands or, or, or

whatever, right, and that will mean in different um places,

um.

That's gonna be very hard if you flatten the image,

but it turns out that neural networks can learn to

do that, right, and be quite flexible about where things

are in the input.

And still give you kind of relevant um and accurate

output.

So we're looking for both or just like ingredients because

Like it would be better if we know the exact

location, but there might be correlation between the location and

the output.

Ah yeah, I, yeah, that second point is good.

I, I don't know actually if that is um.

Yeah, I don't, I, I mean they're almost conflicting, aren't

they, right?

Um.

And it, it may depend on the training data.

I mean, let me go and look into it more,

but I think it might like.

It Say you wanted an algorithm to do something with

um like yearbook photos or something, right?

silly example, but the the reason I'm thinking of it

is because everyone sits the same in a yearbook photo,

right?

There is no kind of like, no one's gonna be

like hiding in the corner of the image or like

facing the other way, right?

They're all kind of um front on, um, photographs.

And so in that instance, it may be that the

neural network is just like, look, I don't, I'm not

gonna, I don't need to be flexible, right?

If I see two eyes in the centre, right, and

that's, that's fine.

In more complex imagery, if your training data has things

in different places, well then it may learn to kind

of have that kind of um um flexibility to search

around uh the image.

So it may be a learned property of the model.

They're certainly not guaranteed.

I think this was in this article, um, and hence

why I make it italic here, then convolutionary neural networks

are not guaranteed to be translation in variant.

Right, they're not guaranteed to have this flexibility, um, but

they can learn to be this flexible, and I think

this is a correction in the literature.

I think it used to be assumed that neural networks

just were like flexible enough to do this.

Um, more recent literature is starting to say, well, no,

they're not.

There are examples where you train a model and it

doesn't have this property, um, but you can induce it,

um.

By like encouraging it to, to look around, right, by

having examples where the phenomena is in different regions of

the kind of image space.

Yeah.

OK, how are we doing for time?

Awesome.

So, some maths.

Uh, we are going to need to, uh, learn what

a convolution is.

Uh, in order to be able to apply it and

see why this helps simplify the compute space of our

model.

So let's just take two matrices of exactly the same

size.

These are 2x2 matrices, uh, and A1 through B4 are

just scalar values, OK?

Nothing particularly special here.

Remember that if you wanted to kind of matrix multiply

these two things, right, you take the first row of

the left hand matrix, you multiply the corresponding elements of

the first column.

Of the second matrix and then add them together, then

you shift to the second column of the second matrix

and multiply it with the first row, vice versa, vice

versa, right?

We saw this, uh, in, uh, week one.

Now the cross correlation, which we're gonna represent not with

a multiplier, but with this kind of asterisk inside a

circle.

It has similarities in the sense that there is a

multiplications and additions, but it's simpler in the sense that

what you do is you take the corresponding positions of

both matrices, multiply them together, so A1 times by B1.

A 2 times by B2, A3 times by B3, A

4 times by B4, and then you just add all

of that up together.

Right, so this is just another scale of value.

So whereas a matrix modified by B is a 2x2

matrix.

A cross correlated with B is just a scale of

value, right?

It's take the, basically overlap them, multiply the corresponding units

and then sum it all uh two together.

OK, so that's the cross-correlation uh operator.

Now, um.

This isn't technically a convolution, right?

A convolution is exactly this, uh, same thing, uh, but

before you overlap them to do A times by B,

you take B and you just rotate it 90 degrees,

right, and then you overlap them and do that.

So technically there's a difference between a convolution and a

cross correlation.

Cross correlation is just take the two things, overlap them,

multiply the units, sum it together.

A convolution, take both things, rotate the right one, overlap

them, multiply them together.

The reason you should just think of cross correlations is

because ultimately, this second matrix here, B, the thing that

you would rotate for it to be truly a convolution.

Right, it's gonna be learned.

So it doesn't really matter.

You don't really need to think about that rotation at

all because you, you know, you just, the numbers are

gonna change anyway because you're gonna learn them, it's not

a fixed thing, uh, essentially.

So, um, although it's called convolutionary neural network.

You should, I think it's easier to think about it

as a cross correlation, right?

You're just kind of overlapping these things and then doing

this reduction rather than also think about this rotation, because

that's really, doesn't really matter because, you know, be rotated

is just, you know, like that, and we can learn

it in either direction, right?

The model will just do that.

So think about it as a, uh, a cross, uh,

correlation.

You'll see what I mean as we get further through

these slides.

So that's very easy if you have two matrices of

exactly the same size, OK?

It's just gonna be a scale of value.

Um, now suppose that A is slightly larger than B,

OK.

What happens here is, uh, you start in the top

left hand corner and you overlap them.

So you've got B1 behind A1, B2 behind A2, multiply

some together.

And so that's exactly the same, or almost exactly the

same as what we had before.

But notice because we have A3 here, this is now

A4 and A5.

So we're doing A1 times B1, A2 times B2, A4

times B3, A 5 times B4, then sum all of

that together.

And then you just shift B along.

So now B is overlapping with A2, A3, A5 and

A6.

We multiply again the corresponding units and sum them up.

Now we've hit here but we haven't done in this

region, so we bring B down.

And we do the same thing here and then we

shift it a long one and we do the same

thing here.

So the cross correlation of a 3x3 with a 2

by 2 is gonna turn out to be a 2

by 2 because you're going to just scroll it along

and work out the corresponding um um sums, right, of

these, these um modifications.

OK.

So, yes.

If you go back one slide for the last one

there.

Why is it?

Yeah, good, good spot.

I will change that as well, um.

What's the problem?

This is A5 times B1, A 6 times B2, A

8 times B3 and we got A7, sorry.

Uh, it's a real pain writing the stuff out in

later, um, but yes, that's, that's an error, I will

change that.

OK, so let me just kind of run you through

that again.

We start top left.

OK, those are the corresponding elements in A, we multiply

them by corresponding elements in B.

And we pop them in the, the top left position

of this new matrix, then we move along.

We do the same thing.

We move along.

And then finally we shift to that last one.

I got it right this time.

Uh, A8 and A 9 there.

OK.

Are we kind of happy with what's going on here?

Awesome.

So this is just a cross correlation, right?

Notice that you take a bigger matrix, and if you

have this smaller second matrix and you cross correlate it,

you're reducing the size of the thing.

That's gonna be very handy.

This is how we're gonna do the kind of feature

simplification of uh a um.

Uh, competition we work.

Now, um, it's not quite as simple as that, uh,

because obviously what happens if like the size of B

changes, or indeed what if A is massive, we can

actually kind of do very different things.

Um, and so basically there are two main properties of

the cross correlation that you need to be concerned with.

The first is what we call the window size, and

this is essentially how big B is.

OK, so, uh, if B is larger, clearly you're gonna

be doing fewer shifts around your data, right, if they're

the same size, there's only 11 cross correlation operation you

can do, and that is just a scale of value.

However, if A was 10 by 10 and B was,

uh, you know, 2 by 2 matrix, then you, you're

gonna do lots and lots of skimming across this, um,

first matrix.

OK, so that's the window size.

It's basically the size of uh B.

Um, And then you've got this thing called stride, so

we've been using a stride of one.

Because basically that's all we can do.

So we start here and then we just shift along

one unit, and then when we hit the edge, we

come back and go down one unit and then again

we go right one unit.

But again, if A was larger, say A was 4

by 4, well we could go 1, 11, 111, or

we could go 1 and shift 2 to get the

other two elements, then come down and do the same

thing, right?

So a bigger stride.

OK, so in that case a stride of 2 means

that there's less overlap in the information used to, I

mean, in where you have a 4 by 4 cross

correlated with a 2 by 2 and a stride of

2, there is no overlap in the information, right?

You're kind of looking at separate regions of A.

With a stride of 1, actually, you know, you're using

the, the second column, first two rows twice there.

And then it overlaps again.

It's only the edge ones that only get used, uh,

uh, once, uh, for example, yeah.

Where do you denote that stride?

Like, without looking at the what it equals, how do

you know that the strike was one and the one

that we did?

These are, oh well, I know it's one here because

I mean we are literally just moving one position.

I actually can't have a stride of anything other than

one here causes a stride of 2 would be like.

8689 and black coming out of the matrix, but actually

in general when you have very big images and you're

not constrained in this way, the stride is a hyper

parameter.

It's something that you choose.

Yeah.

I stride in both directions or is it like moving

like this?

So suppose you have 3 columns and you're skipping by

2, and is it 135, and then it comes, or

is it 116?

Like, uh, I think it generally it will always go

scan horizontally, then reset and move down the stride.

The stride shift, right, so the amount you're moving left

and right and up and down will be the same,

normally.

You won't have a separate.

Unless there are more advanced things that I'm unaware of,

I don't think you would have a separate stride value

for like the X scroll versus the Y scroll as

it were.

Yeah.

Yeah.

And is it possible to have a strike?

Yeah, yeah, you could, so if you had like a

4 by 4 and a.

Uh, yes, I mean, if, if, if you had a

4 by 4 image, a matrix, and a 2 by

2.

And like let's like 55, so like I will only

like the yeah, you absolutely could with a stride of

3, yeah, yeah, yeah, yeah, that's possible.

Probably inadvisable, right?

Because you're deliberately deciding to ignore.

In your 5 by 5, the central cross of your

image.

But maybe, maybe, I mean it could be um it

could be useful, um.

Uh, I mean, the kind of silly example would be

taking lots of pictures of windows.

You just don't need to know anything about the frame.

You just want to see what's through the side of

them.

Um, OK, so that's the, that's the two important, and

these are hyper parameters of the model.

There's the window size and there's the strides.

Um, you'll often see the B matrix referred to as

the kernel, OK?

Uh, and it's a kernel because it's going to do

some very particular operation, uh, on, uh, the data.

So these are silly examples like these are just kind

of arbitrary matrices.

But if we actually set specific values of B, the

kernel, we actually get quite interesting, um, uh, changes to,

in this case, our images.

Um, so, another name for it is the philtre, and

there's just a million names for it.

Kernel philtre, the kind of two ones that we'll refer

to in, in this course.

Um, if you set these values very deliberately, well then

you can actually do things like feature extraction or feature

reduction on whatever A is.

So for A, for us, A is going to be

an image matrix, right, tensor, uh, if it's, um, uh,

colour, um.

And so we can, what we want to do is

kind of work out what B should look like such

that it does uh kind of interesting things to maybe

isolate kind of the outline so I can look at

the hunch of Ed Miliband or like sharpen it so

I can really see the, the flecks of bacon falling

out of his mouth, kind of things like that.

Um, so believe it or not, uh, this, uh, kernel

here, where you have zeros in the corners, 5 in

the centre and a minus 1 on the crosshairs, uh,

will sharpen your image.

OK, because if you think about what it, what it's

doing here.

Uh, it's kind of the, the very centre of the

focus as you kind of stride this along your image

is being magnified.

You're ignoring the soft, fluffy bits around the edges, right,

and you're you're sharpening the contrast.

For the surrounding bits of the of the cross, right?

Uh, because you're gonna do 5 times the centre bit

and then add in minus 1 times the values of

the other things, right, so you're gonna kind of, um,

reemphasize the centre of that image, yeah, go for it.

Uh, when thinking about knels, I'm, I'm aware of that

terminology when when thinking about the select start.

The same behind it that sort of use some additional

information to or is it just I'd never really thought

of it in terms of like Python kernels.

I don't know actually.

I mean like kernels, I mean we have like experts

in kernel science here at the LSE.

I'm never really too sure what the more general um

concept is.

I mean, I can look that up, and maybe that's

helpful to adds inside as well, but um I've never

thought about it in terms of the Python kernels, um,

but, but maybe they are connected.

Um, yeah.

Just to clarify, and then our A matrix is the

like colours of the individual pixels.

Yeah, yeah, yeah.

Think about it grayscale just for the moment, so you

don't have to think about the depth, right, but it

is the image, it's the pixel values of the kind

of grey scale images as well.

OK, so that one sharpens an image.

If you want to blur an image, uh, this is

called a box blur because all the values are the

same.

Uh, you just take 1/9 of the whole thing.

So you're basically taking the average, right, this is just

the average of the pixels in that area, right?

Hopefully that intuitively that that is gonna look like a

blurring.

OK, yeah.

Wait, sorry, just to repeat the a matrix in this

context, like each value is like an individual pixel and

so like that 0 is being applied to one pixel

negative 12.

OK, so it's like regionally sharpening.

Yeah, yeah yeah yeah yeah.

And I wonder if I can open this here.

This is a really good website if uh you want

to play around.

With an A, we can do it.

OK, so here's here's a, here's a big A matrix,

right?

These are grayscale values uh for this image.

The nice thing about this, this is our sharpen.

OK, uh, and you, this is what happens, right, if

you apply that to that, you kind of get this

emphasis of, of contrast.

Um, the blur, I think they use a slightly different

blur.

This is, this is a Gaussian blur.

Not that you've kind of got like the bell curve

there and there and in the, in the corners, right,

so but it essentially it's just a weighted average of

the.

The pixels in that area, right, there are other ones

you can do, uh, so an emboss looks like that.

Uh, and to get the outline, for example, right, you

basically do something very similar to sharpening.

But you're really looking for the contrast, so magnify the

centre and then basically deduct the values from the surrounding

area.

So this is gonna be largest when uh there's a

big um.

Stark difference between kind of er the the centre and

the the periphery of your kind of um window, as

it were.

Yeah, isn't the sharpening like for any anything that we

choose here, isn't it arbitrary as well?

Because we're choosing, for example, to sharpen arbitrarily socially.

Yeah, you're just, you're just portraying yourself as a true

deep learning engineer at this point because yeah, I mean,

it is arbitrary, right?

So what do we do?

We learn it, right, this is just a learnable parameter.

If it needs to sharpen it will sharpen.

If it needs to emboss, it will emboss.

We don't need to set this.

These are just, these are numbers, no, we can learn

these numbers.

That's, that's the whole point of the convolution of neural

network.

You should be confused, right?

We're never, we're, we're not gonna wanna deliberately sharpen, right?

That's far too old school.

We want to learn whether we need to sharpen or

not, and it will decide for us inductively.

Yeah, let's say, for example, a pixel that we want

to sharpen isn't char.

Just because the stride did not catch that pixel.

Oh, I see what you mean.

Well, in that case you just need to make, if

you really want to sharpen every pixel.

Then you have to make sure that your stride is

equal to 1, right, so that every pixel is at

the centre at some point.

Uh, I can't, I can't adjust the stride here, otherwise

I, no, I can't.

Otherwise I would, but um, yes, you're absolutely right.

If you really do genuinely want to sharpen something, right?

And you want to sharpen every single aspect of that

image, you would have a stride of one.

Right.

You'd only do a stride of two if you, you

really kind of want to simplify the image quite considerably.

um.

But then you probably aren't sharpening, right, you might want

to be blurring but not sharpening.

Yeah.

Can we have a matrix of like the same values?

Yeah, uh.

Uh, do any of these, not an outline.

Identity?

No, 00, that's interesting.

That does nothing.

Um, you, yes, you can.

I think I saw it.

I can't remember what it does though, um, but you.

It's really annoying you can't edit these, that would be

really cool.

um.

What would it do?

It would, well I guess that would be lightning or

darkening though.

No, it wouldn't, because.

Yeah, that's actually really instructive.

OK, so why does identity work?

Why does it not change the image?

Well, because you're adding, you're only kind of taking that

pixel value, right, the centre, and returning it at that

point.

So with the stride of one, this is just gonna

return exactly the same thing.

But you're saying, well, what, what if all of these

are ones?

So what you're basically doing there is the sum.

Which words It would be like the averaging but without

the averaging like the the the division.

Um, component of it.

I'm trying to think when that would make it really

bright versus really dark.

It would make it really bright if you have areas

where.

That Oh Would it be like a, would it be

like the highlights shadows type thing, right?

Where, because, you know, the big numbers are going to

be where it's all bright already.

So, and then it will really amplify that.

So it would like really bring out kind of the

sheen of light on your cheekbones or something like that.

Um, and if you have negative numbers, maybe that would

darken it, so it would like really emphasise areas that

are dark.

I mean we can play around with it.

I can't do it now, but um, it will do

something, yeah.

Yeah.

Yes.

So, on the bur that you have, though, they're all

the same, which I think is kind of what you

were just saying, like, their blur makes no sense to

me, but this seems like you're just taking, like, you're

not doing anything, you're just reducing the whole image.

Sorry, if you have the same number for all of

them, are you not just like changing all of the

pixels by the same amount?

You are, but I think the difference here is that

like this is like.

If you were to write out this calculation, right, you're

doing 19th of this, 19th of this, adding it all

together, which is the same as literally taking the the

mean of the pixels.

I think the question is when it's not equal to

the mean, so it's not something to 1.

Would it apply like a highlight or something like that,

because the division here I think is quite important to

get the blur.

Um, uh, maybe in the break, I, I mean, I,

I blur the dog image at some point, so, um,

maybe I'll try with a kernel that isn't, isn't, uh,

kind of, um.

Doesn't have this sum to one property.

Yeah, yeah, so this is the kernel is an operation.

It's performed on 9 pixels.

OK, I see.

And then the, the one we're highlighting is just the

one in the centre of exactly, yeah, yeah, yeah, yeah,

yeah.

So on the blur, right, you'll see this most clearly.

On those aspects where.

You know, like round here.

Ah, that's nice.

It shows you the, yeah, that's what's going on here.

So actually using kernel does not change the number of

pixels between input and output.

It only changes values.

That is a very fantastic point.

What can you notice between the input and the output

image here?

If you look at the input versus the output, they're

both the same size, right, but what is slightly different

about the output image?

Yeah.

So normally what you do is you, you pad your,

it's called padding your image.

So do you see that I can actually put that

like here, for example, what's going on is it.

It looks, you add some zero values to these 6

hypothetical 6 squares there, so that you can calculate a

value for the centre of those things.

And hence you get this blackboard around the edge.

This would actually reduce the size of the image, right?

The reason it doesn't is because you pad it so

that when I want to calculate this original corner piece

here, which technically you can't do because there are no

matric sizes here, right?

You just put zeros.

Right, make it black and hence why you get black

frame at the thing cos you're basically, I mean these

are dark anyway, right?

But then you're taking the average of these truly dark.

Absence of colour.

But still the number of outputs is the same but

just that simple.

Um, Is that true?

No, because technically you can't do that one.

So, so this, this is technically the first.

Uh, place you can Put the window, right, the 3

by 3 windows, it's here, so this is actually the

centre.

So technically we lose that whole row and that whole

column.

The only reason we don't is because we pretend there's

another column here and another row above.

Actually this, this size 3 window.

Does perform a small reduction, right you can see the

reduction is basically losing the um yeah, exactly right, so

this will reduce the size of the yeah, this has

gone from a, I mean what size is this?

Is this a I don't know how, is that 32

by 32?

So that's actually a 31 by 31 image now.

Or actually 30 by 30 cos there's one right here,

one right here.

But yeah, it is reducing the, the image ever so

slightly.

Yeah, if you're adding like an extra layer and how

is it reducing, isn't it increasing technically because they're going

from 32 by 32 to 32 or 34.

No, because you would never, um, these are, these are

really good questions.

You would never ever centre.

So here's my window, right, uh, I can calculate that.

I want to calculate one here for the bottom left

hand one.

I'm not increasing it because I, I wouldn't calculate this

one.

Right, where the centre of my window is actually outside

the original image.

So I can preserve the size, I can't, I won't

increase the size.

The convolution is reducing the image, you can see that

by the kind of black border around these things here.

Uh But we can maintain it technically by adding padding

so that we can actually calculate the kind of outside

edges like this whole edge here with the window looking

like this, but not like this.

But you want the centre to still be within the

image.

OK, so sharpening, uh, blurring, um, more er relevant for

us probably, right, is that edge detection, right, this is

something that we might expect a neural network to want

to learn how to do is kind of like.

Abstract out all of the detail and just find the

interesting bits of the image by looking at the edges.

So you can do this by, for example, amplifying, The

centre Uh, and deducting the values, right, so this is

going to be largest when there's big contrast between that

centre value, so you're gonna make it very big and

then deduct the values of the surrounding, uh, pixels, right?

So you want these to be very small and these

to be very big, i.e. you want this to be

white and these to be black, right?

That's when you're gonna find an edge.

Uh, essentially, OK, so if you take A and cross

correlate it with this edge detection kernel, this is what

you get at the end of it.

OK, just the outline of, uh, the image.

What that actually looks like under the hood is this,

you have these grayscale values.

And then, for example, if I kind of put this

in this top right hand corner here, this is actually

the kind of nose of the, the dog, right?

So these values are very dark, that's the background.

This is where the light is casting off the dog's

nose.

And so, sum up all of these values and make

them negative.

And then multiply this one by 8, so you've got

800 minus this, and then you get this kind of

248 rate, and these are gonna be zeros because there

is no, um, edge here, there's padding.

OK.

So that's how this is working, so you gotta think

about the difference between brightness and darkness.

Um, yes.

So its negative.

So this absolutely this, this, um, point here is that

yes, if you, you will technically exceed this range all

the time, so what we do is we clip it.

So we say it can be no less than 0

and it can be no greater than 255.

And in fact it has to be an integer, but

you can't have a pixel value of 244.7, that would

be 245.

Um, so you just at the end of that calculation.

Put it between 0 and 255 and an integer value

round it and then make it 0 to 255.

And then, right, how do we know which kernel to

use?

We, we, we simply don't.

And maybe it's not just one kernel that we need,

right?

Maybe we need multiple kernels, maybe we need different kernels

depending on different resolutions of the same image, right?

So the kind of beauty of the convolution of neural

network is it makes that decision for you.

Right?

You just say, I want a kernel of size B

times B, right?

It's actually B times B times 3, if we're working

with colour images, right?

Um, and then you say, if you need to learn,

right, the edge kernel, because that helps reduce your loss

at the end, go for it, that's what you should

learn.

But I'm not gonna tell you it, I'm not gonna

instantiate it as an edge kernel.

I'm gonna instantiate it just as random numbers.

And then you're going to change through the same bat

propagation technique to find the, the maps that matter.

Yeah we have one single value like the thing that'd

be far too easy, right?

Like if you want to sharpen one edge and blur

the other one, we just have 1 B1, B2 B3

or is it we are gonna have multiple kernels, OK,

so we're gonna have multiple, uh, different transformations that you're

gonna apply to the same image.

They're gonna generate different versions of the image, and then

you're gonna have multiple kernels that you apply to those

transformations, right, and then from that cross correlate that set

of cross correlations, we'll notice that's getting bigger and bigger

and bigger, right, we're gonna apply more kernels, again, we're

gonna learn them, sorry read because you're not gonna be

like edge detections, there's gonna be some weird acky hybrid.

And what you'll see is that these images are gonna

get more and more abstract because they're losing resolution, right?

Uh, and so what the kernel can actually learn from

the image will change, and so the kernels will change,

and we'll have this whole raft of parameters.

But crucially, like, these kernels, right, uh 3 by 34

by 4, they're not very big.

So the actual number of weights, even if we have

16 kernels, right, is only 4 times 16, it's not

25.2 million.

Right?

So we, we're still totally reducing the, the parameter space

relative to the fully connected model, yeah.

In this part of training, what is it comparing it

to in order to learn?

Yeah, this is supervised training, right?

If you're saying this is the diggening.

It's comparing it to a label of 9, so you'll

have a multi-class classification at the end.

So the end of this model is still 9 nodes,

which you're soft maxing over, and you reward it if

it's applying all of the output to the 9th node,

right?

And it is indeed a 9, and you punish it

every time it goes to things, and then you're just

gonna backpropagate in exactly the same way that we've been

doing through the course.

Nothing changes there, right?

The computation graph looks a bit weird macky, but it's

still just a, a series of nodes that are connected

and you just shunt that loss back in exactly the

same way.

So we'll put some multilayer perceptions not between at the

end actually and a layer.

Yeah, so we're gonna do loads and loads of convoluting,

right, uh, convolving, convoluting, uh, and then we're gonna have

a little mini feed forward bit at the end that

then collapses it down to our 9 out no or

or whatever we want.

You can do this with regression, right?

There there's nothing to stop you doing this with a

regression task, right?

Estimate the GDP of this area, right, where the area

is a satellite image.

You know, if you know the GDP for that area,

there's no reason to think you couldn't predict GDP and

what my student did last year for his um thesis,

right, use CNN's to predict.

Ukrainian GP GDP for like regions because obviously at the

moment it's incredibly hard to get um official numbers.

Any other questions?

OK, right.

Let's have a break then, uh.

Come back at 15 the hour and then I will

show you how we actually build these things.

No, so, um, just to show you.

Actually, I could have intuited what happens if you have

all negative values for a kernel.

If you have all negative values and you sum them

up, you're gonna have a negative number, so it gets

clipped to zero and it, it turns the image black,

right?

So this is like a blackout.

However, I think, um, sending it equal all equal to

1, not 11, but one is more interesting.

Uh, here, alright, it's gonna take me a couple of

seconds to update all of these.

Um, Here you don't quite get white out.

But what you get is some very harsher.

Highlightings, right?

I, it was almost what I was thinking.

It's not quite kind of just emphasise those areas that

are really really kind of bright.

Um, you're really kind of like adding masses of exposure

to which essentially, right?

Um, so you are merging them, but you're also kind

of like making them, um, much brighter.

Um, so that, that's what happens there.

Um, this might be useful, right?

This does at least, I mean this is, this is,

well this is finding the shadows, right?

Yeah, these are the only dark bits of the image,

right.

So if you did need to, I don't know, look

into the shadows or something, this, this is the way

we'll see the shape of the shadow, right?

I don't know, maybe you want to work out what

time of day it is, right?

Then maybe it would learn something like this where it

just removes everything but the shadows, uh, so that you

can kind of see that more clearly.

Anyway, that's just for interest.

This was, I googled like visualise image kernels and this

was like the third option, so just play around with

that if you want to do it.

Let's talk about actual neural networks now.

So here's the pipeline, um, any convolitional neural network.

Uh, essentially looks like this.

You are going to, uh.

Have multiple kernels.

The number is, I'm afraid a hyperparameter, so something that

you have to set, OK?

And you're gonna apply each kernel to the image at

that state.

So at the very beginning of the network, when you've

just fed your image in, right?

And you had, say, 5 kernels.

You're gonna apply each kernel to the image and each

one will generate a copy of that image with the

kernel applied.

So you're now gonna have 5 images, 5 versions of

the image.

Maybe some of them will be sharpened, maybe some of

them will highlight the shadows, maybe someone will be doing

hybrids that we don't quite know how to describe, right?

But the idea is that you're gonna have 5 of

them.

We call those kind of um cross correlated um images

feature maps.

OK, so that's how they're described in the literature.

Um, and then because you have this kind of like

padding issue of like keeping it the same, um, dimension,

and normally you do pad in a cross, uh, in

a convolutional neural network, um, to keep the image the

same size once you've applied to the cross correlation.

To actually get the shrinking down, uh, you do something

called pooling.

So you apply another um.

Basically, um, function that kind of reduces the dimensionality much

more brutally.

And there's essentially two forms of pooling.

Uh, one is average pooling.

So that's a bit like applying a box blur, um,

or alternatively, uh, you can take, uh, what's what's called

the max pool.

So given a window, what is the maximum value in

that window.

Uh, both of those will deliberately squish the image down,

OK, and you apply that to all of your feature

maps.

Now this pooling.

And we will come back to it in a second

again, but just to note, that doesn't really have any

parameters that you learn, OK?

That is something you just said.

You choose max pooling or average pooling, you're not going

to learn to do some other weird and wacky mixture

of the two.

This is just gonna be an operation, a bit like

the activation function is an operation, right, it's not a

um.

A learnable parameter.

So create a set of feature maps, Max pull to

shrink the image dimensions slightly, right, and then you're gonna

repeat this, OK, and just notice that as you repeat

this, I've now got 5 feature maps.

I might have another 5 kernels, so now I apply

each of my 5 kernels to each of my 5

feature maps to generate 25 feature maps, right?

And that's Paul or average pool them again, and I

repeat this a few more times.

Yeah, just saw the structure of the model.

It's all the convolutions sequential or can we make them

better and.

You actually think.

So you do one at a time.

Can you do 2?

The computer can do too, right, cos it can just

send it to different processing units.

Um, it will apply these in parallel.

OK, that's the same, yeah, yeah, yeah, yeah, is that

the more the more layers we have, the more information

we use because we're losing.

Right, so when you have a really high resolution image

at the beginning, you can extract quite detailed information, but

as you convolve pull, convolve, pull, convolve, pull, what you

get at the end of this are very blurry, very

small images.

Which to you and I may have no meaning at

all, right, but they may just highlight black against white

or they may have like just the outline of a

shape or something quite weird, and the computer can learn

from that, even if we would find it very hard

to learn from that.

But the idea is that at at different stages of

this.

Push of the data through, right?

You're able to identify different things because the level of

abstractions of the information is held in one of the

boxes like the edges we apply, isn't it getting worse?

Yeah, yeah, it is.

Yeah.

I I don't think there's any way of getting around

that if it, if it is at the edges of

the image.

I mean, the, you know, you could think of learning

kernel that.

You know, what if the kind of the, the phenomena

in question is kind of on the very periphery of

the image, it's like here's a hard it's cut off.

You could think of like a.

Uh kernel that sits here, right, and exaggerates the left

hand side to drag that information in, right, and so

you could preserve some of that by doing that, but

whether it does that or not, is, is kind of

inductive, you know.

Depends on the data.

Um, but you're right, I mean, there, there is information

loss here, right?

It's deliberate in a sense, you know, if you need

to learn things that require high precision, you have to

learn them in the initial layers of this convolution.

The, the kind of final layers are really just getting

at the, the broadest brushstroke.

Is it curved?

Is it straight, you know, it's not able to say,

is it, is it kind of like a jaggedy line,

is there fur?

Is there anything like that, you know, like we've lost

all of that detail by that point.

So we do this a few times.

This is gonna basically stretch our data to make it

kind of very thin, but we're gonna have, you'll notice

we're gonna get increasingly more feature maps, right, but of

of reduced dimensions.

Um, and then what you need to basically do at

the end is take this very long and thin set

of feature maps and just convert it into like a

normal, um, fully connected layer.

OK, so you kind of do this kind of rotation

to kind of make it fully connected, you maybe pass

that through another hidden layer just to allow it to

learn any kind of transformations uh in the standard kind

of feed forward way, and then you, you boil it

down to however many output nodes you need, given the

task.

So classification.

Be it binary or most class or regression if you're

doing something like GDP prediction.

So to go into each of those steps kind of

a little more um carefully, right, the idea of these

feature maps is that there's, as I said, like, it's

really hard to reduce an image down, right?

There's lots and lots of like information being conveyed and

those things are overlapping.

And so the idea of these feature maps is that

kind of you can kind of separate out that information,

use different kernels to create different versions of these images,

each of which highlights different relevant features that you want

to carry forward um through your training.

So by having multiple kernels that generate these multiple feature

maps.

Right, we can kind of map that kind of very

high resolution but singular image into lower resolution, but multiple

images that kind of emphasise the features that are relevant

for the task at hand, you know, the thing that's

going to help us predict or classify something um uh

very easily.

OK, so.

In a In the kind of dimensional sense of a

tensor, right, you're kind of a grey scale 64 by

64 image, so just ignore the depth for a second,

um, of, of a colour image, right?

You're actually gonna generate a tensor as a result of

these convolutions, which is kind of 64 by 64 by

M, where M is the uh number of feature maps

that you generate or the number of kernels you have,

right?

So kind of our um feature map set C.

Which indexes over the height and width of the image

first and then the maps, right, is the result of

applying to our input image, right, the cross correlation of

the MP map or kernel K.

Right, so we're gonna stack these together, that's why I

keep doing this and making them longer and longer and

longer, right, is because we're gonna stack these as a

depth channel uh in our uh convolution.

Once we've done that, once we have our M uh

convolutions, um.

After the input, right, we, we apply this pooling.

I've already told you, the two common types of pooling

are max pooling.

So again have a window with a certain size, and

then take the maximum value within that window and then

just shift it by the stride.

Um, or you do average pooling, right, which just takes

the average of the values in, uh, the window.

Now this is actually very similar to a convolution operator.

Uh, but it's not gonna be quite the same because,

uh, we are not going to learn the kernel, right,

so we, we're gonna have a very set, um, uh,

operation to do here.

So, and normally also the thing to to note here,

going back to your point about the stride, is that

with pooling, you don't normally overlap because you actually want

to boil the image down in dimensions.

So let's say we have a 4 by 4 input

image like this.

Uh, and we apply a kind of a pooling layer

of size 2 by 2 with a stride of 2,

so there's no overlap, right?

If we're doing maximum pooling, we start here, we look

at what the largest number is, which is 9, and

we return that in the corresponding position.

We're gonna stride to 2, so we're not gonna go

3631, we're gonna go 6415.

We take the maximum value.

We're gonna come back to the left and go down

2.

So now the maximum value is 5, and here the

maximum value is 8.

So this is the max pooling.

So you'll notice you've kind of, you've reduced the size

of this um matrix by 75% in this example.

Right, so in a convolutional neural network, although a convolution

can reduce the image dimensionality without um padding.

You actually reduce the dimensionality by using pooling uh in

a CNN, OK, so you try to separating out those

two things.

So, but first we take image image and and assign

like kernels.

So this is actually the 4x4 matrix.

It's an effect of the first operation, yeah, and then

like this is like a second action that we do.

So this is the feature map rather than the input

image as it were a feature that's the case feature

map.

Right, and we apply this max pooling across each of

the feature maps separately.

And as I said, we do this multiple times.

OK, so take that 64 by 64 grayscale image and

say we have 5 kernels in our first convolutional layer.

So that means our tender is gonna be 64 by

64 by 5.

Now let's say that we did that max pooling, uh,

with exactly the parameters that we just did, so 2

by 2 and a stride of 2.

So my pooling layer is gonna reduce that 64 by

64 by 5 to 32 by 32 by 5, OK,

and then.

I passed it through another set of 5 kernels, so

each of those 5 feature maps is has the each

of the next 5 kernels applied to it.

So that's 5 by 5 in the feature map dimension,

right?

And then we pool it, so we half the size

again, so now it's 16 by 16.

And then we do that again, so 25 times by

5 is 125, we half the size of the image,

etc.

etc.

So you can see that the image dimensions getting smaller

and smaller, but the depth, right, which in this case

is the number of feature maps, so the number of

representations of that initial image increases, um, quite dramatically.

Can we come back to the previous slide, So how

from this image we can have more than.

One reserve.

Ah, so you don't, right?

This is the case feature map.

There will be another feature of a map with slightly

different values cos it's had a slightly different kernel applied

to it, and we will pull that as well.

With the same values or the the not the.

Well, there are no values really, it's a function, right,

it's like, given that these values will change, take the

maximum of those new values and these ones here.

So I don't know how it's how the number of

these maps increasing if we have.

Yeah, no, that's a great question, right, so.

We're OK up to this point, right, we take our

64 by 64 image and we have 5 representations of

it.

That's applying 5 separate kernels.

So now you've got 5, essentially 5 slightly smaller images.

And then let's say we take another 5 kernels for

the next layer, you apply each kernel in that next

layer to each of the 5 feature maps from the

previous layer.

So the first kernel goes 12345, so that's 5 new

images, and then we do that another 4 times, so

you get 25.

That's, it cascades like that.

Yeah, so if you have 5 kernels, are you like

having a row of 5 and then like applying cooling

and then reducing it, or is it sequential or is

it simultaneous?

I It doesn't, uh, well, each kernel can be applied

in parallel, right, cos they're different operations or just copies

of the same data.

Um, but you should think of these as being stacked,

right?

They, they don't interact, you know, the, the pooling operation,

so you take your feature map, you apply your new

kernel, you can do that in parallel, right?

Uh, and then you pull each of those separately, so

again, that can be done in parallel.

It's only at the very end before you get to

the next layer that you want to stitch that back

together into a kind of three dimensional tensor that has

these kind of now 25 channels at the back.

Oh, sorry, yeah.

Um, how do you make sure that your kernels aren't

doing like the same thing?

Like the 5 different ones like.

Is it not the best thing that they all do?

Well, yeah, I mean, but you just kind of hold

your hand up and say but deep learning, right?

I mean, it's like um if if that's what it

needs to do, that's what it's gonna do.

I very much doubt it would do that, right?

But it's just gonna find, given the, the, the finite

amount of compute time you give it and the number

of kernels and the high parameters you set, right, the

best.

The best set of kernels.

And that's just gonna be defined as minimising the loss.

But you don't, there is no kind of um explicit

control to stop the kernels being the same.

It's just probably not very useful to do that.

Or not very efficient at least.

No, no, of course.

And each time you use the same 5 kernels or

no different each time.

Yeah.

Oh, so like each layer, like I know it's a

layer of terminology here.

Absolutely, but it's not as much as if you had

a fully connected space.

You've got in each layer 5 kernels.

And each kernel, um.

Is of a certain size, let's say, say it's like

a um I don't know, a 4 by 4 kernel,

right, so you've got 16 values in each kernel, so

16 times by 5, that's only 80 weights.

The size of the kernels of the can.

They can change between layers.

But I think it's quite unlikely.

I think that that's a a degree of freedom too

many, I think.

Yeah.

Sorry, the kernels, how does it initialise then?

I believe just the exactly the same way, like Xavier

initialization, just not exactly 0, but normally distributed around 0.

So applying cradles also reduce the damage.

I legit.

It would.

Except that we're gonna do the padding normally, the padding

ensures that the convolution itself doesn't actually reduce the physical

size of the image.

We're gonna do all of that through porting, right?

But what that really means with padding is adding black

borders each time.

Yeah, uh, the thing I don't understand is when we

apply the kernel, we're getting a smaller matrix.

Yes.

How does that impact the size of the image?

Just adds.

Black border to make it back up to the size

of the input.

OK.

OK.

And uh when we apply max pooling, uh, are we

choosing the pixel with the highest intensity?

Is that the yeah, exactly, you're choosing the brightest pixel,

yeah.

Yeah.

Was there a question?

No, because I mean that's kind of like as wide

as the image needs to be, right?

I can't.

I, I think slightly different implementations handle it slightly differently,

uh, but ultimately the kind of the logic is, even

though the convolution technically does reduce the dimensional reality of

the image, you just keep it the same size by

applying some padding, either pre or post convolution, and you

you you actually deliberately shrink the image through the pooling

operator.

Yeah, yeah.

Let me look into that more.

I mean, I'll find out before our seminar whether or

when, sorry, the padding is applied, whether it's pre or

post.

um Because it could be useful to reduce the dimensionality.

Yeah, for sure, um.

Let me look up that logic as well, like why

we, why we like.

Do that so deliberately with the polling when you, you

could just have like increase the stride of a um.

Of the kernels and do the same thing, right?

Um, so let me look that up for you and

get you a better justification of that.

Yeah I wonder why the number of cars stay the

same.

It's 555 each time.

Why do we keep it at 5 kernels?

I, I think because.

Um You don't have to, is is the first thing

to say, you could vary that.

It's probably empirical, but I imagine that the, the number

of kernels in general, sorry, is more important than the

number of kernels per layer.

Like I think it will extract more performance.

By increasing the number of kernels per layer rather than

by making there be more kernels in the 3rd, 4th

and 5th layers, for example, there's nothing to stop you

from doing that.

Um, it was still back propagate, it was still trained,

um.

You might think that as you would, I mean.

We're getting to the fringes of my knowledge here, but

you, you might think you might want to actually reduce

the number of kernels towards the end, because like, these

images are so low dimension at this point that like

surely how much can you learn from them?

I mean they're literally like, if you look at the

feature maps, they're like blobs, you know, they're just like

white and black.

Surely there's only 2 or 3 ways we can learn

from that, so maybe you want to shrink the number

of kernels there.

I imagine it's just really hard to do that to

actually kind of like tune your hyperparameter for the number

of kernels per layer, it just takes so long.

And it's probably quite Sensitive that it just doesn't make

sense in terms of the finite, like tuning time that

we, we tend to have a scientists.

Yeah, you said that there are like 25.2 million notes

in the system.

So, uh, in this case, does the computation part just

depend on the number of kernels, not on the like

if you have a 256 by 256 versus 2000 by

2,000%.

It won't affect like the computation.

It will, yeah I mean it will because you've got

to do more striding, right?

A larger image means so there are more operations to

be calculated, so it will slow things down, um.

But normally you're going to do this on a normally

any serious image work would be done on a GPU

where you've got tens of thousands of mini processors and

so you probably won't notice that um um increasing compute

time.

But that, but yeah, it does require more computations if

you have a larger image, yeah.

Um.

Um, Yes, and you're kind of stuck in a hard

spot, right, because even if you increase the size of

the kernel, so you're doing fewer computations, right, you're still

having to do the individual ones, so I, I don't

know what the trade off is there, but um I

suspect there is one, yeah.

OK, um, some other questions that you may have, er,

let me clarify, so, um.

Uh, and this answers the question that we've already been

asked.

So in each convolution layer you can actually have, um,

different numbers of kernels.

Uh, that is totally up to you.

I just suspect it's gonna be really hard for you

to tune that well, right?

It's easier just to default to 5 and then maybe

test all layers at 5 versus all layers at 10

and see if you get any improvement in performance.

I think if you try to fine tune the 3rd,

4th, or 5th layer, that means you're a full-time NL

engineer and you have the kind of capacity to do

that.

Uh, I, I highly doubt we would in the kind

of context that we're considering, um.

As we've already said as well, right, it's important to

remember that from the second convolutional layer, uh, every kernel

you have is applied to every feature map.

This is what stretches that depth channel, right, because you're

applying each kernel to multiple different representations of, uh, the

image.

So you're getting lots and lots and lots and lots

and lots of different feature extractions, um, as you get

deeper into your convolutional uh neural network.

The only complication that we haven't really done here, because

it makes my brain hurt at least, is that uh

we've been thinking about grey scale images.

64 by 64 times M where M is the number

of um feature maps, right?

Now, what happens if you have a colour image where

what you're shunting through is a 64 by 64 x

3 tenser.

Your first kernel will be 3 dimensional.

So your kernel will be, say, 4 by 4 x

3.

So you have a width and height of 4, as

we've seen, and then you have a a depth, 1

per channel of the colours, so red, green, and blue,

and you sum that entire cube up, right?

So, interestingly, when you um.

Squish this down after that first convolutional layer, you're gonna

make all the images grey scale anyway.

So if you look at your feature maps inside your,

and I will try and write you the code so

that you can do this in the seminar as well,

but what you'll notice is that even if you push

through colour images, your feature maps will be great scale.

Right?

But you can learn different kernels in that initial layer

that may exaggerate or downplay different channels, right?

Think about varying the values across the kind of depth

of this cube of a kernels, right?

Uh, so you can still represent the colours, but you

won't notice that you're representing the colours because if you

were to plot it, it would look grey scale, right?

Because you do this kind of cubic, uh, initial transformation.

I find that quite interesting.

OK, um, The last thing to know is that once

you've done this cross convolution, and before you do the

max pooling, uh, you do in fact apply an activation

function still, right?

We still want these nonlinearities, otherwise this is still just

like the linear combination of things, ultimately.

Um, traditionally, in the olden days, i.e. pre 2012, right,

you would have used, um, sigmoid activation functions.

Keep it between 0 and 1, these hidden states.

Um, more often than not now, and Alexnet is kind

of like was the breakthrough convolutional neural network in the

early 2010s.

Um, you use, that is the, um, uh, easiest one.

Again, it solves the problem the vanishing gradients that we've

talked about, um, uh, before.

And so you apply this activation after cross correlating, uh,

or convolving your, uh, input data with your kernel.

Uh, but before you apply, say, your, um, max pool

or your average pool.

OK, so just remember that there is an activation function,

uh, in there.

Hopefully you are kind of starting to realise that like

from a pie torch perspective, like if you implicitly will

think about how you're going to define this in your

head, given what we did in that first one, it's

actually not that difficult, right?

Like we're just going to define a convolution layer, then

define an activation function, and then define a pooling layer,

right?

And then we're just gonna tell Xs first the con

convolution, then the activation, then the pooling, right?

This, this is why i torch is so nice.

Um, and then, as I said, at the very end,

once you've kind of squished this whole thing down in

terms of the, the, the width and height, and you've

just got many, many, many feature maps, right, we need

to take this very deep but thin tensor, as I'm

saying here, and kind of interpret that as kind of

the input into a regular feed forward.

Fully connected network.

Um, and so what you do is you basically just

flatten it, right?

That same thing that I was describing when I was

saying, well, how would you do this with a random

forest?

Kind of take all those values and make them one

long list, right, we just do that, we flatten it,

right, we're gonna reshape it like we did in, um,

uh, last week's seminar to turn this maybe like 2

by 2.

But 3,125 size uh 3D tenser into a 2D tenser

of like, you know, 1 times 12,500, right, so it's

just a vector of inputs at this point.

OK, and then you pass that through at least one

fully connected layer.

OK.

This, the idea here is essentially that you kind of

like splitting out the images, extracting different parts of it

all into these things.

These are kind of represented as a long list of

things and then the final fully connected layer or layers,

right, allow you to make connections between them again.

So you kind of bring it all back together and

mix it together, and it's that kind of final mixing,

which is really important and allows you to kind of

get to the output there, yeah, of course you can.

Uh, how is the red function here, not acting as

the activation function?

Because when we cross correlate, the negative values will be

zero, right?

Uh, yeah, exactly.

So I mean, or, I mean, I don't know if

you even.

Because then, like the random function will output X if

it's positive, right?

Like will output the same value if it's positive.

Well, it will output any kind of like, I mean,

you shouldn't just think of the those kernels like setting

things to to black, i.e. to zero, right, sometimes they

magnify it, right, they make him um like 100 instead

of 50 or something like that.

And in that case Relo would return you 100 or

or or 50, um.

And so this, this can still have an effect, right,

but it's, it's gonna essentially perform the clipping, at least

the lower clipping for us, right, so any negative values

here will be just transformed to to pure black, zero.

At the other end, I actually don't know whether in

implementations they clip it after the convolution, or whether they

kind of leave it to be that, and it's only

when you try to draw it that it, it squishes

it back down and makes, you know, 259.

Pure white, um.

Um My question is, if we're not getting any negative

values from the cross correlation.

And so the other won't uh output zero for the

negative values therefore, we'll just act as the identity function

except that you could technically, I mean without clipping, you

could technically get negative values here.

OK, yeah, yeah.

Yeah, I mean, it's possible, right?

But when you want to represent that as an image

again, it just has to be pure black, it's like,

it's darker than the darkest absence of light, so we

just set it to the absence of light here.

Yeah, and Re's doing that for you.

Yeah.

Yeah.

But the cross convolution and convolutions in general do not

have to be bounded greater than 0, right?

That mathematical operation.

can be any number.

Yeah.

No, no, you're welcome.

All happy.

So what happened?

What happened with colours in that first input to convolution

layer.

Your height by width by 3.

Tenor is multiplied by a kernel, which is window size

in height and width times by 3.

Right?

So you're gonna overlay two cubes now, still multiply each

of the elements in each of the depth channels height

and width, and then sum all of those together.

That's gonna get you.

For each position that you stride, just one single scale

of value, so your three dimensional image after that first

set of convolutions will become grayscale.

Yeah, and how and how about output.

The output for our purposes is not gonna be generative,

it's not gonna be an inch.

It's gonna be like, is that the number 9?

Probability 0.8, yeah.

Um yeah.

But it's gonna be, I mean, you, you can, I

mean it's gonna be computationally way harder to output an

image, right, to expand back to 3 channels.

Mhm.

Gonna take a lot more computer to learn how to

do that, yeah.

Uh, the colour design that could be, uh, use.

Uh, like black and white, golden instead of, you know,

jy one.

Yeah, I mean, yeah, you could convert the image to

grayscale and then just um just have normal regular convolutions

first, yeah, yeah, yeah.

Yes, I mean, when, when, I mean in image processes

like Adobe Photoshop, right, when you black and white or

you increase the saturation or do those types of things,

you are in fact applying kernels, right?

It is doing a convolution or sharpen an image.

We are doing the convolution side of these things.

We're just not doing the connected bit to to get

to a regression outcome at the end, right?

There's no learning there, that preset kernels, so yes.

Tony a grey scale is applying a black and white

card, yeah, yeah.

OK, and then we have the output there, right?

So if we're doing classification, we're gonna have K nodes,

one for each possible class, unless it's binary classification, in

which case you can get away with just a single

node uh at the end.

Right, remember that your final activation function needs to range

match.

So Soft Max, if you've got multi-class, um, er, sigmoids

probably if you've got er binary classification.

Rely maybe if you, you're counting a probability uh sorry,

the number of occurrences or something, you can't have a

negative occurrence, uh, etc.

etc.

And as with any other deep network, right, and this

is why it's so important that you kind of learn

this stuff early on, right, we just use some form

of gradient descent, right, this is just a computation graph.

All of these parameters are have partial derivatives that we

can pre-calculate and store in a kind of list like

we did and then just shunt the losses through to

the uh child nodes, uh, etc.

etc.

OK, so nothing changes there, right?

It's really only this convolution and pooling operator that, that

is new.

OK, so convolutions have been around since the 1990s.

I think it's worth kind of like saying that up

front, like Jeff uh Hinton has, you know, papers on

this from the 1990s, and I almost cited that, uh,

as kind of the starting point.

But really, if you read about convolutions, they became popular

after this paper in 2012.

Uh, and it's not really a paper about like an

introduction to convolutions or CNN's, it's all about GPUs, right?

And so the breakthrough here was to show that when

you use, uh, GPUs rather than CPUs, right, so the

graphics processors, much smaller computers, but just thousands of them,

uh, you can actually get dramatically better performance on very

large data sets.

So whereas before you could really only do what we'll

do in the seminar, which is work with like these

Minist style, very small images.

They showed that this kind of um this form of

network could be kind of parallellyzed across GPUs.

So notice that they've kind of got these things stacked

here.

That's basically because they're sending the, the model across multiple

um GPUs, and they allowed them to train very performance

classifiers that were much better than anything else to date.

On images that were high resolution, so the ImageNet data,

these are, I think these are items of clothing.

Uh, so like, is this a, a handbag, is this

a pair of trousers, is this a, um, a t-shirt,

etc.

Um, they could do that on like much higher resolution

images in kind of like finite time, right?

Um, so if you're interested, go see this paper here

at Geoffrey Hinton, winner of the Nobel Prize.

I gave a open AI.

Guy, is he still there or was he ousted in

the end?

I can't remember, right, but notice that these, these people

kind of recur across like every single paper that will

um um study, hence why they keep winning the Nobel

Prize, etc.

But notice that like we start off with these kind

of grey scale images, right, that have a kind of,

this is their full resolution, then we apply a number

of feature maps, so we kind of pull and and

shrink it down.

And then we do it again, so that we get

more channels, these are more feature maps, right, it's now

5 times by whatever and they're denoting it 27 here.

And then they get wider and wider and wider, and

then you have this is the flattening, right?

This is where you go from your chunk of um

feature maps that are as thin as you want them

to just regular, fully connected, notice, fully connected deep.

Uh, layers, and then you have your final output.

So in this case, it's 100 classes, I believe, uh,

in the, uh, ImageNett, um, data set.

OK?

Read the paper.

I mean, it's a little bit kind of computer science

and boring, but, um, the contribution is, is, uh, incredibly

useful for us, I think.

OK, right.

In the 5 minutes I have left, uh, I just

have a few slides.

I wanted to kind of point out where this has

been discussed in an actual applied field, OK.

Um, so, uh, there's a paper in 2022, uh, again

by Michelle Torres, but not the same one as, um,

uh, where she kind of introduces CNN's to political science,

right, and she kind of thinks about various different applications

of this, including handwriting detection.

Um, but nicely, she has this discussion which you should

read on the limitations of CNN's for us as kind

of applied researchers.

Uh, and so she, um, kind of points out some,

uh, kind of both technical and also conceptual issues.

So the first is that, you know, these CNN's are

not guaranteed to be invariant going back to our original

point, right?

They can be, if you train them well, you can

have instances where, uh.

Uh, they can learn that it doesn't have to be

in the centre of the image for it to be

a bomb crater or, or whatever.

Um, but they're not guaranteed to do that, and that

can be quite complicated.

Um, so, uh, you kind of have two possible failed

cases here, right?

If most of your training data looks very similar, is

everyone square on at the camera smiling, and then I

decide to like turn to the side and I'm looking

down or something, it may really struggle to recognise that

I'm also a human or, or whatever, like the classification

is.

The alternative failure case, right, is that it might say,

oh well that's a face, uh, right, and it's because

it has all of the same features that it's recognised,

it's used to seeing, right, mouth, nose, glasses, uh, but

it doesn't recognise that something's gone seriously wrong here and

this, this probably isn't actually a photograph, right, this is

me trying to be funny, uh, in my final lecture.

um.

So this, so this can be a little bit of

a problem, right, and you kind of need to like

feel this out, you know, have some interesting images in

your test data where you can actually qualitatively assess whether

it's doing what you think it's gonna do, right?

You might have a 95% accuracy rate, but that's because

there aren't that many bu craters, thankfully, right, and, and

therefore it's just kind of being.

Boosted by the kind of nuller prevalence rather than its

actual ability to kind of recognise distributions in the image.

Now they say this is an issue with CNNs, but

it's, it's an issue with deep learning, machine learning more

generally, right, which is that you get a prediction out

of this bad boy, right, this is like 0.9 a

t-shirt, right, and you're thinking, great, that's a t-shirt, but

there's uncertainty there still, right?

That's that comes from the training data that you sampled,

the kind of uh the specific kind of like biases

that you've induced because of your choice of hyper parameters,

etc.

etc.

etc.

And the problem is if you then kind of shunt

that into a regular regression model, right, you're not kind

of propagating that uncertainty.

Right, so you're treating it like that is a t-shirt

at this point.

And the, the regression model is gonna go, thank you

very much, that's an observation, that's gonna reduce my standard

error, but actually it probably shouldn't reduce it as much

as it is because, well, maybe that's a, you know,

a a a shirt, not a t-shirt, or maybe it's

a blouse rather than a, a cotton t-shirt or or

whatever.

Uh, and so I think when you kind of have

these any deep learning model but maybe particularly CNN's where

you're doing some kind of classification and then you're extracting

that feature and putting it into a separate analysis model,

you have to really think carefully about whether your standard

error is too small, conventionally because you're not propagating that

uncertainty, and I think that's that's a big issue that

we're not very good at handling at the moment.

And the other thing is um.

You know, everything we've done so far is actually very

like mundanely similar in terms of target, right?

It's still give me the probability that this is a

t-shirt or kind of predict me the GDP.

Those are big important questions.

um, but what they're not doing is what maybe we

think like deep learning does, right, sometimes, which is, well,

let's just discovered that this is an image of kind

of like violence or like, uh, it's kind of extracting

those latent.

or dimensions directly, right?

Actually, we've had to be very specific, like tell me

it's one of these 100 things or it's one of

these 9 numbers, you are not kind of doing any

kind of like latent analysis, right?

You can look at these feature maps and look at

what it extracts by looking at what the kernel actually

yields, but that's qualitative, that's not quantitative, that's not actually

a kind of output of the model directly.

So those are kind of some um broad limitations of

CNN's.

So Michelle Torres I think actually is a little bit

kind of like, well, for, for classification tasks.

CNN's worked really, really well, but actually sometimes what you

want is like feature extraction rather than feature classification.

And so she presents this other kind of topic, which

she calls bag of visual words, which basically works by

kind of um and you should read the paper rather

than trusting my description, but it has um specific kernels

that extract kind of the most important points of an

image.

I notice that for each image it can extract multiple

points and the number of points can vary, and then

it basically clusters using very simple clustering algorithms.

Uh, the, uh, images, uh, or the points extracted from

those images, and then you qualitatively go through and you

look at those clusters and say, well, for an image

close to the centre of that cluster, what do I,

like, what is the concept being represented here, what's similar

between these images?

And then you can apply that to your whole data

set.

Uh, so it's a way of kind of like extracting

kind of cluster-based features.

Uh, using kind of image techniques, but not CNN.

So if you contrast these two methods, you know, this

bag of visual words is, is kind of very transparent,

right?

The kind of point extraction and clustering algorithms are dead

simple and easily to explain.

There's no where I'm gonna let deep learning decide that

bit of it, right?

Like it's fully transparent.

You can, you could write the parameters out in the

paper.

Uh, you know what you're doing and so you can

interpret the outputs quite easily, right?

But they're very inflexible in that way because there's no

learning.

If there's a better way of doing this, or actually

if there's something hidden that you, the human cannot detect,

but the computer can, right?

It's not going to get at that.

So CNN's conversely are very flexible, right?

And if it doesn't like the measure that bag of

Visual Words is using, which is Pixel intensity, it just

won't use it, it will change the kernel up, it

will learn something better.

But these witching apps are not going to be very

interpretable at all.

You may be able to look at some of them,

particularly the kind of ones in the early stages where

they're just doing some very basic convolution on the raw

image, but as you get deeper down, it's gonna be

very hard to intuit why it needs this particular blob

in this particular way.

But it does, right?

That is the best way of minimising um the loss,

right?

The other interesting thing here is that what we've done

with CNN's right, you get one output.

You know, there is a crater there or there isn't.

This is the number 9 or it isn't, etc.

etc.

With a bag of visual words, because you have these

multiple points, it could be that that image belongs to

cluster 3 twice or 3 times, so you can count

the number, right, or the extensiveness of a phenomena, right,

there are lots of craters here, right, each of these

visual points identified is a separate crater or or kind

of moment of violence.

Um, and so that's a, that's a distinction as well,

right, so I think you should compare and contrast, um,

those two things.

And that's really it.

Um, and, uh, true to form, I am running out

of time, right?

So I'm gonna leave you with the kind of this

big picture of like, these are just like so flexible

and you can see even like very minor changes to

the structure gives you very different models, right, from feed

forward through to auto coders, through to denoising and variational

things, generative.

All of these are deep networks, all of them share

this back propagation logic, but subtle changes to the architecture

that allow you to do very different things, and this

week I've shown you how to do it with, uh,

images.

Just remember that they're not always suitable, right, for the

social scientific tasks that we're interested in.

I would really strongly recommend you read, uh, the work

of, uh, Michelle Torres.

So, have a lovely reading week.

I will see some of you for one more seminar

in week 6, but don't be strange, it's week 7,

sorry.

Uh, don't be strange, just come see me in my

office hours and uh good luck with the exam.

I OK.

OK.

Like I I I invited to.

I just like I, I thought that was like.

And then I think she.

I was really I know I know it's like I

feel like I was.

a lot of time I think it's it's, it's mostly

like I I don't you know she's just.

---

Lecture 6:

Everybody internalise that I think.

I think most people have a deadline, is there a

big deadline in methodology?

Oh, I see.

I it's just gonna be an empty class.

What's the deadline?

I.

Oh.

Maybe yeah I heard people.

Oh yeah There's Of course.

Oh yes.

Yeah.

I took, I took the machine learning class last year

and yeah, I just kind of writes out the algorithms

on um, I see, so there was no.

I see the ideas are pretty good.

Do you have any anything in your head or not

yet.

Well, I think that, OK, actually it's a one website

called Paper Swift code.

And there is like uh there are like scientific articles

like group different topics like topic modelling, I know, um,

sentiment analysis, but they are like very legit scientific articles.

Paper with code.

I can send you.

Huh, yeah, papers with codes.

Yeah, it's worth checking.

It's like, and they're like super clearly explained and each

paper has attached codes, so.

Yeah.

Hey, how are you?

Oh, a lot of work for my super relax, to

be honest.

Yeah, that, that was no joke.

Yeah, yeah, yeah, yeah, absolutely no joke.

I were like able to finish.

Yeah, yeah, yeah, yeah, um, I got over being sick

like just in time to go to Paris, just in

time, so then I spent most of that like so,

yeah, and I finished like 90% of my assignments like

I finished the distributed.

I finished part one of 7 and I had finished

all of 475.

It was like part two.

Yeah, I spent a couple hours ago, that was my

that was because I was.

I was like I don't know with it.

I was like, and it's fine but it's nice though

because like the submission format like it's good that they

had it to make sure like everything got correctly but

I don't think my mom was.

Yeah, that, that's it like was it like pushing it

and I went I was like what like there's a

10 things I can do and I don't think.

OK, show me.

And that was his answer.

He said, You show me.

And I was like, All right, I guess I am.

So I think he wanted you to.

I was like way too much.

No, you didn't waste time.

You learn from it.

Now you're you were you would be spending all your

time on that today and then I didn't have that

one, I would also spend a lot more time on

that.

I tried several different you know.

You know, it's hard to know like it was but

um but anyway.

I, I literally the last not breaking out.

I cannot choose between these two functions for the first

one.

I'm so long in the middle and I literally.

You know this is.

OK, first of all.

Mm Mhm Yeah.

Yeah, but this is not normal.

I I just communication.

Yeah, sure.

So yeah, yeah, because otherwise you're, I did.

I get it.

I get, I get, I get what I really like,

it's like, like I eat so much fish.

No, in February, like no one was I that's really

cool.

Yeah, I honestly ready to live, um, we're, I know.

Um, and I'm, I mean, I'm so yeah, it's the

same like.

But it was great.

I don't know it's an accent, but it's just and

there's so many restaurants I can hear it was like.

midnight.

Well, actually I was I was it closed.

Like a couple of years ago, I, um, so in

the and I was.

It was, however, it was like no option to study

in English.

I had to study everything in French.

I write all of my final assignments in French.

So, OK, well, I, but that.

We have like another measure for for students, so I

suggest if you measure me say OK because you're probably

yeah so I sent him all you know and he.

I said that I don't know, but on the other

hand, I was uh at the same time so I

think that this um and so I just to make

sure that like this OK yeah you the test.

So you wanna make sure that the very, oh, you

have your predictions, never mind.

Oh, OK.

Well, you have your predictions.

I don't know.

I can you run your predictions before you figure out

if that's possible?

Sorry.

I think what you just what you wanna make sure

you're doing is that you're going through all your models,

selecting your select I upload the materials on, on, OK?

I think this.

And then to make sure that you have like that

you have everything printing within this unless or you can

say.

Yeah, just do that.

Or you, or you can, or you can call and

install our mod of like your best model or whatever

it is or you can use it and then call

it in this function and make sure it works or

you can just like copy and paste all within the

fun, yes.

If you want to test how it works, what you

can do.

Mm.

It's like you can create like you just call it

so like actually this is.

So, it's week 7, is that right?

Yes, and this is the function.

And then whenever I create data, to create like some

data.

Oh.

and these are just like this is just random.

This is not like like I was just like it.

Um, so now I have my Y which is like

one of yours and ones that sort of thing, and

then I can just I can run.

So this is my normal like I can here and

I can run it.

It might take a second run but this is how

they're gonna do it to grade it, yeah, yeah, yeah,

because they're gonna call source.

And we're just gonna run the function and that's all

they're but they're just that small because it's closer in

like another.

So I think I realise oh no, to generate the

same email ID to send you.

I was just.

Oh yeah, and then and then my fingerprinted the biology

is selected as accuracy and that's the one, so that's

so I just think that.

I have to figure out how to do a folder.

I think it's, yeah, it should be here now.

Yeah.

It's below the uh the readings.

You see the materials?

Yeah, cool.

I uploaded code and lecture.

Um, I'll talk a bit about this in a second.

There should be back next time.

I'm OK.

Um, so, yeah, uh, I work in the department just

like, uh, Tom.

Um, I think I haven't seen any of you maybe

in the office I was like, um, all my teaching

is pretty much bunched in the, in the last weeks.

I might see you in a couple of courses now.

Um, I broadly work on text analysis and deep learning

methods for social science in my case, mostly economics.

Um, and yeah, um, and I'm currently kind of building

this course alongside Tom who's now pretty much done with

this.

And, um, maybe quickly like recapping the 1st 5 weeks,

weeks, what are your thoughts about this?

Do you feel you're kind of, uh, you caught up

with the, with the materials?

Is that OK, or is it important to recap some

stuff?

Fine.

OK, cool, because we're, we're really building on the thing

now, basically, um, so on all the fundamentals, um, OK,

um.

So my lectures, I think they are structured slightly differently

to Tom's.

I secrecy, what I usually do is I, uh, discuss

some slides.

And then most of the topics, at least the ones

that I think are the most important, I also discussed

with code.

So we'll always have code notebooks.

And it's more like kind of an engineering approach.

So everything that we discussed in the slides, we then

build in code.

OK, um, I mean the the bigger picture or like

the bigger things, otherwise it would take too much time,

of course.

And then depending a bit on the units, we might

have some time at the end of the lecture, and

you can, you can run the code in parallel anyways

when I run it on my computer, you have it

now.

Um, but if you have questions about the code, maybe

at the end of the lecture at times there's also

10 minutes or so, uh, where you can, uh, experiment

a bit with the code, and, uh, it's some kind

of hybrid thing between the lecture and the seminar, OK,

uh, that you have, and then I'll obviously stay here

and I'll answer the questions and just walk around the

the lecture room.

So that's usually the approach.

So slides, code, then maybe a bit of free time

working with the code and then seminars working kind of

fairly autonomously with the code.

OK, um, any questions?

Cool.

I just need a pen.

Mm.

Not yet, but soon.

I think I need one, any of the roles of

OK, correct.

OK, cool.

So, um, if you think of the advances of, of

GI methods over the last years, then there are the

obvious cases in games.

I mean, not just AI in general, in games.

Think of AlphaGo, you know, this, this uh paper I

think was 2016 that Uh, with the system that we

use all in, in the goal match, uh, for Xero

was, was a system that didn't rely on any human

play but could just learn board games from scratch, or

the chess and go.

Um, and these were pretty big initial, um, initial, um,

kind of breakthroughs in AI.

A couple of years prior, maybe 2 years prior, there

was also the Atari paper where an algorithm runs the

Atari that I think was in 15 or 14.

Um, these were relatively popularised.

All of these were from, from this research, uh, from

DeepMind in London.

Um, they also, um, you will have seen, they built

alpha folds on the chemistry no this year.

Um, that's one of the applications in science.

There are other applications, for example, controlling Tromax nuclear fusion

is also fairly active field, but a bit beyond what,

what we're doing here.

And then if you think in the like How AI

basically reaches the public domain and what most people interact

with, and the two obvious big ones are text, um,

and chat GPTs of this world, um, and then images,

uh, also video with it also sound and audio and

so on, um, but the two, big branches are really

image generation and text.

So, next week or from next week onwards, we're building

our way towards getting a reasonably good understanding of models

like GPT, DeepSeek and so on, but that will take

us a good amount of time.

So next week we'll get the fundamentals on Uh, on

these transformer models that, that predict, you know, the next

token and text sequences, uh, in these applications, but then

we need to lead to a higher reinforcement learning, because

it's getting increasingly important in that field for, you know,

higher levels of intelligence in terms of planning of these

language models.

And then in the last lecture, we bring it all

together.

Um, discuss some other topics like multimodality and so on.

But like the remainder of the course will be broadly

geared towards getting an understanding of, you know, what's really

powering a lot of AI at the moment in these

large language models.

Um, OK.

So, and, and today we're going to, uh, we're going

to look at images.

Let OK.

Um, and that's just the prompt I ran, OK?

So please create an image of researchers discussing generative AI,

um, seems the model things unis or offices are very

fancy, you know, it looks more like a corporate office,

I thought.

Um, but yeah, like, I mean, you'll know this, this

was, uh, it was Delhi 3, yeah, stable diffusion.

Lots of other um of these kinds of um software

pieces and, um, obviously these production level systems are incredibly

complex, require Very large amounts of compute, we can't just

put them off on like a notebook on a laptop,

but I think we can get some intuition of how

this works, and that's the goal of this like you

know, time and time committing so.

OK, so to first start with broad categories of generative

AI models, we won't have time to discuss all of

them and they overlap and so on, but just to

give you an idea, you already did the audio coders,

both the standard ones and the variational ones.

We're actually um continue.

Continuing with this here.

OK, this is a good amount of the lecture as

well, um, but we're not using them for like you

used them before for synthetic data creation in terms of

like, you know, uh, trying to missing values in Tom's

paper, but we're mostly looking at them through the lens

of image generation.

As as really generative models for.

Um, and then, uh, another big class of models you

might have heard of them are these scans, generative serial

networks.

Um, they, the seminal paper on this was in 2014

in, you know, machine learning terms, this is a long

time ago.

Um, and, uh, they are declining in importance, um, these

days, these models, like, you know, what I previously showed

you, uh, is powered by these diffusion models, although the

architecture isn't public, but it's, um, uh, it's, it's very

likely and there are some sources, uh, on this also

online, um.

Uh, most of, you know, the stable diffusion, not, not

the image generation models.

So most current image generation models have some diffusion components,

but we'll also, we'll also discuss the Gs.

And all the regressive models, this is really taking the

sequence and then making a prediction of the next element

in the sequence.

OK, then rolling one forward, predicting the next element in

the sequence.

And the big application of this is JGPT, Lama, Mistral,

all these models, right, cloud and so on.

But really, you can also generate images this way, um,

I think pixel by pixel, whatever, yeah, um, flow-based models

and energies-based models, they come more from the physics domain.

Um, I just wanted to list them here that we're

not thinking that that's all of generative AI, OK, but,

um, uh, we'll look at the biggest and most salient

topics, arguably, and then there are lots of hybrid variants

of combining all of these and so on.

OK, so in our course, what are we going to

do?

In this lecture, we're looking at at least kind of

fundamentals of these three, OK, of the variation of autoencoders

of generative adversarial networks, and diffusion models.

And then the upcoming weeks, we look at all aggressive

models, but not the general ones, these ones, but like

really the ones for language.

That's our focus.

OK.

Um, and that's basically in the scores, uh, the, the

GAI kind of content.

OK.

Any questions?

So the remaining 3 are forward.

So the remaining 3 are for.

So they are all, they are all for generating various

data.

Think of it abstractly as just, you know, some vector

and some vector space that's generated through these models.

That vector might be, you know, uh, a token, like

a representation of a token or a word, that vector

might, you know, be, uh, might be an image, Sounds

and so on.

Um, that might have one dimension and it's just a

point or many dimensions that it's in some big space.

But like these models overall, they, they generate, they generate

such, uh, data.

Um, OK.

And mostly they have probabilistic notions other than, for example,

the standard or encoders.

Uh, that you are able to, to draw from these

distributions to generate stuff.

So all these models achieved this, but we are looking

at a subset of models that are arguably the most

salient ones, like, uh, the ones that you are most

likely to encounter, and particularly the, the, the shame about

this.

this is trickier than the other topics, so this is

more technical, and I thought quite a bit about how

to, how to discuss it in a course like this

and whether I should leave it out, but I don't

think it's a good approach to leave it out.

So, uh, so we'll look at like kind of the

basics of that as well.

OK.

Um, so this is how we start, uh, with autoencoders,

uh, and then followed by variation encoders.

So the autoencoders, you discussed them with Tom, the, the

VAEs, right?

So for generation of the synthetic data.

But if you look at this, it's quite a cool

paper and we're trying to replicate some of this with

a new architecture and we're making our life easier.

But that's really the idea from this old paper from

Hinton.

Hinton is the So just won the physics Nobel, um,

for, uh, for his work on deep learning, um, and

he has a host of, of papers and, you know,

days when lots of people were not working on AI

and, uh, and he was using autoencoders to do nonlinear

dimension reduction.

That sounds a bit funky like here in, uh, in

a moment what this means.

These days, you look at these mostly uh to generate,

um, uh, not mostly but like it's one application to

generate data and to think of them more through a

lens of generative AI, um, but we'll also see at

the end of this lecture how they are used in

dimension, even in kind of almost a of the architecture

or the.

Good.

So the simplest, um, autoencoder is this year, right?

So, um, I just have a slightly different, uh, um,

different picture from, from this blog post, which is a

great blog post, if you want to, uh, catch up

on autoencoders.

Um, and you have a neural network just to feed

forward neural network, right?

Um, and the only restriction that you put on like

one central restriction you put on this network is that

you have, um, decreasing number of neurons in the middle

layers.

So you force some kind of bottleneck representation.

Um, and then you input, for example, you know, this

MIS, right, it's a bit like the fruit flies of

deep learning, um, you, uh, you input the image here,

you just like vectorize this or flatness as one big

vector pass through the network, um, through the bottleneck and

then try to predict uh the number again.

Right?

Um, and then you, uh, I always like to think

in terms of what's the loss function.

So what's the loss function of this thing?

Well, it's your standard MSC loss, the only difference is

it's not just one thing in regression in the end,

but you have to sum up overall the dimension of

the final output, right?

That's it.

So then you do your grade and descent on this,

and that's how you get the parameters.

These are the parameters of the encoder.

These are parameters of the decoder, well, you know, they

cannot call them detail.

That's just like.

Good.

So, um, what if the middle dimension was the same

as that dimension?

What would happen?

With this kind of training.

Would it make sense, less sense, more sense, easier, harder.

be no diction.

What do you think would happen when I run this

then?

So I put this in this out, and I have

the same dimension in the middle.

Maybe the Uh it would just learn how to.

Output input.

Mhm.

So like a kind of a like unit mapping or

something like this.

So it would have zero if it if it doesn't

completely mess it up, it would have 0 or close

to zero loss, and it would just trivially output the

input.

Yeah.

So really what we're, why we're how we're making this

thing meaningful is by adding this part of life representation.

Otherwise, this is kind of a nonsensical problem.

OK.

But even if I took, um, the middle dimension of

being the same dimension as the output and the input,

I could do other stuff.

For example, I could look at this the noise in

autoencoder that's linked to what Tom talked about, like in

his part.

Um, imagine I knew some of these of these values,

and then I try to predict the real thing.

Then even if I have a middle dimension that's the

same thing, it's still not a trivial problem.

So then the harder question, why are autoencoders usually, why,

why do they try to compress like so for dimension

reduction, OK, but kind of.

Like it's linked to this concept of like, so these

things are called latent variables, all of these are actually

because they are unobserved.

And, and what is kind of, how would you, how

would you describe why we, why we try to push

things to like lower dimensions?

What, what do these lower dimensions mean?

I would say in vulgar terms, saying it's capturing the

essence of the.

Of data here would be like any shape and colour.

Yeah, so colour, I mean, it's black, white, it's between

0 and 1, but you're right.

So what would happen if we would have coloured images?

Either we would convolute over them, you know that now

too.

Or what we would do is we would just concatenate

them all, like we have 3 colour channel RGB and

then we concatenate them all into an even longer.

And then we run them through feedforward.

Um, but we need way more parameters in feedforward than

in convolution or nets.

That's why people usually use convolutional heads for lots of

images.

Um, you're right.

So it's capturing some kind of essence.

And it's quite a nice concept, if you think about

it.

There is some hidden world that captures lots of this

stuff, and we don't see it, but we can try

to get closer to it and then try to, um,

try to just run computations in that world, and then

maybe reconstruct in the end.

And actually, at the end of this lecture, we'll look

at like just a schematic of stable diffusion, and that's

one of the tricks to actually do to make this,

uh, to make this work.

Good, so that's the autoencoder, OK?

Um, any questions about the, the ordering encoder?

Cool.

So then let's uh let's check out the first quote.

So we're going to build this now.

And then run some dimension reduction that is reminiscence of

Hinton's paper.

I mean, Hinton did this with the like more complicated

architecture, but we're more like, you know, we're using the,

the new school methodology with highorge and relos and everything.

Good.

So that's the, that's the file here.

Um, you have to, I don't know, did, so do

you guys know how to create virtual environments and things

like this?

Not all of you.

I think for imagine like environments in which install everything.

Uh, did you discuss this in this course or or

where did you know?

Did you in programming to create environments and or did

you install everything in base?

OK, because this was like.

OK, because it's one, it's like, uh, it's a bit

pedantic, but it's good practise to not push it all

into base because usually these environments get messed up at

some point and then you destroy base, and then, you

know, you have to delete it.

I mean, it's just a mess.

So usually, for example, for every paper, I have an

environment, uh, for every course, I have an environment, and

then I just install all the libraries in that environment.

If you want, I can upload some materials on how

to do this, OK, um, it's super simple.

Um, OK, um, so anyways, so what I do is

I select the kernel here and I actually see here

I have an environment coding course and that's like loading

all the lines.

Um, maybe I can increase the font size a little

bit.

OK, good.

So now I'm, I'm loading the endless data set from,

uh from the torch library.

I, I actually already downloaded it so that should run

without error would hope or without kind of download.

It's the local library cereal.

That should work for you as well and it should

install these into a folder and this data, you know,

in your folder where you have the notebooks, I have

them here, here's this data, OK.

So, in endless data now lives uh the endless data

set.

And I have the training um part of the data

set and the test part of the data set.

Good, so now let's have a look at these data.

So that's all they go like, OK.

One thing, these are super simple, but this, for example,

I wouldn't even know, maybe that's 1, maybe that's 7

for it's 1.

Um, this is like, you know, has some noise here

and so on.

They are actually not that trivial, uh, and I mean,

they're quite trivial but not as trivial as you might

think, like just a, um, a white number on, on,

on a black surface.

So this is a 5, but you know, it's, it

could be a 9.

Yeah.

So that's how this looks like, OK?

Um.

Good.

So now we turn to this auto module and this

is really building exactly what we um what we uh

what we just saw on the slide.

Um, how comfortable are you with this class notation initiator

methods and all this kind of stuff?

Why?

That seems slightly more hurdle when you use Pytorch versus,

you know, Keras or something like this where it's very

functional and you just write, you know, and you or

whatever.

Um, but this year, in the end, it gives you

a lot of control of how to build these classes

and so on.

And for example, reinforcement learning and like, you know, research

that I'm working on, we build a class for the

agent, and then we saw the parameters with the class.

It just makes everything easier to organise.

And actually, in this course, I'll also build the course

in the code this way.

So, um, That will not just be a feature of

Pytorch, but like a lot of things we work.

For example, reinforce the learning environments, usually we wrap them

into classes and so on.

So just like it gets a bit like if you're

used to more functional languages like R or MATLAB, it

takes a bit of time to be used at least

it taught me, but after a while, like it kind

of, um, becomes reasonably natural.

OK, so this is the recorder, you see a very

simple one.

Um, we have first ina takes 28 times 28, so

why 28 times 28?

The dimensions of each image.

That's right.

All right, cool.

And we push it down to 25.

Um, we then push it down to 128 to 64,

and then to a latent dimension that we can just

choose.

So now we have an encoder and a decoder.

One important thing is when we train this network, we

train the whole thing, OK?

So that's the whole like, you know, you do a

forward pass, you put in your input, run through the

bottleneck, it comes out at the other end.

Um, but in the end, when we want to, um,

when we want to generate new images, we just get

rid of the encoder and then we start from the

middle of that network basically.

Um, that's why they he separated it in two different

parameters, but really it's like one more network, um.

Good.

So here's the decoder and then the activation we just

choose um boring.

OK, so, um, here goes the encode method.

We activate um these individual layers, uh, repeatedly, um, and

then we arrive at the bottom we output this.

The decode.

Um, starts at X, you know, starts at like an

X of the dimension of the bottleneck, he calls it

into the uh normal sized output 28 times 28.

Good.

So, and uh the forward methods combined to, that's what

I mean, the forward method of that network is just

a part the entire network.

Good.

So now I set the latent dimension to 2.

I, you know, I choose these like problems because they're

feasible on my laptop and in a lecture, so only

2 epochs are fine to, to, uh, kind of get

at what I'm, what I'm aiming at here.

Learning rates, try some out.

This works reasonably well.

Uh, my criterion is an MSE, but actually, I'll edit

an exercise later on to think about other, um, criteria

in this kind of setup.

Um, and then this year just like changes the ordering

code, OK, so 0s the gradients.

Um, it lets the images, the loss, that propagates, takes

a step in the optimizer goal, OK, zero of the

grads, um, and does this over, over like the batches

here.

Good.

Um, any questions about this code?

Cool.

Good, so let's let's, let's run this now.

Your computer might be faster than mine when I was

a bit by now um should run within like less

than a minute.

OK, awesome.

Now, one interesting thing is to see, like keeping in

mind what was my bottleneck representation too, right?

So what I'm doing is I'm compressing 784 digits into

two numbers, two floating point numbers, and then out of

2 numbers, I'm recreating 784, right?

That's what I'm doing here.

So now one obvious thing is like, will this just

give me complete junk when I recreate or will I

actually see some numbers?

And when I first saw this, I, I, I found

this quite mind-blowing that actually with two numbers and it's

pretty simple architecture, I mean, this is far from perfect,

but even the errors make a lot of sense, right?

The 1 is pretty simple.

The 7 doesn't quite know whether that's a 9.

The 4 is also close to a 9, right?

Because like only one more, um, dash here and we

would be at a 9.7.

Similarly, 0 is pretty easy.

Um, that testing shapes relatively easy.

2 is too complex, it's like too, too many twirls

and so on.

9 works pretty well, 6 works pretty well, and so

on.

Um, yeah, and we were basically able to compress all

that information into just into just two numbers.

Um, with, you know, significant reconstruction loss, but actually what

we could do, what I didn't do here if you

know principal component analysis, it's not in this course so

I didn't add this, but you can do the same

thing with principal component analysis and then recreate with it

and we'll just get garbage back basically like there's no

way.

So now this is interesting in itself, but I think

the really interesting thing here is why does this work?

Can we get some intuition of, of how, what's happening

here?

And the, the key thing to get that intuition is

to look at the latent representation.

So to really look at that latent space.

Why did I choose two dimensions?

Because it's easy to look at.

That's right.

And, and, and then I basically took these, uh, like

you can plot them into a plane and just have

a look at, at how it's separated.

And it looks quite, you know, quite interesting because what

this thing is doing makes a lot of sense.

And, and let me explain, let me explain this plot,

um, if it's not obvious.

So This year, every point here is one of these

numbers.

OK.

So one image is a point in that plane.

I've pushed every image to two dimensions and now I

depict them in the plane.

Where, where does the, where does the colouring come from?

Does the autoencoder predict this, or, you know, where, where

do I know what do these colours mean?

That's right.

And has no idea about them.

So just expose, I pushed this and imagine this was

all black and white here.

I would just think, well, yeah, it looks like a

mess.

I don't know, um, but if I can colour code

it, I realised, huh, um, so this nonlinear dimensional reduction

actually is able to separate these pretty subtle parts of

the space.

So this year, um, is a, is a 6, you

know, this year goes into like a 2, seems to

be the one is pretty easily separated, by the way,

reflecting exactly what we see here.

So the ones are pretty neatly separated.

If you look at these numbers, the twos look less

well separated.

Yeah, they are much more overlapping here.

Um, this looks really terribly separated.

The sevens in two dimensions, right?

The sevens.

So if we check out some sevens, yeah, looks like

a 9, looks like a 9 as well.

So that makes a lot of sense.

And we can interpret this kind of this lower dimensional

representational manifold.

Yes.

So those are the labels that the assigns, right?

Those aren't like showing the labels or are they showing

the labels and seeing like, oh, this does or does

not separate that's an important point.

True.

So the model is the, the problem with, like, so

people usually describe these things as unsupervised so often.

But if you look at the learning objective, it's really

supervised because I know what the output is.

It's just the output is itself, and I can, it

is not trivial because I have the bottleneck.

But, um, but I don't look at traditional labels of

is this a 5, is this a 7, and so

on.

I'm just trying to create itself again.

And then, um, these every point here is just an

image, right?

That's an image, that's an image, that's an image, and

that's an image.

And then I just colour code it for myself to

see whether what this thing is doing makes sense.

Exactly.

And now I'm colour coding this and I'm seeing, well,

it seems to make a lot of sense.

And if this wasn't non-linear, this would just be like,

you know, a bit like a short unicorn, uh, with

everything on top of the, uh, of everything here.

OK.

So, and, and this nonlinear dimension reduction is, is able

to separate this really well.

Yeah, any other questions?

And the two dimensions are the dimensions from this bottle.

That's right.

So they are Z1Z2.

Yeah.

Other questions.

Cool.

So now, one question is, you know, can we generate

stuff from this, you know, um, and what is your,

I mean maybe you've read the rest of the notebook,

but what is your feeling?

Is this like a You know, is this a model

with which you can generate new numbers?

It depends what's right, the ones that clean break off

from from the kind of pack like like the one,

right, where there's barely any overlapping dimensional space but it

probably couldn't.

Generate a 7 or a 4.

Yes.

So if we took a higher dimensional space though, um,

then good.

But like imagine with the spaces though.

So if you think of the variationalcoder that you talked

about with Tom, like there you could really sample from

some distribution and then it gave you magically data or

whatever the application there was.

Whereas here, what do these dimensions mean?

I mean, the 0 is here, not quite in the

centre.

It goes until 8 here, 6 to -2, 8 to

-4.

What would be sampling from suppose the distribution we learned

the distribution of each number.

So I mean, maybe what you mean is like when

when I first saw this and I thought, OK, how

can we, you know, what do you think will happen?

If I, if I put a number, um, mm.

6 and 06 and 0 into the, into the.

Uh, into the decoder part what times.

Can we, can we not know like um.

I would expect it to generate a zero image.

So let's see, so we go 6 and 0.

It's quite amazing though.

Yeah, so exactly, and it does not create, you know,

this we can pick empty parts of space.

It's not that this is just replicating a number and

something in that space means that's the zero area, exactly.

Um, so similarly, imagine we do -2 and you know,

-1 or something.

You see, yeah.

There you go, OK.

Makes sense.

What do you think happens if I go -2 -2.

Oh no, probably one because it's closer, yeah.

Yeah, but like this is one problem with these so

these audio encoders, they, they've been really developed to do

dimensional reduction.

That space here is not dense, like, as in they

have these big holes, it doesn't have a clear, you

know, you know, gap, goes from 4 to 8, from

2 to 6, whatever that means.

That's.

It's not the focus here.

The focus here is to compress everything into 2 numbers

as well as possible, but not so much to have

a generative model.

What is interesting though is that even this thing can

generate, and it makes a lot of sense how it

generates, you know, like, if we take this little further,

you know, we imagine we go 1 and 3.

And Right, we're just here.

Um, and you know, like if we were to train

a and that's the issue with the diffusion stuff later,

like, I mean, I have this model that I can

train a couple of minutes, but it's not very good.

And even for the MS, we would need a GPU

and we will need to run a couple of hours

to generate decent images and so on.

But this year, we could run in like, you know.

Yeah or even a minute and it's already that uh

strong.

It, it's not very sharp, obviously, even if, by the

way, we um we uh pick higher dimensions, um, so,

um, but maybe I can, you know, leave this for,

for later in the lecture.

Maybe you can, you can, uh, try this out a

little bit.

Um, that's one limitation of the limitation of these models,

and not just because we pick two dimensions and we

have this for example, but in general, they wouldn't, uh,

typically give you an image that looks like this.

However, the Dali 1 paper actually was, uh, to my

knowledge based largely on a variation reports, but then from

Dali 2 they used to move to fusions.

Um, OK, um, any questions about the cold?

Cool.

So we have this old school method, um, but that

actually can generate data and that's also really, really powerful

in dimension reduction.

Um, what is interesting I feel is also that you

can, you can compress this information that's 784 numbers.

However, it's a bit of a cheat because you need

also the encoder, right?

So actually you compress also a lot of the information.

You need to do that operation into all the parameters

of the encoder and the encoder has a lot of

parameters.

Even in the small or encoders, you can easily have

50,000 parameters or something like this.

OK.

So, um, this is a lot of what's happening in

these models.

Um, one interesting thing to, for example, think of, you

know, we'll do this in more detail later, but if

you think of, um, large language models, um, and if

you think of this first step that they are of

just predicting the next word on the internet on a

very large chunk of internet text, um, it's like lossy

compression of all the informa or like, you know, some

lossy compression of the information that was contained in the

text into all the parameter values of the model.

So you compress a lot of information into the parameters,

you know, the sorting part numbers with 16 or 32,

um digits, um, and, uh, and, and it stays in

these numbers.

And if I, you know, I could now send you

two numbers and you would be able to recreate an

image, but only if you had the decoder.

So I also need to send you all the decoder

numbers.

However, only once, right, not per image, but they are

image independent.

OK, questions or?

Cool.

Um, good, that's on the auto programme.

So we already said, I think most of this, so

it's, um, it's a very capable approach for, uh, for

nonlinear dimension reduction.

Um, It's possible to create image as well, but it

has these limitations of this irregular latent space with wide

gaps, doesn't super well interpolate, you know, we can, we

can try this out.

So let's let's take something in between.

What does that even mean here, right?

So like, you know, something in between these numbers.

So let's take this.

This is, this is a bit tough to say, maybe

a 4 point or maybe a 5.

And then a 3.5 or something like this.

And then it's the question, what is this?

Is this going to be a 0 or is this

going to be a 2?

See both, yeah.

So this doesn't separate very sharply, but it's some, some

weird uh juxtaposition.

All right, cool.

Um, and if we don't have a probabilistic framework, I'm

seeing these numbers in a space that I don't have

a good interpretation for.

It's not that I'm sampling from normal or something like

this and then generating images.

Um, it's OK.

So next is the variation of the coin coder um.

Let me maybe first um.

with time, um, try to see whether I can see

one another.

It.

Yeah.

It's the best I could find.

I'll try to find one on the break.

Um, one thing that these models all share that I

thought was conceptually interesting is they really try to map

some initial set, you know, that will be, say that

initial set is called set or something like this, um,

and they try to map this thing into some funky

set of everything that we perceive as being a valid

image.

Right.

And if we shoot outside of that set, we go

here or so, it's noise or it's not the shape

that we feel, you know, resembles anything that's meaningful, and

so on.

So really what these models are trying to learn, they're

trying to learn a mapping, uh, between these initial states

and then, and then like what we would consider valid

images.

And like, to just hammer this one very clearly, but

I think it's obvious to you by now, why is

X an image?

It's really like we can, we can build this entirely

imagine, you know, we have uh an image with uh

with say.

2 times 2 pixels, OK.

So that thing would be a 4 dimensional vector, right?

So that would be X1, X2, X3, X4, say.

So I can represent that image as a four dimensional

vector.

Um, say I have 3 colour channels, right?

And then actually how you would think of this, it's

then like a tensor with like 3 colour channels.

My drawing abilities are limited, but you get what I

mean.

Um, and then I could just line them up.

Um, as now a 12 dimensional vector.

OK.

And now I have this image, it's just a point

in the 12 dimensional space.

And this I draw in 2D to kind of, you

know, make it easier for us to imagine with the

X and the Z axis.

But really, this could be 12D and this could also,

by the way, be projection into a higher dimensional space.

So Z could be smaller and could be projected in

12D or could have the same dimension unclear.

Right.

But all of these, all of these generative models, they

start with some initial vector, and then they try to

bring that vector in some other space in the set

of of feasible images.

And whether you want to generate cats or, or just

what is an image overall that makes sense.

And, and you know, there's parts in the space that

are twos and threes what we just saw.

And then also um uh also parts of the space

that we would think is like almost white noise.

OK, cool.

So the variation of autoencoder is trying to extend this

framework, and it's trying to get a space that like

kind of initial space to uh that lay in space

if you only look at the um at the decoder,

the, the, um the space for generation, but if you

look at the whole architecture of just that bottleneck representation,

and it tries to force that bore representation to be

of a certain form.

That's what this VAE tries to do.

OK, and then it also tries to uh to incorporate

a probabilistic motion of that, of that bottleneck or latent

space that we can more easily sample from it and

with it than sample images.

OK.

So now, that's a um um uh variational encoder.

Um, I if you've also seen this.

I don't know how much detail you've seen, like there's

this trick in order to make it, did you discuss

this that otherwise you can't propagate your gradients through this

and so on, exactly.

Um, OK, so this is just like, you know, a

notation of a conditional probability conditional on, on X.

I'll get a Z, conditional on a Z, I'll get

an, you know, X prime really.

Um, this is the reconstructed this the imports again from

this blog post, if you want to read up on

it.

Um, this is again a compressed, uh, a compressed, um,

a compressed vector.

However, for every X really, um, I get a different,

uh, um, I get a different sample to latent vector.

What's my loss function.

In the simplest form, if I have constant variance terms

and I have, um, Gaussian distributions and so on, it's,

uh, it's just the same reconstruction loss as before, um,

a mean square error, but it doesn't have to be,

right?

It could be any kind of loss that's sensible, right?

Um, what do you think in terms of maybe to

go with, like, later in the exercise, that would be

part of what we could do, thinking of the endless

digits, like, uh, what values do they have?

I think we already said this, like, because they're grey

and white, so Uh, so like black and white and

grey, what is, what is the scale?

At least you can normalise between any sals come on.

So Often they go from 0 to 255, but like

if you think about it for for grey scales you

can just do 0 and 1.

Um, so if now these things are 0 and 1,

how could we try to make, if we think of

our autoencoder before?

What was one thing that actually, or how do we

make life for our autoencoder harder than what it had

to be?

Yeah.

Um, you mean in the variation one or yes, that's

right, but like thinking, um, thinking of the autoencoder that

didn't have any uh any um probabilistic uh features, yeah.

So thinking of the plain vanilla autoencoder, if you look

at It's a bit of a tougher question, but if

you look at the architecture here, um, think of the

forward.

Can the forward generate nonsensical values?

Could we use a sigmoid activation function instead because it

would, yeah, exactly.

Because like I mean to take one step before you

answer, if, if I can, you know, if my outputs

are 0 and 1 are bound between, this thing can

do negative predict negative outputs, right?

There's nothing stopping it from predicting negative outputs.

So I can put some structure on this that might

help me with reconstruction also and so on.

I mean, up to for you to try.

Um, so then what, what we do, like we would,

like you say, we would, um, on the, on the

decode function, wrap a signal around here.

Uh, then we would force every single, usually when you

have many output neurons like the way we think about

this or these IR the softmax functions, right, to make

them all add up to one.

But we don't want this here.

We want, it's a bit weird, but we want to

reconstruct the grey scale image.

So we want every single node to be between 0

and 1.

So exactly like you say, we run a sigmoid over

every single node element wise.

Um, OK, so what else do we have to change

that?

What kind of what would make sense then.

Can we, we, we could also leave it as this,

but like what, what else can we change?

Wrap it around the encoder so that then there's sharper

boundaries.

What can you say it again?

Wrap it around the encoder, so there's sharper boundaries between

black and white.

Uh, where would you, where would you, uh, where would

you edit in the endcorder?

Um Before the bottleneck?

So you would kind of, you're thinking you could force

the bottleneck to be between 0 and 1.

But then, I mean, you could do this, um, and

maybe, you know, it doesn't change much.

These are often empiric or usually empirical questions depending on

the data set and so on.

Um, but here I would be a bit concerned.

There's nothing, you know, that latent world doesn't need to

be between 0 and 1.

So I'm making it in a way less powerful than

it would be otherwise.

Whereas I know the truth is between 0 and 1,

so I might as well give my model a hint

to not minus 5.

Um, but the, uh, the latent one I could be

as flexible as is and with my and plus.

So I, I usually have like kind of linear activations

on, on the bottleneck.

If you see, that's exactly what's happening here.

Um, so, uh, the, the activations are, uh, are, um,

are on all these ones but not on the bottleneck

or because also the re, right, makes it non-negative.

Um, that's exactly the same logic why the rela is

not around here.

OK, so what, what one, what's one other thing I

could change here?

What would be the goal is the goal?

The goal would be to have a a little bit.

I mean, actually, you can also think, I mean, there

are other things you can, um, you can tweak this

model with, but we have this one thing we, we,

the output, we wrap in sigmoid.

But imagine how we have a sigmoid output with between

0 and 1.

What, what do you think?

What, what, like, you know, I don't want to give

it away, but so what, what these networks usually go

hand in hand with if your output is between 0

and 1.

Think of classification problems, for example.

What is different in a classification problem or network than

in a uh regression or a network?

It's a decision threshold.

For example, if it's, you know, it's just a single

one, but a thing does function, right?

Yeah, exactly.

So you would pick some cross-entropylos, yeah, exactly.

So you can do that too.

I mean, that helps or not, you'll see, um.

Would, would you change that?

That wouldn't be changed like within the class function, right?

Because you define the, yes, because the class is really

just defining this object as an encoder, but the optimisation

of that, but I mean, you could also put a

train function in that autoencoder and so on, but the

way we did it here is we separate it.

um, and then, uh, we would, uh, we would do

this here.

So like we do it here, we would just define

a different.

But I did the mean square error because that's how

Tom introduced it in like JR4 or something, and then

also, um, yeah, but like, by the way, this loss

is only that simple here.

That's actually negative log likelihood that loss for the variation

order encoder is only that simple if we have mean

square, otherwise we need like a log likelihood and so

on.

And then the KL diversion.

So what, what is this really doing here?

and maybe we can take a coffee break.

So what's the KLR versions doing here?

So first of all, what is a divergence I let's

start with this.

You need fertilising the food.

I to the exact same.

You see how penalising for the similarity a little bit

for the similarity between the input.

Are you, are you answering because you've seen this in

other models, the KL version, is that where you from?

Uh, yeah, OK, but I think your argument you can

make in many ways.

So, um, imagine, OK, so let, let's take a step

back.

What's divers.

The penalty really is like using it and adding it

to a loss function that you want to, you know,

um, that you want to minimise, but like what is

KL more generally that's right.

So it's measuring the difference between probabilities.

So if I want to use this, I can use

it for many things, but if I want to use

it to be part of my loss function and be

a penalty here, then what is this here this.

What's your So that's like, yeah, it's linked to this,

but it's really what we would refer to in that

kind of context as a prior, a prior distribution kind

of what, what would I like this thing to be?

So this is, this is what I would like to

force my lead distribution to be close to.

And the more weight I put on this, the closer

my lady distribution, if everything else in my network works,

um, uh, should be to that right.

OK.

So, If I, and, and that's pretty much it.

If I want a dense latent space, um, and I

want this latent space to be standard normal, then I

will pick a standard normal here, and then I will

just penalise whenever it's going away from a standard normal.

By this, I will weigh this more than the reconstruction,

so I'm arguably getting worse on like really mapping this,

like compressing this very well and keeping most of the

information because I have that additional thing that tries to,

um, make it standard normal in the latent space.

But why do I want to make it sound at

normal in space?

Because we want to have output a little bit different

than the inputs.

It's the, it's, that's why it's generated in this, in

this image.

What's, what's, where's my standard normal in that image?

I mean, I know these are slightly harder questions, but

you could answer everything before, so I'm thinking like, why

not, you know, so I like this, like, yeah, I,

I'm aware this takes a bit more to think about.

So before, or differently, before my set of possible, you

know, or my pro my distribution of possible Z variables

was kind of funky before in my normal autoencoder, right?

So it had these weird shapes.

So now if I forced this thing to be standard

normal, how would my latent distribution look in a 2D

space?

Maybe you would be centred around.

centre on 0 and then what, what kind of shape?

Mhm OK.

Like, like a ball, basically, like you would have this

and then if we were to go with kind of,

you know, 3D, it would basically just be a normal

around this year centre around 00.

That's how you can think about it, you know, it's

coming like kind of 3D out of this and be

like this normal here.

Um, if you think in terms of like the, the

density lines.

So the density would be that action, that, that axis

basically.

Exactly.

So by doing this, we force the Z set to

be basically like a circular thing in this latent space,

and then we can, and why do we care?

What can we do then?

How, how do we create a new image in that

world now?

See, I, I tell you, I want, I want an

image of a number.

What do I do?

We want to penalise anything which is inside the.

So you, OK, OK, good point.

So say, imagine the latent space goes a bit ballistic

and turns out like this.

If I care about if the KL divergent is there,

I'm pushing it back to be that normal sort of

like whatever I mean, under the second constraint of like,

you know, separating the points.

Um, but I'm, I now want to to have it

more of a shape of, of like what could be

coming from a standpoint.

But in this world, if I want to create a

new image, what's the procedure I have to do?

After, so first I trained this stuff, right?

So I trained this minimise my loss da da da.

I have my parameters of decoder and encoder.

I got this now.

So how do I get the next prime?

Select the point within Z and feed it through the.

Exactly.

So if we think about it before, when we looked

at the autoencoder, we thought about it non-probabilistically.

So we just picked the point.

That's a bit of an odd thing, you know, if

you think of modern models, it's not that I say,

oh, I like -4 and 7.

Like what would I do like to, to generate a

new image?

What would be kind of convenient?

If you, if you imagine you want to invent this

stuff, so what would be convenient.

Exactly.

So what would you sample from?

That's right, so you would suffer from a standard normal,

you run into this thing, you get an image.

That's the trick.

That's it.

So we built something where we can sample from standard

normal, run it through the model, get an image.

Cool.

OK, any questions?

And then we implement this again called kind of from

scratch and see whether that actually works, um.

Cool, um, yeah, that's maybe me that.

Well, how much, how much breaks did you take in

the course before, like 1015 minutes or hm, not 10.

OK, cool.

Let's, let's do 15 pass.

I is it, is it running?

Yeah OK, I don't, I think so are you, are

you running your fun?

Tesco.

Are you sourcing it?

What I would do is I would So.

Oh wait, did you do that?

Is that there?

I'm sorry.

that in your code.

Oh, OK.

And then you're just, you just wanna make sure that

it's running solely in here.

And then.

Yeah I would just, I think they're good.

You I would say don't Oh no.

Where are you calling it?

I wanna see if you can find online.

I would because I just stay in the hospital for

like a day.

I did not um no, no, it's uh looks like

yeah, he's doing, yeah, he's doing something.

So uh so you to wait.

Oh you guys oh, so it's too.

I think it's too it's like we can do anything

too that when, whenever I'm sorry, I think.

you call it, it prints out.

I, I took the uh this one was really nice,

but like.

Let me try, try importing it again because I just

changed something, so it's not gonna cost so try importing

this again the professors.

Because he's like top of the, you know, he knows

what his what his communications are good, good, good, good.

OK.

Well, that hurts, yeah, it's not that's like a really

like all of them like they have this uh um

like.

It's.

Uh, I, I wanna like what's the difference with normal

machine as we I think they just have like a

pri and so you like and then like.

It updates your patients and so that like change like

you don't need to like I don't know how it

works but I the prior and I have no idea

I don't know.

I mean I don't like a I think you just

this is what it's supposed to do that so you

can make much more I guess, I guess this didn't

work.

OK.

Yeah is that.

Yeah, I would just maybe like just make just make

it like a little bit easier where saying but maybe

you can add something where it says like final feature

selected these features and then that's it looks good like

yeah yeah it's like calling looks like it's just selecting

and also I just want to ask.

I'm afraid of, OK, OK, um, I think, yeah, it's

gonna still be like, I I think it's.

Don't submit the HTML don't submit the HMO but it's

gonna be.

Yeah, I think this is the I, I don't know,

like, so this is the I just explained, you need

attention and then I'm afraid.

I'll I'll just have like 5 to top of your

other, yeah, yeah, you don't, you don't want an HTML

that's like just just send your script like you're gonna

submit the script on its own and then they're gonna

call it and yeah, and I also want to.

Oh, I would say that just the big thing I

need to do is make some of these cuts smaller.

So why is it all.

Oh yeah.

I get some so this is fine.

I'm doing and this and this this um, I found

it because I, I hate.

Oh no, that's fine.

I mean, not I, sometimes I have to go.

I I I knew I regret anything.

I was like now I have my what I want

and I have to encode and be cold.

Yeah, so this is my code.

So I think, do you have any?

OK.

I think you should keep the code in code format

and just let all your code be at the bottom

of your report, yeah, like all, all the codes.

I just and when you wrap it in code.

automatically I just like the bottom.

And then for my report, all I'm doing is I

I tried, I tried this and these were my results.

And it's too easy and doesn't like I did it

in, yeah, yeah.

Yeah, I've done.

I just, yeah, I actually just downloaded it but I

just download, you were able to download it from the

from and that's all we want our own.

Yes, so one script and then one where I'm like

gonna explain you guys like to like for you where

you're just kind of like trying to like it worked,

if it didn't work like that I don't like to

spell your code like you know I mean I think

that you want all your codes to be kind of

like they use the same.

Uh, randomization algorithm.

Oh, it's like of course, yeah, she's so helpful.

So by the way, uh, which programme are you.

OK, I think, I think I'm the only one who

um expected this.

Are you honestly.

Yeah, it's hard, that's I think it's it's good that

you, yeah, and I'll tell you.

I don't like it's not feasible to answer the question.

I I put that, um, that's right.

So if it makes you feel good, if none of

your models were good, then you feel bad about it

because I thought I was the problem for so long

and I feel like I'm also taking the uh the

likes of you as well or?

474.

How do you find the two courses together, so there

will be one week that will be boring for you

and I mean, I guess more boring, but like there

will be one that's just repetition, uh, because I'll, I

mean, I have to teach one week on our networks

in the general course of machine learning.

Lots of people take it that don't take this course.

So that week for you will, will be entire like

entirely.

Um, but do you find the two courses, I mean,

we could only run them basically in the same time.

Uh, do they sort of compliment each other?

OK, that's cool, because like, I mean, all the statistical

machine learning is really not in this and then also

thinking about, you know, cross validation and this a bit

more rigorously, yeah, cool.

OK, great.

So, um, any questions on what we, what we talk

about with the, with the late spaces, then let's look

at the next code notebook.

Of your So now again, uh, same objective, we try

to build what we just discussed basically.

Um, this, this repetition, I just, because it's a different

notebook, it runs a different kernel, right, so it has

a different patent doesn't share the variables with this, so

I need to run the same again, same thing again.

I pick my environment.

Which torch are you using?

because I have like a new torch and it doesn't

have this.

Yeah, so I mean, I also got in, so Yeno

deleted it from his website.

I got that error and now it loads it from

some other website though, um, so it has some kind

of, yeah, tree that it's trying on, I guess.

But I remember like a year ago the whole work

was should be automatic.

I have a look.

Mhm.

OK.

Do you all get it?

Is that like a needle roll thing maybe?

No, you get it.

You can download it.

I'm having like an SSL certificate.

Yeah, you have to like unable the verification if you

have, yeah.

How do you do?

I don't know and it's like some unverified content that

you have to allow to load.

Um, OK, cool.

So.

We got the data.

This is the autoencoder, um, same thing as before, but

now actually the encoder.

It's outputting, um, uh, or you know, has this, has

this new and this log are, OK, um, and this

is happening by uh by um by combining these two,

these, uh, these two numbers here, the latent dimensions to.

OK.

So in the end, what is the dimensionality of the

new?

What is the dimensionality of the loqua?

So what are these things?

First of all, what is the new one, what is

Awa?

Yeah, that's right, it's a lot of standard deviation, I

mean more variances, and then it's uh it's the newest

the mean.

What are these these are the two parameters that parameterize

the normal if you exponentiate, um, so what is the

dimension of the new just so that we have this

here.

What will this be?

I know it's slightly weird.

That's, that's why I why I thought it's good.

Looks like 64 and then the latent dimension.

So like, imagine, imagine this wasn't like there wasn't any

fencing you or by you or anything.

So we had that we had that last operation.

So, so that would be just like before in the.

Uh, this year.

So this was basically the bottleneck was of dimension two

variables, right?

OK.

So now, basically, what is this here?

How many numbers is new?

True.

That's right.

And this.

2.

So basically what we're looking at, we're looking at either

2 independent draws from a normal or we're looking at

a draw from a multivariate normal, right?

And if we want to emulate two independent draws in

the multivariate normal, we would, what would we do in

the various covariance matrix.

I this or have you, have you seen this?

Right.

So if you So we can either draw two independent

norms because it's normal, the correlation and independent thing uh

it's easier and it's the same as it's different to

other distributions.

So, uh, we could either draw two from two single

or like univariate normals, or we could draw from a

multivariate normal.

Um, so a multivariate normal is usually parameterized by, you

know, a vector of news.

But then, like the trickier part is the covariance because

that thing now is, is a matrix, um, and that

has the form of like a 2x2 table now.

Why?

Because that's the variance of 1, that's the variance of

two easy, but like these are the covariances.

So this is how draw 1 covariants with draw 2,

and that thing is symmetric.

Right?

And if I have one that is 0 of the

diagonal.

And that's replicating these two draws from the two uni

ones.

But only here, it means zero correlation, um, yeah.

Right?

So that's just a way to write this.

That's a multinomer.

And then you see this generalises really nicely, right?

I can draw from the 3 trillion dimensional space.

It doesn't matter, and I just have a massive covariant

matrix.

Um, the, the covariances get completely out of hand, right?

So they are like, um, uh, the square of the

dimension over 2, but the minus the diagonal, um, because

they're symmetric, they are the same, um, below and above

the diagonal, but usually what people are very often people

just say.

Um, this is that kind of matrix with, say, a

constant variance, say that's the same for everything, and that's

an identity matrix, which would be 0 and 11 and

the constant, and then each would have the same variants,

and these would be zero.

You see that that's the identity matrix of, of 1001.

If you haven't done any linear algebra, don't, don't worry

about this.

But you're all comfortable with kind of matrix multiplication and

so on, right?

Tom, Tom recap this because I think otherwise this stuff

is difficult to understand.

Um, good.

So basically now what we get is a 2 times

12 times 1 lock bar, OK.

Um, and then we have the decoder, we put the

latent dimension in there.

Um, and then we basically, uh, we basically, um, multiply

it up again, OK.

Good.

So now let's, let's try to look through this.

Like this is the reprometerization trick actually.

That's what's happening here.

Good.

So we, we have the encode and the encode goes,

OK, you want it with 3 easy.

Then the final one is actually now wrapped into the

in the approximates for the means and the approximates for

the variances.

So now I have 2 means, 2 variances.

And then this thing is returning actually, mean and variance.

OK, but I don't have that for now.

So what I now do is I actually take these

means and variances.

First, I, you know, exponentiate to get the standard deviation

because that's a lot of variance, um, and, and then

I, uh, um, I, I sample like some added noise

from like a standard normal here and I do this

trick, um, that makes me draw from that distribution, the

mean plus like, you know, some noise times ESE.

Um, good.

So now if I look at the forward pass, see

what's happening here, like that's my decode.

Once I have the hidden, uh, the hidden stuff, I

just like basically take the two numbers and I blow

them up again to my X and I hope that

it lands in the set where the sense of validity,

right?

But like the critical thing is the forward.

So what happens with my one forward pass through the

network?

I enter the image, the image gets encoded out of

the encoded image do not only come to numbers, but

actually now come to means, two standard deviations.

Now, essentially, I'm sampling from that distribution.

That's what this is doing just in a way that

I can still.

Propagate to it, but I'm sampling from this distribution with,

uh, with two means and 270 gs.

So what happens if I sample from this thing?

Like with this multi, uh, very normal, if I sample

from this, I might sample minus 0.1 and plus 0.7.

That might be a draw, OK?

Another draw might be minus, uh, you know, 2 and

minus 0.3 or something like this, you see what I

mean?

So if every draw from that distribution um are two

numbers.

So and these are my sets.

Here I encode, I get these, I cannot draw from

the distribution I have my Z, and then my Z

gets decoded and that's my reconstruction.

So now my reconstruction even in training and in any

forward path is random.

I have a question.

I think I maybe just missed something.

Can you scroll up to whenever you create the new

and the log variant.

To me that just looks like you're just still just

you're compressing the same thing into two latent dimensions.

So how do we know like it's the same thing.

So how are some of them new and some of

them like log variances?

Does that make sense?

Yeah, so basically what you how you think about it

I mean I think about the two dimensions to me

that's a new.

The variance just tells me how, how much if I

sample from these two latent dimensions, how much does it

vary around the main.

But imagine the variance would be zero, then the new

would be exactly the Z, like in the, in the

traditional sense.

So if we force this thing to be deterministic by

degenerating the normals and making new zero, then we're kind

of back at where we were.

If also then we would make the the variant 0

here, you know, we wouldn't draw anything, this would be

0, we would basically just output the.

There's this if you take a normal function and you

degenerate it to the infinite mass on like a single

point, that's like the Roxx delta function.

That's a bit like a, a deterministic spike on one

specific point, you know, there's no variance around it anymore.

It's just, it's just, uh, um, this one point.

And it's infinitely high because it still has to integrate

to one, so it's kind of weird.

Um, OK.

Any other questions?

But I think that's like this code is helpful because

like it's easy to look at, at least for me,

I look at the slides and I'm like, yeah, sure,

obvious.

And then I try to build this or try to

understand the code and then I I see it's less

obvious, um.

Uh, OK.

Any other questions?

These tricks, yeah.

So, like, basically, this is just emulating a draw from

that normal.

So my, my encoder is outputting a new and a

standard deviation.

Now, it's a bit weird, why do we have these

22 dimensional vectors.

But really the standard deviation vector is just there for

me to be able, able to meaningfully sample from the

other one.

so now I need to sample from that distribution.

But if I just have this sampling operation in there

with like a tilda and so on, I have no

idea how I, how I do my back propagation through

the whole thing.

Um, so I need to have something that is like,

you know, reminds you of the standard like plus, uh,

kind of, you know, in the beginning what you saw

when you looked at all the differentiation, um, this year

you can actually take a derivative through, but in the

end, this is the same thing with this you sample

from the distribution.

So there are two, there are two ways.

Either, either I just go to Numpi if I just

want to get the Z, um, like a specific X,

like, you know, I take an image, I take an

endless image.

What does it output for me?

It outputs a new and the sigma.

That's what it outputs.

So this thing now has new sigma.

Um, and then I want to, I want to create

a new image.

So I now with the new Enigma, sample my Z,

and then I would draw from a, from a, from

a normal to new and Z.

Um, and I would, uh, alternatively, I would just say

new plus an epsilon, which is, um, drawn from a

standard normal times the standard deviation.

And that would be the same thing as this drawing

from the distribution to begin with.

And I only have to do this because then I

can optimise this, because I still have to choose all

my weights and biases and so on on that network.

Cool.

So all the training you see here, uh, reconstruction loss,

I I picked MSC again, um.

It should be averaged for having more meaningful comparison between

the Klali but the versions, OK, so let me see.

I run this.

Oops.

It's very quick.

Now the loss is gigantic, but the loss is, you

know, coming from the lock times in the diversion, so,

um.

What I care about is that it's decreasing and then

here you see the network is saturating relatively quickly.

So that kind of architecture won't be able to push

on the loss forever.

Uh, it's now kind of not quite, but like, uh,

that was actually quite a bit of a.

So I might, you know, if I did this seriously,

I might run it for a couple of more Apo.

Check this out.

Boom.

Still separated, still some empty spaces, but pretty centre.

I mean, we don't see all the point mass.

We don't have any, you know, we don't have any

kind of level of how dense these things are.

But what you would see it's, it's pretty much like

radiating around the 00.

Um.

Good.

So now we can basically sample from that latent space

and can we just draw a number.

Here we go.

55 6 9, kind of cool.

So we have a degenerative model, OK.

We sample um uh a drop from sample we run

through the model, um, and then we, uh, uh, we

generate images.

So Just to make here in the this this piece

of code you take out all the encoder and you

just that's right, that's right, exactly.

So sample that normal run it through the decoder.

How do you do this?

Oh, you just.

So basically I do.

So the cool thing is by setting this up this

way, um, my, my object, the VAE that I intentiate

is like lowercase VAE, um, has these different things.

Like the forward is used in optimisation implicitly from Pytorch,

but, um, I can just like, you know, myself, uh,

use encode or Ecode as methods in this class.

Cool, uh, other questions.

OK.

So now, Let's continue.

So we've done variation on online coders.

It's quite nice.

Now we have something that is more resemblance of how

we would think of a generative model.

We can, you know, draw a random variable, when we

have an image.

And if this thing works, we, we push a lot

of even in two dimensions, we push a lot of

random variables into something that's sort of a meaningful image.

It's quite nice.

Uh, we're actually pushing something from a 2D space into

a 784 dimensional space.

I think also we draw something in 2D and then

we get an image out of this.

Um, uh, and then, however, even in very large scale,

um, production implementations like WA one, for example, the, the

clarity and the, you know, the fine grains of this,

this will always be, or I mean, empirically has been

more blurry than other um options like guns or um

or um um or uh diffusion models.

And one thing is this trade-off between the log likelihood.

I want the best reconstruction possible.

But I also want this, uh, latent space to resemble

whatever distribution I want to draw from to begin with,

to create these images.

So I have this, this trade-off.

Um, and then, uh, if my, my data distribution is

multimodal, I have a different, um, uh, multi mode of

distribution is just the maximum of the distribution.

So something that's sometimes a bit confusing is, uh, if

I have a normal distribution.

Like then the mode is the mean, but if I

have slightly frontier distributions, then the mode is not the

mean, right?

So I might have Imo try more distributions or whatever,

then the me might be somewhere here.

Um, uh, and, however, is the only mode, it's the

highest probability density, but like I have a couple of

peaks.

So the, the mode is the highest, um, highest point.

Alright, so multi motor means different kind of, uh, peaks.

Um, kind of colloquially, OK, so, um, the orders are

still not working.

Hm.

All right.

O encoders are still used for um.

Uh, 4 dimensional reduction.

Um, I'll show you one thing at the, at the

end.

Um, good.

So now we want to look at methods that generate

more high quality images, um, and first we're going to

look at generative adversarial networks.

Um, the seminal paper on this has been in, in

2014, this Goodfellow paper.

Um, it, it was refined to a lot of, you

know, sequel research with massive models like as usual in

this space, the more parameters and so on, everything that's

equal and sensible architectures assumed, uh, you get better outcomes

for a while.

Um, we'll discuss like these kind of laws in more

detail for language models.

Um, the basic setup of games is you have two

neural networks.

These could be any functions, but usually people use neural

networks, and one is a discriminator and one is a

generator, and abstractly, they play some kind of game theoretic

game, like Minna game, and I show you the loss

function, but, you know, so uh we, we, this is

actually because it's now used less and it's quite brittle

to train.

I don't have code on this, so I'll have code

again on diffusion.

Um, so basically here, I, um, I generate new images,

um, from the generator, and then the discriminator is another

neural network which classifies whether these images are real or

fake.

So you have data from images from the real distribution.

Imagine you have a big data set with, you know,

lots of real images, and then the generator should get

successively better at generating fake images that look indistinguishable from

the real ones.

Right?

It's very, it's a, it's a very interesting setup.

And then the discriminator tries to distinguish and by playing

this game against each other, they both get better and

better.

That's the theory.

The problem is, for example, if the discriminator is too

strong to begin with, it's just destroying the generator all

the time, and the generator never learns, uh, much because

the discriminator is too good.

So sometimes people make the discriminator, you know, they parameterize

it less powerfully and so on.

So, um, these are kind of the limitations of this.

They, they are quite brittle to train.

Um, but that's the broad approach, OK?

And it could generate very, very impressive data.

Um, and that's like Wikipedia was some small changes for

me.

Um, uh, basically what I just said, again, our favourite

Z vector here, basically, you, you add some noise into

a generator.

That's again a neural network, you know, we can set

it up however we like.

It should be from some latent dimension, Z, whatever that

dimension is.

Through a feed forward or convolution or on that or

whatever we want to use, and then it ends up

at a vector that has the dimension of the final

image, or it could even be like always convoluting, you

know, it could be um a rectangular, uh, representation of

an image in the end if the whole thing is

only convolutions, right?

You can also have these networks to take a really

rectangular image and convolute over it, and you, you, and

we scale and so on, you get sort of the

same dimension back.

Um.

OK, so we notice we add in the generator, generator

outputs, a fake image.

Here we think of it, you know, let's think about

it like a vector again because we can't flatten any

rectangular image.

We have a real image.

They don't go into the discriminator at the same time.

These images are just usually flow charts are built this

way.

The discriminator gets, um, usually gets them, uh, independently like

gets a vector and then has to decide is that

vector real or fake.

OK, so the generator generates a vector that goes into

discriminator.

Discriminator tries to predict zero for that vector because, um,

you know, if, if the object if, if one corresponds

to being real, um image, um, but the generator wants

to create such a good fake image that this thing

is falsely saying it's real.

Yeah, you have a question.

So the discriminators trained before, they are trained at the

same time.

Oh, OK, yeah, so you have already, so it's, it's

all happening at the same time.

I mean, of course, but like they are so you

are, it's a kind of a classification and you're creating

one of the classes.

The fake one.

Yeah, yeah, good point.

So definitely that's classification for sure.

But this is really like a generative model.

Like think of a neural network.

I mean, if we try to write this out like

that neural network could, for example, look like this, um,

could have, you know, uh, X1, let's take that 1

to Z100, OK.

Then come some kind of hidden layer.

And then comes some output, which in our world, say

endless digits 784 dimensional.

OK, so that's the network.

That's the generator.

We initialise the weights at the moment where we initialise

the weights randomly, this network will create some output.

It will also look like white noise if we put

white noise in it, but whatever.

It creates some output now.

So now I have an image.

The discriminator goes, sees that image, um, and, you know,

if we initialise it at random, we'll take 0.5.

But then the discriminator will want to learn that this

is not a real image, because at the same time,

the discriminator has shown real images from the data that's

underlying this.

Um, and it's, uh, is, um, uh, it's trained to

predict one for the view.

So our goal is to, for the generator to increase

in performance and the discriminator to decrease in performance.

No, that so.

Yeah, I mean, it depends it depends, but say if,

if your discriminator is too strong in the beginning, your

generator doesn't get anywhere because it never like the learning

is limited by the discriminator just being too strong.

So you ideally want to have them run simultaneously.

So if we think about this a bit like, you

know, this is just a bit to give you some

intuition.

So that's the objective function from the original paper.

By now, these objective functions are often separated.

Um, but think of this here.

It's one objective functions.

It's, it's not just the thing that I want to

minimise like loss function, but it's really two.

It's like one of them, the discriminator wants to maximise

this thing, and one of them, the generator wants to

minimise this thing.

That's this Minmax game, right?

OK, so.

Over the data, that just means that's an average over

X coming from the true data.

What's the true data, the set of true images.

OK, so that's an average over the true images.

That's empirically if the empirical counterpart of it is an

average over generated images, you know, think benches or so,

uh, in order to optimise this.

Good.

So now, why does the discriminator want to maximise this

thing?

This is the probability assigned to a real sample being

real, right?

So the discriminator wants to assign high probability when a

when a real sample is real.

Um, and the discriminator wants to assign, um, uh, uh,

wants to maximise this year, and how do you maximise

this year?

You minimise this year because it's 1 minus it.

So for a fake one, the discriminator wants to assign

a zero here, right?

So zero probability for a fake image being real, um,

and high probability for a real image being real.

So that's the thing from the perspective of the, um,

of the, uh, of the discriminator.

Now, from the perspective of the generator, it's the other

way around.

I want to generate an image such that that that

this thing gives me a high number here, which makes

this log 0, pushes it down, I mean, closer to

0 being minus infinity, right?

So the limit, um, and then I actually want, um,

I want to reverse here as well.

OK.

So I want this thing to be pretty bad at

like real images, not thinking they are real, but thinking

my um generated images are real.

So they have the object they have counter, you know,

they have exact opposite objectives here.

And I think you already get a feel from looking

at this like this is brittle, um, and, and people

then separate these two, they make a separate loss function

for the, uh, for the discriminator and so on and

so on.

But we won't look into this in detail.

It's not, it's not like a focus of this course.

But check it out.

You can get some seriously good images of this.

Um, I mean, there were these websites like a couple

of years back, uh, this person doesn't exist or so

I don't know whether you've seen these websites with, with

faces when it was still a major thing to generate

faces that looked like real people.

Um, and if you look at this, that could absolutely

be like, you know, real food, real dog, etc.

So, um, these are very big models and they are

kind of Closest to state of the art in the

Gan world, but you also see the papers, they are

from like the these years like they still work on

Gans, but the, the, the big models, they are shifting

water with the fusion.

So what I, what I like.

A lot of reasons maybe um.

This year can also be applied to a lot of

other data to generate, right?

So we, we kind of leave out sound here, but

you might have heard these things like, you know, a

B, like a Bach cantata that you have to tell

whether it's fake or real, um, like some, some music

piece or so often what's behind this, you know, or

possibly could be again, and again that architecture, you generate

sound.

Um, and then discriminators to say, is this really bar

or or is this not really Bach, right?

So like that's, that's the same logic.

Uh, with speech, there are these style transfer things that,

um, you can, uh, speak and on, on top of

your audio is like basically, uh, put the, the, the

voice tone of someone else and so on like um

a lot of kind of uh tools with, uh.

are based on gas.

Uh, the problems here are like, what you've already seen,

like these vanishing grade.

Do you, have you seen this in other neural networks?

So, if you multiply a lot of these to the

chain rule, you multiply a lot of these.

If they are close to zero, like the multiplication gets

really small.

It's been a big problem in recurrent neural networks, which

I think in this course, like, will not focus on,

because these days, but for now, you know, mostly this

transformer architecture has taken Over at least a lot of

them, and they suffered a lot from vanishing gradients.

Um, the guns have another kind of that sounds fancy

mode collapse, which means that you basically, uh, predominantly sample,

um, very common cases from the data, and I have

a nice image, I think that illustrates this quite well.

Um, they are very tricky to optimise, um, and then

increasingly many arcs or to defe models.

Good.

So that's basically a paper, um, also highly cited the

fusion models being scanned.

So let me send this.

That's, uh, big on some of these like state of

the art guns.

You see here, that's the true data of the image

net.

These all flamingos, and this thing just likes a specific

picture of flamingos.

So that's the more collapse, OK?

So it's basically focusing on one thing, and this is,

I mean, maybe they cherry-picked this a little bit, but

this is much more diverse than the images it's generating.

Good.

So, most state of the art architectures, um, utilise variants

of this diffusion.

Um, you might have, like, a lot of it's a

closed source because, you know, and these things are from

proprietary firms.

On that one, the first one that was open source

is stable diffusion.

Um, and, uh, and that's why it's also, I think,

discussed quite a bit.

It's a technical topic overall, um, and And also these

production level or or proper models, they combine a lot

of bells and whistles.

I'll show you like a flow chart in the end,

but, but, uh, don't worry about it.

If you want to read more about this, so we

are going about the key parts of this, but in

the interest of time, I only look at like some,

some basic parts of this whole paper, and I'm more

focused on the general idea of diffusion.

OK?

And that's the general idea.

It's quite brilliant.

Um, so you have, yeah, so first, like, um, you

have two different processes, noising and denoising, OK, two different

processes.

So that's, I start with that image here, and that's

a real image.

That's from my real data set.

And now I add some gauge noise, add a little

bit more of noise, a little bit more of noise,

a little more until I can't see anything, but I

have a pure noise.

I put an XT here, X capital T, you don't

see it because of the noise, but it goes from

X0 to X capital T.

And now there's this reverse process, um, and that here

now is in generated image.

So that model again gets something like our Z, the

random one, but now it's actually a rectangular image.

That's what it starts from.

And then it's step per step, per step to step

generating something and in the end, like another dog comes

out if you're trained on the domain of dog images

or whatever in this example.

Um, so, so intuitively, how does this work?

This year is cheap, right?

In terms of, I have some data, I have an

image of a container ship or a dog or whatever.

Flamingos.

Here I have a dog.

To add noise to it is, is silly in a

way.

I just need to schedule where to add the noise.

This year is trickier.

And there are lots of, you know, tricks to make

this computation feasible and so on.

But I think intuitively, if you think of this process

here, I'm kind of generating training data.

So I'm adding noise here, and I'm just doing this

from images I have.

But now I have these pairs of images.

With this image, I can predict this image.

With this image, I can predict this image.

With this image, I can predict this image.

With this image, I can predict this image.

And what I'm really doing is I'm predicting the noise

in this image, the added noise that was in this,

and basically, I'm removing the added noise in this image.

Then I'm predicting to remove the added noise in this

image that was here in this image and so on.

But like the, the, the simple, like kind of silly

architecture would be, I just take these pairs, predict this

from this, this from this, this from this, this from

this, this from this with the massive CNN.

And now I have a CNN that I can start

with noise.

I iterate it 2000 times.

In the end, I have a dog.

Like, this is not feasible, like, I like computationally not

feasible.

And you can use a lot of tricks of the

underlying statistical processes here here to restrict the model and

make it predict something more precise than really just the

full image.

Really, what you're doing is you're predicting only the noise

points on this and subtracting it from this to to

arrive here.

That's what you're doing.

But that's the, the logic is this you noise up

images, you have training data, you train networks on this,

you iterate these networks forward from again a random Z

and you arrive at images.

That's the key thing models.

OK, so here, I start with an image.

I, this is like you statistically, if you have heard

of these things, these are Markov chains.

Like with one, with one state here, I know the

next state, like, you know, I only need to know

that state for the next state.

Um, this is my Q function, and that's just a

noise process.

So the, my, with my old state, the new one

is I pick the new vector X is my image.

Um, and like B is very small, beta is very

small, so I mainly take the old one.

So the new image is mainly the old image with

a little bit of noise.

That's the identity times beta.

Um, so yeah, and beta is a schedule between 0

and 1 with very small numbers.

So every step, I'm just adding a little bit of

noise and my new image is mostly, I mean, new,

my new image is mostly my image from before with

a little bit more noise, but at some point, I

only have noise.

That's the 4 diffusion.

And then the reverse diffusion.

is to basically predict the noise in this and denoise

it and predict the next step.

Predict the next step, predict the next step.

OK.

And I won't go into all the detail here, um,

but there are these blog posts, if you're interested in

them, but it's beyond, a bit beyond the scope of

this.

Uh, of course, it's a bit more technical, but if

you're interested in this, um, we need that function.

We need to predict basically the probability of what could

come next after an image, you know, when we denoise

it, and we Get that with the neural network, but

we don't just brute force, get that function.

But actually, what we, you know, after a bunch of

derivations, you see, I can just predict the noise and

subtract it from the image, what I said before.

And my loss function is actually in the end, just

the real noise, which I know because I edited mine

is the predicted noise and the square.

So, very similar to noise to loss functions we've seen.

But actually, to arrive there, like, you have to make

a couple of assumptions, and it's, uh, it's a bunch

of algebras.

So.

But that's the kind of high level intuition of this.

And this notebook, I'm not fully happy with.

I want to explain it more because I think it's

not clear otherwise.

I'll explain it more and then I'll upload it.

Um, but it's, so the, the thing, the difficult thing

here is I wanted to first create a diffusion notebook

that was actually what we're doing, we're doing OK on

time, that was actually, um, creating.

Very sharp and digits, but like that would require some

GPU on call up and like a couple of hours

or like some really intricate architecture that I couldn't find.

Um, and this year runs very quickly.

And I found an architecture with a much er already

that kind of pushes it down and what I'm doing

here is I'm this, this predicting the noise that.

Usually you take convolution or networks that have quite a

lot of bells and whistles.

I'm just taking um.

Uh, you see here the Q function and so on.

Um, I'll just take the multilayer etc.

So a very simple no network.

OK.

And that's why it's so quickly on, uh, running on

my computer and so on.

With GPU, I, I can only use the CPU here.

I pushed but the loss stagnates here, so there's only

so much I can do.

However, this gives me a couple of nice outputs in

the folder that I thought are kind of interesting.

Mm.

Mm Uh, let me see, this was, uh, this was

the wrong folder.

I want this again.

Well I mean it's quick.

Here we go.

And then what would come out of this?

It's actually, I mean, it's a bit silly because it's

so simple, the example, but it's the forward and backward

diffusion process and the final outcome, and the final outcome

isn't great, but at least you can see the shapes

of these numbers, OK?

So that's kind of the best, the best I could,

I could see that runs in like 2 minutes.

Um.

OK, so here we go.

That's the diffusion process and the reverse diffusion process.

So first of all, we can now really depict this.

So we have original T0, T 100, T200.

So that's exactly, you see here, I can see a

little bit of the 0 at 100, a bit more

of the zero.

and then there are the, uh, there's reverse process, very

difficult to predict from this, but these are 50 steps,

right?

So like it's, it's a step by 1, but you

see this is a challenging problem.

Um, and then the final ones, I mean, at least

you see something reminiscent of these numbers, OK, so we're

kind of, you know, like, yeah, it's doing this in

2 minutes.

Um, all right, cool.

Um, that's that.

And I like once I'm happy with how I'm understandable

it is, I, I'll upload it, but um, yeah, um,

it's not the focus of this.

All right, cool.

So last slide.

Um, this looks funky and well, I mean, my, my

main point about this, this, these are these schematics that

you see in like actual machine learning papers on production

level stuff.

This is from the stable diffusion paper, uh, the, the

original one, OK.

Now there's like, you know, I think stiffusion like 3

or whatever, but, uh, this year is, is the stable

fusion 1 and The one thing I want to say

here is that we did a lot of this now

in this lecture, and, and you did this in Tom's

lecture already.

This is the image.

This is a decoder, pushing this into some latent space.

So now, this is like in some invisible space, we

are like, I mean, now we, we've made it visible,

but in some lower dimension.

Now we actually run the diffusion on this latent space.

So we take a noise, like, you know, we, we,

we run the diffusion process of this year.

We learn the diffusion, and then we start with a

random point in that latent space.

We create until we converge for 2000 steps or whatever,

these diffusion denoising steps, and then we run the decoder

and we have the image.

So we're running this whole kind of denoising and noising

step all in this latent space, and that makes it

computationally feasible, and that latent space has much lower dimensionality

than the actual images.

Um, and what are the Q, K, and these, these

are queries, keys, and values from transformers and attention, and

that we do next week.

And then also, you see here, like this, this is

a different architecture that some of you might have seen,

but we didn't do in this course yet.

And this is also conditioning because I said in the

beginning, right, you know, give me an image of like

a of, of researchers thinking about Gen AI, um, and

we need to encode this, so we also need an

encoding of the text.

That's also coming from some architecture.

Um, good.

So that's the attention mechanism of transformers.

attention in particular one of them, and we need transformers

going.

Any questions?

I know, I'll stay around for a bit long.

Yeah.

Yeah Oh.

Yeah, I see.

I'm so excited with this, but we power through last

term with like the.

Oh I see.

I'm sorry, I'm your um.

I see, I see.

Is it, is it any week that you know, or

like just this week, like, or any week, I have,

I mean sometimes it's like, so so, um, but otherwise

you just yeah you probably want, yeah, if you want

to speak sooner than later probably.

Yeah, yeah, yeah, that makes sense.

You can just send me like if you send me

an email, then we just schedule something else.

OK, yeah, sure.

Mhm.

Do Had a good reading week?

Well, yes, she's reading.

You've probably all been writing I.

Yeah.

I I.

Hey He was a little scared.

My I see.

It's.

This charity shop find.

So.

S as well suffrage, it's very important for us.

It's always very like springs are very like close and

she's, it's a it's a fact.

She has a.

Oh.

I see you coming to the the history world.

---

Lecture 7:

it's like basically what I can't see if I can

put a say like.

you get.

Did you get it here you go.

I need to otherwise I I'm not eating anything.

I'm eating like one meal a day like, OK, I

need to force myself no sleep no food here running

off coffee basically your coffee.

Good, good.

That's we are reading into the um for space OK

yeah yeah yeah I think we learned one of the,

we discussed one of the papers in the lecture that's

why I why I keep having like flashbacks.

Um, I think the white union membership and white racial

politics.

OK, but we talked about something about biracial white racial

politics, and we said like, Yeah, hm.

That's honestly only the reason I'm coming in because I

need to like legally make my money.

Yes.

just You Um 2 Just.

Yeah, I like, but uh I found out yesterday that

I was like I don't know, it's like sad.

Maybe I'm just like uh I don't know.

I just think it's very yeah, it is.

I just feel like even for me like first time

I went out there was like yeah, yeah, I think

it would have been great happens for a reason.

You never know what's what's next.

OK.

Yeah, I think it's I mean I.

Yeah.

Like it's just I think that is something.

When the pros and cons list comes out, like not

a good thing, um, and then I was like, OK,

I like the best.

10 days yeah I made friends.

They didn't.

I wanted to go there.

I had two nights.

Of your used to camps.

I can.

I just don't know what.

I told you I like because they were not.

You could see them like this.

So my friends that are just sitting inside like.

And they were just excited and they both became like.

like we arrive like what do you think they were

like you know we haven't got this stuff yet.

experience.

I got this I for.

This was, yeah, no, I, I actually had some friends

last year but.

I'd like to know some of them like it's a

lot.

I think the people on the actual.

OK.

Oh.

I don't In I, I think.

The lights should be on mode now.

This all worked.

That's good.

Yeah OK, awesome.

Here we go.

Um, yeah, so I think today is, uh, yeah, very

exciting topic.

Pretty much all the weeks from here are continuing to,

uh, be quite like close to I mean, in some

ways to the frontier.

And this week is going to be about language models

and about transformers.

So one poll, how many of you already know how

Transformers work?

A little bit, sort of.

OK, because I'm wondering like how kind of how commonplace

this is getting.

OK.

Um, and the tension, have you heard of the tension?

Sort of.

OK.

Cool.

And, um, yeah, so we'll discuss all of that today.

Um, and I think we have enough time.

And then, uh, depending on how much time I have

in the end, I, uh, I may otherwise show you,

uh, next time we need some research using LNMs to

link this more to social scientific methods.

Um, OK, cool.

So last week we studied image generation, um, you know,

with these approaches that we discussed in quite some detail

like variation of all encoders and other approaches that we

discussed more briefly such as diffusion.

And we concluded with the notion that for a system

to generate specific images, we would have to encode it

prompt in a very high quality way and then feed

it into such a network.

Um, and today will be all about encoding and generating

text.

That's how this connects, OK.

Um, and overall, if you think of currently what we're

seeing as advances in intelligence levels, a lot of this

is driven by text, um, in one way or another.

So, uh, what we're doing here is relevant for lots

of architectures, many things are built on these, uh, systems.

So that's the outline I thought about for today.

I start with some foundation on, um, on text processing

and, and, you know, machine learning models, um, because we

haven't had, you know, the foundations that we would have

in like your text analysis course.

That's the first part.

Then I discussed like sequence models and a little bit

of the current neural networks.

There are also some questions about this.

Um, and then from there we move to the transformer

architecture, uh, using that architecture, we discussed base models, uh,

from, uh, language models, um, and then we go up

to supervised fine tuning to make them ideally respond to

questions.

Um, and everything else is, is, uh, reinforced the learning

driven to some degree, and we're going to revisit this

after we spent the subsequent two weeks of reinforcement learning.

That's kind of the flow of, of the next weeks.

OK, so.

The other foundations and the key question is this, how

do we move from text to American representations, right?

Because computer can't deal with with text as we do

so we need to encode this in America.

Good.

So imagine this is like, um, you know, very simple

data set with 4 patents, invent a good dimension, bad,

dimension less good dimension.

OK, so that's my corpus.

I don't have any other documents.

That's all I have in terms of data.

Um, and if I tokenize at the word level, then

there are 5 unique tokens in total.

So if you look at these documents here, there are

5 unique tokens.

And the tokenization um dictionary or tokenizer is really just

this kind of dictionary, uh, with integers mapping to these

tokens, and then we can go from tokens to integers

and back.

Um, so imagine that or have a look at that

document, a good invention, that document would just be 031.

So a language model actually doesn't read in a good

invention, but it reads in 031.

If it encodes it, it spits out some vectors.

If it breaks the next token, it might break 42,

token 42 in whatever dictionary.

Here, we only have 5 tokens in total, so it

might, you know, a good invention, um, yeah, nothing really

fits here.

Maybe, uh, something could be a sensible bridge.

But more or reading the sequences of numbers and then,

and then if they, if they return um text, they,

they return another number and then you can nap it

back into the.

Any questions about that?

So that's like kind of a for example now on

to something, um, slightly more or like certainly, um, more

realistic and, and what was actually used by current um

state of the art models.

So intern we talk and, you know, your computer we

need a bit more time for this and asking coding

and so on.

Unicode maybe first this computers, um, represent characters and with

character, I mean, you know, could be.

Um, that could also be holdings of, of emojis and

so on, but obviously also traditional letters.

Um, all of these are represented as sequences of bits,

as everything in the computer.

And then, you know, just the definition is 8 bits

are 1 byte, um, just to make it a bit

more manageable because you need usually like at least, um,

1 byte sequence to represent characters and unico, these sequences

are very.

Um, courses like 472 discuss this in a bit of

detail, and I believe also Ryan to do some text

analysis.

So now our computer represents these things as zeros and

ones.

Now imagine you, you represent a very long text just

as a sequence of zeros and ones.

That's just like the internal representation.

And now the whole trick about tokenizers is to look

at that sequence those sequences of bytes, which are very

common.

You know, by whatever might be followed by another bite

very frequently.

Then you bundle these bytes and those are tokens.

Sequences of ice.

That's not like, you know, if you look at the

current state of the art or like, you know, models

like the, the old series or GPT4, um, from OpenAI

or, um, the, uh, the cloud series from anthropic, then,

um, these tokenizers, they, you know, for this model here

has the newest one has around 200,000 unique tokens.

Where does this come from?

It's, it's quite a bottleneck for some, uh, um, things

in these models.

We look at some failure modes if we have time,

um, and they come from sensible splittings of these byte

sequences into commonly occurring, uh, commonly occurring, um, themes or,

you know, tokens.

Um, so that's, that's where these numbers come from.

But just to give you a ballpark, the previous tokenizer

had 100,000.

So there's a lot of, um, that one was GT4,

I believe, um, like the legacy models, um, that's roughly

15 years.

Um, good.

So now, I use that tokenizer actually, and when I

tokenize this, London School of Economics, we have 4 tokens.

And actually, this is kind of nice here because we

can explore this and try different things.

And it makes it more concrete.

So this is the tokenize I chose, OK.

So now, London, London School of Economics.

So you see we have 4 words but 5 tokens,

and what's the deal here?

London is usually not spelled with a lower case l.

That's why we have 5.

So here's here, you see exactly this, right?

These common sequences.

lower case letters is not common.

If we do this, presumably yes, we have 4 tokens.

OK, so here we can explore stuff and we can

um explore.

There's also a Python library, it's TikTok library, you can

just import it with a two liner transform text tokens.

Sometimes I do this for example, in research when I

um want to prompt the model and it has a

maximum context and I want to see that I only

exceed that context.

So I take a long concatenation of text, transform it

to token, measure the length of tokens.

It's roughly 4/3 times the words, roughly, but it can

vary.

Um, and, and so I have an exact measure.

So that's, for example, how I use this in Python.

But this is nice in terms of visualisation.

Um, OK.

Great.

So, these are tokennizers, um.

Any questions?

OK, so to represent meaning, however, of these tokens, so

far, we just have numbers, right?

So this year's London School of Economics if we know

the dictionary to map it back, but there's no meaning

in such numbers.

So how can we make a computer, like, you know,

we would actually have to assign a vector to all

of these, right?

Or like some kind of list of numbers, maybe a

unique number vector dimension, uh, one that could already be

that one, an integer.

But if we want to encode more, then each of

them should carry some internal representation.

That's how, how the model should work.

Um, so, and it turns out you will have heard

of these, uh, there are the so-called embeddings, and they

are vectors encoding photons, and that's the computer's way to

deal with whatever we think of meaning in language.

OK.

So think of I have this, I think it's maybe

a tricky to read this, um, stylus example here in

three dimensions.

I have these words bank and loan.

I imagine we tokenized at the word level, bank and

loan, basketball and football, um, they should be in similar

directions of this embedding space if this model works.

Right?

If we initialise them randomly, it would be all over.

But once the model, uh, was trained, if, if it

worked, it's likely that we see pictures like this.

Now, uh, traditionally, like around 10 years ago, there were

these static embeds that you could train on Wikipedia.

Actually, we can do this.

Um, you know, you can, you can, for example, download

from Ben, the word to back model, and I've tried

this.

You can train it, you can download Wikipedia yourself and

train it.

Um, and then you get representation for, you know, the,

the common words in Wikipedia if you.

Less common ones, and they have very sensible geometric relations.

Uh, we study this in more detail and text analysis,

of course, it's quite interesting because as you see how

the, how the computer is working internally with this language.

But for us, the most relevant thing is after training,

these things will be positioned or hopefully be positioned in

a sensible way.

And that's how this computer, um, or like this model

managed, just, uh, manages to be so, so capable.

So these here have been, um, have been developed just.

To give us these word vectors.

Now, we are more looking at token vectors.

And if you sometimes I have, I worry about this

myself, or students ask me, can I not use transformers

to have contextual embeddings?

What's the meaning?

This year gives me one word embedding for all of

Wikipedia.

So inflation will be the the same embedding, whether it's

next to grade or whether it's next to monetary.

But it doesn't have the same meaning, right?

For a capable model, that wouldn't be great.

But this year, they give me these averages and average

embeddings in the underlying data.

Now current language models depending on the sequence, they will

give me a different embedding, depending on what's next to

it.

However, usually they give me embeddings if I download these

massive pre-trained models that I can only get with cloud

compute.

They give me embeddings at the token level, so it

might be a fraction of a word, might not be

that helpful for me.

Plus they all the time switch, depending on what the

context is around it, and so on.

So, um, it's not that easy and, and kind of

tractable as this.

You might have seen some research here where people track

these embeddings that are static, but over time, then they

could look at semantic shifts, like, you know, certain, certain

stereotypes and so on.

There's a bunch of social scientific research with this.

However, the the modern models, they learn them sort of

oppa, right?

They learn them while they try to minimise their loss,

which might be next to open prediction, might be the

objective, and they try to achieve that.

And while doing that, they learn these embeddings because that's

the way how they can predict the next.

OK, questions.

Great.

Um, So then before we go to the transformer, let's

look at a bit of history and, um, and, uh,

what people refer to as sequence models and intricately linked

to these recurrent neural networks.

Only a few years ago, this would have been a

bigger part of a course like this, undoubtedly, a bit

like CNN's.

And you never know with these tools.

They might come back in one way or another.

But for now, we, um, the transformers have at least

in many, many use cases, um, uh, um, taken over

from, from previous, uh, recurrent neural network setups.

So let me show you this first of all.

When you see this task, I mean, one of the

biggest, as unexpected as funny lots in science is that

we get these levels of intelligence by breaking the next

word.

Um, this is not a new thing.

So there's this paper, um, from 2003 of uh neural

language proneural language models by BenGo, um, and, uh, they

use a model to you see really do something very,

very similar to what SGPT does today.

Um, and they even have like, you know, embeddings, so

they, they call them slightly differently, um, uh, for, for

a context of words, obviously a much shorter context.

I looked it up.

I think they ran it for a few weeks on

CPUs and so on.

Um, and, and then they, um, predict given that context,

even the word from today's here, they predict, uh, they

predict the next to.

So in that kind of architecture, you can either do

with an MLP, MLP and simple feed forward neural network

are used interchangeably.

So that's just that network that has fully connected layers

and processes information from left to right.

All our networks are feedforward that we're looking at here,

um, but the MLP is the simple one with fully

connected layers, no convolutional layers, no layers.

OK, um, great.

So either these simple ones, you know, or we could

use these recurrent our networks, um, to, um, to predict

the next, um, next item in the sequence.

So RNNs are fairly natural to like the the logical

set to achieve this and and.

Yeah.

Also, um, they have been used widely in machine translation,

um, in sentence completion, and sentiment analysis, image captioning, you

name it.

Um, and there are many variants of these, but to

just, you know, categorise them broadly, um, think of, uh,

this is just the standard feed forward in our network,

that's the feedforward block.

OK.

So that's the X being mapped into a Y through

my parameters.

And then you can think that way of uh recurrent

neural networks like rolled out traditional neural networks.

So you have an entry point, and X goes into

the network.

Now you have something that looks like in the plain

vanilla recurrent neural network.

That's a feedforward neural network.

Um, and, you know, it might or might not.

Emitter state that is in itself a terminal state, but

it also emits another state, you know, think of this

like Y or Z or something like this, and that

goes into the next network in combination with maybe if

we have many to many or many to one, maybe

with the next with the next X.

Yeah, so we have this roll out, um, and now

the broad architectures are one too many.

Think of this, for example, could be an image.

Um, and that could be like outputs describing the image

if each of the blue ones are token embeddings, right?

That makes sense.

And then many to one could be a sentence, and

you map it into sentiment, positive or negative.

If it's 1, then, you know, binary number 01.

Many to many, these are sequence models, um, that go

from sequence to sequence.

And that is, for example, translation.

Imagine a sentence in German, a sentence in English comes

out.

Um, so this, this is what is meant with students

to sequences.

Um, now, these different ones, they all have this kind

of roll out architecture and the state is passed on

or not depending on like the state is always passed

on because between the current units, but sometimes also terminal

states are or um are omitted, uh, and sometimes not

depending on the architecture.

Do you have questions about this?

And the simplest way would really be take a feed

like I mean not feed forward but take this MRP.

And that MLP actually gives you, uh, gives you, um,

two outputs if it's one too many.

Um, and the next one you fit, uh, you feed

into the next MLP.

And imagine then, really, I have an image that goes

in as a vector.

Um, and then in the end, uh, I say, um,

it's a classifier, whether it's a cat or not, you

know, something like this.

Um, and then it could go through here.

Um, but more likely it would be, I would make

use of that, that would be possibly better suited with

the convolution or that.

But imagine I want to describe the image so I

have like a sequence of talks.

Good.

So, these things, maybe that's intuitive already from how I

drew it out.

They struggle to remember, uh, long dependencies.

That's been ever since, uh, uh, introduction and, and trying

to incorporate realistically on sequences.

That was a struggle with them.

Imagine, um, I have an input of the, of the

degree that we're looking at these days, you know, we're

looking at context lengths of hundreds of 1000 tokens.

Even if I Say a couple of 1000 tokens, and

then that function needs to be so expressive or nonlinear

that if I change a token, 242 out of 10,000,

that really the output might change.

So you need to really remember these things.

Um, and then there was work, because it's like an

obvious limitation of this.

If we just roll out these traditional, um, fully connected,

uh, networks, there was work on this and the most

kind of commonly sighted units, uh, you know, units are,

are.

Kind of think of like in these cells are LSTM

or GRUs, um, uh, gated recurrent units or um long

term, long short-term memory networks, often people call them um

LSTM networks, um, but these were also a specific kind

of cell that retained information.

Um, so these were mainly meant for, uh, for being

able to remember those things that were most relevant for

classifying, you know, say the final thing, predicting the next

token or whatever, yeah, um.

Right, many to one could also be next to, for

example.

But these architectures have a bunch of challenges.

I mean, every architecture has its challenges, but this one

here had a had a set that made it tricky

to train on very large data sets relative at relative

at least to the transformer.

So one thing are the, you know, commonly account of

vanishing exploding gradients.

Why do you think that's a bigger issue here?

I didn't add it to the slide because I thought

maybe, uh, it'd be good to discuss it.

So why could vanishing uh gradient be a bigger issue

here?

Yeah, exactly.

So, um, each of them, think of them, each of

them would have its own back propagation because each of

them is own network.

And now you have this back propagation through time, um,

that actually goes through all of them and gives a

very long multiplication.

So your likelihood that this goes to zero explodes just

by that part.

Um, if we think of, um, uh, networks like this,

say we have a decoder part, like something that, you

know, I'm sorry, an encoder part, so something that encodes,

say a sentence in, uh, in German, um, and then

it passes it on to another section of that recurrent

neural network, um, and that.

Edit it at some point in French say, then that

bottleneck here has to carry a lot of the information.

Um, so you have to pass a lot of the

information from encoder to decoder through like very few um

parameters relatively.

That's the encoder decoder bottleneck.

Now, by the sequential nature, also the paralyzation of these

things is limited because I always need to be propagate

my errors.

Like, it's tough to do this in parallel, whereas the

transformer I'll show you later has a much easier time,

although it's not so natively a sequence model, um, but

it has an easier time paralysing, um, uh, sequences of

different lights.

Um, OK, um, and, uh, yeah, so today transform for

now, Transformers have taken over a lot of the tasks.

If you are interested in these.

These are two, like, you know, hugely popular blog posts,

but you see here also by the dates, um, um,

these are like this gives you a kind of a

flavour from when these concepts like LCMG are used, uh,

when they were discussed.

So, for example, this one here is a great blog

post on really discussing, like, see, this is what we

just said, um, and this is looking at the specific

cells that try to retain information.

They are quite complicated.

Um.

OK, yeah.

I mean, if you're interested in this, someone wrote me

um about recurrent neural networks, um, but if you're interested

in kind of the history of this, then, then it's

uh it's, it's uh great to study them as well.

These are good resources.

Um, OK, other questions.

But because for now transformers have, they are like pretty

much kind of a copy paste architecture to some degree

for lots of different applications.

That's the obvious architecture for us to spend like the

limited time on, um, that we have.

So now on to attention, um, which is the key

part of the, of the transformer.

Funnily, it's actually not being developed in a transformer.

Um, there's this paper from 2014 where they developed this

for RNNs precisely to You know, counter this bottleneck stuff

between Ecoda and Dakota.

Lots of these papers were in machine translation in the

beginning.

That's where the sequence to sequence comes from.

Lots of them were wondering about, oh, how can I

translate a sentence from language A to language?

Um, and now, this was an issue, so they tried

to have these units attend to other units.

So basically, get information, like selectively search the other units

to try to get the information from the ones that

matter most, maybe from the word that mattered most for

a specific translation, etc.

Um, you see it here, it like really resembles a

lot of how we think we projected that the use

of fixed line vector is important that can improve the

performance and go to decode that's what we talked about

and proposed to extend this by allowing a model to

automatically soft search for um parts of a source sentence.

Um, for relevant, um, that are relevant to predicting a

target.

Funnily, they first thought about calling this RNN search, so,

uh, uh, we wouldn't have that name attention today.

And then, um, then, uh, I heard this in one

lecture, and then one of the, uh, of the authors

was suggesting attention and that became like that has become

such a prominent name and kind of machine learning over,

over the last 8.

Good.

Um, any questions about this?

We'll discuss in lots of detail, of course, but just

to give you some history, I think that's kind of,

uh, important as well.

So now the transformer architecture.

And this is actually a hint to the paper time

of the transformer, um, which you might know.

So in the end, like, you know, you could make

RNN a lot better with this, but if you just

took attention on itself, like with it, like, you know,

as itself with like some nonlinear transformation and vision.

Turned out that was enough.

OK.

And then they pointedly make their paper attention is all

you need.

That paper, just to give you an idea, is published

in 2017, I think has a 160,000 citations.

I know something like this.

Everything you see in like, you know, language models these

days, all these advances, all transform architecture.

Interestingly, that paper was originally written for machine translation again.

Again, an encoder and a decoder part.

Today, when you use models like Chad GBT or BERT,

then BERT is an encoder transformer, and shared GBT is

a decoder transformer, and this combination of machine translation is

not commonly used, uh, as commonly used as the other

ones.

Um, when you read the paper, also you see that

they developed this very neat and very carefully thought through.

So a lot of hyper parameters I used that way

architecture, but I don't think anyone when you, when you

build this can understand in what many ways it will

be used later on.

So it was really Um, a very good machine translation

paper.

Um, but then afterwards, it developed, like, uh, um, yeah,

I mean, a massive life in its own, like it's,

uh, like it often happens in research on, uh, on

very different tasks, um, like language modelling of like prediction

or, or encoding.

Um, OK.

So this looks a bit cryptic, but I think all

of this will make a lot of sense, uh, um,

at the end of the lecture of this unit.

And really, lots of this is still.

Very similar in in pretty good architectures today to the

degree that they are public.

Um, one thing is the norms are switched, like they,

they come before the forward and before the attention and

so on.

But like that's the kind of detail we're talking about.

Other than this, like these two blocks, they are, they

are, um, uh, conceptually at least at the larger picture

um used in that way.

Good.

So E transformer block, that's what these guys mean with

N times here.

That's a, that's a transformer block.

That's an encoder transformer block, decoder transformer block, um.

This year has this double multi attention.

We focus on what is self attention, a sequence, you

know, looking at elements of its own sequence.

There's this cross attention for this translation things where one

sequence in German was looking at elements of the sequence

in English that we won't use anymore.

But other than this, like, we use attention and feed

forward, they make up a transformer block.

So attention feed forward in our network.

Um, so we'll discuss this more generally before moving, um,

uh, you know, to, to decoder transformers that drive large

languages.

Good question so far.

Great.

OK, so let's make this as concrete as we can.

Say we have a sentence, language models are interesting.

That's all sentence.

Now I ran this through the tokenizer and it would

generate these numbers, you know, that's just like integers in

this lookup table that we, that we discussed in the

beginning.

That's it.

Now, entering the transformer, these things.

Get input embeddings.

This is just a lookup table where each of them

gets a vector.

So each of the tokens in the, in the tokenizer

gets a vector.

In the beginning, how do you get this?

Randomly initialise it.

Um, these things here are, are learned through back propagation

as well, and they change, but they are static.

Whatever enters into them gets the same number in the

lookup table.

The contextual embeddings and embeddings changing happens throughout the transformer.

But they enter the transformer, you get a static embedding

for the, for the, um, for the token, because you

have to, to continue your computation, um, and then you

land at, at this year.

So you have these 4 tokens now being represented as

4 vectors coming from that lookup table.

Now, the next thing is you want to encode that

position as well, because one thing is to know that

this, the first word here is language and the second

word is models or token, but you also want to

know that languages at position 1 in the context, um,

models at position 2, etc.

So you add a positional encoding.

This could be another lookup table, so another vector embedding

that you pull from a table, um, and then learn

through gradient descent.

Or it could be some function, um, they like on

based on a sign or cosine if you check on

the paper.

That's a bit like an empirical question.

Simple implementations, simply look up the positional encodings and like,

um, you know, imagine.

Imagine my encoding dimension or my, my, my embedding dimension

is 1000, 1,024, and my context is um 2048.

Then I will look this up in, um, in the

gigantic table of as long as my context, each, uh,

with the dimension of these synthetics, and I just take

a, take a phone.

Great.

So now I have embeddings to start with, and they

enter the first attention.

Um, so we're now here.

Um, they enter the, uh, the first attention and now

we're going to discuss attention.

So now.

Quick word on attention.

Very often people focus on this particular implementation, that is,

you know, the economical one, that's not attention.

You might have seen these keys, queries, values, and so

on.

And that can get quite confusing, at least when I

first saw this.

The fundamental thing, however, is, you know, a lot more

general than, you know, how you precisely implement it with

matrices.

And, um, in, in, in one of his lectures, Capafi

has this nice example of, of, uh, you, you imagine

even much more generally, I have a network, OK.

And I have notes, and these notes are, you know,

say node 1, not 23, not 4.

And each of them carries some data, OK?

So each of these nodes has a vector, so it

carries some data, and that's not feed forward necessarily, that's

just the network.

And now each node could, when updating its vector, attend

to the other nodes.

And we could, for example, update that vector here.

As export Prime.

Being like a weight on its on its own, Alpha

4 X4, and then a weight on X1 plus a

weight on X2 plus a weight.

Times X3.

So we're really updating this as a linear combination.

And where do these alphass come from?

We should learn them depending on what we want to

achieve.

But attention is this general thing of thinking of a

network, and nodes relate to each other.

And then I can update data that's stored in nodes

as a weighted average of, you know, the node itself

and all the other nodes.

Now, in the transformer, this will have this kind of

feed-forward way to do this, but the mechanism itself is

much more general.

So you have a lot of data points that connect

to each other, and by the way they connect, I

can compute how they are updated.

They are updated as weighted averages of their connections.

Where do I get the alphas from?

Depending on my objective, whatever I want to do with

this, I probably optimise and try to find the right

fits.

And, and as simple as it is, but that's why

is this called attention because say my alpha here is

0.9, I attend a lot of the data in that

note, right?

My alpha is 0.001.

I don't attend a lot of the data here.

And yeah, as confusing as I said, but that's, that's,

that's the essence.

Um, it's, it's pretty simple.

Um, and now to the transformer.

So we have this, you know, just to recap, we

have this sentence here language models are interesting.

After doing the initial stuff, we have these 4 vectors,

OK?

So we arrived at the first attention block with these

4 vectors.

Um, we would actually normalise them now.

We would have some have some norm on them, but

let's abstract from this.

So they arrived there.

Um, it's 1, it's 2, yes.

So that the number of dimensions after the embedding and

the position do we add a dimension.

No, so yeah, good point.

So it's just imagine they have the same dimension.

So if I, if I have the simple addition here,

the two vectors say X2, um, and, uh, the position

of X2.

The positional encoding, um, if I can add them, they

are vectors of the same dimension, so you, you can

have variants of this, but very simply it's often.

These are usually run on, on, on GPUs and so

on, but I, I may, you know, try to build

like a very simple one with one head or so

where we can actually go through codes step by step.

Um, but it's, uh, but, you know, it easily gets,

gets out of hand until to, to get something that's

actually, uh, predicting the next to reasonably well.

You, you often need like to computers and GPUs.

Um, But uh yes, OK, cool.

So we have the, we have the uh the X

ones, they enter the attention.

So this is just these are the 4 for us

variable, whatever how many sequence points we have, but for

us they're really just these 4, these 4 vectors that

we have.

So now the first thing is to transform them in

what people then call a product attention of value.

But it's insanely simple.

It's just X1 being, you know, multiplied with the, with

the matrix.

So we have the value matrix, and that is multiplying

X1.

Imagine this one here of dimensions, say 10024 times 1,

then the value matrix could be 1,024 times 1,0024.

Um, and then the resulting vector is again 1024.

Um, just it gives it some more flexibility.

By the way, if the, if we chose the value

matrix to be the identity, then it would just be

X1 and X2 enter it directly.

Imagine this would be the identity matrix, right, this one

here with the ones on the diagonal.

Then multiplying, multiplying X1 with it is just X1, and

yeah, so that would be like the simplest model.

But we give them more yet a bit more flexibility

to actually not average over the original axis, but average

over some small change of them, right?

So like, really what we're averaging over are these value

vectors, but they are just the original ones times W.

And now the alphas, and that's it.

And the new X1 is the weighted average of all.

The X2, um, has different weights.

It's alpha 21, alpha 22.

They all have their own weights.

Um, alpha 2T, um, and then the second one would

be alpha 21 times this plus alpha 22 times this

la la la, however many there might be, uh, times

alpha 2T times this value.

That's it.

We update all of them, attention done.

So it's simple weight and averaging, but by choosing the

alphas, we can exactly choose to which token here to

attend, right?

And and which token to to conclude, uh, to include

in our, in, uh, in our um average.

Now, when you hear about queries and keys, um.

You know, when you read materials or or or attend

all the lectures and so on this topic, then really

what the alphas are, I like to think about this

first in terms of alphas because it's so simple and

in terms of this way, but really these alphas, they

are functions of further weight matrices, keys and queries.

Next slide and it's also intuitive, but it makes it

a bit more, you know, more notationally intense.

But this is the core, um, just averages over, over

these embeddings.

Now how do I get the alphas?

That's the question here.

They go alpha 12.

So what is alpha 12, Alpha 12.

Is the, the, the weight on the second embedding vector

in the updated first one.

That's it, right?

So alpha 12.

How do I determine it?

First, I have to compute what is a query vector.

Again, think of WQ as a first initial randomly initialised

matrix and then learn to gradient descent like everything in

this architecture.

And again, I do something like this.

Imagine it also comes at 24 times 200024.

I multiply again the same X with it, and I

make it yet a bit more flexible, and it's not

quite the same X because now it's X1 query.

OK.

And now I do the same with every other token,

but with a key matrix.

So I now have a vector X1 key, X2 key,

X3 key, and so on.

So what I have for now are just 3 transformations

I have.

The next one.

X1 Q An X1 OK, OK.

This here is simply what I average over.

That's my value, the value attached to each node, so

I average over the Vs, and these here are just

heck to get the alphas.

OK, good.

So how do I get the alphas from this?

Um, and let's go through this slowly.

Um, say I want the weight of the second of

the second, so I have X2 here, X1 here, I

have X1 here, X2 here, blah blah blah.

And what we're now wondering about, well, when I update

X1, what is the weight of this thing?

That's, that's, that's what we're aiming at.

That's alpha 12.

So we want this thing here.

How do we get alpha 12?

So now, for X1, we compute the query vector.

For X2, we can compute uh we compute the key

vector and then we take the dot product uh to

them.

So why, why does that make sense?

And a couple of reasons.

First, think of query and key.

It's like, what is Token X1 looking for query?

What is Token X2 offering key?

So now you take the dot product, buy the dot

product, because the dot product is a pretty good measure

of similarity.

That's why.

So the dot product between two vectors, I imagine I

have these two vectors.

I have 12, and 5 and I multiply this with

a vector to for.

And say 6 or so, then that will be a

high number, OK.

If I have minuses here, that will not be such

a high number that product.

So do products in themselves are some measure of similarity.

Interestingly, if I were to normalise each vector by its

length, so say I were to divide this thing by,

so say I have a vector X and a vector

Y and I multiply them with the top right, we

said that's broadly a measure of similarity.

Interestingly, if I divide them by the length of the

vector, Then this is the definition of cosine.

So when you see in, in when you know, similarity

between words and embeddings and so on, the definition of

cosine similarity is this.

So what we're doing here, although we don't divide by

vector length variably, we're pretty close to cosine similarity here.

We just normalise to make it numerically more stable by

the number of dimensions of the vectors.

So we divide by the square root of the dimensions,

has some numerical advantages.

Um, but what we're doing here is conceptually pretty close

to like similarity measures.

It makes complete sense to get the alpha this way,

because what's the weight of the 2nd token on the

1st?

Well, what the 1st token is looking for, what the

second token is offering, take the dog product of the

2.

If it's a high number, it's going to be a

high weight.

If it's a low number, it's going to be a

low weight.

How do we know what they're looking for?

All of this is learned through gradient descent with the

sole objective to maximum or to minimise the loss function,

or maximise an objective.

OK.

Good.

So here, um, what, what is this kind of cryptic

thing?

First of all, the dora may not be positive, right?

It might be negative.

Um, so we take a good old kind of soft

max logic here, same thing that we discussed in the

classes.

So I wrap it in an exponential, and then I

normalise this to make sure all the alphas add up

to one.

That's all that's happening here.

So this is X1 is asking for something, X2 is

offering something.

Um, this is, this is the top product of the

two, and then I do this X1 is asking with

who, whatever is answering, like all the other tokens, and

that's how I get the weights.

I first initialise, say, imagine I initialise all the W's

randomly, then my weights will be broadly uniform.

I will get weight, I get alphas that are broadly

won over and.

Um, but then through gradient descents, these weight matrices gets

updated, and I will learn alphas that are actually um

picking up meaningful connections.

I think you had a question, how do we decide

if 1 and 2?

Yeah, so I mean, imagine like we initialise this randomly

with the Ws, and we think on average the alphas

will be 1 over N 1 over T, you know,

the, the sequence length, OK, so it will be uniform,

the alphas, there will not be weight on much.

So now, if we want to predict, say, the next

word, then all of these, we can stack all our

Ws in this, in this gigantic parameter vector.

We do gradient descent on it.

And then we will endogenously learn Ws to pick high

alphas, or, you know, high Ws or high do products

where things should be connected.

And when you say whether to pay attention, think of

a low alpha, something close to zero.

That's meaning these two have share very little connection in

what our objective is to predict, uh, you know, an

output and minimise the loss.

So we start by calculating the weights and then we

decide this is what we're going to.

I mean, it takes, it will take us a bit

of time, but what we're trying to do is we're

building a network again.

We're initialising everything randomly.

We can do a forward pass, just like you did

before.

We're just building a funky architecture, you know.

And then we can do a forward pass.

Um, and once we can do a forward pass with

randomly initialised parameters, we just learn through great interscent.

And then these things will like get very meaningful the

ways you can make these attention matrices on what pays

attention to what, but that's just endogenously resulting from this

thing trying to, you know, um, minimise its loss in

language models predicting the next to.

But you know, it's easy to get lost in WQ

WK and so on.

Like, I mean, the notion is weighted averages of things,

it's a linear operation as well, this one here.

So you, you do weighted averages of, of inputs.

inputs are encoded as vectors.

That's it.

How do you get the Alas, in a very clever

way with the stop product attention, if you use the

product attention by defining these three matrices, see that is

characterising the entire attention block these matrices.

I average over the values and the Qs and the

cases.

Give me the alphas, give me the weights.

So are those three separate weight matrices, exactly, exactly, but

you know, like you stack them, you combine them in

one gigantic and then you you separate them again in

the different computation like when you look at actual code

of these implementations, it's a bit messy because you also

have the patch dimensions so you have encils everywhere and.

On, um, it's the usual thing with this, so, um,

but, uh, but conceptually that's what's happening here, yeah, so,

um, but you might sort them imagine you might sort

them in a, in a long table with one here,

one here, one here, and then we might separate these

and reshape them, and then we do the computations and

then combine them again and sort them also.

Other questions.

Good.

So now, intuition, what we already said query, what information

is the token seeking?

Key, what information is the token offering?

The product between the two, how similar are they?

In some pretty like pretty robust ways, how similar are

they?

And then using softnes over all these similarities to see

what are relatively most similar in terms of looking for

an offering, and then computing the alphass with this.

That's one implementation, how to get weights.

The fundamental thing is this, weights between different pieces of

information.

You might have, like, this is what we discussed, by

the way, if we, we didn't do this, but if

we had done a cosine similarity we didn't, right?

We divided by square would be, which is just the

number of dimensions, not the length of individual vectors which

we would need to compute as well.

We always divide by the same number for numerical reasons.

If we had divided by.

It it would not be feasible.

But if we had divided by individual lengths, this would

actually be cause and similarity what we would have done.

So, um, so that's just like another piece of intuition

why this thing works, because the top is some measure

of similarity.

It's actually pretty close to kind of our main measures

of similarities.

So our challenges, and maybe you thought a bit about

this.

Imagine like you have a huge context length like think,

you know, loud or something, 200,000 tokens and now we

do this kind of stuff that means that every token

has to compute a dot product with every token and

attention, that's what people say it's like the complexity is

squared.

You need to, every token needs to compute a dot

product with every token.

So it's Tx T do products.

It's huge.

I mean it's computationally and memory wise incredibly expensive.

And these very large architectures with like 100 thousands of

tokens, they are often private.

Um, and you don't know it for sure, but very

likely they use things like this is flash attentions, certain

flash attention papers that are very efficient ways to store

and retrieve, um, these weights.

Or then there are, you know, approximations like, uh, there's

another paper on trying to approximate these things, um, and

reducing the complexity to something that's, you know, not square.

That's linear.

Because this is, imagine I have a context of 100,000

tokens.

It's 100,000 times 100,000 dot rights.

That's like just very expensive.

That's an issue with this simple dot attention, but it's

in the original transformer paper, um, and it's an intuitive

way to teach this and also build simple architectures, but

likely very big architectures use these newer variants to make

them, to make them even people.

Good.

Um, other questions?

What's the, um, that depends, right?

It's all indulgence.

So it might be, might be low depending.

Like I think to your point.

This is the updated X one.

But really, if you look at the full architecture, there

are also these skip connections.

Did, did you discuss the connections with Tom or no?

So imagine I have a, I have a graph and,

and like a, like a network like this, and something

flows through like a fully connected layer.

So I do brian descent and I, I do back

propagation to the parameters in that layer.

But I can also define a residual pathway or a

so-called skip connection that just goes around it.

Sounds a bit mad because why do I do all

this thing if I just add up the input again,

but I'm now adding the original input plus the transformed

input.

So really, what after doing all this attention, what you're

also doing is you're taking the X1 prime and adding

the old X back because of the slip connection.

So it's even added another time.

So what Alpha is really depends on like the specific

objective that you're trying to learn, what, what this thing

is choosing.

Um, I'll have a picture where this is, um, where

this is very clear, I think.

But like, OK, stepping back, this thing, I mean, if

you think about how it powers like what we see

in terms of, and we can even with this err

attention but very serious architectures if we hire some cloud

compute, um, and architectures that we can wrap in a

couple 100 lines of code.

Um, and I appreciate it's a bit confusing in the

beginning, but if you think like, just check out these

LSDM cells and these networks with all their different gates

and so on, this is so simple.

It's just the way it average between things.

And now comes the second component of the transformer and

maybe you'll laugh a bit because it's just what we

did all the time in our network.

That's it.

So that's why they needed the tension is all you

need because you need these weighted averages and then you

needed one nonlinear transform that you also sca stack because

what we're doing here is just a linear average, right?

So we need it likely to have expressive functions to

give good predictions.

We needed some nonlinear transformation.

What's a nonlinear transformation, a good old multilayer perception forward

neural network, right?

That's it.

So we have say um XT, uh, it becomes double

prime, prime comes out of the attention layer say, um,

and then, uh, it goes through a feedforward network.

Um, good.

This parameterized by weights, by biases, and then an activation

that activate.

Have you seen GO activations?

Yeah.

So, OK, um, I added the slide, but, you know,

you see, it's just like one more and like an

infinite set of activations.

It's, um, it's, uh, it's used a lot with transformers,

works pretty well.

Uh, one slide, one moment.

Um, but the key thing is this year, um, that's

the nonlinear transformation and the transformer.

It's stacked.

There are many of them, but like that's what that's

the nonlinear part.

And then another key thing is, here the vectors are

processed independently.

Here they all talk to each other.

They all get weighted average, and so on.

And here they are passed independently.

So once this, once they come out of the attention

um layer, I just run them through the feed-forward layer,

vector by vector.

That's also kind of an interesting um piece of that

architecture.

So there's no communication here.

They're just individually nonlinearly transformed.

So onto the relogeno type of thing, um, the reno,

you know, uh, by heart now I guess that's the

geno.

It looks funky, but it's very simple.

It's just X times the commutative distribution function of a

normal.

So if you think of normal distribution, subject to my

drawing abilities, say like something like this, OK, if I,

uh, that's the PDF, the probability density function, if I

look at the CDF, the cumulative distribution function.

Um, then I would have something like this year.

Um, and at some point, like this is minus infinity,

plus infinity at some point, and then I just read

probability of one.

So I will draw something, um, that's, uh, smaller than,

you know, like a very large number with, with close

to probability 1.

OK.

If I use that function times X, I get that

activation.

And that's just an approximation.

If you look at code, that's why I edited here.

If you look at code, you see that and that's

just an approximation of this here, the, the part between

X and the CDF, um, and how does that look

like?

Super similar to Relo.

Um, even though it looks, um, uh, it looks more

complicated, but it's very similar.

What you achieve is non-zero gradient around zero, and that

can help with learning.

So your neurons don't turn off that quickly.

When, uh, when something that's like mildly negative would immediately

give you zero, now gives you still, uh, some non,

uh, negative, like some non-zero number and with that easier

learning.

Right Just because you'll see them in all architecture, so

um that's that.

OK, um, What do you think?

Shall we, uh, I think maybe to combine them, I

would need maybe 5 more minutes, or shall we do

it after a break to recap?

I see by you would be better.

All right, cool.

So let's take a break until um.

Um, yeah, let's, let's do like, uh, let's do like

7 pass or something like this, OK, um, 10 pass,

OK, so 10 pass and then we uh we continue.

Yeah, I'm really proud of it.

I'm like, how do I make.

Were you saying you guys are you guys staying in

the room?

I think so.

I thought it was really I feel like I don't

understand.

Yeah, yeah.

And I like the way.

AI, yeah, yeah.

Yeah, I I I I was 5% 5%.

Yeah.

I'll help you with that.

I that's probably uh be safe.

So it's apparently like feeding you everywhere highlighting the ones.

because if you're thinking about.

I think that was a thing.

No, I still a few years.

You see.

That's the it's like, oh God, now I'm like I

used to like just.

I know it's whatever.

It's fine.

Yeah, it's just my like counting of you.

I walked and then I saw you guys.

I was like, oh my mom we're fine.

I guess let's see, it's, it was uh.

No.

I was I was I was like before the lecture

could have come had I realised was it good?

Yeah.

I know.

I was like yeah I don't know.

I think they are not that's where I used to,

right?

I mean.

Yeah, no, no, I'm reading.

You're gonna move on to, yeah, now I'm like, OK,

some 556.

Is I.

Yeah.

OK Consider this.

OK.

OK.

Right, that's all.

I passed.

So if we combine them all, we all, all, all

is just, um, tension and feed forward neural network, then

that's a transformer.

That's it.

Um, and I drew it out here, um, and you

have X, like each embedding actually in the beginning, a

sequence of these.

So this is really a sequence of all the X's

and here the same sequence, but now X by X

goes into this.

Um, the sequence goes into this, gets normalised, gets to

attention, and then there is something that these transformers usually

have is multi-headed attention.

So what we just said was one or we discussed

was one attention head.

So you just have a bunch of them in parallel,

each with their own WQ, WK, WV.

They all do their thing in the end, you combine

the vector again.

They might operate on smaller vector spaces and then you

concatenate it to the original dimension and so on.

Nothing deep about this.

It just gives you more flexibility, yet more flexibility in

this function.

Which, you know, ultimately you want to have to, you

know, fit some complex loss or like reduce some complex

loss and fit some complex function.

Um, OK, so like once it comes out of multiple

heads in parallel, again, you, the add, I mean it's

just an add operation, the computation graph of the residual

connection.

So all of this comes X, I add the X.

If you think like, I'll link this, I have this

link later in the, in the um The series, there

are these really nice videos also from Three Blue 1

brown on Transformers and like this, this combination of after

like this attention and feedforwardness of um particularly the attention

um of having the original vector.

I imagine that's a vector that um what was our

example was I think, uh, language models are interesting, OK?

So say.

That's the original vector for models say, uh, for the

token model it actually happens to be a full word.

After this attention layer, it will probably move closer to

the vector of language.

So that's what's happening here.

So through these attention layers, we'll, in this space, and

here's a much nicer visualisation of this than I can

draw here, we'll move towards this.

So the new vector will be the old vector, plus

some shift in the vector towards this.

That's what this is like the old vector plus this

shift gives me the new vector.

And the shift comes from the From the from the

attention.

Now run through for network, same thing, uh, with the

SI connection, skip and residual connections are the same thing.

Here you see the quotable modern architectures, these things are

moving very quickly.

Um, the only major shift from the, um, our paper

is, uh, is where I put the norms, um, in

front of them.

Um, and yeah, each block has its own W, E,

uh, each feed forward network has its own, so each

attention.

Um, block has these, each feedforward network block has these.

Together they are transformer blocks, so that's characterising the transformer

block.

This is just kind of hand wavy at the end,

I think of the double prime again as an X

and run it through the next transformer block.

That's what I mean with this.

Um, and then I sack them and I run my

sequence through these blocks.

So imagine the model is trained when we do a

feed forward, uh.

So you're, we're shifting the words in the vector space.

If the, yeah, I mean, so, OK, like, more like

less fancy, uh, if we do a feed forward, we

start with a sequence of vectors, we get out a

sequence of vectors after a huge amount of computations.

Now, these sequences of vectors that we get out, they

might even be depending on the the same dimension and

the input sequence, just heavily transform.

For nonlinearly through the 4 and then um you know,

being connected to attention.

So they sit somewhere else in that space.

And imagine our vectors were not 12, 3000 dimensional or

4, but they were 2 dimensional.

And maybe I start with a moral vector that sits

here and after running it through my transformer, that moral

vector would probably shift towards language because I mean language

model.

Yes, it just catches on cortex.

That's right, exactly.

That's it.

That's it.

And the attention is what shifts them, right?

So the nonlinear narrative moves them all into a different

space and not in a linear transform way as in

on videos, but in a non-linear way, but these, uh,

the attention is just shifting them, um, and, and, yeah,

making them, uh, connect and be closer, uh, depending on

meaning.

Good.

So all of these are, are, are combined, and then

all parameters across the transformers to defend loss function.

So far, that's why it's a good point.

I didn't fully define forward paths and I didn't say

what a loss function is.

But at least so far, I just wanted to show

you the two main things that make up a transformer

attention and neural networks.

And that's where the title comes from attention is all

you need.

Um, good.

So now, two broad ways of how transformers are used

today.

We don't care that much about translation.

We might use these tools are like not as relevant

for us as, say, large language models in our research.

So the two broad things in which we use them

are encoder transformers, decoder transformers.

I will focus more on decoder transformers, but one slide

on encoders, because I know people use them a lot,

there are these bird models that you fine tune for

sentiment classification and so on.

So the tension that we discussed so far, we would

call fully connected.

Each token is communicating with each token in the sequence,

right?

I had like 4 tokens, they could all depend on

each other.

That means models could depend on language and art.

So language models are interesting, so models could depend left

and right, huh.

That's good if I want a really rich encoding of

that input.

If I want like some very good numerical representation of

what that sentence or whatever means, like sequence means.

Um, and that's why it's an encoder transformer.

It really gives me a rich encoding of that input

sequence.

Now, imagine I have these, like, and what we just

discussed was pretty much an encoder transformer because the main

difference is in the self-attention, and I'll show you the

decoder self-intention in a moment.

The main difference is only where the weight goes, like

the same thing.

It's very, it's, it's nothing complicated.

But what we did so far was encoder.

Things enter this, go through this attention and our network

block, intention, communicate with each other.

In the end, I get a sequence of vectors out.

Now, these vectors, I could put into a classification model,

right?

And there would be a super rich embedded, like representation

of the text.

And now with that representation, I could, you know, combine

them or pool them or whatever.

That's, you know, up to debate, uh, there are choices

to be made there.

And then I could use them a classification model, um,

and predict sentiment of a sense.

I have the encoder of the sentence with transformer, like

er me.

This.

And what, what it means to fine-tune birds is either

to just put a fully connected layer at the end

of bird and just train these parameters, get the full

encoding from BER, the full one, and then just train

a tiny amount of parameters to map it into sentiment.

Or you could update all of bird's parameter values, but

they come pre-trained.

Um, and you could update them then to improve sentiment

classification.

But then you quickly, you know, I ran this on

like a small data set or it took hours to

be marginally better than a random forest.

So we also, it's important to think about when it

helps and when it doesn't.

OK, but that's, that's the bur thing.

Now, the only thing we're missing for bird is what

is the loss function.

We have the architecture, you know, we're passing through attention

and, um, uh, and the network layers, attention is fully

connected, fine, but what's the loss function?

And we won't go into details because our focus is

more the language models, but loss functions.

Our next sentence prediction or mask language models.

And this I saw on the video and I thought

it was a nice depiction of this.

Um, just for my encoder, think of like, you know,

it's from the burn page.

Um, this is just a special token for a start

of like text.

And then I and then I mask this, this red

card.

So I mask that token.

And then it's, I love this red card, say.

So you see, it's a bit like a denoising autoencoder,

huh.

So I knew certain things in the input sequence.

I try to predict it again.

And by this I learned these like these deep structures

of, of the text and, and I'm able to encode

them very well.

What does fully connected attention mean?

Just what we said, every token is.

Nearly combining with every token.

But if I combine them, often you see heat maps.

Like this is all uniform, but you know, you might

have, uh, red might depend more on car and so

on.

And that means that all of the tokens can depend

on all of the tokens, not just forward or back,

but, but from all sides, because I want to.

Good.

Any questions?

That's like kind of work in a nutshell.

It's also much easier to understand now with, you know,

having revised, um, uh, revised attention and, uh, particular attention

block the rest you knew already.

Good.

Um, now we can move to our language models, uh,

making our way.

If, I mean, I, I don't want to rush, uh,

through this.

So if I don't go to the end, I'll just

recap it, um, uh, next time because these things are

very important.

It's, uh, it's important to, to discuss them and make

sure, uh, you understand them and can ask any questions

you might have.

Um, OK, so large language models.

People call large language models, lots like lots of things

I call large language models.

I I've seen papers where people use word from 2019

and say it's a large language model, maybe, but like

tradition like as of now, the most, if you say

large language model to say computer scientists work in that

space, they probably mean all regressive models of language predicting

the next token being transformer based.

So basically looking at the old Benji at loss, like,

you know, objective, like trying to predict the next thing,

loss would be cross-entropy, whatever the right word is, we'll

discuss this in a moment.

But then having a much more flexible function than what

they have in these days.

Why?

Because we have the architecture now that works very well,

and we have like a lot more confusing more a

lot, right?

So from 2003 to now, we, we just have a

lot stronger computers.

Um, but yeah, and how do we do this?

We do this with decoder transformers.

Why decoder transformers will become clear in a moment.

So The context, uh, which can be taken into account

is, is huge by these models, right?

So roughly 200,000 tokens.

It's just a coincidence, by the way, that that is

also the vocabulary size of the tokenizer I looked at,

like the tokenizer for GPT 3.5 or 4 and 100,000

tokens.

Context is not the same as vocabulary, right?

Context is, now that's also here.

If people say, you know, we need more, you know,

like this language model has a larger context, that's just

meaning.

Well, how many weights do you have?

What can you pay attention to?

That's the context, right?

If something is out of context, I don't have an

alpha to actually take that into account.

That's where this comes from.

Um, and the tokenizers just, and how many byte sequences

do I split my language?

These happened to both 200,000, maybe, you know, I should

have taken the older tokenizer, and then it's 100,000, that's

clear.

Um, good.

And the following, I followed GP2 architecture.

Now you might say, wait a minute, isn't that old?

Yes, but it's open source.

And um the other ones are very quickly then hidden.

Now you have other architectures like LA, for example, that's

open source so now deep see.

And what is interesting is that like, small parts changed

of this, but the big picture really remained the same.

The number one thing that changed or the biggest variable

arguably, that changed was just the size of these marks.

And that's also really interesting in itself.

If you go from GPT 12 or 3.

Before and you see how much more capable they become.

And although we don't know the precise architecture, it's, it's

likely that a major part is just the size of

the model, meaning more layers, more parameters, and so on.

And that then has these emerging capabilities of becoming what

we would view as a lot more intelligent.

Um, and yeah, these are very deep questions and I

think like, you know, there's an area of very active

research why this happens and so on.

Good.

We look at GPT2 again from Wikipedia to have something

kind of sad that you can look up.

Um, this is, I put a til there for IS,

the GPT to architecture, because for some reason on Wikipedia,

they put the layer norm here, um, uh, they have

the SIP connection into this year, and, uh, so basically

you have the SIP connection depart from after the layer

norm.

Um, let me see.

So here, the skip connection departs from after the layer

norm.

That's just like adding it back, basically, that's the residual

pathways in another word, and here from before the layer

norm.

I think that's odd, the implementation I've seen have it

one way or the other.

And that's how we tiny detail, but just that's uh

that's what I meant with Matilda here and yeah, that's

all pretty like should hopefully be pretty clear.

You have input here, you look up the initial embeddings,

you, um, there you go, input initial embeddings, you add

positional embeddings.

Now dropout optional, you can use that you've seen.

Now on these transformer blocks.

Each transformer block looks like this.

What's happening?

Attention.

That's masks mean map mile is like the Ws.

If you don't have do product attention, and masks you

see in a moment, that's the way in which direction

the attention goes.

That's the only difference, um, with this decoder transformer.

Now you apply dropouts and, uh, dropout and so on.

Um, you combine this more dropout, and then that's the

feed forward block.

You see, you project it onto some like hidden layer,

geo activation, correct.

Does that make sense?

Or is like, see, now, also, like, I would hope

over the course of this lecture, these things now they

should, they should make a lot more sense than before

if they hadn't made sense already to you because you

knew it already.

OK, so key difference in, in attention, and that's it,

really, attention is forward looking, uh, sorry, it is, um,

it's backward looking.

So I am trying to predict the next token, so

I can't tend to the next token, right?

Because then I know the next token and it's trivial

to predict that I put an alpha of one and

yeah.

um OK, so basically, I, when I, my X1 can

only depend on X1.

My X2 update can only depend on X2 and X1.

My XT update can only depend on X1, X2, yeah,

that's it.

And how do you do this with a mask?

You zero out the other attention weights.

That's where the mask comes from in the, in the

schematic.

And now also, you see here one advantage over the

recurrent nets to revisit this why we uh why we

study this and why people are using it so much.

Look how paralyzable this is.

This is um already um a way to predict X2.

So I can based on this now predict X2.

Based on this, uh, based on X1 and X2, I

can predict X3.

Based on this, I can predict X4X5 from this, and

then based on this, I can predict XT + 1.

So during training, I can just with the things that

come out of this, after all these layers of, you

know, after all these layers of, of, of attention and

networks come out, say, of the same dimension, these vectors

again, but heavily transformed.

And now, based on this vector, I can predict X2.

Based on this vector, I can predict X3, and I

can just paralyse this very, very well, um, and train

this very quickly.

Now you might say, isn't this crazy because you make

your entire prediction based on the final token, and it's

really like this.

It's like, if you predict like if we at inference

time, we just, you put a prompt there and it

predicts the next word, predicts the next word.

Really the next word prediction is based on the single

vector.

Out of this whole transformer inference time comes the final

token embedding, this vector, that vector then is scaled up

to the vocabulary size.

With a fully connected layer.

So say I have 200,000 um uh 200,000 tokens, so

they go here.

I have numbers here.

I apply softmax to this.

I sample from this, put the next number, I put

the next uh token there.

That's how the model works.

So XT + 1 is um at at when I

really generate fully new text, it's just based on this,

but by that architecture of XT just one like everything

being condensed in this final thing at the very final

block and that allows me to in parallel by training,

predict X2.

With this, with this, explore with this, and so on.

And that allows me also to make this thing very

flexible and predicting from sequence inputs from varying lengths because

when you ask JGPT a thing, your prompt has many

different, you know, ways in which you can write huge

documents, more documents, and so on that are all taken

care of here.

That's the only major difference to the enco transformer, the

direction of attention.

For us, OK, not for the, for the people.

We can use our favourite example, language models are interesting,

but now we don't want to encode it, but we

want to predict the next thing.

I mean, not not our soul, uh, we also encode

it on the way, but our, uh, sole objective is

to encode it.

Again, these are initial vectors.

We add the positional encoding.

This could be another lookup vector or like some function,

um, of the position of the T, um, and then

we apply entrance former blocks.

We run the attention, we add this back, um, so

this gives me the updated thing.

Um, I normalise it forward and so on.

Um, and then in the last transformer block, I used

the last embedding of a sequence to push it into,

um, you know, to scale it up with a fully

connected.

Layer to however many tokens my vocabulary has.

Now I have numbers, and these numbers could be -100

plus 2000, whatever they are.

Now I run them through a soft max and they

become probability.

That becomes, that becomes a probability close to zero.

That becomes a probability very high.

Right, so E to the -100 is close to zero,

so very low probability.

Our probability vector, I sample from it, that's the next

token.

That's the forward pass.

So now we got the full one.

Do you have questions about this?

But when you're saying that you sample from it, doesn't

it just select the one with the highest capability?

What do you mean?

Yes, great point.

So what, what it actually, if you, if you ask

APT, um, you know, you, you write a point in

it, you get different answers.

The main reason why you get different answers is because

it doesn't pick the mode of this, but it samples

from it.

So it actually predicts the probability distribution.

It doesn't predict the next token.

It predicts the probability distribution or model.

That's why when we looked at the function.

This function is not token, it's probability overall the next

tokens.

So really this predicts the probability.

That's, that's a great point.

So I predicts the probabilities over all the tokens.

Then I sample from it.

Now, you also, when you use GBT or the API

or so, you might have noticed this parameter temperature, or

you might have heard of this.

Do you know what this is?

This is just, you know, imagine in the end I

have a, I have a distribution here.

Let me try to draw it.

Like some tokens are super likely, and other tokens are

a bit less likely, some tokens is very unlikely, and

so on.

High temperature is just making this thing more uniform.

So bringing it more to similar and then makes it

more experimental, because then I'm sampling the things with more

similar probability.

Low temperature makes it more degenerate, makes this thing close

to one and the rest, and then I almost pick

the mode.

And, uh, temperature of 0 would push that distribution to

have the most likely would be very, very, uh, high

probability.

And then I just pick the most likely word and

the output is more boring, but maybe more, um, more

rigorous.

Less temperature, just scaling the the final.

Good logic just means that's just like language, um, no

net language for like whatever, you know, floating point numbers

come out of this break through a soft bags, um,

ready for past training.

All right.

So that's interesting because, I mean, too much for this

lecture.

But one major question is what data do you train

this on, right?

And how do we curate that data?

Because the better the data, you want large data sets

because we'll talk about these shading laws in a moment.

But also you want good data sets, um, and high

quality data, and that's a huge endeavour in itself.

And one nice resource is this year, um, on, um,

the hidingha website, they have this open source, this, this

fine web data set where they really, um, source a

good amount of data from the internet.

I mean, I think 44 terabytes.

So to give you a, a perspective, when I downloaded

Wikipedia to train and work like model a couple of

years ago, this, I think was 15 gigabytes.ext is very

small.

So all of Wikipedia 15 gigabytes.

So in one way, this is huge.

This is a huge amount of text.

In another way, we can buy a couple of hard

discs and put it on these hard drives, right?

So it's still pretty little, um, because it's just text

data.

We couldn't just buy the computer necessary to train the

model that would be millions.

But to store the data to train it on, yeah,

no problem.

Uh, that would actually, it's only 44 terabytes and that

you could get a very serious model out of that

data set.

Um, if you have the money and the GPUs to

train it, that's the bigger problem.

And then they, you know, they, they go back it's

interesting, they go through all these different ways.

They, they philtre out, um, uh, um, problematic URLs.

Then how do you from the web source code extract

the text.

You don't want HTML text and excessively, so you want

to extract also a lot of text.

And then do you want to train multi-language?

Do you want to philtre certain language?

They are so voyeuristic to philtre certain language, might not

be perfect, but you, you know, so it takes a

long time.

Lots of things.

Um, you might want to, like, no, you might, you

definitely want to remove the personal identifiable information at that

stuff, and you have other philtres.

So it's a huge amount of work that goes into

this.

But immediately there's this paper in textbooks is all you

need is all you need, where they really train as

a model a pretty small data set of very high

quality textbooks and they can show if we only had

data at that scale and that quality would have have

much better models.

Um, OK, so quick excursives or digression on this.

Once you have the data set, you have your forward

pass, you initialise everything randomly, um, you sample the next

token compared to the actual token runs.

That's just like classifying, you know, whatever we were classifying.

Here we're classifying words.

I sample the token, one of, let's call them words,

like one of 200,000 words of tokens, and then I

compare it to what was there on the internet.

That's why this objective is so appealing because I have

almost infinite amounts of labelled data.

I don't have as many images saying what's in the

image, but I have so much text telling me what

the true next word is, right?

So that's why it's kind of very salient to try

to do this, um, yeah, OK, and then you minimise

across entropy loss.

So now say we have trained the same, OK.

Now we have the encoder and we've trained it, um,

so the Ecoder transformer that predicts the probability of the

next tonne.

What is inference and inferences, uh, like how's that happening

and that's happening when you write a prompt to JGBT.

So step one, you write a prompt, and that prompt

is language modelling gets encoded as this.

JGPD gets this samples the next token I say because,

OK.

Then step two, now writes because, um, you know, if

it's your prompt, it might start with a new sentence

here, but like this is all completing say your sentence.

Um, probably actually JGPT would autocomplete your sentence if you

just put this into JGPT because there wouldn't be any

other sensible answer.

So then it goes because, and then sample to you

concatenate again, put into the model.

That's the auto regressive nature.

And then you iterate forward, at some point, you sample

an end of, um, of text token or some special

token and the same source.

That's inference.

Or call infants.

Questions.

All right, so recap, what we did is we built

something reasonably close to kind of state of the art

architectures of, of trying to predict it.

And I think with this already understood a good amount

of our tools like JGPT work to the degree of

a base model.

There's much more to making them, uh, be helpful assistants,

uh.

Both behave properly and then also just answer questions and

so on.

Um, and, uh, part of it is based on a

lot of it is based on enforcement learning, so we'll

discuss this in detail in the future.

But part of it is also based on just repeating,

um, the same thing that we did with the same

loss function and we look into this.

Before that, however, like a couple of, I find very

interesting conceptual points.

So first one, so far we've developed this space model.

And we have developed an auto-complete, you know, engine for

the internet.

That thing can just like, um, there's this term like

that you see kind of online like dream basically internet

text.

It's just like emitting and creating internet text, right?

So you start with the text and it's just continuing

the text.

You also see now there's nothing necessarily truthful in that

thing, right?

So that's why it's hallucinating as well, because it's just

predicting the next token whether that sequence of token to

begin with, makes sense or not to us or was

some true statement or not.

Um, so that first step that I find quite interesting

of training you can think of as a lossy compression

of all the information on the internet into the parameter

values of the mark.

So think of the model being characterised by all these

W's and the and the biases.

That's just a bunch of floating point numbers.

State of the art models have maybe 1 trillion of

them.

But like take that as an example, the number 2

model had the larger one had 70 billion parameters.

Um, it was trained on 10 terabytes of internet text,

and you condensed all that information in 70 billion floating

numbers, which are roughly 140 gigabytes of text.

So all that information now can go on all of

our hard drives, basically.

Then we need 500 lines of code.

It's a bit cheating.

You don't just need the parameter values, you also need

the inference, and the function to infer from them.

But that's like a couple of lines of code.

Um, and that's it.

And then we have that compression, and we really have

that compression and an odd way of predicting the next

to, but it turns out that compression carries a lot

of the weight apparently of what concepts and information is

stored on the internet.

So how do we get this?

It's really expensive.

At that time, roughly, like all of these are, I'm

sure very rough numbers, but you use 600, uh, 6000

GPUs around 12 days to develop that full model, even

that model with 70 billion parameters, roughly 2 million that

base model.

Um, and you then have that kind of wet or

completion engine.

OK.

So now let's look at this.

Um, because it's quite fascinating to, to, to try to

look at the, at the workings of these things.

So here I'm basically using a website where I'm, where

I can access the base models, and for most of

these, uh, like they exist for the open source version,

so I use this very large 405 billion parameter Lama

model, uh, from the Lama 33 series, um, the base

model, and here I can enter a prompt, OK.

So one thing I can do is, for example, that's

the base model, it's like this auto completion engine.

I can say who found it.

The NC.

Do you think it will answer to me?

I'm here, right?

OK.

Looks good and see what I could, it's um wait

here we go.

Let me see.

So who founded the LSC?

let me try this a couple of times so I'll

show you the probability.

Who founded the LSC.

Siri, and why does it matter like odd stuff, right?

So let me run this again.

A new you yeah, OK, so that's the thing that's

not quite myGBT.

However, that seems to carry most of the weight in

some way, um, but it's for now, it doesn't know

what it means to answer questions in the way we

that we would think.

We can try to prompt it in a way to,

to kind of make use of its auto completion nature.

So we could say the LSE was founded by.

Let's see.

Very More reasonable at least, huh.

OK, so here we're trying to build on the or

like to, to notice this auto completion agent.

And now just to illustrate this in a bit more

detail.

So say, uh, or like, like say we take some,

I usually take this example some kind of obscure dinosaur

or something and um.

We take part of this.

Wikipedia article on which it was likely trained.

All right, so we take this here.

Let's see, uh.

So I just pasted this in here.

And that's all.

Ah, I think there's like um to.

Doesn't like the the new pair and it hasn't bark

I saw that before, so let's do it like this.

So I just like this year just, I just pasted

that that first paragraph and now let's go, though their

white wing membranes probably included.

The white wing membranes probably included.

So this thing is really like, I mean, it has

seen Wikipedia and all its epochs as part of a

huge data set, right?

So think of like the terabytes was only a few

gigabytes of Wikipedia.

It's iterated over this a couple of times and at

some point it will digress, but say here an upward

to walk on the 3 fingered hand.

There we go.

It's pretty remarkable, right?

So that kind of compression we achieved, and we can

do this with a bunch of numbers.

Uh Um, any questions?

OK, so let's try something.

So the LaA model was trained, I believe, until 2023.

So that's the day.

So now we just take the 2024 Paris Olympics.

Um, The games were the first held in France since,

OK, let me see.

No, it just makes some stuff up, right?

So like, it, it can.

I mean, if, if I'm right that it's been trained

in 2023, um.

See, it makes up how many athletes were there and

so on, right?

So because there's nothing inherently truth telling here, it's just

autocompleting.

Cool.

All right, so that's the, uh, that's the base model,

OK?

Just autocompleting not answering our questions.

Um, good.

Anything else I wanted to mention here?

OK, so now we can use, for example, the instruct

model.

So where's this?

It's small one.

Um, Let's see if we can do this.

There we go.

There we go.

Do you know this example for like that's one kind

of famous failure mode of this, right?

And Um, if you now, I, I tried this yesterday

with with 40 and it's not answering this correctly, you

wonder whether they are coded or I don't quite know,

um, or maybe they gave a lot of examples now.

Um, but there are some very silly failure modes, and

this is actually not entirely clear where this comes from.

People are researching this particular example.

Um, a couple of conclusions here.

They are not very good at counting.

If you think of how they operate with predicting the

next to, it seems unlikely that they're amazing at counting,

right?

They may be better at mathematical proofs like sequential but

than at accounting, um.

And then secondly, the tokenization is the key here.

So for example, you have seen this, this, uh, um,

you will have seen this example of how many Rs

are there in strawberry.

Um, and, and they're all pretty bad at this, um,

and say.

Check this out.

How many tokens are strawberry?

So if that is your sequence 3 or 2, you

know, 1618 or so like that that will not be

great at predicting the letters and the words.

So really tokenization is also a lot of where the

limitations are and so on, but then it's a bit

like, you know, it's this cliff walk of where sometimes

you have to be, you know, you have to be

careful not to walk into a territory where this is,

these models are very poor, uh, or like very, like

not very capable, um, but then in other territories like

they can, you know, now ace questions in malys, um,

they really have like the reasoning models in certain, um,

in certain domains PG level knowledge and so on.

So it's just this, um, pretty incredible combination of ability

and ability.

Um, yeah, um, OK.

Great.

Good, so I think that should make it, that should

make it kind of more tangible.

Another very interesting, um, interesting thing are the so-called scaling

laws.

You might have heard of them, um, and it's.

When we haven't looked at something that's the eventual task,

we just look at this auto completion engine.

Um, and we, uh, we haven't aligned it to answer

our questions or something like this.

But it turns out the better that auto completion engine

is in terms of its own loss.

The lower the losses, the better it is in lots

of downstream capabilities, the more intelligence it is, in lots

of downstream capabilities.

So it seems that the quality of our compression of

the internet matters a lot for all the downstream capabilities.

We didn't just have to show it how to answer

questions and so on.

We look into this, it's also super fascinating.

We look at this into this in a moment, but

like this is also fascinating, namely how regular this is.

So that's a, a pretty important paper from 2020, um,

by looking at, um, test losses in terms of compute.

Um, use data set size and parameters, they say you

have to, to increase them in tandem.

It's not that you can increase them all at once,

but like usually you increase them in tandem, and then

you really decrease the loss incredibly predictively.

For a while until 2020.

So now in 2020, like they, they like that's kind

of those were open AI groups and they put a

lot of weight on that the scaling laws held, and

they just increased the model sizes and yeah, the rest

is to some degree history with PT and so on.

And the key question for our current paradigm, because also,

even all the reasoning models for now, they are based

on these base models.

They start with this base model because it's so, so

much data to train it.

Um, and then you, you know, you, you would enhance

it through, through other steps.

But like, the key question remains of how far can

this loss go down.

And for example, the GPT 4.5, which is again, 10

X times the parameters of the previous model, is now

super expensive and slow.

But between each full iteration of 3 to 42 to

3 was 100 X of parameters.

So the question is also, like I've seen some posts

of all is this like, you know, an indication that

the scaling laws don't hold it anymore, is this thing

not getting more intelligent?

But two questions.

One is what happens with a 100 X as well,

like because those are the steps between the bigger models.

And then the next thing is also, even if the

base.

Model like the base model is only one piece.

You then need to build reasoning models and so on

with it, and then they might be a lot more

capable again.

So yeah, fascinating.

I mean, the regularity is just mind blowing, um, and,

uh, and also a big big question in this whole

field of how long they hold.

Yeah.

Wait, questions.

Yes.

Cool.

So now, what we can do today to make them

ideally answer questions more reliably, uh, we can look at

supervised fine tuning.

The rest reinforcement learning, we'll do that actually.

I could do that too now.

I'll do this in other courses as part of such

a lecture because this course we have kind of the

luxury to actually study reinforcement learning properly.

We can do this in much more detail and, uh,

and I believe it can be more interesting in the

end.

That's why.

I, I divided it this way.

Um, so how can we make this thing to answer

questions?

I took from, like, that's kind of a canonical paper

on this.

That's the instruct GPT, the preliminary, uh, thing, version of

Chat GPT of the first one.

There's a paper from 2022.

That wasn't the time when they were still open, open

sourcing these things.

Um, and they have three steps.

Steps 2 and 3 are reward model reinforcement learning.

We look at this in the future, but step one

is super simple.

All you do is you write, uh, you look at

prompts.

I think they looked at like certain sets of prompts.

I don't know whether the API or other about the

details.

So they had a bunch of prompts that people were

asking the model because they had these models live, the

GP3 and so on.

And then they had humans at that point writing proper

answers to these prompts.

So now you have a data set, and that's kind

of, I just thought that's a nice example of that

paper.

So here's some examples from that paper.

They took a GPT-3 model and they took 3 different

sizes, so 1.36 and 175 billion.

Now check this out.

The data set of tracingrons was 13,000.

That's just absolutely nothing in comparison to before.

So you write for prompts, these answers, you concatenate all

the texts, and you do next to prediction on them.

You repeat your loss.

That's it.

Maybe you zero out the problems because you're already, you

know, maybe you've seen them, but like you, you, you

update your gradients based on the answers.

I've heard that that like someone from our basements told

me that that's what they did.

But these are kind of technical details.

So you concatenate all the documents and then you predict

the next token.

Here they had a bunch of documents of, of, and

then, uh, like how labelers were answering them with few

shots certain examples and so on.

Um, same loss, um, and then the main, the model

with these steps is able to, for some reason, not

entirely clear, I believe, to generalise following instructions.

So we only show them 30,000 examples and we write

a poem about the LSE as an answer to a

poems or something with a lot of effort, but then

it can also all of a sudden write poems about

other things because it could write them all along.

It just didn't know what it meant to answer questions.

That's the key point.

So really what these things are doing is they are

teaching the morals what it means to, or it seems

so to follow instructions.

Supervised fine tuning wouldn't give you the levels of intelligence

and compliance that you see.

You would need RLM on top of this, um, and

they have these two further steps, reward money reinforcement learning.

Human feedback means human written answers or humans rating answers.

Now you can have more capable AI models right answers

and rate answers.

So you have a lot of that kind of stuff.

Um, and then you have more sophisticated planning components and

reasoning models like DC, and they're all based on, on

RL.

Um, you had a question.

For fine tuning, do we take the pre-trained model and

just continue training on the new, that's right, exactly, because,

so, um, also interesting here is the fine-tooth model is

like a very simple objective.

It's like what you would call kind of mathematically greedy.

It's just going one step look ahead.

Uh, we'll also see these examples in reinforcement learning.

So one step look ahead.

So it's not very sophisticated.

If you think of chess, for example, take a chessboard,

and then you predict the next move, right?

So one simple way to build a chess engine would

be, I download a lot of historical games from lead

chess or something like this, um, of certain levels of

people with EOS.

And then I predict the next move.

So I just run a regression between the board with

the CNN on the next move.

They a deep CNN, a bunch of tweaks, enough GPUs,

and then probably I can build like a jet engine

that's reasonable.

But that is just like next move prediction of humans.

If you look at things like Alpha zero, what it

actually did it played against versions of itself without actually

being initialised by the next prediction of humans and was

just choosing um these actions now.

Bit of a preview, but you can think of the

next token thing that we've did all the time as

choosing the next token.

If you're answering a question yourself in a discussion, you're

deciding what to say.

So you're choosing the next token.

You're also arguably planning of where you want to go

with an argument.

So it's very similar to this testing.

And the big question is, can you do something from

zero?

Maybe at some point, but if you think of all

the internet, how much initial information it carries about the

world model and, and how everything works, it's also as

a base model, even if it pushes it arguably into

some suboptimal directions of being trained on silly text in,

in parts.

Um, it still carries so much information that it would

be difficult to get rid of this, uh, entirely, at

least for now.

So you initialise it with internet text and then you

put like planning stuff on top of it that's actually

strikingly similar to things like chess and go.

You just plan your next, not your next move on

the board, but the next token you want to say.

Um Yeah.

Cool.

Other questions.

So these examples, like um, you know, also like kind

of the, the, the LLN and so, uh, the, the,

the base model LLM, there's this like it's, it's beyond

the scope of this course.

It's I think 3.5 hours by Andrew Capay, this, uh,

lecture that was published a couple of weeks ago, um,

and goes to a lot of more examples and so

on.

But if you're interested in these things, that's a great

resource.

That's also great, um, and it's, you know, this has

been references also for the slides, um, uh, and, uh,

so have been also a lot of other papers that

are all in here if you want to look them

up.

Um, and then, um, this is fantastic from 31 Brower

visualisation of also how these parameters store information, how vectors

get updated, and so on.

Um, these are blog posts if you don't like videos

or or lectures like to read, um, uh, and then

that's like parties for lecture series on neural networks kind

of from starting from scratch, has lots of hours and

so on, and lots of bit similar and, uh, we

in different places we also did here, but if you

want to kind of study these things in depth these

things here.

OK, so.

The last thing, any, any questions?

To show you a bit of research, um, you know,

that's closer to our domain, um, I, let me see.

Um, a paper or like the kind of the line

of research I've been recently working on is, uh, is

trying to do interviewing with language models.

So the language models might have be some kind of

bridge between qualitative and quantitative methods.

Now, they come a lot like this says a lot

of caveats.

These models, they need like 30 minute conversations.

They are not able to build the personal work.

For want of a personal interview and so on, but

you might be able to scale these simple interviews to

a degree that you couldn't scale human interviews.

So there's some dissection between the two, but also they

fall short in many dimensions of what the human interviewer

could do.

But in the interface of written chat interviews are actually

pretty capable.

And, um, you can go to, um, Yeah, let's see.

Um, so here's, uh, the interview repository and we open

source it.

Obviously the language model we're using is an API so

that's an open source, and we use a very capable

one such that it's as well aligned as we can,

um, make sure to for it to behave properly and

so on.

Um, there's a call out notebook, um, so there's a

full code you can download call notebook where you can

try it out.

Um, all that you need is you just need an

API key which you enter here.

Um, and then you can run this, um, and you

can, you know, change your own interview outline.

You can, you can say do an interview or whatever,

and then try it out with this function here.

OK.

So that's easy and if it can be helpful, then,

you know, you can.

---

Lecture 8:

And And the.

to make I think right.

Yeah And they, I think.

And It OK Which is exactly like this.

I I.

They're just like I don't want to sit too far

back.

Yeah, yeah you recovered 100%.

Oh, that was a bad thing, a really bad thing,

yeah, well, we all had like a.

Yeah.

It's.

She's.

Well, yes, it's like a hybrid and now they.

but in his dreams department usually hurt people.

I'm going.

Yeah I was like I used to work at.

So that was yeah, it's I mean he likes it

and it's like, but yeah, it's not like I think

that.

and here you know they have some separate countries.

However, they so they rather in the I remember having

a conversation with you I was like.

and like it's not.

I just refuses.

And.

and I walked in and I was like 5:45.

Yeah right now yeah it's that's fine.

I think I have.

It Yeah And have that like that yeah.

OK.

Oh yeah, I was like um why you keep quiet.

Yeah, because it's not like I think it's like there's

some confusion about the format of this, uh, because for

instance on classes there's like no room for questions.

It's only like, we're doing this.

it's like super super and uh yeah and I I

wasn't able to.

I was like I didn't I'm not sure if I

like I I remember that when I I had, you

know, it was, it was also like very like it's

very common for like just you know I do.

Yeah, yeah, he's a little bit closer, which I like

he had like.

I didn't think this is this is.

OK, OK.

Well, OK, I can imagine a situation just like change

their mind and like I mean I think.

Yeah, yeah, I was thinking I was like maybe.

OK.

Yeah I think 60 pages at least 100, yeah, so

like to me I'm like.

I think it's um um.

I.

I heard about things like it's called.

I I I love, um, so one reposing, um, how

many of you have, have worked with reimposing before?

So how many of you have seen part?

or something like this, OK, cool, um, good.

Um, we're looking at some basics so the idea for

today is to look at um the function-based methods, in

particular fuel learning, um, and look at a couple of

kind of code examples um to try to implement um.

Yeah, so without further ado, let's go.

So where we stopped in the last lecture was, uh,

at like a very quick, uh, discussion of supervised fine

tuning.

We'll, uh, have a longer discussion about this in the

last lecture.

That's more about these topics.

Um, and then, uh, we, we spoke about that a

lot of, um, yeah, I mean, the most discussed current

models, the reasoning models that they use reinforcement, you know.

To see these things can change and so on, but

that's how things are at the moment.

And um in order for us to study this in

a bit more detail later on, it's good to revise

reinforcement learning, also reinforcement learning.

Broadly, you can think about it like It's an oversimplification,

but this figure you might have seen it of like

supervised learning, unsupervised learning, right, and then RL basically like

as that being machine learning.

Um, and so.

Supervised learning just to, you know, kind of hammer this

home?

It's, I have a, an X and a Y and

I want to learn the function between the two.

Simplified, of course, like you will, you will find exceptions

to these things, particularly at intersections like semi-supervised or something.

So let's say unsupervised learning, that's I want to find

patterns in X.

If we think in this kind of uh um statistical

setups of X and X and Y and reinforcement learning

really is quite different.

It then uses things from supervised learning to, you know,

improve and generalise better, but really, it's about solving dynamic

problems.

That's what it's about.

So it's heuristical approaches to solve what people might refer

to as dynamic optimisation problems, to get approximate solutions to,

you know, you might otherwise call them dynamic programming, uh,

that you would solve for things like value, function iteration,

or, um, policy iteration, and so on.

And to get approximate solutions to these, um, sometimes also

accurate ones, but in realistic settings, usually approximate ones, but

get solutions to dynamic problems that are otherwise out of

reach.

Yeah, good, but like it's quite different in that sense.

Just we use, for example, function approximates from, from supervised

learning to learn these policies and our behavioural rules, but

ultimately, all these uh tools are for uh for optimising

dynamic decisions.

OK, so that's in one sentence.

Reinforcement learning is gonna start with learning and analytic the

optimal decisions to trial and error.

So an RL agent, and agent is a bit of

a stretch here, but people usually say this, um, uh,

think of it more like a function making decisions, that's

it, yeah.

Um, so in a policy function, we'll, we'll define that

in a moment.

So an RL agent observes a state in these kind

of setups that we look at here, observes a state,

takes an action.

So as a reward on the next day, takes the

next action, etc.

The key thing here is, you might have heard of

algorithms like bandits or so that you know might place

ads on websites or other applications.

The key assumption here is, and we're looking into the

kind of the details in a moment, that my action

changes the future state.

So here the action of that RL agent or the

policy we we train is actually influencing the future state

and that makes it um what we later introduced as

kind of uh markoff decision processes.

Um, when we see how this, how this pans out,

but that's kind of the high level of intuition.

OK, and that's like an image you usually see in

variants in different courses.

I just took the one from the Satan book.

Um, have you heard of this book?

Um, so it's those of you that maybe have done

a bit of RL.

There's this, um, book which, you know, it's, it's a

pretty, um, uh, like it's been around for a while,

first edition, and they revised it and they had a

second edition.

I think like it's made a small revision.

It was originally 2018.

20 or something like this.

So pretty new.

Um, RL has been in a field that's been, you

know, around for a long time.

Uh, the connection with, with deep learning, um, really came

massively to the surface with this 2013 DQN paper that

we're going to look at actually, um.

And uh before this was around and you would, you

know, either do tabular methods which we also look at

or like use simple function approximates like linear functions and

so on.

So this is not something that just came around in

the last few years or so.

Um, and the general setup of these like that's basically

an image for such a Markov decision process or like

just a figure.

Is you have this agent and think of this agent

like that agent usually has something that's called a value

function that gives that agent, you know, defines the value

of each state.

And then how I like to think about this because

agents are sometimes confusing people because they think agent-based models

and so on.

Um, I just think of this as a policy function,

and the policy function are defined, but it's, it's the

thing making decisions.

It's taking a state in and making an, uh, picking

an action or picking a distribution of the actions.

And that's exactly what this image also shows here.

So agent or policy function takes action.

Action feeds back into environment key.

The action influences the environment actually.

I'm not abstracting away from this, but I'm shaping this

now with my action here.

So now the environment emits a next state.

That becomes the state the agencies, um, and it also

emits a reward, you know, that reward could for a

while be zero, could be negatively positive, whatever the scaling

is and so on, but it emits some signal, um,

about the, you know, how valuable that action was for

the agent.

OK, do you have uh questions about this kind of

picture overview.

So maybe already sees this like this nests a lot

of things, right?

The key thing is if you try to kind of

wrap your head around this when you see this for

the first time, one key thing is my agents, my,

my, my agent's actions or the action he actually changed

the future state.

That's a fundamental property.

OK, here we go.

I said, like, uh, the deep reinforcement learning, trying to

combine this with networks, that's what it means.

Um, that came into, um, I mean, at least public

appearance in in 2013 with this paper on the Atari,

the DQ networks.

OK, so that was a computer that could learn just

from screen input to play an Atari game.

And I wrote it up here how this maps into

the previous uh structure.

I have a state and that's the screen image.

You know, I could say, let me take the last

three screen images or something like this, OK, and then

I take them in all colour channels or something like

that, and that's my state.

So given that state now, I choose an action, and

I chose the from that paper, I took the screenshot

of the arcade here, like, you know, I mean, maybe

I, I don't know if you've seen these games like

you go left, right and you have to, uh, you

have to um uh let that thing basically bounce back

and uh get rid of.

Of the wall, uh, north of it.

And, um, I can go left and right.

So it's a very simple action space.

we would call this, um, not continuous, just left, right,

um, and just 2, and, um, so, yeah, break, break

out, and, um, I can observe a reward depending what

I hit.

In games, it's very easy because my score changes.

Here we go.

So that's nesting this almost perfectly.

It's not that I see a reward, like, you know,

20 hours in the future, but like, immediately, almost after

every action.

OK.

So now comes, um, comes, uh, the, the new screen

image, right?

So I'm hitting something, there's an update to the screen,

um, that goes again, you know, it's my new state.

I choose a new action.

I might observe a new reward if I hit something.

Imagine Space Invaders or so.

Do I hit a spaceship?

I get a reward, but I don't a spaceship, I

don't get a reward.

How do I start?

I actually explore randomly here.

So I'm interacting with this world, with this environment just

randomly.

And then successively reinforcing those decisions that led to rewards.

That's where the name comes from comes from.

Is it clear how this maps into the previous picture?

Yeah, OK.

Good.

So let's just like go through a couple of things.

Another famous quote unquote kind of thing here was, uh

was the AlphaGo um uh paper and project and then

subsequent projects that, you know, we, we're improving on this,

um, learning to play without any human knowledge and learning

to play like the Alpha zero across a bunch of

games.

Um.

They combine reinforcement learning, that reinforcement learning with a couple

of other tools like Tre search, like that that's more

like statistical search algorithms.

Imagine like a chess game and you search through all

the branches of the tree that could originate from a

move.

I'm thinking chess, for example.

It's all the rules are clear, right?

So if I move a piece, we can compute all

the other pieces that you could move.

Problem is the tree is so large, uh, that results

from this that we can't conclude it.

But there's even a theorem in in game theory that

if you have these kind of games, you could backward

and use them.

Um, the problem is the game is too large for

us to, uh, to compute it in terms of chess.

Um, and here, that game is vast, right?

So I can place these, this is the, the, the

move number, I believe.

Uh, this is where I, uh, I placed these, um,

uh, black and white pieces on the board.

It has a huge amount of configuration.

Imagine the combinatorics and how many ways this will be

configured.

Um, so what they did is they took this port

as a state, right?

Um, the action was the, the move of the algorithm.

The next stage was, it's kind of imagining the other

opponent as part of the environment, right?

So if this thing is playing against EO in this

AlphaGo match, it's moving and then there's another person moving,

but that's just from the perspective of that algorithm here,

uh, the environment moving back.

OK.

And then, uh, new state, new move.

So question to you, if you think that way, like

these kind of states, say Atari, state gold, you already

see some resemblance, right?

They are both images.

Uh, so what do you think is a neat way

to represent these states?

Or to, to extract like the possible actions from them,

etc.

If we try to think this through, so we want

to make an action based on a state or a,

yes, so for the Atari, for example, we would take

the last 3 images, like, you know, it's like continuous

continuum of and then the last 3 images in our

colour channel here, yes, like you said, just a screenshot

we can take this.

Um, or like, you know, account configuration, we don't even

have to take a photo like we do, but it's,

it's a, it's a spatial um thing with, uh, with

an X and a Y X, right?

So it's like, uh, it's an image from that perspective.

Um, OK, but imagine I want to pick an action

automatically based on that state.

So there must be a function that maps from the

state to the actions.

That thing must exist.

How do you define a function that maps from something

so complex to what I kind of piece I move

next?

And that's why this wasn't solved before uh neural networks

or before kind of deep learning or function approximators whether

they are neural networks or other, other function approximates of

enough uh enough um expressivity or flexibility to, uh, to,

um, to extract from that state, um, uh, the plausible

or strong next action.

Um, and we're looking to algorithms how we find strong

actions, but, uh, so what neural network would you use?

Let's, let's take an MLP like sell out etc.

How would you even encode this as a state for

an MLP?

What would you do?

It's a vector.

OK, so say we have, you know, I'm not a

goal player, but say you have a certain amount of

of squares, yeah, so you, you can, you disretize or

like you, you say you flatten this in a vector,

and then each, each position on that board, like a

chessboard, um 64, you would, you would give it some

kind of uh data, what pieces on that position, etc.

With this though, we would, we would kind of Yeah,

I mean, not utilise this nice, uh, this nice spatial

structure.

Also, the problem with these, these MLPs is, as soon

as we create these long vectors, we need to do

these fully connected layers.

That means we need to multiply a huge vector with

a huge matrix to map it into another huge vector,

that means a huge amount of parameters.

Imagine we have a state with, you know, a couple

of like 1000 inputs.

Imagine, for example, um, what we did last week, the

language models, right?

So we have the vocabulary of 200,000.

Imagine I take 200,000 and multiply this into something else.

That's a humongous amount of parameters.

Actually, they do this, these models at the very last

um piece of them.

Um, but if you can avoid this, you try to

avoid this and you try to use different kinds of

layers.

So here, what do you think Alpha ago they used?

What kind of neural network would they use?

Yeah, that's a good point.

There was work on this actually because of the auto

regressive nature of this.

But if we even make it simpler, imagine we we

proxy the auto regressiveness by just taking, you know, the

last image of the last three images and saying that's,

that's my current state.

That's all I need.

Yeah, so you, you would just take something that is

strong with images and you would think of this is

not like as you correctly said, not really an image,

but abstractly it is, right?

So it has the same structure.

I can even, I can, I can do a computer

represent this and it almost looks like a grayscale image,

maybe with a couple of more channels depending on the

pieces on it and so on.

Yeah, so it probably has to do with convolution on

our network, so, OK, um, good, other questions.

Cool.

So just I want to give you an overview of

how vast the applications of that kind of stuff are.

Um, there's been a paper in 2022 on controlling the

To mac, uh, that's a, um, uh, reactor for, for

nuclear fusion, and with nuclear fusion, um, an issue is

that the, the plasma is very, very unstable and to

control it, and, uh, you, yeah, you have to have

very kind of thin technology um to not make it

reach states that you don't want it to reach and

become unstable.

So This is actually something that is slightly more complex

than the other things, also in terms of its mathematical

abilities.

Like I, I spoke with one of the authors, and

they, they actually only had part of the state and

so on.

Um, but you can still map this exactly into this,

you know, silly Atari game logic.

Um, it's, I observe a state here, I have core

currents and magnetics.

I choose an action, um, some voltages applied to this

control coil.

I obtain a reward.

They define this.

They define the reward such as has has the plasma,

the shape I wanted to have, um, and then the

next day comes, um, and that's next core currency magnetics,

you know, um, giving me a summary of the state

in that, in that reactor.

Why is this a quote unquote full RL problem?

Because again, my agent, my agents are like my actions

here changed the future state.

It's always that.

Otherwise, you know, it would be a more trivial problem,

and we would use something like a band-aid also.

You might have seen this in other courses we, we

won't have the time to discuss them in detail here,

but it's basically, um, abandons, usually they are using what

people call online, so I just they're basically set and

then like data is arriving and they make decisions, and

they base their decisions either on just the global scale

states.

So just an average.

Imagine like something placing ads on a web shop.

So you go to a website and it shows you

like an ad in a certain colour.

Chances are that if I go to the website, it

shows me the ad in a different colour because this

thing is experimenting on us and trying to place them

in the right colour, right proportion, right, or, you know,

where on the website.

This thing is not thinking it changes the world through

its actions of placing the ads.

So I just put in the ads somewhere.

Maybe it's as far as a contextual bandit, that means

if I arrive, I have some cookie or browser history,

and it knows that and places the or it shows

me something that I have maybe bought before or so,

but still.

It does not assume that I changed some it's budget

constraints or something like this, whereas this year really, uh,

the decision is like, well, the assumption is this, uh,

the correct assumption here is that the actions changed the

states.

Any question?

Do you see how this is the same as before?

Like from a kind of math perspective, it's just the

same thing.

Here, um, you might not want to represent this with

the CNN.

I will need to look into the details.

Um, they might have used like, uh, actually another network,

but they had a huge amount of states, and also

what I know is they had continuous action values.

Um, whereas the previous one, we had discrete ones, right?

So where does the piece go?

And here they actually had continuous values, um, which if

you think about one.

Continuous variable has more possible values than all of the

disre ones, right?

So these are these different kinds of infinities with a

bit of a map of this.

Good.

OK.

So further, further fields in which this has been applied.

The prominent examples are games and robotics, um, but dynamic

decision problems are, of course, ubiquitous.

Um, and the social science just because of what my

field.

It's like, um, it's, it's useful of course to many,

but in economics you can think of typical problems are

dynamic resource allocation, planning problems, expectation formation, how do I,

because all that planning involves forming expectations, how do I

build more realistic expectation formation and how we model humans,

um, macroeconomic policy, you know, imagine how do I respond

to certain.

Um, uh, inflationary shocks what is good fiscal policy for

me to do a dynamic econometric models and so on.

At the end of the lecture, I show you kind

of a project I'm working on where we try to

apply this for, um, for econometric, um, for an econometrics

topic, and that causes.

OK, um, and then obviously at the moment, it's probably

most frequently used, at least in the kind of Public

researching and natural language processing, but very, very, uh, wide

applications.

The problem is how do you get with simulators?

Either you have a docker Mac and you can just

like, you know, or like a robot and you can

learn with the thing or a game, but if you

say want to plan dynamic resource allocation, you don't have

a simulator of the economy, right?

So this thing here.

This kind of simulator of I pick an action and

the world feeds me something back, that is very difficult

to get.

And actually, the paper I'm showing you in the end,

we're trying to build this from data with all inference.

But, um, but one reason why reinforcement learning maybe hasn't

taken off as much as, as, uh, as other fields.

Unless compute reasons and so on, uh, because this is

less suitable for GPUs, but to also arguably because it's

difficult to just transfer this to the real world and,

and because you can't experiment on the real world, so

you would need to build some news.

All right, that's an introduction.

So one thing I want to show you, we can

wrap this thing here in code, OK?

Um, and usually, and we'll see this in the code

examples.

The the code is like there's a lot of stuff

going on, like, you know, we instantiate this object before

it's like a class and so on.

But really, we instantiate the environment.

It has a reset function.

It gives us an initial state.

Now comes our action.

Um, it takes a step, gives us the new state,

a reward, whether the episode is over, like, you know,

is the Atari game over, and some more info, and

that's it.

So really that is the whole thing basically usually looks

like two lines of code and then you know, stuff

going on in the background, but really this is it.

And then how do we pick the A's, the actions?

Well, we picked that with an RL agent or an

RL one.

And this environment thing keeps track of what the current

status, which images next, etc.

Good, so now we can introduce this slightly more kind

of formally.

And an MDP is this Markov decision process and sometimes

people include the horizon to the top and so on,

but um.

Uh, this year's definition with, um, with the set of

states that's always in the definitions, uh, the sets of

states like out of which set kind of like, you

know, the set of images, the set of uh currents

for the, for the robot, etc.

um, a set of actions, um, what actions can I

take, and this year is Is this function that says

the probability or distribution of what could be the next

state of the world is dependent on the current state

of the world, obvious, but also dependent on my action.

That's that assumption.

Now, I can model the probability distribution where it just,

like, you know, it does not depend on the action,

but then, you know, it wouldn't be a full NDP.

Um, the reward is a reward function of the current

state.

I took that action.

I landed in the next state.

That's when I see the reward.

That's, and that reward would be my delta and score

on an Atari game.

That's why these are such nice playgrounds for these algorithms,

because we need the score.

Um, discount factor, because also here, um, like usually, um,

I can also some variants of these discount factors we

look at next week, uh, they can lead to more

stability and training and so on, but usually I care

about immediate rewards more than about future rewards.

Um.

Think of like, you know, economic applications.

Why do I usually assume this discounting because at time

0 right now I could take the money and invest

it, uh, and then I would have more in 10

years.

So, um, uh, if you give me money in 10

years, it's just worthless to me because I had it

now I could invest it.

Some opportunity cost.

That's one way to think about it.

All right, good.

So if a problem can be mapped into this MDP

structure, we have a good shot at attempting it.

If it's humongous, like we might not have enough compute,

etc.

um, so things like sometimes these things push the boundary,

for example, that paper here.

At the time, like, this is, like, you know, now

it's simple, but then they applied this in 2019 to

Starcraft.

I don't know whether you've seen this, like that's like

a, you know, like a strategy game, kind of a

multiplayer game, and this like, it's very, it has a

lot of details, high resolution images and so on, and

this really was pushing the, the limits of the computer

available at the time, um, and they could, you know,

play as well as like pretty good players but not

be the very best players.

Um, but that really kind of took it to quite

an extreme, um.

OK.

So extensions, these POM NDPs partially observed Markov decision process.

If I just don't see the full state, actually there

is, there are aspects of this that are partially observed,

makes it more complicated.

We look at the fully observed NDPs.

OK, so now we can define a bit more rigorously

what we've been talking about for a while now.

So one is the policy function, the unspectacular, given the

state, I choose an action.

Think of this thing probabilistically.

So given the state, I have a probability distribution of

actions.

What is neat about this way of thinking is it

actually nests the deterministic one because you could think of

a probability distribution that just has all its mass on

one action.

And then it's still a probability.

It's the delta function if you know this from probability

theory with all the mass on 11 point and then

it's as good as deterministic, but we can still think

about it in terms of probability.

So anyways, so you can always think of this, but,

but usually either like it's a distribution over things or

Or a specific action, I think.

Um, so my goal in all this, um, all this

endeavour here is to find the policy which maximises the

expected discount rewards.

And here's where the dynamics coming in.

Um, that is not the reward right now or the

reward tomorrow, but it's the sequence of rewards until the

end of that finite process here.

And for very large T it's, it's for many intents

and purposes and applications as good as kind of um

uh as good as uh um it is.

Mhm.

It was a bit brittle but should work again I

think.

Great.

OK, questions about um this year.

So conditional on a policy, given my policy, um.

So I want to pick this and then give my

policy because that changes how the whole environment moves afterwards,

I want to pick the policy that is maximising this

because this year is between 0 and 1, the um

the power of this decays, right?

So at some point like far in the future, I

don't care anymore what rewards I get.

So how we can define value functions, um, and we

define optimal value functions, and that is just this sequence

of rewards with the optimal policy.

This year was our objective function.

I would like to do this, and this year if

I've actually picked the optimal policy.

So if I've picked the optimal policy and I'm at

the current state, what is the value until the end

of that game or until the end of that, uh,

you know, episode, um.

Usually one step people call period.

There's lots of names against all these things, but one

step in the game you would call it period.

The whole game you would call an episode.

All right, so, and then at a current state, um,

this would give me, um, this would give me, uh.

S 0, this would give me the expected reward.

So now thinking of, of this formulation, one way in

which these things are often introduced are simple grid world

problems.

So if you've seen the Sutton and Barto book, for

example, they do this, uh, heavily.

So imagine I have.

Like, you know, some RL agent that lives on this

board, OK, and starts here.

That's the start.

OK.

So now, all of these board positions give me zero

reward.

I can go north, you know, south, um, west and

east.

So in 4 direction, um, or up, down, right, left,

and I want to arrive here because that gives me

a reward of one or whatever reward bigger than the

rest of the fields.

So, now I randomly try out stuff.

If I go up here, environment throws me back on

the board.

So the environment function would throw me back.

We could code this up, um, so the environment throws

me back on the board.

Um, and now at some point, I'll hit this.

And then I'll update my knowledge of that environment and

we go into this in the detail in, in, in

a moment in the details, and I'll then find the

trajectory to this successively more easily.

But I could have different things.

So for example, I could have a cliff here, we'll

look at an example, and I could have some wind

blowing and you know, there's a certain probability I fall

off the cliff, so I better don't take that path,

but I learned how to take that path, right?

So, um, OK.

And then with that logic here, imagine I am, I'm

here in that state.

What is the value of that state?

The final state, that that's like my last period.

I'm not in zero, but like I'm in in capital

T, um.

So the value of that state, if I'm there right

now without, you know, state discounting is 09.

It's one, right?

So I'm there right now.

But then we can actually say for an optimal policy

that if it was sitting, say, imagine the agent is

here, an optimal policy wouldn't just go there, no, it

doesn't make any sense.

Optimal policy would go there.

Optimal policy here would also go there.

Optimal policy here, however, couldn't go diagonally say because we

assumed this all would go these ways, right?

So now we can compute the values of these states.

What is the value of this state?

If we have discount in 0.9, what's the value of

the state?

09.

That's right.

So 0.9 and 0.9.

What's the value of this date?

Is it 0.81.

So 0.9 to the 2.

Yeah, OK, you see the logic and here 09 to

the 3, etc.

So this is how we can in the super simple

world, we actually know the values of the optimal policy

because the optimal policy is so obvious for us to

see.

And that is what the value function needs, the optimal

value function.

That thing for every state gives me, um, uh, gives

me a value.

OK, so we can write this recursively and now we,

we drop the, the time indices that you might see

here.

We dropped them because this year holds at any point

in time, wherever I am in the sequence, that relationship

here holds.

And that means that um my value on that state

is my value on the next state.

Discount it because it's tomorrow.

Plus, the reward I'm just getting.

And what is this year?

It's just all the probabilities that could, you know, the

next iteration could produce and what that reward might be.

Because that reward depends on where I land in the

next state.

So this year is all what the next state could

be.

Um, it's like some kind of expectation over the, over

the environment dynamics.

OK, and that's just a different way to write this,

um, and that is still the optimal value.

It's just this recursive way.

It's sort of, you know, you might have seen recursive

functions in programming, um, it's depending on itself.

It's saying if today is optimal and tomorrow is optimal,

well, then the only difference is I discount tomorrow plus

whatever reward I get today, and that's still optimal.

OK, and then there's another bellman equation, um, or another

value function that's more important, um, and that is a

state action value function.

OK, and that value function gives me for every state.

The, uh, the discounted sum of all the future rewards

when I take the action a and thereafter behave optimally.

So this thing in like, you know.

So for example, here, if I, if I, the, the

Q value of, of going down, going onto this square

say.

From here, um, it's 0.9.

I'm going right into the uh into the uh into

the final square and it's one step ahead, so it's

0.9.

Um, but the, the Q value going going here won't

be because then I'm taking an extra step.

Um, and it's just a different way to think about

it.

It's, I have a value function that gives me the

optimal, like the, the, the, the value on that state

on the square I'm standing at.

If I behave optimally ever in the future, but I

have this one move, and I now have a value

if I'm in this grid world, so I'm standing say

on this square here.

You sometimes see these things with like kind of numbers

here, I have a value for all moves.

So now I don't have a value for the state,

but I have a value for moving here, a value

for moving here, value for moving here, value for moving

here.

This will be, um, this will be the worst because

I'm going away.

This will be kind of, you know, second worst because

I'm staying on this square.

These values will be higher because I'm going on the,

onto the next square.

So now each of the squares has like 4 values.

It's not just one value for a state, but 4

values for a state and all the possible actions.

That's what the Q function is.

OK, so how do we learn these, these, uh these

behavioural rules, right?

So you might have heard of these traditional methods, um,

called, you know, from dynamic programming called value function and

policy function iteration.

We can actually prove that for these, uh, you know,

for, for these problems for these NDPs, they give you

the exact solutions.

So the obvious question is why do we do all

the reinforce anything?

Because you can only find these real exact solutions for

toy problems.

Um, and for realistically large problems, there isn't enough compute,

uh, to find them.

And also, it's not just this, um, it's, they also

require knowledge of the dynamics.

So in order to do this value and policy function

itation, I need to know what is the probability to

land an S prime given S in the action A.

I need to know all of this.

Whereas if I just am be on this board and

I'm wandering around and trying things out, I have no

idea about this.

I just kind of see what happens and then update

my value estimates of all these squares.

That's how reinforcement works.

Whereas if I solve this from like knowing the full

problem, I need to know, um I need to compute

the expectations, I need to know the probabilities, etc.

Right, so we will, first of all, look at an

example that we relax that assumption, just like knowing all

the transition probabilities.

Um, we will yet have to look at an example

that scales to bigger state spaces.

You will see this one here doesn't work well.

But it's a nice way to start.

It's this kind of, kind of canonical reinforced learning algorithm

called Q learning.

OK, and the idea is instead of knowing, you know,

this probability, this transition uh uh function of, of the

NDP, um, I simply sample experience from it.

That means I randomly interact with the world.

And then I hope that I learn the expectations, sort

of, because I see draws from the world.

I work on that square, see what happens, nothing in

that square.

At some point, I hit the square with the treasure

or the value, and then I kind of learn this

way.

And I'm sampling experiences from that, from that, uh, from

that world and ultimately distribution function of, of states that,

that are hitting me.

Um.

At every point in time.

If I have an initial estimate of Q, which I

might initialise just at 0, I say, you know, every

state action pair.

So for example, maybe to do this a bit more

slowly, how many Q values are on that grid here?

How how many key values do we have as a

table for that grid?

OK.

32, yes, so it's 88 parts and then times 4

actions.

That's right.

So we have a queue table.

If you write this stuff down, we would have a

table with 32 rows.

Yeah, and for each of these Q as things here,

we will have a value 42 or minus 10 or

whatever.

Here we would have um have uh things between 0

and 1.

Yeah, OK, so.

I can at each step when I observe a reward,

I can construct a new estimate of that current square.

So imagine I just wonder on a square.

Well, then, my estimate of that square now is the

reward I saw, plus the next state, I would go

to the next optimal state, given like, the best action

afterwards, right?

So, that makes complete sense.

So I just go to that state.

I see 42, or 0 here, because there's only 1

here, I see 0.

and then I do 0 plus the value from the,

from, from here.

And that's my current estimate for that square, and I

can compare that to my old estimate, which was just

pure um essay here.

How do I get the initial estimate because I need

to get started on something.

I just initialised that table.

I might fill it up with zeros.

And that's pretty much true learning and then um for

each square I update my estimate in an ongoing way.

I say.

iterating and walking through that world, my next estimate of

the square is 1 minus um 1 minus alpha and

alpha is a small number of my old estimate of

the value of that square, plus the new one, and

that's called target.

So I'm basically fading in these new estimates, because imagine,

imagine we have a more complex NDP where quite randomly

I sometimes can see large rewards here, but very rarely,

but I step on this thing and I see a

reward of 50, but it almost never happens because sometimes

I'm falling off the cliff otherwise.

Now I'm seeing 50 and I would massively upscale my

Q value if I don't fade it in with this

moving average.

And that might be overconfident.

So usually, you know, that's a hyperparameter to tune, but

we're faring better if we, if we, uh, sample these

experiences, um, uh, into our overall average at a smaller

rate.

Why?

Because these are only individual observations, right?

We, we can have infinitely many interactions with that world

here.

Why update our Q value so massively based on one?

OK, cool.

I think one thing that like I just have to

think about that when we discussed this year, one key

property of this, and that's by the way, where the

Markov comes from, is, is the, is the assumption or

like the property of these problems that all the information

I need is given in my current state.

So I don't need anything else.

I mean, that's why all these functions look that way.

Um, I don't need to know anything other than my

current state to determine the value of something, and that's

the underlying Markov problem if you might have seen Markov

chains or something like this.

Good, so Q learning.

That's the algorithm.

Um, we initialise the Q function.

We get an initial state.

Um, You know, might be given by the problem, and

then we, uh, we compute this convergence or until we

don't want to anymore.

Um, we sample an action.

We haven't yet said how we picking it, how we

do this actually, because like what we did so far

was, you know, hm, I'm clear.

So let's like visit this next slide.

So we sample an action, however we do this, and

then we get the next state.

Cool.

Um.

If that is terminal, I just get this.

Why is there no V?

Like, where's the V gone?

Yeah, exactly, exactly.

The V would be 0.

That's what this means.

The V is 0.

and then I sample a new initial state back to

our code example from the slides.

I go reset with my environment and I can run

again.

Otherwise, I construct the target.

I have my R and my optimum will be here

from tomorrow.

Um, I, uh, end this year, um, I update this,

update the state, continue with the next episode on the

grid.

If, uh, the episode is terminal, what, what would be

a terminal episode in that world?

What's the only way to end that game?

Yeah, reached the treasure in the end and we look

at these kind of examples, otherwise this thing is wandering

across the board.

Good.

So now if you look at this thing, what would

be the obvious way to pick an action?

You have two values for everything.

Maybe after a while you even leave them a little

bit and not initialise at like zero.

So you have a Q value now.

What what's kind of the obvious thing to do?

Exactly, like for every, for every square, go through the

QSA values that you have at your disposal for the

highest, and that is what what is referred to math

as greedy.

It's only one step look ahead, um, and it's just

taking the, the largest immediate return.

And this brings us to a very important part of

all these algorithms.

They have to balance two things that are called exploration

and exploitation.

It's like, you know, I mean, the the standard example

that everyone says is checking out restaurants or something, do

you go to your favourite restaurant or do you try

new restaurants, these kinds of examples.

OK, so that's everywhere and learning happens that way.

Um, uh, that whole theory of reinforcement learning has, has

a lot of roots in, in, uh, cognitive psychology and,

and, uh, neuroscience and the reward signals and so on.

In the book, they have some chapters on this.

So here it's like, I need to balance this in

a way.

I need to keep exploring, checking out the board, not

overly committing on some states that I've just seen once

and think are great, but then I don't find the

treasure basically.

And, um, and then here the, it's, it's, uh, that's

the, the way to do this.

Um, in learning, a common approach is what is called

epsilon really.

It's insanely simple.

It's just, I take a random action, you know, up,

right, left, uh, um.

Um, um, or down, and I picked that with probability

epsilon, um, and, uh, I otherwise pick exactly what you

said, the largest Q value with probability 1 minus epsilon.

And then I can maybe run in some decay of

epsilon, so I can do the learning, decay the epsilon

with some holistic.

And then at some point, I'm converging more towards like

sticking to more optimal behaviour, just picking the Q values.

Um, but I don't want to decay too quickly because

then I don't explore the board again.

So that's kind of, uh, the trade-off here.

During inference, if we think of it like kind of

in the language from the language, which more or like

like the neural network, if we really want to display

the best policy we have, of course, we stopped doing

the Epsilon stuff.

The Epsilon is for learning.

Um, if we try to walk across the cliff without

being, you know, blown off the corner or something like

this, then we just, uh, we just, um, uh, we

just set the epsilon to zero and evaluate the policy.

Our policy function now is implicitly defined over this.

It's my policy is with absent probability, pick a random

action of the 4 feasible ones with 1 minus absolon

probability, take the action of the highest value.

That gives me for every state um a way to

go.

That's my policy rule.

That's my pie, um, you know, choosing a even and

state.

Questions.

So just to repeat, the Q value for a square

is the value of that square plus the value of

The action after that.

It's the value of the entire game, if you still

want, until the end.

So the Q value is until the end of this

thing.

From here on, I'm behaving optimally until the end of

this game.

Yeah.

The only thing is, I'm behaving optimally, not, I mean,

if it's a state value, I really say from right

now, I'm behaving optimally.

The cute thing is like, I have the state, and

you can give me one of the feasible acts.

And that thing I'll like, you know, that action I'll

take on the grid world or whatever the problem is,

and afterwards I follow the optimal policy action isn't that's

right, because it needs to be defined for every state,

we need to have these 4 QSA values.

So even if they are not optimal, we need to

define them somehow.

But only one of them, if it's clear cut, will

be the right decision for that kind of problem.

We have one right way and that's towards the one.

What if we have infinite actions, infinite set of actions?

What do we do in that case?

Yeah, so for example, um, with, with this stuff that's,

but imagine one continuous action.

I have an action on any real interval between 0

and 1 between, you know, there's this thing that there

are as many numbers in 0 001 than there are

in.

No, so these kind of tricks with the real number

line, but like any tiny interval on which I pick

real actions, uh, will have a huge amount of numbers

and infinitely many uncount.

So, uh, there are, we, yeah, I think I'll do

this next week.

I'll, I'll include this in the week.

So.

Actually, it's a great point.

These algorithms struggle with this.

Um, there are ways to implement this, to my knowledge,

but like kind of the slightly more negative ways to

implement continuous actions, how they also did this stuff with

the Toker Mac.

They used a different kind of algorithm, um, uh, that

is like a class called policy gradient methods, and that's

next week.

And to give you a preview of how this is

done there, I learned the policy directly.

I sort of, I maximise the sum of discounted rewards.

That's my objective function, and I take a derivative that

implicitly takes a derivative through the policy.

So I'm doing gradient descent, trying to find the optimal

policy and gradient ascent because I want to maximise my

rewards.

In these models, now you could have a neural network

with a soft max and then you would still have

discrete actions, right, not helpful.

But imagine you have a neural network that outputs a

single number and you say that's an intercept and that's

the mean of the normal distribution.

So now you have a normal, you have a neural

network, it takes its state as input, it outputs a

single number, say there's only one action.

But that number is not in training, not a deterministic

action, but it's the mean of the normal.

Afterwards, you, you pass that through a normal because that

normal network.

Puts out two things a mean and a standard deviation

maybe.

We put them into a normal sample from the normal

and you're back into this, you know, back in this

game with exploration.

Like now I have some random move around the mean

and if I actually evaluate that policy later on, I

get rid of the normal distribution and I just play

the, the um the mean action of the normal distribution.

So that's how you would incorporate continuous action spaces.

Um, all right, next week, I think this will make

more sense, um, but yeah.

Here it's a bit tricky, but we look at examples.

A lot of questions.

How are you, how are you feeling?

Shall we, should I show you code for the Q

learning or do you want to take a break now

and then?

So How tired of reinforce in the world?

No.

Alright, so, OK, I can show you that, I can

show you the code and then I mean it's very

short, you'll see.

So this is some kind of This is the code

that you would get if you these days, you know,

I prompted all one and then changed it a little

bit.

That's the kind of code you would get, OK?

So these are kind of samples.

I uploaded them, but if you do that from, you

get slightly different code, but it's working.

And, and then actually we can now look at it

and make sense of it.

And it actually, it implements these canonical ones.

I didn't just quite prompt one give me Q learning.

I said because I work with this reinforcement learning and

know there are these gyms, these, uh, little, uh, playgrounds

to train these algorithms.

Um, and I asked it to give me a Q

learning version that works with the gym and so on.

And the gym is just the environment, you know, I

could walk around the cliff like that kind of stuff

that they think of as the gymnasium as the library

these days.

It started with OpenAI as gym and now it's in

some kind of open source foundation as gymnasium.

Um, each environment has an observation space, so how many

states are there?

You know, so here we have 8 states, right, simple.

Um, and, uh, and then how many actions are there,

um.

OK, cool.

So here I initialise the Q values.

That's my queue table.

It's tabular Q learning.

I initialise how many states and the actions.

That's exactly what we just said.

I, I set an empty list for my episode rewards.

Um, good.

I reset the environment.

Turns out it's not just the initial state, but it

also always emits some info, which is usually like an

empty dictionary or something like this, but sometimes it carries

some information.

I set a flag to false, namely done.

What is, what do you think Du refers to?

That's right.

When the episode is over.

That's it.

As simple as this.

Here we go, Epsilon really action selection.

Um, I pick a random number.

Um, if that's, that's coming from uniform, if I just

do this coming from uniform distribution between 01, this thing

is smaller than epsilon, I'll just pick a random action.

That means given my action space, that would be, you

know, up, down, left, right, pick a random one.

otherwise, I pick the maximum action of the current state.

So I have 444, you know, columns in my table,

pick the one with the highest value given the role

in the table, which is the same.

So execute that action.

That's why I showed you previously.

Now the environment, the world gets the action, and the

world throws something back at us.

World throws at us, next stage, reward and a flag

for done, which is true or false and some info.

You know, is the, is the episode truncated and over,

and, you know, some other information.

I updated a few values.

Here, they actually more gave me some fancier code back

with like, you know, some, some kind of plus equal

logic and so on.

But then I thought it was a bit, uh, harder

to see that this is actually the same thing we

did.

And this is really just a weight average over my

estimate times the learning rate, um, plus the learning rate

times, and this is my target, the new reward plus

from tomorrow I behave optimally.

And that's my current estimate now of that state, or

my, my, my very current experience of the, of the

state in which I am now.

So I'm trading off like each state, just to get

that clear, each state is given by has a value.

And that value is from now on behave optimally.

Um, I had an estimate before, you know, condition Q

QA values now the Q values also have a connection

right now, and then I now go to that thing

and I observe a reward.

But then I also know what the best square is

that I currently think if I go to next.

So I just take that value of that square, which

is the optimal uh value ever after.

I add the reward that I just earned, and that's

my current estimate of what that stake right now was

worth, and I can compare it to what I had

on my table.

OK, so I override state with next state.

I add this to rewards.

Um, I sent the rewards.

Um, here, by the way, there's some fancy, slightly fancy,

like you have like some multiplicative factor with which you

multiply the epsilon to, to, uh, to bring exploring down.

Then this, I like, you know, us.

To add here because when I want to plot the

learning through time, so these things are interaction, interacting with

our grid, it can use, like, sometimes it falls off

a cliff, so has very low reward and so on.

So you don't really see the learning very well.

What I do here is I just take a moving

average over the last 50 episodes, and I plot that

moving average.

It's much easier to see.

Um, OK, I created a figure of this, um, and

then in the end, I render the final performance if

I so wish.

Um, and, uh, OK, so you can see these kinds

of environments here.

One is frozen lake.

So as before, um, I, it's pretty tiny, um, I

created an environment, um, so you know what we discussed

in the classes, so I create environment, um, added these,

um, edit these things.

Let me See if I can go to the no.

Yeah, it's type one.

OK.

So what's there, that's there, and that's the tabular cu

learning.

And maybe like this.

And I just run it here.

Why do I not run in a notebook these days?

Because my notebooks kept crashing on me with this because

of the rendering of that world.

Like there was some issue with the Jupiter kernel and

there was a GitHub issue where no solution.

So then I, I decided to, uh, to just use

the console.

Um.

Let's see if this works.

Here we go, super fast.

What happened just now, let's see.

I Had 10,000 interactions with this world.

It's a great world.

You see it in the moment.

It's called Frozen Lake.

There are a couple of, you know, ice parts of

the lake there, on which I don't want to fall.

Um, and I chose, um, true for rendering, which you

will see now.

So that's the learning.

It's super easy to solve after 2000, uh, interactions with

this thing we're done pretty much.

It's just randomness.

Um, and let me close this, and here we go.

Spectacular.

OK, that's it, yeah.

OK, so we love the part, but if, you know,

I mean, you are laughing now, imagine we do one

of these, uh, now we try to pull this off.

OK, I don't like it.

Let's say, um.

Hm.

OK, no learning, and no, let's see.

So I can take a while now.

Yeah.

So the Q values weren't updated and this thing is

just sitting there stationary not moving.

Otherwise, if you try like learning for a little bit,

like basically, um, it's just like, uh, yeah, it's not

going, like, yeah, it would fall into these like, um,

holes in the ice.

OK, so we can do other things as well with

the much discussed cliff as well.

So then we can, I, I linked this here.

You can pick this environment, uh, and the code actually

applies with these.

Uh, that works with these, if you install the necessary

library.

Um, OK, let me close this here.

Ah, I think I have to.

Yeah, so they started to learn, but I, I still,

so.

Ah, pretty good.

OK, so that's the cliff, um.

See, like that, that was so simple to, yeah, OK,

um.

And we, we have that and I think very few

episodes.

I think the interactions take a bit longer.

Here we go.

Basically, we converge pretty quickly.

The um But nothing more to see.

We can make this harder though.

Uh, we can, uh, put this like a slippery, uh,

option.

And then there's some probability the environment now the market,

you know, the NDP now will suffer.

And like if I walk close to the edge, like,

you know, there's some probability of me moving around and

so on.

So, and then it will learn different policies for like,

should, um, let's, yeah, let's go for 1000, might take

a bit longer.

Now it's, it's much harder to learn.

So I'm having more, there we go.

I mean, we, we sort of converged.

I think we're done pretty much, uh, but it took

longer because our exploration took longer and you also see

these episodes, they take longer now.

Because it's a bit just meandering around.

But we got to go.

Try to stay away from the cliff.

OK, good.

Very good.

All right, cool.

Um, good.

But yeah, so see that code now should make, I

mean, of course, it's quick.

I'm aware, but like this is really just the algorithm

that we looked at that we discussed in a lot

of detail and that's the way to implement it, not

that many lines of code.

We can try this out with these scripts.

World environments we can build our own grids, um, yeah,

so that's a learning for you.

Problem is that doesn't scale very well to, uh, to

big problems, um, as you will already imagine.

And yeah, I suggest you take a break, uh, and

then we can continue.

Let's meet again 20 past.

Just It's fun with the.

I Yeah.

I, I, I definitely.

Oh, yeah, because she's like I also she's mind, yeah,

I am.

It.

Mhm yeah, yeah, so like a big part of the

parents and I like um social media.

Destroy, there are some um like different policies.

I feel like, I feel like I haven't manage the

fact that I didn't have that much.

So like they say one thing and then and then

another, and there was like some for me like.

You can't have like uh account on Facebook until you

draw and you have the whole policy have to target

children at the age of 9 and 8 to engage

them like because they think is that um usually the

first platform that you engage will be we have to.

Yeah.

Can you help me work.

OK.

Uh, OK.

Mm yeah.

Yeah, yeah, well, that was it yesterday, right?

So if you say something you said, yeah, it's so

scary.

I, it's terrible.

I wasn't that yesterday?

huh?

I thought.

Yeah, I just put that in the responsible to like

do it well, well, interesting because like that was my

impression because they, I think that they said like Instagram

said ones that um.

They will release like a cushion of a certain age

and and we don't know the children will only interact

with the friends, so.

Like I like I, I, to be honest, because I

had that should be a different different option for until

like some like 16.

Unfortunately, and then, and very interesting like finding from his

research was that uh this is really the research shows

that it's much more impacts.

Or social media influence, and they do not see such

a big problem in my life but like the uh

like my age was awful.

Yeah, I don't know.

sad.

They're very close like being on the other side of

the other hand, and they couldn't like and I feel

like I I had to like I'm like.

I just yeah I I yeah, yeah.

OK with this, with this pills, it's like to keep

your attention and then like more so like, you know,

and all the and then it's like.

And you can see like this yeah, no, you see

this like like, you know, like a home.

I'm just because you know everything is like so, um,

I like a really I knows I about um in

the US like just like yeah, like they, they come

on, but then that is like the next recommended video

is like, like.

You like um.

Oh, I think a long ass one starts uh-huh, uh-huh

OK, you're like, yeah, yeah, yeah, but like that's like

I'm like you like you're like.

Well, well, if, if no, no, no, like, like we

usually use it together so well he can play some

video games or like he's addicted to food, you know,

so that's his main activity of life which is like.

Yeah, um, yeah, yeah, so, so certain stuff with us

on the on his tablet, but you know all of

that.

So if you wanna do YouTube and know to check

something then we you know and like and there's there.

That.

You know, it was like but yeah and on the

other hand, the good thing is that I think you

know it's the school 10 years ago it was more

than.

And and they like oh like online I think that

you know if you start education at the very early

stage, then you know it's not like OK they already

call like.

and then have some workshops how to be safe, but

you know preventing strategy I think it's smart.

I also like try to avoid situations that humans is

closed in his room and we don't know.

you know like yeah I don't think it was very

sport.

I just like and I like and stuff yeah yeah.

I like yeah, I know, and yeah, it's I think

it's like I like I know.

I was like I'm trying to show so much we

do a lot of the, you know, go out to

the show 3 streets away, but we haven't.

So only optimality are the same.

So we said before there are methods um from traditional

dynamic programming that come like get the the optimal, the

truly optimal um behaviour.

This is so simple, you know, like, uh.

Mm, every sort of method that's remotely reasonable will get

it.

Um, but you can actually show that with QLearning under

certain conditions of assuming certain convergence rates, um, uh, or

like, you know, certain schedules for the learning rates and

so on, um, you can show that it, uh, gets

optimal values.

And how does that work?

Imagine if I have an epsilon probability of visiting everything.

That's not too quickly going to zero, then, you know,

in the limits in infinity, I will visit everything.

So I will know everything and I'll just pick the

best decision.

I mean, that's probably what turning their proofs, but like

kind of that, that is like giving you the main

intuition of this.

Um, yeah, and, mm.

The problem with this is though, if you think realistically

large state spaces, and you are already saying this on

the action side, but imagine what you were saying, take

one single continuous state variable, has infinitely many values, right?

Just a single, not just like 1234, where am I

on the grid, but like really just a single variable

telling me my velocity or something like this.

Um, so now what I could do, imagine I have

10 variables or so, and I discretize them into 100

steps each, and now I'm running into this thing that

you will have heard in every course here like this

curse of dimensionality and so on, where, where I would

say, you know, imagine I have 100 grid points per

and I have 10 variables, then that is just completely

infeasible.

And how does that, this, this kind of square nature

come about?

Imagine I have 100 values to go through.

Like 100 values of my state.

So I discretize a continuous variable into 100 different, um,

buckets.

And then I have another one, and I need to

check out all these states.

So that's already 100, um, um, or like, you know,

uh, 100 times 100.

So 10,000, if it's 10, maybe a bit easier than

it would be 100.

And then I have another 10, so I have a

cube already, so I need to walk through a cube

and then hypercube and so on.

So it's just like, uh, a lot of states.

Um, and very quickly this becomes infeasible through this quatic

nature.

Um, OK, and the solution, maybe not so surprisingly, is

why don't we try to approximate that Q value with

the function approximate and just to get the intuition immediately.

Imagine I have, you know, I could take your network,

but also people have been doing this for long with

other with other things.

Imagine a linear regression.

Um, and the, the benefit of taking some function, um,

to approximate this is even if a state comes that

I've never quite seen, my function will form a prediction

of the value of that state because my function is

defined over the entire support that it has.

Imagine I have a state that's, you know, one variable,

and that variable goes from 1 to 1.

And now I, I begin to learn the function, and

maybe, you know, the value of that state is something

like this.

But I've never been close to -1.

But my function by fitting it on these values here

will form some guess over what -1 will be.

So if I hit that state, I will already have

a value estimate.

And if the surface of that value is, is remotely

predictable, that it would be better than, uh, better than

random.

So the whole logic of this is I have a

lot of states, they're all continuous.

I've never seen, probably I've never seen a state exactly

that one possibly before, but it's close to things as

I've seen before, and my estimation of the value is,

uh, is reasonable in that neighbourhood.

OK, and that's like a um a sketch kind of

of the core idea of this EQ and paper.

If you want to look at the details, that's the

paper.

And that is the original Atari paper, right?

And the idea is to approximate, approximate these, uh these

um two values.

Um, with the neural network that takes the states as

inputs.

Now I have a state.

What was the state for Atari, the image, the screen,

the frames, like the last frames.

I input this into the network.

Um, I get a, uh, a prediction.

I actually get a prediction for every action now as

well, say I say spacing data, say it's left right

or something like this.

Um, I have that prediction, and I, uh, I now

pick um greedy, uh, the one that that gives me

the highest, uh, the highest uh Q value, or I

randomly move that right ho, uh, with Epsilon.

That's the logic.

So the, the key question though is how do I

learn this network?

So how do I learn, first of all, what is

this network?

Well, that network is just a network like the ones

we've seen before, um, with a parameter values.

So now my Q value is not just a function

of SNA, but also it has, you know, if we

are careful, um, in terms of our notation, it has

these, uh, parameters in there.

And actually, I'm tuning the parameters to get sensible cu

estimates.

That's what I'm Doing essentially.

And also, I actually have two networks.

I have a target network that gives me, has slightly

different parameters for now I'll say.

Um, and it gives me the, um, the estimate of

the value of tomorrow and ever after.

Um, and that one is my main network.

So why am I saying, like, not really two networks,

because I only initialise the parameters of that network.

And then I just share them with the other one.

So I initialise parameters of one network, that's my main

network.

And then I have this other thing to decouple them

a bit, and that makes it more stable.

And this one is lagging that network.

That here is stable for a while.

It's in the beginning, I set the parameters equal, then

I learn and continuously update that network.

And then after a while, I set the parameters again.

of theta equal to the minus and then I begin

updating theta again.

I said it equal to the minus and so on.

So this other thing is some kind of leg version

of that.

And imagine one logic is if I do very quick

overconfident updates on this, then at least my baseline, my,

my, it doesn't move as well.

So it's like kind of, I give this a bit

of time before I update the other one.

OK, so the logic is the same thing as in

the tabular, um, what I like, not the same, but

very similar.

We have a target, and that is my current I

observe reward for wherever I am in the Atari game.

Um, my, um, estimate of the value of where I

currently am in that game, given my current optimal or

like my current policy that I'm currently learning.

Um, that is how much I feel that current frame

is worth where I am, like, what did I see

in terms of the score, did I hit the spaceship

and Space Invaders or something, and then, uh, what will

I do, um, from the next frame onwards, and then

this year minus what I estimated this thing to be

worth before.

And then, for example, a squared loss.

Try to bring them close to each other.

I, they have to be close to each other, right?

Because they should be the same thing.

If I learn them in a sensible way, they should

be the same thing.

This is, this is the Q value from today forever.

This is the queue value from tomorrow forever plus the

reward today.

They are the same thing, and if these functions make

sense, and my, my making sense of the world makes

sense, then that loss should be small.

So if I minimise that loss, like kind of, yeah,

that's um that's a sensible, um, uh.

Sensible loss to minimise.

Now that's a square loss.

They do other things like, um, for example, um, loss.

Do you, do you guys know um Huber losses?

Have you heard of this?

It's pretty simple, but it, it can make a difference.

Um, that's a mean square loss of Zubba loss.

It's basically from a certain value onwards, um, you know,

if it's bigger than this, uh, this, um.

Alpha or the gamma.

Yeah.

If it's bigger than the gamma, then it basically, um,

the delta, then it basically, um, becomes a linear function.

So it becomes, it's a mixture between an absolute loss,

an absolute value loss, um, and a square error loss.

So in the immediate neighbour of zero, it's square error

loss, but then in order to avoid these erratic changes

and erratic updates, we then for further, for changes that

are further away, we try to, um, not be as,

um, as strongly opinionated on them as we would be

with the square term.

Um, that's the logic of this.

Um, by the way, if you're in 474, what we're

doing tomorrow, we are looking at some tree regression models

where we can set loss functions and we could, for

example, the Uber losses and like kind of state of

the art 3 libraries like uh like GBM and so

on.

Anyhow, so that's that.

A couple of other tricks in there.

One trick is this double network.

I mean, it's actually just one network and passing on

the parameters to the other one.

yeah.

And OK, uh, I can show you a code.

That's obviously will be a bit more involved than the

previous one, but also I think.

I mean, we can make sense of a lot of

it.

Very simple, um, queue network.

No conss, nothing, just like fully connected, takes the stage.

Um, you know, these will now be, again, gym environments.

And, you know, I'll show you the ones later, like,

trying to get a car up a mountain or trying

to, you know, jump over like something or so.

Um, and, um, I have to be a bit mindful

to not be too complicated ones.

They are pretty cool ones I'm landing a spaceship, but

I'll need to train it all night or so, and

then I couldn't show you in, uh, in the lecture.

Um, if you have to use this thing is faster,

um, but, uh, but it would still take, um, uh,

a good amount of time.

Um, OK, cool.

So that's the function approximate I set up for my,

for my few values.

Now comes this thing, you know, on the slide, I

actually say here, connect experiences through interacting with the environment.

And that experience is collected in what is referred to

an experience we play buffer.

OK.

So that means like I have some list in the

wider sense where I saw all my environment interaction.

So, you know, the states I saw, the actions I

picked, the rewards I saw, the next state I saw,

and so on.

And I saw them in this thing.

And now my train function um has a bunch of

parameters.

This, for example, is the learning rate.

This is where my epsilon, um, for the epsilon really

starts.

So I'm just being like fully random, the policies I

pick, and then it has a decay.

It goes to 0.01.

I can set this, um, and it takes, uh, where

it says here.

Um, Oh I do this in the Epsilon decay steps.

So within 500, um, 500 episodes here, I'm going to

almost don't not explore anymore.

Um, that's the, that's so I'm going from 5% exploration

to, um, to 1%.

By the way, the policy gradient methods that we're talking,

um, about, they, they, they kind of have a more

native way to explore and this sort of hacky epsilon

reing thing here.

Um, OK, good.

So how much do I store on my buffer?

Da da da.

So, OK, I initialise an environment that again lets me

render it and depict it later.

What's my state space in terms of like how many

states do I have?

Here now, I can have continuous state variables, right?

No problem, because I can pass them to my function.

Again, I can pass it through a linear function.

We could set that up too.

Um, it's just that it wouldn't be as good in,

in, in approximating the value of certain states and then

if it's not as good as approximating the values of

our decisions if we pick the biggest um Q value

won't be as good.

Here's the Q net and the target net.

The target is the part of the network that I

use in this target value, R plus tomorrow's value.

But you see here, it's initialised as a network, but

then I immediately share, uh, with the same weights.

So I this other network, we immediately gets a copy

of the weights of, uh, of the original one.

I accept, um, you know, the usual A optimizer, um,

and create an empty buffer, which is just to store

these experiences.

And and then I create a schedule for the epsilons

to go from the maximum value to the minimum value

over the amountity process testified.

Um, OK, so now I go and experience this environment

and the episode starts.

Uh, I reset our good old reset function that always

kind of depicts these environments, um, and, uh, I have

an epsilon.

Now, I pick steps in terms of I interact with

that environment and again, if I, um, if I sample

from a uniform smaller than epsilon, I will pick a

random action, um, and that means from the safe space

of possible actions, random action.

Otherwise, um, I would just You know, just evaluate my

policy.

I will get the state as a tensor.

I will pass it through the network.

Network will give me two values.

I will take the maximum action value.

Simple as that.

Just now with the with the with this true value

network.

Um, this is for rendering that we can see these

kinds of animations.

Now comes the next stage, environment set function.

Next, um, stage reward, done, truncated is that episode truncated,

and this is info.

Um, and then I increased the reward.

I saw that now this is the part where I'm

just interacting with the environment, right?

I'm not learning anything here.

I'm just interacting with the environment, and I saw this,

um, in my, in my buffer.

So I pushed this, that's the method that they defined,

um, here, I push this into my buffer, so that's

now stored there.

So after interacting with the environment for a while.

Um, if my, uh, if I have, uh, if I

have enough, uh, collected enough of these, of these interactions

bigger than the batch size, I can also set in

this implementations.

I get the states out of this buffer, like, so

here now these are the buffer states.

These are my experiences.

I compute for all these data points, the Q value

estimates.

I compute um for for all these ones.

I also get all the action values, yeah, I also

get that.

And then what I do is I, uh, I compute

the, the next two values, um, and that is just

this target network or this other network which until in

the beginning had the same parameter values.

I also get, uh, the, um, like I get the

actions.

this.

And then with all of this, I can, um, I

can compute the target value.

That's the reward.

I just observed, the gamma, whether I'm done, exactly your

point before, if it's the end, then there's no value

in the future, right?

So then it's just the R.

That's also the same thing as in the tabular implementation,

um, because then 1 minus the boolean, uh, if the

boolean is 2 with 0 I think.

Um, now here I can define the Uber loss, you

know, I found this, I just changed this from like

I, I tried to Uber loss depends a bit on

the, on the, um, setup of the chale.

I take a gradient step and I'm minimising the distance

between these two.

Um, if I'm hitting a truncation step, I don't want

to have more interaction with the environment or it's over

the, the episode, I'm just stopping this.

Um, OK.

Um, That's it.

Any questions?

And this basically is the learning.

Um, I'm storing all the, the learned rewards.

Um, and then I am, uh, I have like something

to render how it looks like.

So, OK, let's, let's look at this acrobot.

It's like some simple robot that tries to perform some

kind of gymnastics stunt.

Um, I think this should not run terribly long.

Um, and in the meantime, we can discuss other stuff.

OK, um, good.

So let me run this, so DQM pie.

OK.

You OK Mhm.

OK.

1 2nd.

See if I can fix this in like one minute.

You That OK, I just need to install a torch.

OK.

Now let's run this.

So take a little bit of time.

Um, OK.

So in the remaining time, um, see, here we go,

like that's 50 out of 200.

Um, and that's the epsilon.

So now it's decaying.

So basically, I'm just playing mostly random, um, and now

taking these steps, I will play decreasingly random, here we

go.

So I'm, but I'm still hitting the lower mark of

the reward, basically, like that's the lowest I can go

on that problem.

Um, and let's see if we can kind of make

this thing learn reasonably in the amount of time.

Um.

I spoke with some of you about kind of applications

and then the class today, we look at a bunch

of social science applications or like social science research relevant

applications, I would say.

Um, it's a bit of a difficult balance because we

want to discuss all the methods in the lecture, but

then also we, uh, we don't want it to be

like only like a computer science course or something like

this.

So I think the, the right way to do that

balance is, um, is, uh, via the classes.

And then though, I want to again, show you some

kind of social scientific, uh, if you so wish in

the widest sense type research.

And one thing I, um, Uh, and co-authors worked on

this like um a paper trying to use reinforcement learning

for, um for kind of for real world application, it's

an econometrics paper, but really the, the problem of this

is fairly um broad.

Um, and we're trying to, uh, in one part of

it, basically try to build a simulator from real world,

OK?

And this problem is this from the paper.

It's basically, you know, if you want to check out

the paper, um, it's like, you know, you'll, you'll find

it on, on archive.

Um, but this is a presentation I gave about it.

So imagine you're like this policymaker, you sit in this

office and through your door comes a stream of people,

and the people, they become unemployed, um, and then they,

um, and they can request unemployment, um, kind of.

And like job training for you, you know.

And the best thing, of course, would be to give

job training to everyone, but you don't have enough funds

to give job training to everyone.

So you want to give funds to, you want to

give job training to people that you believe, you know,

benefit most from it or require.

Um, OK, so you observe everyone arriving in a sequence,

and if you reject someone, they might not come back.

Um, you can't accept everyone because it's costly to you,

not to them.

You, as a policymaker, give that job training, um, and

then, uh, and then, uh, the, the people arriving, they

don't pay for it, but the budget in your office

is not sufficient for everyone.

Sounds fairly, you know, random and and like the discussion

we had before, but it's again exactly that problem.

So can you see how this, how this naps into

an MDP from just what I told you?

So imagine you're that person, what's your say probably to

make that decision whether you give job training to someone

applying for it or not.

Uh-huh, yeah, time can be covariant.

So where are you in say fiscal year.

Uh, so we have this thing that you need to,

you need to use your budget by the end of

the year.

So depending on where you are in the year.

But like, what is the main factor of what's the

main thing driving your decision?

Whether to give someone that's exactly.

So and say to proxy this, we have a couple,

we have information about the people.

We have some covariants about them.

So imagine like what was their job before, their earnings,

education, whatever the data set gives that.

have, but you might use different things, not use other

things and so on.

All right.

So your state is a combination of who's standing in

your door and they, uh, and they are covariants, like

if like giving information about them.

And then the time in the year, if you want

to build like a more complex policy, and then also

critically your budget.

And why is the budget so important?

So what, what's like kind of if you think of

the NDP, what was this critical thing about the NDP?

To, to, why is this an MDP?

So what is the problem I just told you makes

this thing like an NDP and with this like a

reinforcement learning type of problem.

Once you decided to spend on someone, your money decreases.

Exactly.

And if your money is part of the state, your

budget, then you are changing the state.

You are changing, you are, I mean in the in

the centre.

And that's why it's like kind of mathematically abstractly.

It's it's super similar, um, and it's again an NDP

just like in a.

Completely unexpected setup at first.

And now the big question is, of course, how can

we build a simulator because we don't have an Atari

game about this.

How can we build anything sensible for this, right?

And, you know, just to give you a very quick

overview, we, um, basically need to do this in like

two steps.

So we build, like, here you go, you, you will

recognise this.

So we need to build this environment from the, from

the southern picture.

And we need to estimate, and that's, that's like some

kind of causal inference if you take the courses here.

Um, uh, we estimate for every individual.

We try to estimate or get an idea of the

individual level treatment effect.

So if they were to get the job training, how

much would they benefit from it?

And whatever your outcome is, we take like, you know,

the delta and their earnings or something like this, depending

on what the data can give us that we are,

then we are training on.

Um, how can we get this?

We have an RCT because otherwise people who accept the

job training might be people who are more into education,

etc.

So you have all the selection highs going on.

Here we are.

So it's like the day is over and it's trying

to get across the bar.

Let's see how it does.

Mhm.

Uh, here we go.

OK.

And that's the learning already averaged, but you see it's

like really volatile, but like you see a clear upward

trajectory.

I had to fiddle around quite a bit with like

kind of the hyperparametters.

I can tell you also from the paper, these are

very sensitive to hyperparametters, um, less so with like the

algorithm we're using is PPO it's, it's much more modern

than the DQN, um, but, uh, but that's a, it's

a general feature of working with reinforced learning algorithms.

OK, good.

So that's, that was this one here, um.

Good.

So we estimate how much everyone will benefit.

Um, and then we, um, we also need to estimate

kind of our, for our simulator, how they are they

arriving at our door.

When do people become unemployed, what are seasonality patterns, and

so on.

And we get that also from the data set.

And we can get that because the data set is

a randomised control trial data set, and we don't have,

where in, you know, the 1980s in America, um, people

were randomising job treatment to people.

Um, and we need this because otherwise we could not

exclude that if we just took how much people earned

at the job training, we would have all that selection

going on, that the people of what I said previously

that, you know, are, you know, more into education on

average, they would select that more and they would earn

more also because maybe, you know, they, they study more

on average and so on, of course, like.

Uh, all of these are on averages.

But you don't want selection into this, and the RCT

randomised control trial where it was randomised, it's very helpful

for this.

So then, based on the RCT, we can try to

estimate for everyone how much they would benefit.

And then also we can from the RCT, we know

when they actually became unemployed and requested this.

So we can try to estimate kind of kind of

seasonal arrival patterns with this.

Um, yeah, so how interested are you in the course

of probably.

Yeah, not so much.

So, uh, let's like, let's show you like, so this

is the, this is the seasonality of arrival patterns.

So we cluster people into groups and we see when

throughout the year they tend to become unemployed, these groups,

and all of that is really just trying to build

like a very simple simulation.

Of that real world setup, but all from data because

we don't have simulation, we don't have a Docker Mac

and we don't have an Atari game, so we need

to build this.

This is really, and then really the, the deeper point

here is we need to know what happens to that

world if our agent makes a decision.

Where in an Atari game, if I go left and

I shoot, I see the score.

I see what happens on the screen.

But if I take data and I give someone a

job training, I have no idea what happens to them.

So I need the whole causal infershipbang to get like

an idea of how much they benefit from this.

I need to estimate the causal impact of my actions

on the world.

And to get that from data is not trivial.

Uh, we could also do this with other causal inference,

um, uh, um, you know, tools, and we do it

like with, uh, with individual level achievement effects estimated from

an RCD.

By the way, I see that you're writing there will

absolutely not be any exam questions on this, right?

Like that, that's just like for, for, uh, for, you

know, general kind of research because you ask kind of

what what we could do in social science with these

methods.

Mm, yeah.

All right.

Good.

So, and then, like, let me show you like how

we actually build like once we have, so what do

we get?

We have for every person I benefit from treatment, treatment

is stop training.

We get that from the randomised control trial.

And then we know in which class they are and

how they would, you know, when they became unemployed and

so on.

So how we build the simulator and the previous version

of the paper, I know it's slightly different, but like,

so how we build the simulator is, first of all,

this tells us at what time we are in the

year and time is over at the end of the

fiscal year.

So this agent also has to learn to to spend

the money within the fiscal year.

Um, OK, so, where are we in the fiscal year?

And then we can say, um, from which of the

four clusters does someone arrive?

So we, we randomly sample that cluster and then we

take our data and from the subset of the data

of people in that cluster, maybe younger and, you know,

whatever kind of queries they have, we sample someone.

That person now is sampled from the data set.

We have this RCT data set and stands in the

door of that reinforcement learning agent.

Well that image that I showed you before.

The reinforcing learning agent observes the covariants, observes the current

budget, and has time and has a primary policy function

because we have policy um function like the policy variant

methods that we discussed next week here, and then samples

and action.

The action is either treatment or no treatment.

If, if treatment in terms of job training, we decrease

budget by a constant cost, you know, or variable costs

if we make it fancier, there are variations of this,

of course.

So, and then our budget is completed.

And we draw the next time increment, like you can

show that, you know, if these things have certain distributions,

we can draw the time intervals between people.

We draw the next time interval until the next person

comes.

We update time.

Here we go from the beginning.

At a given point in time, from which cluster does

the next person arrive?

sample the person from the data set, etc.

etc.

Um, good.

So what we've done, we've built an NDP here.

We've built the transition matrices, like all that is now,

and now we can actually wrap this again in like

some kind of, uh, reset and set function.

OK.

So I have reset.

I have the agent that makes decisions.

Um, I may take the next step, and that's all

in this.

And then I can basically train on this.

Um, How would that look like?

Yeah.

So, for example, like this, um, this is, that depends

a bit on kind of what rewards and so on.

But a level of 1 here would be just randomly

giving out treatment to people, 50/50.

I have no idea who's arriving.

I'll just like randomise this, and I can get like

multiples of 3 with more fancier functions, multiples up to

7 and so on, over, uh, random treatment by, by

knowing who's arriving and having, uh, how much they have.

There are lots of limits or limitations to this, of

course.

I'm estimating this on historical arrival.

they might not hold anymore.

There are huge issues in that domain, you know, if

you think of like what covariants do you train on,

what's the inherent bias in these covariants and so on.

So to implement something like this in, in practise, uh,

would be very, very challenging, not so much for the

technical reasons, but for, uh, for all, um, uh, for

the ethical challenges.

Also, if you think about this, if you think this

little further, um, these algorithms, they are.

They are incredibly scalable.

Um, if you have some say corrupted government individual who

does bad decisions in like some local authority, that's bad

enough.

But if this thing has a buck intentional or not,

um, we could scale this to so many people.

That's one thing.

So also, we could put the individual in front of

the court who puts this in front of the court,

the usual things, right?

So who even Undertas this like which policymaker that is

elected or which bureaucrat that is overseeing this um could

be reliable for this um by understanding the intricacies and

so on.

So really like the devils are are the devil is

in the details in, in this case much more than

the than the tech, and these are very interesting questions

to think about for social scientists.

Um, and very important ones too, OK, um, rewarded?

Yeah, so, so great, um, reward here is.

The treatment effect of how much the people benefited from

it.

So I assume the reinforcement and the agent sees basically

our causal estimate of how much they benefited.

So that's how through the causal estimates for everyone in

our data set, we have a causal estimate.

Um, and we, uh, we can see, like, basically how

much in our estimation they would have benefited.

And then we can tell that to, um, like a

score change in the game, and then the agent will

reinforce that behaviour.

And by the way, how would you like to simply,

like, um, have certain adjustments to this.

But to get a very simple estimate of this with

an RCT is, is very, very simple.

You have a treated group that was in the treatment

group.

They have received the job training.

You have a group that wasn't treated.

They have not received the job training.

Um, now, you estimate, say simplest thing, estimated linear regression

model on this, try to predict the wise one, estimated

the linear regression models, these are the covariants.

These are the previous education, um, earnings and so on.

Estimated regression models to break the Y 0 of the

control group, swap the models, you now have like why

heads for all of them, and then subtract them and

you have an estimate like a crude one for how

much they would have benefited.

And that is trying to get at this fundamental problem

of causal inference that I can't observe a parallel universe

where people do both, right?

And I'm just trying with a very crude approximation to

get something here.

And why also, I would have actually said the same

point that you just made, because here, one neat thing

is rewards arise naturally.

The rewards are just the treatment effects we get from

causes infants.

But one major problem.

The problem with reinforcement learning is you need the rewards.

And if you don't have rewards, you engineer them.

But if you engineer really intricate rewards, then where's even

the learning?

So what do I mean with this?

Imagine with the grid world, I have, I have this

really kind of like complicated grid with lots, lots of

kind of walls in there and so on, and the

treasures here.

Um, and say there are lots of walls and it's

a tricky one, like, you know, these maze type of

games and actually where the DQN paper, um, they could

solve the majority of Atari games, but on Sunday they

failed and they failed on.

Games that were hard exploration or like sparse rewards, meaning

I barely ever get a reward, because why?

I start randomly, I just bump into the walls all

the time.

I never go anywhere.

I never reach the end of the game or I

never hit any spaceship.

By the way, if you think of go and chess,

they are tough also because of this, because I just

win a game at some point.

It's not that I get a treatment effect at every

point or an Atari squad change at any point.

Um, so that's tough.

And then here there are sparse rewards.

But now imagine, I just craft and engineer rewards and

I put on this grid like a trace of rewards

around this thing here, um, up to the, up to

the treasure or something.

And then, oh wonder, my agent will learn, right?

So because it will pick up these rewards, update its

true values, and will just like at some point walk

there.

But I've sort of engineered the whole thing.

So one challenge for reinforcement is also you need setups

where you have naturally arising rewards.

If you just plaster your world with rewards, um, at

some point you will just, you know, uh, you will

just create the behaviour that you want to have, and

there's not much learning going on.

Um, yeah, great.

Yes, yes, you have, uh, how do we then extract

the number of individuals that are chosen and the characteristics

of them because that's what we're.

Aiming to know, right?

Yeah, so in the end, like, you know, so this

thing is like observes the covariants of the individual, um,

the, the time and the budget, um, and, and then

given this, like that's its state, that's it, that's the

screen from before or you know, the pendulum thing or

whatever the state of that thing, velocity position and so

on, and then it's um picking a uh an action.

Um, if it's and if it's action, I know what

the treatment effect is on the person.

I saw that.

And that's how I can create these learning curves here

because I know the average effects of how much people

benefited, uh, from treatment, um, and I can compare that

to a group of people I just randomly treated.

Um, yes, and then and then I can look at

the characteristics and so on, like who has higher treatment

effects and so on, so we also have these kinds

of.

Good, cool.

If you have any, I don't want to keep you

here longer, and I mean that that's, I just wanted

to give you like a social science example.

If you have any questions, let me know.

I'll stay a bit longer, um, and otherwise see you

in, in the class.

a I would call her and it is not.

Yeah.

Because Yeah Yeah, well, I probably should get her up

at some point.

I don't know I've been giving her like I've been

feeding her.

That's so.

What I think's a bit more hardcore but it's still

just like over the counter.

Yeah, really.

melatonin hardcore.

That's so nice.

right now because they adore him and I'm like it's

OK.

Sure that was me when I had like I just

just have a nice time.

So Oh yeah, yeah.

Yeah, yeah, so I, I actually, I put them on

for all the remaining weeks of the time, you should

see them now.

I think there was kind of, they weren't visible, so

I had to put them a bit on me.

I think so like they, uh, yeah, I think that

I have a new idea regarding like the, yeah.

Yes, yes, sure.

So I mean, yeah, sorry, I kind of, it's sometimes

difficult with this mix between it's with uh with the

kind of Zoom and not Zoom office hours but it

wasn't, you weren't kind of aiming at the funding deadline

or something.

Of Mm.

I Yeah, I took my fountain.

Well, it's changed everything.

I like this.

Yeah, I can't change these.

There's many things in life are changing.

Like coffee has no.

---

Lecture 9:

OK.

And Yeah Yeah It's OK.

Something will come up in the.

Yeah, yeah, that's true.

I I always tell them to send me.

And.

So.

quiet.

All right well, That Um, Really in your chair.

It was like it's a really good, really good.

Yeah, yeah, no, I will, I'll get that um.

It's she's that.

Um, yeah, so I'll go back.

I'll probably pick up some like the British s from

Sainsbury's on my way back and then walking to the

airport.

I want to squash squash.

They're like And they're like the most interesting texture they're

almost like soft erasers, but they're really.

I promise.

I've always you're not ro pillow.

Oh Get myself, I myself, but I really don't adore

when I'm like studying, but if I'm just like working,

I'm like I don't study very much.

I work and I work on problem so I work

on this is me like how higher.

I work.

But I should still do myself.

I totally I don't think.

Oh, like, I, I like yeah.

Yeah I like clean my room and all that but

it it really wasn't like I'm not like yeah it

was it was like she was like a week maybe

I would start to feel it.

So It.

Yeah, I'm I'm I feel like I'm just like I'm

gonna say I can't.

31 To me.

I like and.

Yeah.

Oh we get another one.

see what happens.

OK.

I know it's not.

You can see you.

I think it's it's a problem.

Yeah, I know that is so depressing.

So I, I, I haven't yet.

I don't think I I I think everyone's yeah, that

makes sense.

I have the questions that's because a lot of them

so it has like um.

I mean it's for like 10 so yeah.

I mean the in or created in other sections but

that all further potential for this surgery.

I think we do have something like that.

But my And it's it's so.

Well I'm also doing it.

Like it's part of it.

All right, I don't know.

online but it was like OK.

How many Well.

Uh, the table of contents and uh obligation details and

I would just like.

OK so it's download start you kind of like it's

kind of like an article that's what I don't know.

Because I have so much it'll be fine.

I have to change the room.

Oh, that's tricky.

I have to be here.

I I know what I'm sorry, I, I am mixing

up your yeah yeah but like uh that will take

some time so.

Can you, can you move.

I don't know how long we have to.

OK, let's try.

Learning like so now like I mean we we just

said there.

Is super yeah, sorry guys, the computer here we had

to move because of the NAB of outage and the

computer doesn't work here it seems.

Mm.

What I will try to do is I'll try to

get the lecture slides on this computer here and then

I'll try to call someone to help us setting this

up for the code.

It could be even on this.

Let's see And as you see Let's see what Then.

Yeah.

OK.

OK, call the tea.

I don't know.

I.

Hello, uh, this is Frederick speaking.

So I'm in room B 11 and, um, and I

can't connect my laptop to the.

I don't, you know, I don't find a functioning HTME

cable.

Um, if someone could maybe drop by, that'd be nice.

OK.

Thank you so much.

Cheers.

OK, so let's first try this here's that.

So.

Oh I like concentrate.

To.

Oh you're on that like you'd be super honest.

I would be the last day.

I'm I'm gonna watch some boys but your company doesn't.

It's a pretty twin skirts.

It's Saturday.

Is this something Well, I feel like 13 degrees.

It's just a lot probably even warmer now.

I have been but not since I was I was

I was um.

like the grass see I don't know if that's OK.

How do I start presentations on Windows shift on Amazon's

controlling Amazon.

So I tried to, I mean, I found some obscure

HI here I can get it on this.

I managed to, but I would need for like later.

I would need to connect my laptop and I don't

know how I connect it here.

What we can do for now so I can connect

it to this one, yeah.

And that's and then it's projecting it on this, yeah,

yeah, OK, let's try it out.

I got so funny yeah.

It's so funny.

I should work because it was yeah, it shows here,

so we show here yeah OK also in like a

normal size, yeah, yeah, OK, cool.

OK, OK, great.

One thing is you're done if you don't OK, thanks

so much.

Thank you very much, fixed.

OK, great, um.

There isn't, yeah, there isn't kind of a functional HTMI

here, but I can use the one from the uh

from the projector here.

um OK, amazing, great.

Yeah, sorry for the hassle, but we had to move

from from an AB, figure this out, um, because of

the power outage, but everything should work now.

Um, OK, so for today we're discussing the two, you

know, the second big branch of current RL methods, those

are policy gradient methods.

Um, last time we, uh, discussed value-based methods and we

discussed it with the example or like, you know, we

discussed an example of Q learning which is one, arguably

the canonical one.

And last time we also introduced MDPs and so on

because this is a framework how we how we think

about um um.

Problems and reinforcement learning.

Um, Good.

So today's policy gradient methods, that's going to be probably

the most technical lecture of the course, I guess.

Um, I was thinking what I should do here, and

you know I first disclaim you know these derivations of

of this particular lecture.

I won't ask you in the exam or to do

intricate derivations, but my, my thinking here is basically we'll

implement the thing in in uh in code and um.

We, we won't derive all the formulas, you know, you

don't need to for this, for this lecture you don't

need to derive the formula step by step, but I

will do so such that it's clear where they come

from in the code, because otherwise you will see if

we don't do this, it might be a bit overwhelming

while going through it, but otherwise the code is a

complete black box and there's no way to understand why

it learns how it's working also.

These are the algorithms that are used to align all

the LLMs at the moment, so if we didn't do

them or didn't even do the base, we do the

most basic version of them.

If we didn't even do this, then you know I

don't think it would fit the course very well, and

I don't think it would it would define the purpose

of your programme here.

I think we should study this and then also you

will see later when we look at code, all of

this will only make sense because we actually derived the

gradient and we have enough time, I think so.

You will also have time to really study the code

and ask me questions about the code, and then everything

should hopefully for the basic kind of policy gradient algorithm

should be pretty clear.

OK, so that's my motivation behind it, um, and let

me know at the end of whether whether I kind

of succeeded with this plan.

Alright, so last week we did NDPs and Q learning

as one canonical example of value base.

Why canonical?

If you think of the original Atari paper which really

drove the surge in deep reinforcement learning, deep reinforcement learning

because we approximate value of policy functions with deep networks.

That's that's why.

And this was really kicked off in 2023 in 2013

by this by this Deepin paper.

Um, they use QLearning, so that's usually what people are

deep learning.

Q learning itself is much older.

Um, that's why uh why it's, yeah, it's like one

of the when you talk about deep reinforcement learning, it's

usually what people bring up.

That's why it wasn't a lecture.

Now we, it's a basis of a an example of

a value-based method.

What does that mean?

We learn the value function, either the state action or

the state function, and then we derive policy or you

know.

Optimal actions indirectly based on the value function, you know,

so for every state we just went for the action

that that gave us the most reward, right?

So even if we approximate Q with a network, we

had a network that output, say if we had 4

actions, output or 2 actions that output at 2 numbers,

and then we just chose the one that was higher,

you know, the the a value.

You see how that is not that easy to imagine

with continuous values, right?

So that's already, if you think of continuous control, the

To example I gave you trying to control the plasma.

But even you know when I worked on kind of

decision problems and macroeconomics where I choose consumption, consumption is

not discrete, so it could be 2.78 or something in

a state, and then I already needed these continuous control

problems.

Um this is relatively easily to uh to implement here

and time permitting.

We'll also discuss this today.

Good.

So another big option is to not determine the best

actions indirectly, but to really learn the policy function directly,

to just learn this function pi that we talked about

last time.

That's the whole gist here and then we're going to

derive how to learn this.

OK, so, um, the variants that have um you know,

recently been um.

Be used in LLMs have been policy gradient methods and

examples are, for example, this instruct GPT paper here, so

very likely they also use something like this in shared

GPT in the current one, but these are not proprietary.

But also if you look at Deepeek, they use an

algorithm that is a modification also policy gradient method.

These are slightly more technical ones you can tell me

after the end of the lecture whether you're completely fed

up with it, um, otherwise we can also discuss this

in more detail in the seminar, but I think otherwise

it would be an overkill.

Um, alright, so why do we even care, right?

um.

They are not as as sample efficient as Que learning

arguably, um, and that means Que learning had this replay

buffer, right, so it was just collecting a lot of

experiences and then estimating the queue values based on that

experience, so it was creating more and more experience interacting

with the environment.

And then estimating Q values from it.

In the tabular version has this simple weighted average between

you know what it sees now and what it has

in what it has seen before.

And in learning, deep learning it stored it in a

buffer and then did regressions basically on this and ever

increased the size of its data points.

So with this, it's like what I mean here with

this more sample efficient, it needs to interact less with

the real world to to arguably to uh to get

enough samples to train.

Imagine now you you.

You know you have like like say our example with

the job training.

The reason why that I showed you last time, the

reason why we train this in RCT data is because

ethically it would be infeasible to just you know, run

that algorithm experimenting in that setting.

So uh that's why we run it on data.

If we however had a setup where we were really

need to learn on through real world interaction with a

real world environment, then we might want to have something

that takes us as little data points as necessary to

learn.

OK, so that's an advantage of this year.

Um, in comparison to this, the policy gradient methods, they

can in some setups increase the stability of learning.

So all of this is notoriously unstable.

I mean, there's been a lot of work and it's

gotten a lot more stable.

In the paper, for example, eventually we only had to

tune the learning rate of the policy function.

That was the only thing we actually changed from the

default implementation.

So that was kind of like if you compare that

to a couple of years back, that was like it's

a major change because we used this PPO.

algorithm, and, uh, however, it still was very sensitive to

the learning rate of the policy function.

Um, you would get entirely different policies depending on on

the learning rate and you would achieve much higher reward

with some and so on.

So reinforcement learning, there see dependency, so.

For example, for every final policy function in the paper,

I ran like 50 or 100 seats, so I start

with random initializations of the environment and of the parameters,

and then I just pick the best one.

You can also pick a distribution over seats and show

how much variability there is between random seats.

So these are all the things you will face when

you work with reinforcement learning.

Now it's therefore been a massive quest to increase the

stability of them, and the policy gradient methods in some

setups can be more stable, particularly the most advanced ones

like PPO and so on.

That they they clip gradient updates also in addition in

order to not overconfidently update policies and basically fall off

edges.

Alright, so um they explore in a more native way.

So before, how do we explore just to make sure

you're not asleep so yeah.

Yes, so we basically had this constant fraction of epsilon

and we we just with this two random actions like

a really kind of constant and sort of silly way

to explore the world, but it works and the set

up here we'll actually explore based on the state and

we'll explore more if we feel we know less about

the world and we explore less if we are more

confident that our action is already good.

That can lead to under exploration if we're overconfident that

our state is, you know, our policy is already good,

but it's more adaptive.

By the way, like, so how many of you take

a reinforcement learning course in parallel this year?

You do it.

Um, anyone else?

No?

OK, good.

Um, cool.

So, um, good.

So we have that smooth exploration.

Um, we can restrict policy function classes in cu learning,

you know, we had the value function, we picked the

best action, but here we can, what I mean with

this is also it's quite simple.

I can say the policy is logistic function, linear, or

the policy in the neural network policy is whatever, yeah,

I wanna say.

With this, if I force it to be a logistic

function, it can be more interpretable, for example, and in

the paper here, the point is made that if you

apply it to real world settings, then if you if

you can restrict it to a certain class, you might

be happy even though your predictability is lower or your

overall fit because you can interpret it in a better

way.

Que learning, you know, you would just pick the action

value at least in the base version that maximise the

function.

Good, so and it's relatively simple to incorporate continuous actions,

which is a huge space continuous control right so controlling

environments with continuous action spaces.

Alright, so what's the overall objective?

What are we trying to do here?

We're trying to maximise the so-called return, yeah, so sometimes

in software so you see the return as uppercase R

and the rewards as lower case r.

Um, I'll stick to the limitation of the return as

as as G and the rewards as capital R because

that's how we define them all along, right, to not

make it confusing.

Um, so the return is the discounted sum of rewards

in an episode, right, assuming kind of a final episode.

Good, so that's the objective.

So if that's the objective and we're on the policy,

how do we do this?

Well, it's all conditional on our policy.

What is our policy?

It's a function.

That gives me an action or probability distribution of actions

actually given the state and I choose parameter and actually

my policy function is defined by the function I set

up like the network or the logistics and then only

the parameter vector.

That's it.

That's what defines my policy function.

What do I mean if it gives me a probability?

If this is a neural network, how will it most

likely do it?

So I have 3 actions.

So what will be most likely the last layer of

that neural network.

Mhm.

Exactly, so it will output 3, you know, 3 values.

These will be 3 logits you pass them through a

soft max you have the probabilities.

That's what I mean with it output probabilities, um.

OK, so our goal in this entire lecture is to

pick a theatre that maximises this.

That's it.

And with the theatre we pick the policy, but you

see how this is getting tricky because by picking the

policy we pick the actions, etc.

right, so we pick the whole interaction with the environment

and.

Um, OK, questions about this.

This is kind of the goal, yeah, and in the

end, once we've picked the theta, we plug it into

our policy function and we have an optimal controller for

that environment.

So the first thing is I'm following this lecture by

Abi which has a neat trick of simplifying the notation

to make this more tractable.

So we denote denote these states here by tau, so

states and actions.

So these are entire trajectories or sequences.

So imagine I interact with that environment, just picking action,

observing a state observing the next state, picking action and

so on.

That's the trajectory I generate.

So through interaction with the environment, I can just, you

know, act randomly or whatever.

I'll be, I'll be able to create these trajectories, sometimes

they're called rollouts, OK?

So like I'll, I'll create these.

Now I can with this notation, if I denote this

by tau, I can just write my functions more simply.

I can write the return just as the return of

tau, you know, like it's just the STs and the

80s easy for the entire sequence.

And then I can write the expectation just with the

probability of seeing these trajectories if I if I pick

that parameter.

So I rewrite my expectation over the discounted rewards as

the same thing, you know, but just in a different

notation, um, discounted rewards, and then seeing all the different

trajectories depending on what policy I choose.

So these are the probabilities of seeing that trajectory.

Given a certain policy parameters Pitta, and then I sum

over all possible trajectories.

So this is really just rewriting the expectation, you know,

an expectation is probability times outcome of the rent or

like variable, you know, realisation value of the value of

the random variable times the other value of the random

random variable times the probability, etc.

If it's if it's continuous, it would be an integral

here it's just basically the probability associated with the with

the trajectory times the return of the trajectory plus then

the probability associated with the next trajectory times the return

of the trajectory standard expectation.

OK, good.

So now we have this slightly obscurely looking expression, but

it really is just uh the expectation of discount reward

so easy, and that makes complete sense, right?

And sometimes one thing I want to say here early

on is I've I've spoken with people, researchers, and they

were saying, you know, reinforcement learning is just you know

controlling these these environments and it's picking action and it's

maximising reward, OK, but it really is doing dynamic optimisation.

That's like a really important insight here.

This is not just, you know, maximising average reward or

like the reward of period 3 or something like this.

It's trying to pick actions such that the discounted sum

of all rewards is maximised.

So if every time in that environment in episode 2000,

you know, with some small discounting, a massive reward comes

along.

And I have limited budget and you know my paper

case I should wait until that 2000 period, right, because

otherwise I would not maximise this so this will you

know entail building expectations over the future and so on.

So my policy function will encode all that my optimal

behaviour will actually build expectations implicitly over that, in that

environment and so on.

That would be, you know, come handy when we think

about language models and so on and reasoning models.

So it's not just like a static controller, it's actually

a tool to find decent solutions for very complicated dynamic

optimisation problems, dynamic optimisation problems because I want to maximise

the sum of discount rewards over time.

OK, so restating like as an optimisation problem, I just

say, well, max, find the theta that maximises this and

just to be super sure I wrote it out again

that maximise this, that's just that's just different.

So OK, how do we get the theta, right?

So that's the, that's the price question here, um, probably

gradient ascent this time.

Why, why ascent?

Super simple question, yeah, because we're maximising so we need

to work out the function and then the function needs

to be concave right?

so uh like when we do this kind of stuff

we're relying on that the underlying function actually makes sense,

right?

So if we're um you know uh like a gradient.

Of course, so a gradient ascent in a in a

convex function would just not find a maximum, right?

It would just like keep going basically.

So if that function has it needs to have some

kind of points, ideally maximum cell points or something like

this where we can get your derivative at some point.

Um, OK, so how do we usually get like gradient

descent if we take it seriously?

We need to know the derivative, OK.

And here the thing is the derivative of the specific

policy function, I mean I won't recap all that Tom

did and take a derivative of the neural network and

so on, that would also just overcomplicate it, but conditional

even on the on the policy function we need to

take a derivative of that objective function, right?

So in in when we do supervised learning.

Right, our objective function, our loss function is mean square

error or cross entropy, you know, in the two canonical

examples of regression and classification.

Here our objective function is this expected return, so we

need to take the derivative of that expected return with

respect to the policy parameters, and that's what we're going

to derive here.

Any questions?

And then we can do grading descent with us.

Great, OK, so here we go, yeah, um.

Yeah.

But you'll see, you know, it, it looks definitely um

maybe it doesn't look complicated to you at all, um,

and then it's definitely easier than it looks otherwise.

So that's the objective function and to recap, that's just

our way to get rid of this expectation of para

and ride it out, OK, so that's simple.

So now we want to get that gradient.

So we want to take the derivative of this with

respect to every single theta.

That's the that's the plan here.

So that's just rewriting this.

So that's the derivative of the sum.

Now you will know that the derivative of a sum,

say, you know, 2 X2 plus.

Um, 3 log X or something like this with respect

to X, well, that's just the derivative of 2 X

squared with respect to X plus the derivative of log

X.

Right, so yeah, that's just like um a differentiation applied

to sums.

So first step is, so that's just this.

The next step is simply adding a scalar that's one.

So I'm not changing anything here.

I'm dividing the probability by the probability, you know I

can I can add anything here, but I'm doing this

because I get a nice expression afterwards.

So I'm just adding basically probability divided by um by

probability in that step.

OK?

Now.

I'm shifting one of them to divide or yeah to

the um the denominator here, OK?

So I'm moving this here and I'm dividing uh this

to this, and then I can rewrite the whole thing

as this here.

And when you for example look in the Saturn book,

these things like look a lot uglier if we you

know there's a lot more notation involved if we don't

take these uh these like you know this notational tricky

with the trajectory.

Um, good.

So can you tell me, that's I guess is one

of the crucial steps here, um, why, you know, what

is this here?

Why is this lock now all of a sudden?

Where does that come from?

So first of all, is everything clear until here or

do you have any question until here?

So I'm just like creating this.

I could do 42 divided by 42.

I can do anything, right?

So here I'll just divide P by P fine.

And then I just move one of them below this

and the other one stays in the front.

I've not changed anything here in the derivative either.

Good, so what, what is, where does that come from

here?

Why, why do they then do Nabla, you know, like,

you know, this like this operator here, like, you know,

the gradient operator of, of the lock now.

Try to, try like, I mean try to attempt to

see it.

um, what is the rule, what is normal?

I, I'll give you like successive hint, OK, so, but

don't, don't like check out on this basically.

So what's the um what is the uh derivative of

um of uh a log, OK?

Does that like remind you the the step before, does

it remind you of this?

So if you take what's the derivative of block X?

Exactly, so that reminds you a bit of this, right?

So the step before has this flavour.

So imagine I have more generally, a lock of a

function of X, OK?

What's, what's the derivative of log over function of X?

Let's try to do it, so.

Like, tell me, what is it, so I know derivative

of this is what the function.

Sorry, yeah, let's like, let's start with the outer one.

So so one over um.

One over the the yeah, not quite.

Like, yeah, so, um, went over the uh.

You're saying like I think I understand, you know, that's

what you're saying, right, exactly, uh-huh, yeah, sorry, I missed

this, yeah, OK, and then.

OK, cool, so this here.

We could write here, OK, so now we have derivative

of the function.

Divided by the function.

It is exactly this.

So this is just a short notation for this.

If we say the log derivative of a function.

It's simply the river over the function.

OK.

So this is just derivative of the function over divided

by the function, and we can just write this in

a succinct way as take the derivative of the lock

of that function, uh, and that would be this year,

and it would be exactly the same.

So, like it's just simplifying this, yeah, I mean it's

it's an equal sign here, um, and it's it's the

same thing, but it's a notation thing through the chain

rule, right?

So you kind of have to see, yeah, um, know

that this is, but imagine we do this now, we

take the derivative, so it's 1 over P divided by

log P, so it's exactly this.

So if we were to take that derivative, we will

arrive here, but it's such a simple derivative that we

just write it this way, um.

OK, other, other, um, any questions?

Is there something I'm divide the probability and then why

do we not?

Yeah, so why do you feel we do this here

because we want to get the lock notation here.

And to get the log rotation here we need to

bring this below it and that's why we need to

get something in the denominator of like being of a

tau and to get that we do this, we move

it in here, then we can write it as a

lock, but then we're left with this basically because we

will have we'll have a lock derivative here because like

you know it will make our notation simpler.

Because, OK, so I think the next step is also

answering the questions.

It's a great question.

So the question is why do we do all the

shebang?

one is to get the Locky and I so locks

are always good then you know, products are sums, etc.

these things that we use when we derive these functions,

but check this out now.

It looks like an expectation, right?

Because this year did not look like an expectation anymore.

This was the derivative of this thing times the returns.

This now actually has the probability function in there again,

the measure, OK, so this year is an expectation actually.

You see this, so I just rewrote the objective function,

you know, like the objective function.

Was this expectation, and I just rewrote it by weighting

the returns with the um with the probability of all

these trajectories.

Good, so that's just a way to write an expectation.

By adding this thing here, um, I actually have the

log here, but crucially, I can make this thing an

expectation again.

So this thing is really the expectation.

Over Over this part here.

OK, so it's the, it's the expectation um over uh

the, the log derivative of the gradient um times the

return.

That's what this is.

OK, so if this is an expectation, we can approximate

this, like, you know, how do we approximate, you know,

if I roll roll a die.

Uh, my expectation of the dye is 3.5.

How do I get this?

I roll it repeatedly, take averages.

Law of large numbers at some point, pretty close to

the average.

Good.

Um, or to, you know, my average is pretty close

to the expectation.

So the same thing applies here, but it's not so

easy to roll roll the die here, right?

So here that expectation is given by the probabilities of

seeing the trajectories induced by my policy function.

You know, I'm interacting with the environment, so whatever trajectories

I see depends on the theta.

That's what drives this, right?

The probability here depends on the.

So I can approximate that expectation.

By interacting with the world for trajectories with a given

policy and averaging over it.

And then you probably see the logic.

I would update that policy, interact with the with the

world again and get another approximation of the gradient.

So the gradient of the loss function, uh, it's not

of the loss function, sorry, of the objective function here

we want to maximise this.

The gradient of the objective function is the expectation of

the log gra gradient of uh of this thing here,

OK, um, of, of the, um, um, basically the expected

uh the the log probability times the um.

Sometimes the return.

Could we approximate that expectation, but just like the same

logic like with the dye.

I know I average over how many rolls of a

die I did here I average over empirical observations of

this, and these are individual trajectories now.

I write these trajectories with, you know, upper parenthesis.

OK, questions so far.

There aren't that many slides in the lecture, so I

think it's worth it to to to make clear these

slides are clear, and I know it's a lot of

notation, but, but everything will also build on each other,

so it's good to make sure things are clear at

every point.

What did we do so far?

Taking a step back.

We want to learn theA.EA is the parameter like the

parameter of the policy function.

We want to control that world in a dynamically optimal

way.

We define an objective function that's the discount.

of all rewards that's that's making us behave dynamically optimal.

If a future reward is really high, I'll wait for

that reward, otherwise I won't maximise that sums, you know,

intuitive.

So now how do we get that?

We define our objective function that's this expected discount sum

of rewards.

We pick a theta to maximise that objective function.

What do we need to do for this?

We need to get the gradient of the objective function.

Why do we want the gradient?

Because we want to learn theta and we want to

do gradient descent.

That's the big picture, OK, and this is our way

to getting this gradient.

We do this notational trick and we do this because

then we have an expectation here, the expected value of

this expression here.

And the cool thing is we can approximate it just

by experiencing or interacting with the world in this reinforcement

learning flavour of exploring and taking samples from the world

observing states and taking actions and observing new states, and

then we have a direct approximation and estimator of this

year.

So I haven't yet described how we get these, these

terms, right?

So we, we don't even like, you know, I told

you we're interacting with this environment, fair enough, but you

might ask great, but how do we get this obscure

piece in here and how do we, how can we

compute this because to get a gradient we need to

compute something, right?

So when we took a, you know, gradient.

We could evaluate it at a certain point.

How do we even evaluate this?

But we arrived at this gradient, and this gradient is

an approximation or it's an estimate of this expression here,

and this expression is an expectation because it's weighted by

probabilities over all possible trajectories.

Yeah, good point, because if a certain trajectory is more

likely, by interacting with the world, I'll observe it more

frequently.

You know, so imagine I don't have a fair dye,

but I have like one that gives me, you know,

the one more more frequently.

Um, I still divide by the number of of tries

with the dye, but I just see more ones, so

I'm implicitly doing this here.

Here I will see more parts of the trajectory, so

I'm still approximating that expectation.

Yeah, good, good question.

So any, any kind of um these things they are

always helpful for, for understanding this and everyone is thinking

about the same things anyway, so um.

Other points.

We're good here, OK, so let's continue.

OK, so first thing, let's try to get intuition of

this gradient, OK?

That's just me taking this from the last slide, you

know, to not like go back and forth.

So that's this thing.

First thing what we've discussed um plenty of times now

is we're doing gradient descent.

OK, so what this thing is trying to do is

it will.

Like, you know, so this will move into the direction

like so what what is the what is the gradient

of a function?

Oh.

It's like that gradient vector, it's the direction in which

you step to get the steepest ascent of that function.

That's what it means, OK?

So the gradient vector is a direction and it gives

you the right direction in your parameter space over all

the thetas in which you increase the function the most,

OK?

So if this is like the pro here I, I

actually the gradient of the um original function is some

function of the gradient of the probabilities, OK, um, of

seeing certain trajectories um um and.

Um, and this year, uh, you know, seeing these trajectories

is defined by the policy function, yeah, so we will,

we will eventually see that the policy function shows up

here.

Good, so what this means is.

If I increase the probability of seeing certain trajectory, you

know this is this is giving me like the steepest

ascent, so I try to try to get my my

parameters such that I maximise this function, and this is

the steepest ascent.

So if I multiply it with a positive number, I'm

increasing these probabilities.

So this is the steepest descent for increasing this function,

the log probability.

That's what this derivative is.

This is the steepest, like, you know, the way in

which I can increase that probability of seeing that trajectory

the most.

OK.

Um, if I multiply it with a positive number, I

keep that direction.

If I multiply it with a negative number, I go

into the opposite direction.

So if I, if I want to increase the probability

of an action, I'll need a positive reward, um, or

you know, a positive sum of discount rewards.

If I want to decrease the probability of an action,

I, uh, need a negative number here.

It's not, you know, quite as simple.

They could all be positive, and then I'll increase more

if some values are more positive and by this implicitly

by the soft max and decrease the other action because

they need to sum up to one, right?

But you will see like that's already hinting at how

we can improve this because this makes um if they

are all positive this is not great.

Then one is more positive than the other one.

There's no real reference point for a certain reward space,

you know, what's an average reward that we could achieve

and so on.

So we probably modify this.

Um, but the logic, uh, is, is this here, this

is the direction of steepest incentive of, of, of seeing

more of that trajectory.

That means picking actions that see more of that trajectory.

If I multiply it by a positive number, I go

in that direction.

I see more of that trajectory.

If I'm multiply it by a negative number, I see

less of that trajectory.

I decrease the probability of that.

That's the intuition here.

Oh, and by the way, we also scale this whole

thing by the magnitude of G, right?

So um if, if we have a huge uh reward

here, we'll scale it by, uh, we'll scale it by

the um by the magnitude, um.

Always here, so a huge reward will increase the probabilities

more than a smaller one.

OK, questions about this.

Is there a notation for the action in this?

Coming basically like next slides.

I was thinking maybe, maybe I should first introduce the

actions maybe then it's more obvious, but this is kind

of the basic formulation of the of the of the

gradient, but this here is a function of the actions,

right, because I see certain trajectories more if I pick

certain actions, right, so these trajectories they are endogenous, uh,

like my agent is choosing these trajectories, they are not

happening randomly.

So by increasing the probability of seeing certain trajectories, I'm

actually increasing the action, you know, or making the actions

more likely that make me see these trajectories.

So that's why it's directly a function of the action.

But so far it's just more abstractly the probability of

these trajectories, but they are driven by the actions.

Can the trajectory like the sequence of sequence of the

states and actions, the trajectory.

Um, is, is given by, um, you know, uh, state

zero, action 0, or, you know, if, if I choose

an action right here, um, and then whatever until the

final, the final, um, period, um, each step usually we

would call a period, the whole thing we would call

an episode like, you know, um, wording might vary a

little bit.

And what does this mean?

Um, this year, for example, is rollout or trajectory I,

uh, so I basically, that's my interaction with the world

I.

I do this end times.

Mm, that's what this means.

So the probability of seeing that trajectory to your point

is, of course, like, you know, the probability of seeing

these actions and the action determinate the futures, you know,

they determinate the future state by, by the way we

define these NDPs, right so.

To be like, you know, recapping from last week, um,

a fundamental feature of these reinforcement learning problems of these

Markov decision processes is that my actions change the future

state, the probability at least over the future state, right?

If they never change the probability of the future state,

then I can use simpler algorithms like bands and so

on.

Good.

Another key assumption is in the MDP is this Markov

assumption that all I need to know to determine the

probability of the next state is and to take the

action and so on is just the current state.

I don't need the state from 20 years ago.

I just need ST.

That's sufficient to to determining to determining my action.

OK, great, um, other questions.

So is the thing of updated?

With each like gradient, is it, it's not the, it's

the probability because you're increasing the probability of that.

So the thing that is going to do it is

just so we do this like we have theta that's

a point in space, and then we move into a

direction where we increase that point we move into a

direction where we increase the objective function the most.

So our step in the gradient descent is just the

derivative of the objective function times the scalar and because

it's a linear approximation of that by being a direction

and we want to not take too much of a

step, but we go in small steps.

Um, cool, and, um, and that turns out through our

derivation is implicitly a function of these probabilities.

These probabilities will be driven by actions, OK, but really

this expression, you know, makes a lot of sense but

could be technically anything and other problems um if we're

trying to maximise some other objective function.

But this expression comes from our particular objective function of

trying to maximise the discounted the expected discounted sum or

the discounted sum of expected future rewards.

OK, other questions.

Great.

Good.

So on to also your next questions recapping again, taking

from the previous slides, that's what we're trying to uh

to um to estimate um but how do we compute

these things?

How do we get this and how do we get

this right?

Because this seems to be, you know, a function of

our environment dynamics, our actions.

There's lots of stuff in there.

So how do we, if we don't do model-based RL

where we know the whole not know, but we're estimating

the entire environment dynamics, you know what is the probability

of the next state given the current state of action

so if if we're not, you know, estimating this, like

how do we even get this here and the algorithms

we're looking here are actually from from Mo 3RL.

OK, that will become clear here.

Good.

So, OK, my, my.

Taking a step back, we're trying to still get an

empirical, like we try to get an estimate of that

derivative to do great in descent.

That derivative is this here, the empirical estimate to compute

this thing, I need to know this and I need

to know this.

That's what I'm what I'm after here.

OK, great.

So let's look at the first part of this tricky

part, you know, the second part is this, OK, so

like, you know, this is like the second part is

extremely simple.

The first part is this.

Good.

The probability of seeing this trajectory is the multiplication of

all these um all these uh these functions.

It's picking an action, and that gives me a probability

times the probability given that action and that state to

land in the next state.

And then I'm just like, you know, factorise, so I'm

just multiply all these probabilities.

And I'll arrive at the cumulative prob like the the

total probability of arriving at that specific um at that

specific point.

Yeah, so the total probability of a trajectory I is

given by first of all, the trajectory is I I

is given by all the states and all the actions.

That's what it's like what you were saying.

That's what it's defined by.

So if I want to get the complete, you know,

the probability of that whole trajectory, what do I do?

Well, I know all the the actions in all the

states, so I, I get the policy.

I get the environment dynamics.

I might not know them, but theoretically we can write

it down.

And I'll just multiply all these probabilities, you know, like

I go through a tree, um, and I arrive, you

know, at different points in the tree, I can, uh,

you know.

I can multiply the probabilities in the tree to arrive

at a final point.

This is the logic here because these, these things are,

um.

Uh yeah, can can be multiplied with these conditional densities

here and the the more important point is that this

part is the dynamics of the environment.

That's the model, right?

If you think last time with this this picture from

the Sutton book, um, if you input an action into

the environment, it gives you the next state and the

next reward.

That depends on, or that is just like a nice

picture of this function which was part of the MDP

definition.

And that was part of the NDP definition because this

is kind of the key part of these environments of

these NDPs in terms of the dynamics that are generated

by the model.

I give the model an action.

I give the model a state, and observe the next

state to make this a bit more tangible.

So take the paper we discussed last time.

Our problem was, you know, people arrive at this policymaker.

Policymaker decides whether to give them job training or not,

and then the next person arrives.

So what is the P function there?

My state is a bunch of things about the person

I'm currently considering for for job training, my budget, and

the time left.

OK, so once I pick the action, and given that

state, the environment brings the next person basically to the

office, and it gives me abstractly, it's a probability distribution

of who could arrive next.

In our example, we the person from the data set

and then we you know we have that person arrive

at the policy function, but really like more abstractly this

is given by the data set who could arrive next

and that's the next step.

OK.

So in the computer game, the status of the current

screen my action, you know, in in like I don't

know space and data might be left right shoot or

something like this, and I picked this and then that's

the probability of seeing the next screen.

Why is it not like you know that probability, if

it's not really like you know if it's it's determined,

then the next screen might always be the same.

Then it's all probability mass on a single next state,

but more likely than not in Atari and space embedders,

I don't quite know which spaceship arrive next, etc.

There's a bunch of things that I conceptualise here as

probabilistic.

So given the current screen and my action, what is

the next screen?

It's not 100% clear.

It's probabilistic.

OK, good.

So that's the, the, the environmental model uh dynamics part,

and that is the policy function to your earlier question.

Where's this?

How is this in there?

That's how this is in there.

um, good.

So now we use what we did like first we

write this, we write this out.

Great thing is lock of a product.

So log of A times B times C is equal

to log of A plus log of B plus log

of C, right?

OK, nice.

That, that's, that's like these are the usual rules.

So we just do this um and the, the cool

thing here is we, we take the lock of the

entire of the entire sum and then we can divide

it like this, OK.

Because this is just another product.

Good.

So we use what we had here, a derivative of

a sum.

We can move the derivative, um, inside, um, each part

of the sum.

So what we had here, the derivative of 2 X2

plus, um, uh, 3 log, um, X would be, uh,

the derivative of the individual parts of the sum.

So that's just this.

And then what we realised here.

Is given a certain this is given like conditional conditional

on these things, they have already been realised, right?

So they are given now conditional on them being given

this is not a function of the policy parameter part,

right, so this is a function so what's the derivative.

Say, what is the um.

I don't know, it's a bit like, say, um, what's

the derivative of X2.

With respect to why it's 0, there's no Y in

there, OK?

There's no change of that function with respect to, uh,

with respect to a variable Y, it's just not in

that function.

OK, so here, um, the derivative is 0, so I'm

left with this derivative here.

So I can really rewrite the whole thing here, just

with a lock derivative of the policy function.

And now that's quite cool and convenient because we only

need to know the policy function.

We don't need to know the dynamics of the environment

here to achieve what to compute our gradient updates because

of all of this is still trying to get the

derivative of the objective function to do gradient descent.

Right, and to do gradient descent, we don't need to

approximate P here or nothing.

We could make learning faster if we also were learning

P, and there is a whole field of model-based RL

that is learning on the site, so they try to

build a model of the probabilities of next states and

so on, and that can increase learning speed.

But in 2 weeks we won't have time to do

that as well, and the big algorithms are like model

3 RL algorithms, these ones here, and model 3 means

I don't need to model this probability distribution of what

could be the next state.

I'm just experiencing the world.

I'm collecting experiences from the world like storing them and

averaging um you know, estimating the gradients and doing my

parameters.

Good.

So, um, we can rewrite this through like this little

piece of algebra, we can rewrite this as just um

the derivative over over all the um the future actions

given the states.

And this is model freeRL because we don't model this,

we don't model this here and it's not a function

of any parameters that we're trying to learn or so.

So here these are the policy function parameters.

There's not a function of policy function parameters, so um

so the derivative is 0.

So that's all we're left with.

And now luckily, the second part, that's what we're after

next, is a lot simpler.

So like no derivation needed here, really this is, yeah,

I mean, what is this?

We defined this before, um, the, uh, the return of

a trajectory is the discounted sum of rewards, right?

Any questions on that part?

Good, OK, so we can put this together now.

Um, and this is again taken from the previous slides,

our old derivatives that we got, you know, just to

make this as clear as possible, um, and now we're

just inserting for this this year.

We're inserting the log derivative of the policy function for

the P part and then this now we have to

slightly change notation because at every point in time we're

only looking at the um at the discounted rewards from

that point in time.

So if I take an action in period 20.

It wouldn't make sense to scale it by the rewards

starting in 0.

So I did this previously to make the notation a

bit simpler.

Here's like all of these things need to correspond to

each other.

So if I'm at the trajectory at A20 S20, I

want this thing to start at 20.

So I would actually discount period 20, not at all.

It would be R20 plus gamma times R21 plus gamma

square times R22, etc.

OK, so this is just what this means.

Start with T, start at at um at 20 or

T, OK.

This is the node here, and that's our gradient.

Um, and that's not something we can get with data,

right?

This year, the derivative this I leave out because the

derivative of the lock policy depends on the policy, right?

So we have example of example with the logistic policy

in the code, but like, you know, imagine this is

the neural network, then we would need the whole um

sheang from the previous weeks, right?

So we would need to do back propagation or like,

you know, to take the chain rule through that neural

network and so on.

But this is depending on the function approximator of that

that policy function.

So whatever I said, whether it's a logistic function or

neural network or whatever, but the derivative of our objective

function, the discounted sum of of future or expected rewards,

well, this is the final form and that depends on

whatever I approximate my on my policy function.

OK, great, um.

Questions about this one.

Maybe going back to the, it makes sense.

I mean, I understand mathematically, but it makes sense that

the uh the trajectory or the actions that go beyond

the first action are also dependent on the policy, uh,

but given that they're random, is that, is that their

role in them not being important, given that the probability

is also not deterministic.

Uh, you mean as in like uh how if the

if the actions, I'm not sure I understand.

So maybe your point is how if the actions are

probabilistic, are future states dependent on them in in like

in a clear sense because they could be anything because

they're probabilistic or what do you mean?

No, I, I mean like the, the future actions that

follow the first action are dependent on the policy.

They're dependent on the state, but the state is dependent

on the policy, yes, yeah, so policy function is a

function of the state, sorry, yeah, so I understand mathematically,

but here we remove them because they're not dependent on.

I think like when maybe you maybe you have the

same when I first saw this I was like hmm

why is this 0 because these things are functions of

the policy.

Is that what you're thinking?

Is that what you're thinking?

Yeah, OK, so I thought that I thought that as

well, but the key thing here is it's conditional on

them.

So these things already happened.

So if they have already happened, they are just they

are their numbers.

They are like 7 or something like or you know

action 2 or whatever, and now they are constant and

this derivative is not dependent on them.

So that's one way to think about it.

They are empirical like at that point they are given,

so we don't, we don't, you know, we we uh

we don't like we condition on them as given and

then they are not um variable anymore.

We can't change them by changing theta.

Is the action in the state always constant for the

policy chosen, so if I choose a policy today.

uh, and I get an action in a state, if

I choose it like in the next 2 hours, will

I get the same action state.

So the, the state is given by the environment, the

next state is thrown at us through that function.

Depending a current like action and depending a current state,

let's make this more concrete there now, depending on left

right shoot and Space Invaders, depending on the current screen,

what is the next screen that's thrown at us from

the environment, yeah, the actions we choose.

Your question tomorrow will I have a different action?

That depends if tomorrow if, if you put the same

state into the policy function tomorrow like the policy function

actually gives you a probability over actions.

It doesn't quite give you an action.

OK, so I actually put the screen into the policy

function for Atari and then I get a soft mix

over.

Left right shoot.

That's what I get from the policy.

Um, if I put that same screen into the policy

tomorrow, I'll get the same probability distribution unless I updated

my theta, right?

Because if I change my policy on the way, which

we are doing here all the time, then no.

um, but if I, if my theta stayed the same,

my parameter vector, and I put the same, um, image

into my policy function tomorrow, it will give me the

same left right shoe probabilities in space invaders say.

Um, right, the, the actual action might be different because

I'm sampling from that softmax.

I'm picking a random action given that soft max, but

the policy always outputs this vector of probabilities of what

could be next.

I might then choose one of them depending on the

probabilities, and that might be a bit different next time.

Yeah, OK, other questions.

So I think we're reaching a good point for a

break in like um I think in one slide, yes,

um, good.

So this year is us like this we don't know

how to compute this we know how to compute.

We take the derivative of the policy function, we observe

the rewards of our trajectories.

We can compute this thing here.

We can compute this thing here once we took the

derivative, we can compute the whole thing might be 10,

OK, whatever.

Now that's just the number of this thing here.

Great, um, with this then, or you know, depending, it's

actually a vector of numbers because we have a vector

of parameters theta, right?

So our our policy network is probably parameters by many

theta.

um, great, so we have this and since we have

this, we can now degrade in the centre.

So at that point we can create a learning algorithm.

It just won't be a great learning algorithm.

OK, so equipped with this we could train this policy

and what would we do?

Same thing as before, we would randomly initialise the parameters.

We could then we need data, right?

So the the the difference here to supervised learning is

in supervised learning we have all the data we have

images and what these images are or something like this

here we need to create data first.

So by initialising the policy we just run it through

the environment and we create a batch of data.

So like now we have states and actions.

We chose actions pretty nonsensical ones, but we observed states

and.

We just have this, right?

So now we have a batch of data here, um,

over, you know, maybe 3 trajectories each for one episode,

something like this, OK?

So I run through the world 3 times, um, until

the end of the game until I am on space

and data, some, uh, some spaceship shoots me or something

like this.

OK.

Do that 3 times.

I have a batch of data of states, next states,

actions, rewards.

Great.

I can, with this, I can compute all of this

here and then I can update my theta.

With my updated theta, I applied the policy to the

world again, create a bunch of data, update my theta.

That's it.

This is how this would work.

The problem here is that this, this part here, you

know what we said before, so if this is positive,

it updates it, it makes the action more likely.

If it's negative, it makes the action less likely.

But what if all rewards are positive, then the very

positive rewards make it more likely and that's pretty slow.

So it would be great to normalise this in a

way, maybe to centre this around 0, you know, that

actually I have that feature here of if this thing

is positive, regardless of whether I'm Rewards are positive enough,

but that if this thing is positive, I make the

action more likely.

If it's negative, I make the action less likely.

So that's what we're doing next with baselines and so

on.

Um, but this is our first learning algorithm and we're

almost there, you know, we need, uh, this, and we're

at the algorithm and that's the, that's basically the final

piece of the lecture, OK?

Um, so for now we have this simple algorithm, but

it's not very efficient.

Um, uh, it would be very variable and so on,

um, and that's, that's going to be the next slide.

So let's meet again, um, 20 past, OK.

Cool.

I've I I was like, did you already gradients in

your other course did last week.

OK, OK.

Was it clear you felt or sorry, was this clear

you felt or it was clearer than in my other

than just went through it in 5 minutes because that's

the thing, you can teach this very much like a

wall of Greek letters.

I mean, it's anyway it's challenging.

It's for me too, but like, but if it's like

at least like this year you can like go step

by step through it.

It needs some time, of course, yeah, actually it was

good that I came here.

OK, yeah.

I mean, we'll, well from this we need to, you

know, you did this probably the same thing.

We need to add baselines now and then take the

value function as the baseline.

I'll go through the steps.

I, I that's.

Yeah, what is this gonna look like kind of what

we need to know how to apply it?

Yeah, so, um, for example, when we talk next week

about how we build models and so on.

We are we even align them to answer questions.

We'll need this for it.

That's, that's the crazy thing.

Yeah, we'll know, but the mad thing is basically you

can think of a so the language model is a

probability, like a function that takes a state.

It gives you a probability of what could be the

next token of a word, right?

Yeah.

Right.

That's what the language model is.

And so far we in the in the lecture before

the fossil learning thing started, we had this thing of

being this dream of.

Function of the internet.

There's autocomplete function of the internet, right?

So we put like a chunk of the internet in

the computer.

You can think of this thing as a, as a

policy reinforcement learning.

That's the crazy thing.

So you can think of it as taking a state

and your action is the next word.

And now all of a sudden you generate language as

a dynamic optimisation problem.

And you're saying, well, where do I want to write

in my argument?

Let me try to get there.

Let me sample actions, my words, um, you know.

To, to get at the point of my argument.

So that's basically why to really more deeply understand how

language models work, we need to do this.

Um, I mean, it's all, uh, a function of, you

know, how deeply we want to understand this.

I could, of course, kind of do this, but it's

also a major part of what people think of deep

learning.

So I think it's like, um, yeah, I mean, uh,

and then applications, for example, my paper is one.

There aren't that many, um, because the thing is that

for applied reinforced learning in social science.

You need simulators of the real world, right?

So that's a challenging thing, and we sort of build

our simulator through the empirical data that we get from

the team.

There is some work, you know, I can start this

in the seminar on cooperation, there's lots of work like

trying to play multi-agent games with these things and so

on.

So lots of real learning agents learning at the same

time to cooperate and clean up some public space or

something like this, you know, you can study like social

dilemmas with this.

Tragedy of the colours and so on.

I feel like it, it's like, yeah, I'm sure it's

like all the hedge funds you know because like, you

know, my state could be some kind of state of

the stock market and I could pick an actually whether

to settle or buy, right?

So I don't know, whatever.

no, so yeah, each token is an action, but the

state is the concatenation of each previous token, and that

action, that's the new state.

That's the thing in language.

So this, it's really cool, yeah, so the state is

actually the prompt, that's the initial state.

The action is the next word.

Let's concatenate this.

That's the next state.

So just by knowing this, you, you see how you

can frame this and the same thing.

You just relabel your your function.

It's a policy function now.

And then you continue training it with reinforcement learning to

maximise outputs that you deem as as good or high

quality.

Yeah, so it's the original transformer.

So we take the original pre-trained transformer, pre-trained with next

steps, and we just make this thing the policy and

then we continue training its parameters.

So we required an incredible amount of computer to to

update that original function to predict the next words.

We still require a lot of computer to keep all

of that memory and so on to update it with

reinforcement, but luckily we need way fewer documents and so

on.

So really the the the biggest energy step is the

pre-training stage that we already did to basically condense all

the internet into into into the values of that function,

to the parameters of that.

But yeah, this is how it's all built on each

other, right?

So we're basically learning how deep like this is one

of the three branches of machine learning, so that's why

we do it here of deep learning in particular, um,

and then it's there, you know, like alpha 0, alpha

go they're all based on that kind of stuff.

And then it's now used for the language modelling.

So that's why we're using like what we're doing it.

But I mean, yeah, so if you find it like

that was, that's my rationale of putting it.

we can also do more weeks on.

you know, maybe other supervised learning, but I feel it

would be an incomplete course without reinforcing.

But I know it's, it's not easy, of course, because

it's kind of a switch in perspective because we're now

choosing actions rather than doing supervised learning.

But the cool thing about next week is how we

can sometimes rethink the problem and then just reframe it

in this way and then continue optimising and figuring out

a policy function that generates answers to prompts in a

more dynamically optimal.

Way and that's quite mad, but like it's it works

pretty well.

They might in 2 years they might do it differently,

but for now that's how a lot of these, uh,

a lot of these, uh, um, firms wants.

Yeah.

And Deepsek and so on, that's how they work.

That's why Deepsek is so, that's why 01, they don't

publish it, but probably the O series from, you know,

the clause 3.7, that's not a thinking model.

They're all based on talking because they're planning tokens like

we'll look into this like.

So you're gonna teach us how to use it, so

you're gonna teach us how to, so yes, so next

week will be we try to understand how to use

it for alignment.

We don't have enough to run it ourselves, but how

to use it for alignment and then we try to

understand how reasoning models come about with this, why this

is used for reasoning, and that takes us pretty close

to the frontier like kind of the the most intelligent

things that we've been creating so far.

Yeah, yeah.

every year it looks like are going down you're just

trying to oh no, but this is like with the.

and also like a lot of like, I mean not

a few people in the NBA but maybe.

Yeah, like the like the fight for this.

Oh God, well, it's, it's like.

at the same time I also have with me I'm

still not verify because I can't verify there's no way

you can, if you're going 45 because I need time

to go to the reception but um yeah.

Oh, she was on the street.

She was on that yeah I think yeah, yeah, yeah,

they'll just like Oh, the EU, not in Europe.

It's so good.

Oh my God, so the most.

Oh my God, she's she's so funny.

I didn't really think she should like consider.

I feel like all of the things that come out

with their fingers, you know what I mean like it's

every single one of them.

Uh, I was trying to understand why this one is

like some of this.

So I was thinking, so, uh, each, each end, like

each eye, it's like a, like a sequence of, you

know, the, uh, a round of games.

And then like this function times like, in this case,

you break down to every step of the game, right?

That's right.

And then I was thinking, why not, like, Each step

it times the reward of that day.

Because it's, it's, it's each step times of everything you

can get because the action in that step should be

optimal with respect to everything you can get in the

future.

So yeah, timing the future of rewards.

Cause I was thinking like in this, in the original

one, it's the, so this thing like, yeah, trying to

get the Like, maybe like the, uh, the probability of

the, you know, of the, the, this, this particular game,

given the action and the state you are taking.

And then for this probability of this particular, you know,

like, uh, game, and, and you times the sum of

all the rewards of this particular game.

OK.

Would that be just like with each move of the

probability times the reward of that step?

So like this should like this is, yeah, so we

try to get this here um and this is the

key part, this is the other part, um, and then

we need to combine the two, but then this ultimately

depends on this part because at every step here, the

start of the second one should be the the.

Point of policies take the action and intuitively the policy

should take the action at that point in time T

such that it's corresponding to maximising the future.

The morning routine has now merged with um the Atlantic

being like fine like.

I I it's bringing some.

He's like now I think the first one was a

he's he's like the eggs and the eggs I like.

Oh, I've only seen that one.

you know if you are the types of.

I want to try and the questions that people ask

and types of different types of questions that can get

you.

Well, I think they think they're that you like in

your like.

like I'm just because I thought that's.

I'm a finer multiple times for like and then fine

for like 100s like 3 times.

The thing I have my samples.

Yeah, that, that's, that's for another thing, but my exes

went well, but he kind of, I went in another

direction, but the thing that I'm doing like a macro

analysis, so I know my country and I have only

35 countries.

It's like.

Oh yeah, it's such a small.

I have to I have no idea what OK good,

so let's, let's continue, um.

Where are we at the moment?

We built this simplest way to to learn the policy

function direction, OK.

Why do we care?

Because most currently used algorithms, kind of state of the

art, or a lot of the state of the art

algorithms that are currently used in RL are policy gradient

methods.

So that's why we're studying them more concretely in this

course, given the current times, a huge like a large

fraction of what we're doing is thinking about language models

and reasoning models because they are at the forefront of

current AI turns out you will align and make reasoning

models at the moment among other possible methods, but like

the key also for reasoning models is reinforcement learning.

And why?

just to maybe give you a quick sneak peek that

we're not losing, like, you know, tracking all the all

the Greek letters here.

I think of like kind of what we did two

weeks ago was this, you know, function that could dream

the internet basically like, you know, kind of that's the

usual term from kind of this um.

Researchers use about this and then you say you have

this autocomplete function put like a part of the internet

in and we saw it could recite like in Wikipedia

article or whatever.

OK, cool.

So now, um, um, you can think just relabeling thing

after pre-training you can think of this function as a

policy.

And your state in the policy is the prompt that

you write into JGBT and the action of the policy

is the next token.

That's just relabeling.

So you take the model that we had to predict

the next token, we just say, well, let's think of

it as a policy.

So my state is my prompt and my action is

my next token.

What's my next state?

Well, it's my prompt plus the token.

That's my next state.

Now what's my action, the token?

Why is this a reinforcement learning problem?

Because my action changed the future state.

Right.

And then also that makes the whole generation of language

makes it a dynamic optimisation problem at some point.

So I'm not just greedily building the next token, but

I'm thinking of where should my argument go.

So where am I even trying to get get with

this?

So like this is how this is, you know, connected

to state of the art reasoning models and so on,

and this is also why we so we're starting this

for various reasons, you know, that would be like this

is the basis for things like alpha alpha zero some

of the biggest breakthroughs in AI.

So in the course on deep learning this.

Needs to be part.

Second, though, is it's as for now it's like a

crucial part of the current reasoning models.

So, um, and just as a sneak peek, I'll discuss

this in more detail next week, but that's why.

So for now we got the gradient of this objective

function after some annoying work.

I'm aware, but now we can update this.

It just turns out it's not a great update because

what's if you know now we're updating rewards more that

are more positive than other rewards like, you know, think

of games.

I win a game or 0, so I.

Um, it's kind of, yeah, it's not ideal.

So cool thing is very simple, we just introduced the

so-called baseline.

And you've probably already thought about this, to be honest,

so think of this year like why if we want

this to increase our probabilities if things are positive and

decrease our probabilities if things are negative, why don't we

just subtract the mean, right?

So if we subtract the mean and anything that's above

the mean, uh, you know, increases actions for these trajectories,

everything that's below the mean decreases actions.

So and that's basically the simplest baseline would be to

subtract the mean reward that we currently see.

That's like a super simple baseline, um, and, and that

makes learning a lot quicker because we know more what

to what to maximise for.

Yes, so here, yeah, it wouldn't have to be a

good points, so we could make it constant, we could

just make it 42 or something like this if we

know the mean reward of the environment on average, just

subtract that and make it state independent.

We can make it state dependent.

We cannot make it action dependent because if we make

it action dependent, then this is biassing the whole thing.

Otherwise I wanted to spare more math, but if we

take the expectation of this thing with this year, we

actually see that the expectation stays the same.

So I expect the gradient stays the same as long

as this thing doesn't depend on actions.

OK, so exactly like what you were saying, we could

make it depend on, um, we could make it depend

on.

Uh, we could make depends on states, um, uh, or

not on anything that's fine either.

It could just be a number, but it couldn't be

dependent on actions.

All right.

So what does that help intuitively imagine that's the mean

reward, then I'm updating rewards that are bigger than the

mean or smaller than the.

So that's like kind of the easiest thing.

Imagine I create.

A lot of rollouts in the environment with random policies.

I compute the mean.

The mean is 100 for like, you know, the discounted

sum of rewards, so I just subtract that every time.

The problem is like they're all for different time periods.

So at the end, like I only have 2 periods

left, 3 periods left, so it's not quite that simple.

So just subtracting a number for everything that's 100 is

probably not ideal.

So when I say baseline is a function of ST,

the kind of, I mean that's a bit of a

step, but if you think a bit more about this,

the op like sort of the obvious baseline is a

value function.

Because the value function is nothing else but your estimate

of what all returns are to come.

So why don't you subtract your current estimate and see

if what you see is better.

That's the logic.

So the baseline would be a value function very commonly,

um, and then that value function I update concurrently and

that's my estimate of, of, you know, the value of

all the rewards under the current policy that are yet

to come.

And then if I observe in my rollouts more than

what I thought, I update the actions to go more

into that direction.

If I observe less than what I thought would come,

I opted them to go less into that direction.

Um, so, OK, before I go there, one more rewriting,

people often call this thing advantage.

Because it just makes the whole notation simpler.

So now we're really down to that's our gradient.

That's it.

And when you look into papers, that's often how the

gradient looks like lock derivative of the of the policy

function.

So that would be one over policy function times, you

know, policy function if it's univariate say, um, and then

an advantage and the advantage is nothing else but the

discounted sum of rewards yet to come minus the baseline.

Positive advantage.

I increase the actions leading to positive advantages.

That's the nice thing about it.

Negative advantage, I decrease the probability of these actions.

Reinforce the learning and reinforcing the actions that increase more

rewards.

That's where the term comes from.

And basically the algorithm I'm going to show you um

uh was originally termed reinforce, so like the 90s.

Um, great, so that's just like me writing this as

like, you know, following a convention writing this as a

so-called advantage.

Um, Great.

So now what we already said is a kind of

natural baseline function we can take lots of them.

There are many of them.

They are optimal baselines, you know, that like you could

do.

speak about this for longer, but the one that's that's

most commonly used is just picking a value function.

Why is this a sensible baseline?

Because that's my current estimate of the returns yet to

come from state T.

If this is what I saw in my trajectory and

just experiencing things in the environment rolling, you know, and

in this roll of this trajectory, if this is bigger

than what I thought, increase the action.

If this is smaller than what I thought, decrease the

action.

That's the logic here, but now you might say great,

we already went through all the pain to get the

policy function.

How do we get the value function in addition, right?

I thought this was policy gradient, and that's right.

Here we care about the policy function.

We pick CETA to get the policy function to control

our environment, play our game, play our computer game, our

board game, whatever.

But we, we just need to estimate this now along

the way.

And they are broadly, I mean that's just like one

slide on it, um, there are broadly two ways in

which you can estimate this on the way we tra

we collect these trajectories.

That's just what we did so far.

We have our AS and S like by rolling like

by applying our policy to the environment and then we

have two options here.

Um, one is simply.

This is just linear regression.

This is trying to find theA such that the distance

between the two return and the predicted return, the square

distance is as small as possible.

So that would be at every step we we run,

and it's called Monte Carlo because it's based on the

experience and the current Monte Carlo is like I sample

things, um, you know, I might draw things from a

distribution, and here it means like these are the rewards

I I got from uh experience my environment and I

try to get the value function close to these experience

rewards.

That's it.

I square this.

If you look at linear regression.

No, um, linear regression has this loss function here.

Mm.

Say That's for this case here.

So I pick alpha and beta hat such that I

bring them close to the truth, and here I just

pick my my value function is parameterized now by by

say this um this file here, um, and I try

to uh try to bring it close to the true

discounted rewards because that's what the value function should be

what we discussed last lecture.

That the Q or the V functions should be the

discounted sum of rewards.

That's option one.

Option two is to learn it iteratively because when you

look at the lecture slides from last week again, um,

the value function from tomorrow plus the reward today should

be the same thing as the value function today.

Because the value function today is the discounted sum of

rewards, so I might as well take today's reward and

take the value function from tomorrow.

So these two things should be close to each other.

So the value function tomorrow, plus the reward today should

be sort of the same as as the as the

value function just from today.

So I can also bring these together, not the focus

of our lecture, but we need to do this step

to increase, uh, to improve the learning here, OK.

Good, and now we're, we're there.

Um, we can create this algorithm and then we implement

in scratch and Python.

That's the remainder of today.

And then I think the other parts like, you know,

I have one implementation that uses no py torture at

all, so we do everything for logistic policy, everything with

NumPy, so I suggest we do this today.

And then to not, you know, to not do too

much material, we do, we do the torch stuff in

the in the classes as part of the classes.

But again, I want to because it's it's a bunch

of maths want to take a step back and say

what we're doing here.

OK, we have even like you know, for some of

you that have seen more of this before, it might

be a bit boring, but, but I think it's important

to to to kind of know what what we're doing

here and why so.

We have selected a policy function class, and that is

our pie, right?

So class, I mean a neural network, a logistic function,

whatever we've selected to play an Atari game, to play

a board game, all of these things that are MDPs.

So we've selected that policy function.

That policy function is.

Parameters defined by CETA.

The neural network has a bunch of parameters theta.

OK, great.

So given we've chosen this, how do we choose the

theta to play that game well?

Um, that's what we're doing here, right?

So how do we choose the theta to be good

at, you know, a board game or at the computer

game or so on.

Well, we need to do gradient descent.

Um, to learn the theta or hear gradient ascent like

we've had, we have been used gradient descent because we

minimise loss function here, we want to maximise rewards.

So how do we how do we get good at

the game?

We uh maximise the discounted sum of expected rewards in

the game, in a computer game, these would be scores.

For example, in an Atari game, this would be how

much does my score change with certain actions.

OK, so, um.

We need to do this.

We for doing this, we need the gradient.

We derive the gradient that was like the slightly annoying

part.

It took us a bit, but now we know that

it's coming from.

So once we have the gradient, we need to, we

thought it's nice to subtract this baseline to make the

updates more natural, say if the if the advantage, you

know, the return minus the baseline is disadvantage term is

positive.

I want to make the actions more probable.

If that term is negative, I want to make the

actions less probable.

So that was the next day and then putting this

out and then to to a good advantage is the

value function, so we need to learn it too.

Um, and now we wrap this into an algorithm and

implement it and that's it.

OK.

After the algorithm I show you 232 slides on what

current methods are, um, uh, but this is the thing

we're focusing on.

This is the old reinforced algorithm, OK?

But even current ones that you know are used for

GPT deep see, and so on are just variants that

make that more stable.

And you can tell me if you're interested in them,

I can cover them in class as well, but this

is, this is really the fundamental piece, and then you'll

be able to understand the other things.

Um.

Great.

So here this is the algorithm.

We initialise the parameters of the policy function.

We initialise the parameters of the value function.

We pick a policy and the value function.

In the code I'm showing you, I simply pick a

logistic policy that outputs ones or zeros, and I pick

a value function that's linear.

That's just basically the Ws times the state, that's my

value.

No neural network, nothing.

OK, so just to make things super simple.

Great.

So um I um I initialise this.

Now I iterate over how many training loops I do,

right, the usual thing.

I collect a set of trajectories by executing the current

policy because I've, um, initialised my policy.

I can now apply it to the world.

I see the initial state.

I get the action in the next state, pick the

next action, da da da da da until the end

of the game.

I'll be pretty bad at the Atari game, but I'll

collect a bunch of states and a bunch of actions.

I do that a couple of times now.

I have a data set.

Great.

With that data set, I go through that trajectory and

at every point in that data set, I compute the

return that's yet to come at that point.

That I observed, that's a pretty bad return because these

returns come from random play, but like I have data

now, so I compute the discounted return at every point

and with this I can at every point compute the

advantage.

That's just this discounted return minus the value function.

Why can I compute this?

Because I've initialised the value function and I will get

some nonsensical value in the beginning, right?

So, but I'll I'll have Value now so I can

compute that too.

Nice.

With this, I need to um like like with this,

I need to update my policy and I want to

learn a better value function because it's pretty bad.

How do I learn a better value function?

I run linear regression on this here.

I try to minimise the distance between the returns I

saw and the values.

I just replaced this year with GT and I replace

this year with the value function at my estimate.

And then I'm running these I have I have now

through the rollouts I can compute and I can run

a linear regression with them.

That's exactly what you see in the I can do

this via gradient descent or I can just run linear

regression, whatever, you know, I want like OLS ordinary Esquares,

whatever I want to do to solve this.

This is just to get a better baseline, but the

main thing is I want to get a better policy

and to get a better policy.

I know that's the derivative, and you see, I could

have showed you but like we've derived this right, so

that's the derivative of the policy function log derivative of

the policy, the derivative of the objective function.

Lo derivative of the policy times the advantage.

What's the advantage this year.

I've computed this all.

I can update my policy now.

I continue.

Next one, I take that policy, apply it to the

world, collect more data, compute all the returns, compute all

the advantages, make my value function better to have a

better baseline.

Um, with that, I just update my policy function.

Now go again, experience like, you know, run through the

world, pick less silly actions, um, observe new data, etc.

So I know it's all, but there's like this doesn't

learn by magic that's, that's how this works.

It's, it's gradient descent um based on some function that

is, you know, um, um.

Um, the, uh, the discounted sum of expected, uh, rewards.

Yeah.

If we have, uh, for example, a new network doing

the.

Uh, policy estimation that could be you could have a

neural network here and a neural network, uh, do you?

Once you did everything in your eyes to the end.

Do you update the weights or do you restart you

retrain the whole model again?

Yeah, so, OK, I, I go through this here, um,

like, OK, so I, I have a current policy applied

to the environment, get data.

With the data I compute returns with the data I

compute advantages.

And now come my update steps to your point.

This year gives me a new fight for like our

site for like giving the um the the um giving

me a new parameter for the value function.

With this here, um, in linear regression I learn alpha

and beta in here I learned a new phi to

give me a better value function.

That's that step.

And here I can now compute this and I do

gradient descent on my overall objective function to update my

theta.

And the theta is like just the theta old um

plus, you know, uh, the theta is equal to the

theta plus alpha times that that gradient.

Yeah, so basically like I initialised this, I update the

parameters here, start from the beginning again, um, like now

with the new.

Policy function, get new data can get these things, update

my value function, update my policy function parameters, get new

data, etc.

you know.

So here the thing is like my policy is creating

the data, my policy is interaction with the environment.

It's not that I get a data set from someone

else and run supervised learning, but my so that's also

why I need to not mess up my policy because

otherwise all my generated data will be nonsense and I

won't learn.

Yeah, um, so that's where new algorithms kind of come

in and they try to stabilise these things, um, because

that, that's where I was thinking the beginning was the

bananas.

So it's basically the beginning it's just like this thing

is interacting randomly with the world, but you want this

in our old exploration versus exploitation trade off from last

lecture.

Um, because you want in the beginning to just do

wide exploration, you want to try everything and see what

happens and then reinforce what worked well.

That's how this, how this works.

But also to that point, if you have tough exploration

like hard exploration games, imagine I start here in the

grid world, what we had last week, and I have

this kind of thing.

say here, you know, I need to like really like

make it to a gigantic maze.

It will take a lot of random actions to make

it through here.

That will be really, really slow.

And if it's like some weird spiral or something like

this, I might never get there.

And actually in the original paper, the Atari games they

couldn't solve were these games where just with random play

you wouldn't get anywhere.

Um, back to the language models, that's why we initialise

them.

Uh, that's why we do some pre-training before we do

reinforcement learning, because otherwise this thing wouldn't generate language.

It would take a long time for it to generate

language and so on.

Um, yeah, OK, nice, um, great points.

Other questions.

This is putting it all together, basically.

So for example, here in the Atari game.

Uh, would it be wise to start with, uh, player

collected data, people playing games?

You could, like, so for example, on the AlphaGo paper,

they did this.

They first estimated their policy.

They trained the policy based on historical go games.

Then they had it initialised, and then they did that

kind of stuff with Tree surge and because the games

are so, you know, there are so many options in

these games, you can combine a bit more because The

problem in these games is they have sparse rewards.

After gazillions of steps, I either win or lose.

That's not fantastic.

I would like to have a value for every step

and so on.

That's not so easy to compute.

So they had to do a bunch of other tricks.

But yes, they initialised it with random play and then

they, in the original Alphago paper, and then they continued

training with this, and they, they were advancing, like they

were getting better because they wanted to.

Beat the best human that was currently there.

So if they were training on human data, they were

not going to beat the best human, right?

And then in the Alpha zero papers they actually just

trained from scratch.

So they had they had an algorithm that was enhanced

and that could even with random play from random play

against other versions of itself through these kinds of loops,

uh, learn how to play go without any kind of

pre-training.

But the language models like the pre-training is essential, like

yeah, so but I mean, DeepS seek is already going

into this direction, trying to get rid of some of

these um of these um supervised steps, but still the

pre-training of these language models for now remains crucial.

But like someone from computer science was telling me it's

a bit like the alpha zero all over again because

in in in.

In AlphaGo, this thing was like you were saying, initialised

with human play and then to reinforcement learning.

Now we initialise it with all the internet and then

we do reinforcement learning.

Maybe at some point we generate language from scratch, but

it's tough to imagine how this would even because the

space is so large, where do you even start with

random play?

Well, there's this thing of a monkey producing Shakespeare, but

that will take a long time with random random keyboard

action.

OK, good.

So, um, I, I, I wanted to add a couple

of slides because this is our last lecture on reinforcement

learning.

So what are extensions in practise if you were to

study this more?

Obviously this would take more time and so on.

You would need to, um, but maybe some of you

want to do research.

They were actually like former SDS students, they, they went

into AI PhDs and we actually now are working on

reinforcement learning, so it's not something unheard of in this

programme.

Um, current methods are um.

adding a bunch of other things.

They have generalised advantages.

They have a generalisation of the 80 term.

They normalise them, have zero mean standard deviation one making

things a batch of advantages to make things stable, these

kind of hacks.

They clip the parameters updates of the policy function.

So if I update my policy function a lot in

one epoch, I'm suspicious because the problem here is if

I mess up my policy function, my data collection will

be messed up, right?

So it will be all auto correlated.

So now I, I want to be very careful with

updating my policy, updating my policy because otherwise like I

run into problems and will never learn.

I can add these so-called entropy terms which make me

explore more because the problem of this is sometimes I

don't explore enough.

I exploit too quickly in these worlds and then I

never learn where the maze is, but I'm trying to

exploit some small payoff.

Imagine there was a huge thing here and a small

payoff here.

I might always go for the small payoff, um, etc.

Um, OK, great.

So two slides.

One is generalised advantage.

You know, if that's like a bit technical, don't worry

about this, uh, here, um, but you can compute advantages

in many different ways.

I can take the first reward, the value from tomorrow

minus my value.

I can take the first, the second reward, and the

value from the day after tomorrow.

I can do this like and then ultimately I just

take all the rewards minus the final one.

So what is optimal here?

And this is kind of balancing like how I estimate

this first part either only based on the current rollout

or based on on another estimate of the value function.

So there's this work on generalised advantage estimation that has

this form here with an additional discount factor where you

can show that this additional discount factor is weighing between

these two options, either doing this recursive formulation of only

today's reward plus value function from tomorrow minus or all

the rewards minus this, and you can show that training

is more stable.

So PPO, that's why I'm saying this.

PPO and these algorithms are used to align.

Language models, they all use generalised advantages um like and

they pick another hyper parameter lambda.

Last slide, if you want to look at current variants,

um, like one massively cited paper is, is trust region

policy optimisation TRPO it was continued into PPO.

That's for example what instruct GPT, um, the last publicly

discussed, uh, alignment of a GPT model was aligned with,

um.

Uh, the main thing about this, that's why trust region

is they apply heuristics to clip these over confident updates

of parameter values, and the key intuition that's more important

than kind of the math here is why?

Because our policy collects the data.

If we update our policy in an erratic way, it

would collect bad data and we will not learn.

So that's why we need to be careful to update

our policy and that's why there's a lot to be

gained and careful updates here, um, OK.

Um, good.

Other advanced algorithms, uh, where were these used, for example,

PPO here, a variant of PPO in, uh, in the

Deepse model.

Um, OK, great.

Um, the, the code is on, um, the code is

on Moodle, but we'll also, you know, have time in

the, um, in the classes to discuss it.

Um, you see, I can implement the whole thing and

not even 200 lines of code.

Uh, so, and I don't even use Pytorch here.

So I do everything myself, or I mean, you know,

um.

Uh The code does and um I define these functions.

I define the sigmoid function.

I define my policy function, it's just the sigmoid, right?

So my policy function can output um will output some

probability number between 0 and 1 and that's the probability

of action of my action one.

My value function is not a neural network because I

want to do everything from scratch here.

It's simply the state times uh its parameters times its

low.

It's a linear function.

Um, I'll discuss this um again in in the class,

OK?

So, um, but OK, this year runs an episode and

that means collect our trajectories.

That's what this function is for.

Um, I reset the environment.

Um, I define empty lists for states actions and rewards.

And now, um, I, I just appended um the initial

state.

I compute what I would, um, whether I would take

so with the sigmoid policy I can only take two

actions, 001, that's the policy 001.

With that, I um.

I, uh, with my pi I compute the probability of

action one and now I don't always take this if

it's bigger than 0.5, but what I'm actually do is

I sample, so I take action one if this here

just draws a random number between 0 and 1.

And if that random number is smaller than P, um,

I, I take, I take that action here, and if

it's if it's bigger, then I, I take action.

This is just a way to write basically with the

probability P, I take action 1.

If the probability is 0.7, I take action 1 in

70% of cases.

I take action 0 in 30% of cases.

OK, because I want to keep exploring.

I don't always want to go for the maximum action

that I'm currently want to take with my policy function.

I want to keep exploring.

I append this action because I'm creating, I'm getting my,

you know, my, my, um, if you want my towels

here, my rollouts, OK.

Great.

I get the new state, the new reward, whether the

episode is truncated, I don't use this more info.

I don't use this in a simple example.

I append the reward.

In the end, what I get here, um, is, uh,

is a rollout like the data from this episode.

I get all the states from the episodes, all the

action taken in the episode, all the rewards taken in

the episode.

That's what I get from this function.

I run with my current policy.

My theta is defined by like that defines my current

policy.

So for my current policy I get data.

That's exactly what we, what we discussed.

Um, now I need a function to compute returns, right?

So I needed these for these advantages.

What were the returns the discounted sum of rewards.

I find this terminology a bit, you know, confusing, but

that's what people use.

So returns is the discounted sum, rewards are the rewards

given out by the market.

And this you can, you know, go through this.

This is reversing the order, but it's really just in

the final period, the return is just the final reward.

In the second final period, the return will be the

reward plus discounted the last one.

In the period, so imagine we have 100 periods like

our, our um episode has 100 periods.

After 100 periods it's over.

In period 100, the, the return is just the reward

in period 100, might be 2.

And then in 99, the return will be whatever we

saw in reward in period 99 plus the discount of

2.

And then in '98, it will be whatever we saw

discount in 99 plus discounted twice what we saw in

100 and so.

So with this year we just for our rollouts we

we collected these rewards for these rewards we can just

compute returns.

And now for every point in time we have the

discounted sum of rewards still to come.

For the first point in time, all the rewards still

to come, for the 2nd point in time, all but

the first ones still to come, etc.

um, in the last period in time, we just have

the final reward, that's it.

So this is just computing the returns, um.

Great.

So this year is, um, is updating our, um, our

value function parameters.

I added two functions here.

This you might be more used to.

This is doing gradient descent on the value function to

minimise the square loss.

This is kind of what you did in the course.

This is actually what your data package does.

Um, if it solves your linear regression, it just, um,

it just does X to the minus 1 X Y.

like, uh, that's the solution to the optimisation problem.

You can do both, um.

This year doesn't need a second gradient descent loop to

learn the value function.

I just thought this is just a single step.

It's just giving me the the W.

What are the Ws?

These are the parameters of the value function.

Now, um.

Update my policy parameters.

I need to compute the probabilities.

I get the log gradients.

Um, I, um, uh, I have the advantages, right?

I add the advantages as input here as well.

Um, I average over them, and then the new is

the old plus the learning range times the average, the

average gradient.

And the average gradient of the objective function is what

the log gradient of the policy times the advantage, just

what we had on the slides.

This is a function to render this, and this is

the main training loop.

I create the environment, I get the state dimensions.

This has 4 state dimensions like what I'm showing you

has some things like angle and velocity and so on.

Um, I initialise theta and W as small random numbers,

um, and then I loop and I collect these trajectories.

Um, you know, say trajectories per it means how many

episodes of experience do I collect?

Maybe 5 or something.

So for per update I run through the environment 5

times and I con 5 times and I can get

the the all I concatenate all, um.

I get my values, I get my advantages, I update

um my value function, I update my um policy function,

I print out the return.

OK, um, that's it.

Running this.

It's very quick, even like, uh, you know, without any

Pytorch, because I feel otherwise we're getting maybe a bit

too reliant on Pytorch.

So like we'll do Pytorch in script two, but this

shows you like for the simple case where we can't

take the derivative of a logistic function easily, we can

write it in 200 lines of code.

OK, so here we go.

You see it's extremely fast and it's increasing rewards.

Yeah, so it varies a bit depending on how good

it is in each like kind of uh in each

uh training episode, but you see it's like it has

massive increases, and this is balancing a pole on the

card, yeah, so this is like this simple environment with

two actions, you go it's balancing it pretty well.

Uh, so these are the simple robotics environments, um, and

it learns how to, how to do this basically, um,

no pie torch.

Cool.

So in the class we do this in Pytorch, the

same code, but in Pytorch that you see again like

your usual code.

And then we through Pytorch, we can also have neural

networks of these functions, so it's much simpler for us.

And then I can also, because someone asked me last

time, I can show you continuous actions as well, um,

uh, how this works.

OK, cool, um.

55.

Any questions that I can still answer?

So what do we do?

We derived all this stuff, we build it in code,

and we learn like a simple environment.

OK.

Why do we care?

Because it's a major part of deep learning and it's

the foundation of reasoning models and others in terms of

life.

Great, awesome.

I have a question.

Uh, have you seen a lot of applications of reinforcement

learning in economics in economics, yeah, for solving policy functions

and macroeconomic models.

One, my paper maybe so we're trying to, uh, we're

trying to allocate treatment to people.

So imagine I have limited funds and I want to

give people like job training or like maybe drug and

medical.

Tris or something for us it's because we're economists job

training and then we're allocating job training, um, depending on

like whether someone might benefit from it and how much

money we still have in in some government funds.

OK, so I presented this slide like last time at

the end of the lecture.

Were you there last week or no, I was sick

this week.

Yeah, yeah.

So I did it at the end of that's like

whenever there's like a dynamic interaction, that's right.

Yes, OK, so think of any kind of dynamic consumption

problem.

I want to pick consumption tomorrow.

I have an income.

I have an income.

All these kind of problems in economics, whenever they are

dynamic, you might, you know, from micro on one plus

these are static, right?

So I pick apples and oranges such that my margin

rate of substitutions is equal to, you know, um, to,

um, uh, what is it.

Like the marginal rate of transformation is equal to the

rate of substitution, this kind of stuff.

Do you know this from the, yeah, OK, so that's

how I pick X1 and X2 optimally and like standard

like static problems.

But in dynamic problems, how do I pick X now

that it's dynamically optimal in a sequence of things because

tomorrow the economy.

You might be hit by a shock and so on.

So when you like maybe in your undergrad already, but

in masters and econs in macro, you do this dynamic

optimisation.

So how does, how does a person choose their consumption

such that it makes sense in the rent and like

a dynamic sequence of events?

All these things you can solve with this because all

these things they are MVPs.

Just an economist wouldn't call them MDPs, um, but, but

they, you can solve them and people are increasingly Ben

more here is like, for example, beginning to work on

this.

Very interesting.

Yeah, and Is You And you felt it sort of

like, I mean, I know it's a bit more of

math than it was, but you felt it made sense,

yeah, yeah, I mean for me it was very, very

helpful, OK.

Mhm.

changes because of yeah so if you have your own

computer, there's no HDMI here, but you can just plug

this out and put it in the computer like that

was the main thing I need to do this or

not?

No, no, I'll just use that one should be OK.

Yeah, also the movement and fix the stuff first.

I don't.

And that So CKK is still closed.

CKK I think so yeah that was that's my last

info kind of yeah.

So this is possible if you want to connect this

with this here and then um you need to go

on dockcam I mean otherwise you have.

---

Lecture 10:

Come on here.

M All right, last week.

stitched together and say.

Not that much time.

I Me too.

Nice.

What's the the the answer?

Um, not sure yet.

Yeah.

I spend more time.

I got an extension because my what was your weekend?

It's good.

Was it like your hibernation you guys needed?

Yeah, I don't know, I know.

I remember when this class was so crowded if you

didn't get in, you'd be like.

I kind of like this because I do feel like

it's the kind of thing where like I have a

lot of pushback on some of what I just had

breakfast at the jobs.

Why don't all of us have 10 job offers.

You know what I mean like there's a big disconnect

between like you're gonna go home.

I have 7 to 7 a week and then I

have like I said.

and then when it actually comes to hiring, they don't

know what they don't know what the people are going

to do.

I don't know how much you can HR can know

if you're like, what skills do you need?

Oh my God.

There's a big disconnect the growth that it actually happened

like oh my God.

I did, but two people truly kind of like you

just like 4 days.

Oh yeah, it's like a.

but like this can't be true because I feel like

another person is.

I go through any conversations that you talk about.

Yeah, I think that they are like having, but I

feel like all of them need a back.

There is a skill that is like I can stop

busting their passes.

I know last night I forgot that this class is

at 9 a.m. I was like oh it's 8 a.m.,

not 9 a.m. tomorrow.

Yeah, I, I was.

I felt like And here's.

So I think um I'm thinking.

But I, but I always go and then you guys

like not related.

OK, that is.

So for example, like how, I don't know, like what

the Trump did in the USA relates to what happens

in like Brazil.

Oh, that's interesting.

But to quantify this, yeah, yeah, I, I found already.

I already.

Oh, I think, oh my goodness.

I I I'm right here, so I would be able

to see you and I might say everyone just kept

switching rooms.

It's probably be chaotic.

Maybe if I can you can do a printout of

yourself.

That's really.

Yeah, that was such a great inflation is it's lower

than.

Oh yeah, that was like he was like, oh that

was like the last week of 10 hours.

Because that was, you know, yeah, I know about this

and I'm need to join.

I think that you know showing up.

I was like a couple of times, and I was

like, no way.

I don't know about that.

Oh, I know I feel like yeah, I know.

I I feel more comfortable.

I think it would be I was like I you

know I was thinking, you know, I was like so

much I had adrenaline and then I had like uh

I was like I'm not.

I was like.

I, I find that people are doing, I was like

I was like, yeah like I like I.

I have.

good people like this.

Yeah, this is really something.

Cool.

So, uh, welcome to the last week of this.

Um, I think today we're kind of wrapping it all

up with, uh, you know, to combine different topics we

get so far.

Um, before we do this, let me see.

There we go.

Um, so we, uh, are going to briefly revisit the

code from remor learning, um, and then you will see

how this all connects with the language models that we

then, uh, continuing.

Mm.

So I believe you know we arrived at this at

a plane on a policy gradient algorithm um and just

to recap we would parameterize our policy function.

Um, and our value function, we would then do these

iterations to learn more about um their parameter values, and

we would do this by having our current policy, just

trying it out on the environment, acquiring samples with the

samples, computing these returns which every time that are the

rewards to come, um, and then compute advantages, which is

this minus the current value function estimate at that point.

Then run an all less regression between these and the

value functions because that's what the value function should be

to update our value function, um, and then update our

um our uh policy function with this derivative here, which

was, uh, our kind of main task last lecture to

figure this year out that that's the update of the

policy function parameters, um, and then.

We close with uh with looking at this algorithm and

they uploaded three different samples.

Um one is basically doing this from scratch um ourselves

with really defining even the sigmoid function, the policy function,

which is just a logistic one, and here just a

linear, um, value function.

Why is this linear value function certainly the dot part

between the states and the values, uh, so the, the,

the weights, the, you know, the parameters, just the linear

value function.

And this is um as closely as possible, um, resembling

all the stuff from this algorithm box, um, you know,

um this is basically running on these trajectories, trying on

the policy, the theta is giving you the current policy,

that's what it's what it's defining.

There's something from it and then taking actions to that

environment and so on.

Um, here we can put the returns.

It's just this sum.

I tried this to resemble like the box as closely

as possible, um, update the value function, we can do

this, you know, you might have seen this from linear

regression.

You can also do this with anotherigerian descent loop, um,

yeah, and then what was left basically, um, was, uh,

was to look at that with.

Uh, with torch and then give you kind of that's

a bit beyond the scope here but a hint of

how continuous actions would be incorporating these algorithms because some

of you asked this.

OK, um, cool.

So this is just like let me complete this is

the training loop you initialise the environment, you know the

dimensions of the data, you initialise the theta, um, and

your value function parameters.

Um, and then you just loop and gather experiences and

then update your, um, your value function parameters, um, and

compute advantages and update your, um, update your details basically.

Um, yeah, so you might have, uh, you might have

revised this after the lecture, but let me, let me

show you, show you this here again.

Yeah, so.

OK, so now I'm running this.

See.

You could download this on the core side I think.

Mm.

OK.

Yes.

Um, it should work.

Mm.

So here we, you know, we run this for I

think 2000 such steps, um.

It's 2000 times collecting experience and then updating, um, and

why is this, uh, why are these things maybe as

a quick review, um, this just, you know, balancing this

pole to left and right movement here, um.

Why is this, um, uh, why is this so brittle,

um, reinforcement learning often because fundamentally there is no data

that we can just change the supervised functional.

We don't have X and Y just from an external

data, but our policies acquiring the data itself.

Uh, so once our policy gets a bad update and

does some, you know, bad actions, then, then this will

not work anymore because we can't really learn anything because

we're in the wrong part of the action or of

the behavioural space here and we're just collecting that sample.

Um, that's, that's, and then fundamentally, like, you know, this

year it gives you particularly policy gradient discussion.

I know it was a bit technical, but it gives

you the basis for understanding all the algorithms that are

used with elements at the moment.

Um, and in particularly these ones really try to, to,

um, you know, truncate policy updates that seem overconfident.

So there's a lot of kind of, you know, clipping

of policy updates and so on, and all of that

is for the policy not to collect data samples and

then berate design basically it's always the same kind of

logic and to conclude this year, I basically.

Went to, um, to, to try to replicate broadly this

simple thing here where we, you know, took the rid

of ourselves because it was logistics, etc.

to implementing this torch again, right?

And this gives us some flexibility.

So now I can have a um a multilayer perception

for the, for the policy one.

I can have a multi-layer perceptron for the value function,

you know, I don't like now, I don't have to

take these words annually, um.

So, and then basically uh I kept the function here

with compute returns, etc.

um, again, I can render the final policy and in

the end now, you know, I'm updating my parameters with

uh with Adam, um, as, as you've seen the work

before, that's the same as before, but now I'm, I'm

updating them through through Torch basically.

Here is the value function that's the same thing as

before, that's just the means for error loss on the

value function, um.

And uh and then that's the policy loss, that's exactly

what we derived, the advantages times um the lock props,

that's the um uh that's the derivative here.

OK, so let me show you this first, um.

So that's again the environment we're solving and just to

give you an idea, so here we go.

These are these things and we have an action space

that's discrete and we have um uh we have this

observation space here, um, which has these possible values.

OK, so we have kind of 4 continuous values and

we can push the car to the left and to

the right.

That's what this, what this environment is.

And with this neural network.

We can solve the same environment from before.

We just need a lot less um iterations, um, and

essentially these, uh these um.

Uh, this epochs and very, uh, they take a long

time at some point because uh your network is just

perfectly mastering this.

See, after 58 missions already very high reward, and then

I think I saw this after 100 or something like

this here we go, and we can balance this again.

And that's not done with the neural network with, uh,

with one hidden layer, 16 neurons, one layer, 16 neurons,

you know, I could also make this more flexible, I

believe, for example, the open AI kind of, um, basic,

uh, implementation, say 64, 64, but yeah, for this environment,

you don't really need this.

I mean, we could after all solve it with like

something much simpler.

OK.

And then just as a hint, so this is a,

a big field people would refer to it as continuous

control, like how can I have a policy function.

That is not doing left right or something like this,

um, but that is actually choosing 3.7, right?

So that's, that's, um, and if you think of the

talker example, the nuclear fusion example, um, that's how they

did it.

They controlled the Terex through continuous actions, so they needed

an RL logarithm that would deal with this.

And you don't have to worry about the intricacies of

this wholeium just to give you um intuition how this

works.

All that really needs to change is that the uh

the policy function network does not output, um, you know,

a softmax over actions, but it's just outputting a mean.

Um, and a, um, and a log standard deviation.

OK, this is not even in the policy networks, it's

external.

It's just a parameter in the overall like, you know,

this is not affected by the forward pass.

It's just, uh, you can think of it like an

intercept in the network, um, and this is the mean

which comes out of.

You know, you pass it through all the layers, that's

just a multilayer of perceptron here with redo activations.

You combine all the layers in the very end, what

your output is not logits and then soft max to

choose your actions, but it's really just a number.

And this thing we think of as mean.

And then what we do, what do we do here,

um.

We, uh, we take this parameter here as the standard

deviation and our former path to become exponentiating this, um,

and then, uh, we return mean and standard deviation.

OK, so far so good.

Why do we, I thought this is kind of interesting.

Why do you in these implementations, why do you see

a log standard deviation?

People deter and then they exponentiate to get the standard

deviation again like, why, why lock?

Yeah, for the backward pass because the bus instead of.

Yeah, so you can, these kinds of arguments or sometimes

with log likelihoods, if you multiply all small numbers, you

rather want to add them up to because of limited

precision.

But here really, imagine I want to learn a number,

that's just a learnable parameter.

The lock can at least be positive and negative, whereas

the standard deviation cannot.

So it's much harder to learn your.

Only the positive number.

And then we learn this and we exponentiate it, and

then we always have a positive number at the end.

So what is this already hinting at?

It's hinting at that we're learning the mean and standard

deviation of the normal distribution or whatever distribution.

That's what we're actually learning.

And then our action, when interacting with the environment is

to sample from that normal distribution.

And that's how we can do continuous actions.

So imagine, um, Imagine, you know, I have something more

complicated, say these guys with the poker or say like,

you know, I have 3 actions or something, then my

policy uh network will output um something like this year,

you know, um 3.7.

Something like this, right, that will be coming out of

my policy network this year.

I will just take as the as the um, I

will think of it as the mean of a normal

of a multivariate normal, and I'm sampling from this.

Likely I'll sample the action 3.7, 3.5, 3.9 or something

here, and I'll sample the action around 0 here.

I'll sample an action around 1 here.

During evaluation, I basically forced the standard deviation to be

zero during evaluation, and then these are my best guesses

to play.

But like during training, because, you know, when we had

these algorithms before we, we were sampling from soft maxes

or samplings from the logistics to make actions.

We had a state what action to take sample from

like whatever the policy function probability just output it.

So here this is tougher to say, but.

The policy function just outputs these values, we wrap them

into a normal, and we sample from the normal.

And then we learn mean and standard deviation um during

evaluation, we shut off standard deviation and then for a

given state, that's the best guess for our continuous values

to play.

Yeah.

And we need to trick with the normal because also

previously our policy network would, you know, in our logistic

case in the first file, the policy network would simply

output a number between 0 and 1, so it might

output this, right?

So when we were training, like our action.

Um, came from like And a certain step was sampled

from that probability, so it was one with with probability

0.7 and 0 with probability 0.3, and that's the equivalent

for continuous control.

Um, OK, and if you, yeah, for example, I don't

know, I try to solve models and macroeconomics with this,

you know, imagine you have consumption and saving decisions, um,

then, then you need these things because there's no discrete,

you can discretize them, but then we have gazillions of

decisions, um, so rather take them once, um, OK, you

need this in robotics, you also need this in, you

know, any kind of social science application with, uh, you

know, some multi-agent systems or as soon as actions are

contained.

Um, there are social science applications, the paper that I'm

working on that I showed you, or other things, there's

work on cooperation and multi-agent systems, so you have a

bunch of these things learning at the same time and

to document and what reward functions that you define, they

start to cooperate.

more and so on, but like really one major field

how to use this is just emerging in large language

models and that's why I mean, I know some of

you might have thought this, yeah, like why do we

do this kind of technical and so on, but I

hope I can convince you by the end of today's

lecture.

OK, um, great.

Any questions about this?

Yeah, I was just thinking like if you come back

to the terminal and then uh oh, by the way,

I wanted to show you the continuous one.

let me show you this as well.

So the continuous code, like I mean that's just what

I discussed here, but um here the environment we we

we look at like a simple environment, OK, to not

make a mistake because otherwise this can take hours on

the CPU.

Um, this is a car that I control my action

space is now -1 and 1.

maybe that was your question, like one thing here is

like how do you normalise these action spaces because the

normal, say it's centred around 0, it gives you, it

gives you numbers around 0, right?

But we can transform anything around that in that inter

into that interval.

Imagine my macro problem with savings and consumption.

I just said I normalised it between -1 and 1

and then I mapped it.

Back into the normal values so you can do these

things.

Um, and here we have the same the action space

is like not to take, um, to move in certain

directions is between -1 and 1.

The observation space um are a bunch of numbers.

Yeah.

So what's your er yeah, to the like uh the

iterations.

I'm, I'm thinking that why the Um, reward, the average,

the average is like a reward, you know.

The, so the return, so that what I mean it's

slightly confusing the terminology in this field.

There is the reward that's just in every interaction with

the environment.

I might observe a reward.

That might be zero sum of rewards so the return

is basically every reward that I'm expecting from a certain

point in time in the future discounted, and that's just

the definition of the value function.

That's in a given state.

What am I to expect to come under optimal behaviour,

and these numbers they are so different between different iterations

and because like intuitively it should be, they should be

more you feel why are so different because this thing

is exploring, uh, it's just like, you know, it's not

always picking the optimal action.

So for example, think of this example say, given the

current state.

70%, 30% B actually, yeah, so I'm not always taking

the mode of the distribution.

I'm not always saying action A, but I'm, I'm sampling

from this, so I'm continuing to explore the environment because

I want to learn more about the environment.

So this is like all endogenous, right, because my policy

function, once I updated the policy function, the probabilities for

the left, right, or whatever actions they change.

OK.

So, OK, two reasons.

One is agent exploration for this to vary, um, then

like in instability in learning, maybe at some of these

epochs like the agent is better than in others and

it's moving around a little bit.

And then also the environment, for example.

In our paper, just the rewards are extremely variable per

episode, and that's completely beyond the control of the agent.

So it might be that in one episode you you

have a lot of kind of lucky draws and observe

very high environments in your Atari game and sometimes you

really don't.

And even though you're behaving completely in the same way

here, that could also be, OK, so it depends on

the reward variability in the environment.

So here there's no like clear uh.

Uh, like tendency that the more it work there is,

the difference is lower.

Yeah, no, there is, there is definitely.

I mean, you see this here, like, OK, let's first

view this amazing car performance but it doesn't work.

See, OK, that's it, like I think that's exactly your

point here.

I tried this, this.

This thing does not leave.

I mean, maybe at some point it does, but it's

really, really slow.

When I let this run yesterday, I mean it's making

its way up slowly.

When I read this one yesterday with the same amount

of training, it needed two tries to reach the hill.

So just by a different, there we go, but just

by a different run, like we arrived at a much

better policy.

So for example, in our paper, I ran like 50

random allocations through 50 like 50 random seats of initial

parameters, and I mean and that's nothing on GPUs you

can run thousands and thousands and then you, you either

show a distribution of the seats of your policy, see

how good it is about over initial initializations of parameter

values, or you pick the best one.

But this is not giving you, we don't find the

globally optimal behaviour.

These problems are too complicated, um, and the law functions

aren't and so on, um, but the reward functions.

But here what we find is we find pretty good

behaviour for things that are otherwise completely out of reach.

You can think of it this way.

Imagine I have 10 states and I have 5 action

variables.

The mapping between 10 states and 5 things I can.

in a dynamically optimal way and that's so wild already

that if I were to search over this with, you

know, there are exact methods that I didn't cover in

the first lecture, it would take me, you know, 9

years already, uh, if I just test it smalls.

I could search over everything, try to find really the

optimal behaviour, um, but, uh, the, the reinforces on things

give me like algorithms give me good solutions, but not

perfect ones.

OK.

And they have higher run to run variability.

In particular, this is like kind of the plain vanilla

policy gradient algorithm.

Um, if we had more time, more lectures, we would

look into things like PPO, which is used for language

ones, which do these clippings of updates, and they have

less variability from run to run.

Um, but they still have variability from run to run.

For example, in the paper, we use PPO.

We still had a lot of variability from run to

run because our rewards are very variable.

Right, so we ran it many times.

We picked the best policy function or you can show

a distribution of functions.

That's by the way, also why people were not so

excited about our language models for a while, you know,

because like it's so brutal, um, and, and people think,

you know, yeah, like you run it again, you see

something different, and so on, but like, but we'll see

it, it seems at the moment quite crucial for the

levels of intelligence with the other questions.

And it is really, really fascinating.

So I hope you kind of find it as interesting

as this.

All right, good, um, cool.

So over the last weeks we did this reinforcement learning,

we looked at these canonical algorithms, value-based, that was Q

learning.

We were learning the value function or policy based.

We were learning how to behave in that environment just

by learning the theta parameters of the policy function directly.

Um, so they are not just applicable to games or

robotics, um, but they are, um, they are applicable to

language generation itself.

So that's kind of the mad thing.

You can think of language almost in exactly the same

way as you can think of as a power game,

um.

And we will focus on what is referred to as

post-training, OK, um, and that's our focus today of LLMs.

We did pre-training in the lecture 3 weeks ago, but

because we did it 3 weeks ago, I added a

couple of, um, really new sites, OK, to make sure

everyone's on the same page.

Um, that's our outline for today.

So we review what we did before, how to get

a base model, and then we focus on most of

our time on post training.

Um, any questions?

OK, that's cool, um, cool, so that's, um, on, on

the review part, um, our side of things, um.

At a fundamental level, you can think of something like

check GPT as this auto regressive function.

You put in a prompt.

Your prompt, um, is, uh, is split into tokens.

Um, now your prompt is a sequence of tokens one

to the end of your prompt.

And what CheEP does is it predicts the probability over

the next token.

And again, it samples from that probability.

It doesn't pick the most likely token, it's samples from

this, so most likely it picks the most likely token

for definitely.

That's why you see different outcomes when you write your

prompt into J GPT, uh, even if it's the same

prompt, um, and then it puts the token there, appends

it to the sequence samples C+1 and C T +

2 append that to the to the sequence samples plus

3.

That's what all regressive means.

It's like rolling forward.

OK, so.

Can you tell me if I predict the probability over

the next token, kind of, what is my um like,

what is the uh the final layout of the network

was the dimensionality of how many things to work probabilities.

Yeah, so it was 200,000 and that that was just

why is it always doing itself just how these things

work or why.

That's it.

So it's like we think of the number of tokens

and the tokenizer as our vocabulary.

Um, this thing gives me a probability for everything that

could be in the vocabulary.

That's it.

Perfect.

OK.

All right.

So, um, the thing, this function here, you can learn

in many ways.

We saw this 03 paper, you know, that did like

an NLP or an R like a recurrent nene, um,

and they already had like a context of a few

words predicting the next word.

They call it probabilistic neural language models, I believe.

Um, and today, these, these functions we could in principle

parameterize them in lots of ways just what we found

out it works and that and what we discovered so

far is this transformer architecture.

It doesn't have to be the only, I mean, it's

most certainly not the only way to do this, but

it's the best way we have at the moment, and

it works well with our hardware and so on.

So even if someone defines a better way, a better

function, it's not like obvious or better like thought they

did, what better even means.

It's all like a mixture between hardware, can you optimise,

paralyse this, how does it fit our hardware and so

on.

This is what we have for now, these matrix multiplications.

They're extremely well paralyzable on EPUs, um.

And um and the uh the one appeal of the

architecture is a high degree of paralyzability.

And I'll show you this is what I mean with

this in a moment.

All right, so this is um a beautiful drawing I

created um on uh on how this kind of forward

path looks like an inference.

OK.

So one example that we looked at is language models

are interesting, so we tokenize it actually into 4 tokens

we saw, um, with the tech token 200,000,000 base.

Um, this thing I believe or I concept I think

it's the transformer.

OK, that's my plan here.

Um, now, I run this through the transformer.

I take only the last, so through lots of layers

here.

Only the last embedding that comes out of this, of

the last token, this thing, I multiply with the, with

the matrix with, um, with the dimensionality as, uh, as

high as my overall vocabulary.

That's what I just meant.

I have a bunch of real numbers here.

I apply the softmax.

I have the probability.

OK.

So that's how I get the probability that's, that's, that's

this function probability plus one at a high level.

That's it.

Now we quickly review this part here, the orange part,

um, and then, uh, we move to um uh to

both questions.

So that whole process is just to get one next

token.

That's right.

That's right.

And then, and then if you were to, um, uh,

you know, if you do inference so you write sort

of my MY47 my problem set or something like this,

JGBT, then this is my initial prompt.

That's the next one.

had this to this and then run it through the

model.

Um, it can pay attention to as many of these

as it has um it has context, yeah, and, um,

and at some point it will like this will be

more than it has context and then it will not

pay attention to them.

These are these ways between uh between things.

Why does it take, um, why does it sample from

the distribution?

Why does it not take the one that's the most

likely?

Yeah, so, um, it's like, um, by we'll need this

crucially in reinforcement for exploration, but also here you can

think of it this way, the loss function, um, OK,

so during training, the loss function is just, you know,

this cross-entropy thing where you have The lock of this

Uh, Times The true label.

So we predict this probability, and that's how the lock

operates in cross entropy.

Um, but during language generation, I think 11 reason is

that there's not necessarily a right answer to many questions.

So you can make that's a good segue maybe into

what you know this, have you heard of this temperature

parameter language models?

Or that's extremely easy to explain.

If you don't know it already.

What this model will output in the end is a

probability distribution, right?

So it might that token, it might, you know, give

me a high probability, these token roughly the same, this

token a little bit more, a little bit more, etc.

right?

Something like this.

And then most likely I pick that token, but I

might pick that token as well.

What, what a high temperature is doing, it makes these

things more uniform, right?

So a high temperature brings them closer together.

It scales down these and scales up these.

You can think of it this way.

So it brings them back together and then the model

is more experimental.

It gives me, you know, uh, more, more potential tokens.

And a low temperature makes this more degenerate.

It might make that even larger probability relative to these

things, and then it will almost always give me the

same token, but not quite.

So the extreme thing would be to take the mode

of the distribution, just commit on the maximum probability, but

for many things where there is no correct answer, that

might be not the correct way to approach things.

OK, but, um, for example, when we want to have

as kind of robust outcomes as possible when we analyse

transcripts with these interview studies, we pick temperature zero, for

example, because we don't want this thing to experiment, we

want this to tell us its best guess.

Yeah, this is when you query the API.

It's not quite a hyper.

Yeah, I mean this is for inference that means for.

I training, yeah, it's already trained.

The trained model gives me this probability distribution here, um,

and then I can scale it more to uniform, more

to look like this here, like, you know, a bit

like all equal probability, or more to some things having

higher probability, um, yeah, uh, constant in it.

If, if you just add a consonant to the soft,

why does this go in does it, does it go

in the soft?

You mean temperature?

Yeah, so I, I mean, I don't know their exact

implementation, but what this does is the soft outputs you

these probabilities like a multinomial probabilities.

And however, then they implement temperature, it's scaling this to

be more uniform.

If you think of a bar char, these bars bar

chars are now like these bars are now roughly all

the same height with very high temperature or very degenerate.

One of them has a lot higher height than the

other ones, and I'm mostly sensing this.

OK, cool.

So that was like for you to check out the,

the last, like the lecture slides of the respective week,

but we discussed this attention mechanism and, and the feed

forward was kind of trivial because it was only an

MLP this multilayer perception, um, and the forward path during

inference, um, uh, when we create new data, that's what

we mean.

Once this thing is trained and we want to create,

uh, we want to create answers to autocomplete sentences.

Um, language models are interesting, and then we would encode

and look up static embeddings.

These would come from a gigantic table.

We would randomly initialise, and now we've trained it say,

uh, we have these here.

We add some positional encoding might come from another table

or might just be a function of the position.

Sometimes people do that too.

It's just a function of some, you know, data about

the, um, like, um, about the position itself.

But this might be from a different in simple implementation

from a Different embedding table as well.

And that embedding table gives me vectors for each position

in the content.

This gives me vectors for each word.

So hello would have, you know, a different embedding here.

But then this will give me the encoding, the embedding

for position 4, and that will always be the same

across all these tokens.

So I combine this into my embedding, um, and then

I run this too.

I normalise, um, and then I apply these, uh, these

communication steps.

Um, you know, we had this attention of X1, 23,

X4, and we pass this through this model.

Um, into export Prime.

And that's a simplified version of Prime here I actually

say we also need to add skip connections and normalise

to get Prime here I make this simple.

Um, OK, and, and I'm saying like these things, they

all depend on the ones before them.

Yeah, so the form.

Depends on all of these, and these are basically weighted

averages of all the previous ones.

So that's just, you know, a function of itself.

This is a function of this one and this one.

And what people you might see this when you read

about this, this would be called causal or masked attention.

It's not fully connected attention.

Fully connected attention would be this one can, um, so

this one can also pay attention to this one.

This one can also be forward looking here and so

on.

That would be fully connected.

For the language models, we do this massive attention, uh,

which says every token can only pay attention to the

token in the sequence before it.

OK.

Why does that make sense?

So now first, like, you know, this year will be

then, then each of these things afterwards.

So that's one attention, um, layer, and we, you know,

paralyse them and run lots of them in parallel to

this multi-edit attention.

And then each of them will get passed through an

MLP like a feed forward neural network, but each individually.

To be transformed and give me uh the double prime

version.

Again, I'm extracting for norm and skip connection.

Skip connection means just adding the original thing back basically.

So this is not quite just alpha 11 times X1,

but also plus X1, plus X2, etc.

Um, OK, so I run this through an through an

MLP and then I have the final transformed version.

Um, After, let's say here we have, here we have,

this goes to an MLP blog.

But crucially in the MLP.

Um, We do this thing for each vector independently, right?

So here, they're all talking to each other.

They're all, um, um, you know, weighted sums of each

other, and here it's just I take this one, run

through the MLP, this one run through the MFP.

This one run like here they don't communicate, OK?

And, and now again, I don't have the layer norms

here and I don't have the the adding stuff itself

uh here.

That's, that's my simplification here on the whiteboard.

And now in the end, like I do this here,

I do this end times, and then I said, I

take the embedding for this one here, I project it

into vocabulary space by a softmax, and then that's the

next token, right?

But like, um, and that's exactly what I I wrote

down here, OK?

Loins apply softne next token.

So during training, and that's maybe to your earlier points

as well, you, do you see one really cool thing

about setting up attention this way is that we have

a final embedding for this one as well, a final

embedding for this one, final embedding for this one.

We have final embeddings for everything, and for every final

embedding, it's only depending on everything before it.

So actually, we can, we can use the same soft

mat logic to use this to predict.

M X2, use this to predict.

X3 predict.

X4, and this to predict X5.

Do you see this?

Like we can use big we set we set this

whole thing up such that our next token prediction depends

only on the final embedding.

That's what we did.

But by that trick during training, we can just paralyse

this super well.

We can, with this one already predict X2, with this

one predict X3, with this one, predict X4, and we

can with one pass through this network run a bunch

of predictions in parallel, and then also back propagate all

of them and update our model much better.

Um, and that's also a great thing of this causal

of this mass detention only being backward looking because otherwise

we couldn't do this.

Imagine this one would go forward, we couldn't predict with

this.

This would be nonsensical because it would be part of

the embedding.

Um, yeah, but like basically during during a training, the

forward pass would actually be running over batches of data

and that would give us, um, would use all of

these because if we just predict the next token.

Um, in the sequence during inference, we basically don't, we

disregard all of these embeddings that also come out of

the final layer, right?

The final layer gives me um X double primes for

all of these, uh, and if I just predict the

next token T + 1, I just basically don't look

at these, uh, when I, you know, when I have

the final trade model and I'm running inference on.

But during training, I also look at these.

I wrap them all through soft maxes, and I use

them to predict the success of the next token with

them.

OK, questions?

So now we can also wrap up the, the parameters

that make up this model.

Um, so we have an initial, um, uh, an initial

embedding matrix and initial, if we say this is a

matrix positional embedding matrix.

What's the dimension of this just to make sure kind

of.

On the same page.

So what do you think is the embedding dimension, the

initial embedding.

Say we, like I haven't given you one information, say

we look at embeddings that have dimension, each of them,

uh, 2048.

The light dimensions.

That's right, in the vocabulary.

So we have again the 200,000, so this thing will

be of dimension 200,000 times the embedding dimensions.

This thing will be of dimension.

Say we take the same embedding dimension for position, so

2048 again, but what's the other number here for position?

Yeah, but position was the physician.

Yeah, yeah, that's the question.

So position encoding is I, I get my encodings for

this, I would actually, when they enter the transformer, my

word before the first attention layer, I would get the

first embedding by lookup table, looking up the embedding of

the word.

You know, language, and then looking at the embedding of

the position, position one, adding them both up.

That's how I get position and that's how I get

my first embedding entering the first attention here.

So what is the dimensionality here?

2048 times what?

Exactly, because I need a positional uh embedding for every

single position in the context.

Great.

Cool.

All right, so then I need these waves, values, queries,

keys for all of my attention heads that run in

parallel.

Actually, if I have, I omitted this, but if I

have attention heads running in parallel, I need to combine

all of these vectors back into my original embeddings.

So I need another projection, uh, here, another matrix, but

you know, don't worry about this, but for us it's

to learn, we need to learn these parameters as well.

Why?

Because we have all these heads now running in parallel,

these attention heads for each attention um block, and we

need to, they all give us.

12 primes, and we need to combine them all into

one vector, so we need another linear transformation.

Um, and then each neural network, um, has its has

its own here, we might also have biases, by the

way, not just weight matrices, uh, has its own, um,

weight matrix and bias.

Um, and then we have a final weight matrix, taking

this token and projecting it back into the vocabulary.

Um, that's another weight matrix.

By the way, you might see weight tying or somewhere,

um, if you were to look into these things, and

that's basically using the same.

The Matrix that is different for input embeddings also to

project it back into the vocabulary space because the dimensionality

is the same here, um, uh, because what are we

doing here in the final layer it gives us 1000,

like 2,048.

We need to project that into all the vocabulary, 200,000.

That's the same dimension as this thing here.

So we could use that again if we want to

save parameters because let's, for example, think about this.

Imagine we have a transformer with 2048 embedding dimension, 2048

times 200,000, that's already low parameters.

Yeah, so just the initially, that's where all these gazillions

of parameters come from.

So that's why sometimes people in smaller models make this

force this model to have these two be the same.

Um, but you know, a bigger model is more flexible

if it has two different matrices, it works better.

Good.

And then we have parameter like, you know, you did

normalisation with Tom, right?

Like they have normalisation and so on, yeah, so you

might have different hyper parameters or parameters that you can

also learn during training.

Great, OK, so this is the model and that was

only our objective to get the probability of the next,

um, of the next token, and that's a super flexible

function to achieve this, um, that we got and we

could randomly initialise this function now and it makes bad

predictions, OK, as, as usual.

And then we, we were basically train this now.

What is this?

It's just a classification problem of predicting the next um

class, which is over 200,000 potential ones, which is, which

is, you know, the vocabulary size, predicting the next to

comparing it to the true next to.

OK.

Um, to emphasise this, just like we're saying this, oh,

we trained this on the internet, but if you think

of these labs or firms, they would have dedicated teams

only to assemble that data.

That is such a humongous amount of work, you know,

clean that data, make sure it's high quality, etc.

If you look at these small models from Microsoft, these

fine models that you can run on your computer, um,

they actually come from a paper, this paper called Textbooks

is all you need because they trained a very small

model on very high quality data.

From textbooks, OK, so like a lot of the the

things these models they learn a lot more about you

know the world if you so want um through, through

the internet because it's just a lot wider the information

content but also a lot lower quality.

Um, so, um, what we discussed last time to clean

it.

Good, and then we can run inference, OK, inference again

means I'm always trained.

I'm just inferring what it would tell me, given an

initial problem.

Um, and here for now it's just an autocomplete and

like an internet document simulator.

OK.

I say language more interesting, say I sampled the token

because I appended it.

Now I sampled today, Iend it and I sample R,

etc.

right?

So this is how language works.

Why is this so at some point?

Because we have special end of statement tokens that we

usually add in post training, and if we sample this,

it's so, um.

Great.

OK, so that's also from the last lecture, but just

to wrap it up with two slides, um, what we

have now is, um, is a simulator of internet texts.

We don't have, we don't have something that answers questions

or so, but we have something that actually, you know,

was one way to read all these texts, and we

saw with this reproducing of this Wikipedia article that it

has perfect recall over some things.

Um, at some point limited recall, but it has impressive

recall of the documents.

Um, and this, this function now has a lot of

knowledge, although its objective is just to autocomplete the internet,

but we can just change its objective and leverage all

the knowledge it has learned.

Um, you can conceptualise this as a kind of lossy

compression of getting all the data that you saw on

the internet, you know, in some lossy ways stored in

the parameters.

Why lossy?

Because the internet that, for example, AA2 was part of

the internet that LAA 2 was trained on was 10

terabytes.

It's only contained like the whole model can be wrapped

into a file of 140 140 gigabytes.

Um, each of whether these gigabytes come from, uh, these

are, it's 70 billion roughly parameters, and each of them

is stored at a certain floating position, maybe 32 digits

after the, after the period or something like this.

So 16 could be 8.

That's a way to make the models smaller.

When you see these, um, quantized models or so, they

only have a precision of 8, and that makes the

model a lot smaller.

You Don't need 140 gigabytes.

But how, how expensive is this?

Very expensive at the time, that's it's estimated 2 million.

OK, so the base model training is to bake all

that information that is on the internet into these parameter

values.

It's just a very expensive endeavorable computation and, uh, mind-wise,

energy wise, etc.

All right, so we looked at this here and how

we could, how we could, uh, prompt this space model.

But that's also the last slide on recap, um, but

I think good to mention again because it's so fundamental.

Uh, there are these scaling laws and the scaling laws,

um, that, you know, this is 2020 paper, but, um,

but they have been holding for a while now.

Um, if you at the same time increase compute data

set size and parameters you said you saw for a

long time and still seeing um.

Um, uh, at least broadly, uh, reduction in test loss.

This is some out and like, you know, holdout data

and then applying the model to that holdout data, predicting

the next talk on the holdout data and seeing how,

how low, uh, the cross-entropy losses.

Um, and if you scaled compute data price parameters at

the same time, you were getting this lost down.

Now we must say, why do I care about the

loss on next token prediction because it correlates with a

lot of the other metrics of intelligence and problem solving

that we see.

OK.

Last thing to mention here is compute is, is doubling

the or least in like in a most sense the

transistors.

If this thing is growing at that rate, and we

allocate all these scientists to keep its growing at this

rate of approximately that rate for some time.

We will very quickly run out of data.

Um, like we can't, like that's always the thing with

exponential growth at some point, like, we don't have enough

resources for these things.

And even if the compute keeps at this, like growing

at this pace, we're already entering a part where we

are lacking the data.

That's why you see public discussion of, you know, people

wanting to use textbooks at scale, like a lot of

proprietary content now to add more data, uh, because they

have so many parameters and so much compute that they

can't scale it at the same time.

That's why.

Um, and then also this is also tied to one

of the big discussions at the moment of synthetic data.

You might have thought about that yourself, you know, why

not let LLMs generate data and train on that.

That's also what they did on these 5 models to

create small data sets that were very high quality.

So they had LLMs create synthetic data of very high

quality in lots of different domains.

Um, however, if you take the LLM, it's been trained

on the internet, it cannot really magically create new information,

right?

So it will, it will create some combinations of the

internet text that you haven't seen before, but it's not

that this can give you an infinite new data.

So there are clear limits to this as well.

OK.

Questions.

So hold tight you.

Shall we continue for 5, 10 more minutes or before

break?

Good.

Cool.

So, basically this post training, that's the focus for the

rest of today, is split into these two broad things,

OK?

And I'll show you examples for, for, um, for all

of them.

Um, one is supervised fine tuning, and what that means

is just continuing what we did so far, but with,

with data.

Tailored to what we want to achieve, and I'll show

you a bunch of examples and then reinforcement learning, uh,

using all our knowledge from the previous weeks.

OK, so that's worth finding.

We have this thing that has stored all this information

from the internet, has basically read the internet.

That's how you can think of this pre-training.

Now it can stimulate the internet, but not very helpful

for us.

We want, however, to preserve its knowledge of the internet.

Um, and all the texts of it.

Um, so how do we make it answer questions, you

know, behave, uh, ethically, etc.

We repeat the same training objective at the start of

next token prediction, but we make it predict the next

token on documents that we curate, and these documents are

prompts and answers.

So think of like a prompt is.

I don't know.

Give me a summary of the, of the transformer architecture.

And then at the beginning, some human was sitting there

and taking like 2 hours to write a good summary

of the transformer architecture.

And now you have the question, give me a summary

of the transformer architecture, a summary of the transform architecture.

The model knew it all along.

It just didn't know how to answer it.

Um, you then basically combined the two into one text,

and you were doing next talk prediction onto that text.

When I spoke with someone at the time working on

this, uh, she said that they were actually zeroing out

the gradients on, on the prompt because the prompt was

not very uh helpful for them, but you might think

of it like, you know, Being the exact same objective,

and I don't know if if all people do this.

So you concatenate your prompts your answers, you do next

talk on predictions on these things, you teach the model

what it means to answer a question.

It already knows what the transform architecture is.

It just doesn't know how to answer a question.

So now you give it, um, these.

Examples across many domains, you create new texts across many

domains by writing questions and that's the original instruc GPT

paper with lots of humans.

They did it at the time.

They were taking questions, writing lots of good answers, and

then training this thing with the same objective.

They trained the GPT 3 model, and then they had

to instruct GPT which was answering questions.

How does that work if we train it on, you

know, what is the transformer, why doesn't then write me

a poem, because it knew that too, it just didn't

know how to answer my question, and all we seem

to have to train here primarily is to answer questions.

That's also something quite interesting there.

All right, so that's just to recap of that original

paper.

I think that was also a slide, um.

Uh, so here, what they did is they took this

GPT 3 model in 3 sizes at the time that

was already a pretty big model here.

um, then they, uh, they, uh, looked at that data

set and Just to appreciate how tiny this data set

is relative to what the base training was on, it's

just, it's nothing.

So they took this, these were 13,000 training they had

different data sets, but that was one training prompt and

what was this?

These were prompts from the API that people put into

the API at the time to ask GPT this was

da Vinci at the time, the G53 model, um, and

then they wrote their own um questions and they mixed

it.

And then to all these things, they created answers.

They created plain answers.

Just, um, you know, giving, uh, giving the plain answer

to a question, few shot where they added to the

prompt a couple of examples and then gave an answer,

um, or user base, they took things that the users

asked in the API and then wrote better answers to

them and so on, yeah.

So that's how they created the data set and then

they contrained, and they continued the training on that data

set, um, and it's much smaller, um, but it turned

out the model was able to generalise following instructions.

OK, questions about this.

And this was universal across languages.

It was like universal because like this only English or

yeah, good, good question.

I'll show you in a moment.

Give me one second.

Um, If you think of like, you know, the original

transformer paper was this thing that read in a sentence

in one language, and I'll put a sentence into, into

another language.

But what is also so surprising about this, maybe not

anymore, is that by just taking a model that's only

to decode a part of the transformer and only predicting

the next word, I can still have it translate things

because it knows all the languages.

It has some internal representation of all of this, and

I can just say, oh, that's my English and, um,

you know, that's my sentence in English, give me the

same sentence in German.

And without it being a translation model, it's just like

also does this on the way.

Um, yeah, but, but how does it do this?

Um, first step, the underlying training data, you know, the

spine web, for example, needs to be in many languages.

And for example, JGBT, if we're talking about German, um,

JGP it's, it's worse than the English one because there's

less German text on the internet and there's English text

on the internet.

So the first step for is to speak multiple languages

to actually see the languages and doing pre-training.

Um, and then we also need to align it on

different languages, although it will transfer across languages to some

degree because internally all of this is represented as numbers,

right?

So, uh, which, which will have some uh general structure.

Good.

So broadly, this is also what alignment is.

It's giving it prompts and answers, um, showing behaviour that

you want this model to display, OK?

So you want to refuse answering certain questions, etc.

So in your SFT data set, you, you do this,

you, you, you call refusals into this, and you give

a lot of examples.

And then I'm sure, I mean, I don't know how

they set this up.

Sure they also have hardcoded things which LGBTE, like some

certain safeguards that they just, uh, detect with classifiers and

then stop.

But there are too many such things for them to

hardcode everything.

So they really need to align these models to lots

of SFT examples to behave in a way that they

want it to behave.

And then it will have some odd behaviour in other

ways, and then they try to iron out that odd

behaviour through through successive training.

That's also, by the way, why JGBT is continuously evolving,

right?

Because they probably always have someone, um, you know, broken

and made it something, do something that they didn't want

to, and then they continued, um, fixing that part, etc.

and that will always change behaviour in some small ways

in other domains.

OK, and this is an open back there, the SFPA

that is of course not public, but this is a

public one.

So let me show you this one.

Great.

So this is how, how this looks like and to

your earlier question here is, for example, one in Spanish,

OK.

So the SFT data set, and what are we talking

about here?

This is an open source module.

There's 2 to 3.

We're talking about a million examples.

That's what this data says.

So it seems last one, it's nothing in comparison to

the tracking data of these models.

OK.

Um, there's lots of different, uh, question answer pairs, and

then also what's kind of interesting, it's mentioned in the

picture is like this hard coded thing here.

Um, this is like what makes up this data set,

and hardcore really is what they, they give certain common

questions and they give desired answers.

They don't tell them all answers like they train them

all on this and with SFT again.

But for example, this, tell me about yourself.

You might have seen this on social media in the

early days, people were trying to get this model to

say what it is and if it has a soul

and whatever.

But like this thing is just predicting the next token,

and it's just auto completing what it feels is sensible.

So it will say, yes, for sure, or whatever, like

auto completing the.

So here then, this is why these days, these models

will tell you IDPT4 I open they have something like

this because they fine tuned on on this, OK.

So here you see, tell me about yourself, and then

it's this content, um, uh, of the machine.

I'm a chatbot assistant created by by this firm.

OK, that's where these answers come from.

Another way to get these answers, and they will combine

this is to put it into the front of, into

the front of the model.

So there is some hidden part of the problem that

you don't see a system prompt or something like this.

So you write in the GPT, but there's another part

of the problem that they always.

and that process today is April 1st 2025.

By the way, you know the product is this year,

your name I don't know, you are, you are this

and so on, and then the models are competing based

on this, OK, and then display, um, uh, more kind

of acceptable behaviour for humans.

OK, and this has a lot of challenges, right?

So if we align it to those preferences of a

certain group, they are not necessarily the preferences of another

group.

Um, malicious actors might align it to goals that, uh,

that are dangerous.

um, then also, even if a model is aligned to

the.

Well behaved, people try to, uh, to break this, uh,

this alignment and try to trick it in certain ways

to still answer some questions.

OK.

So also for social scientists, just in this space, there's

a lot of research, uh, uh, being done already and

a lot of possibilities for, for future research on how

to align these models and, and, uh, how to reflect

some kind of aggregate preference in this alignment of, of

societies.

Um, OK.

Are we doing with time?

Yeah, I mean, let's, let's take a break.

Well, um, good.

So uh let's take a break till.

Um, so 2 or 15 so.

And I think.

um.

I don't like.

and then we can like take maybe like the the

most common people I give like a sample answer.

Yeah.

It's OK, 3 Sit down.

Is you have.

That's why I, yeah, so.

Since I continue the, uh, oh yeah, but it was,

it was, I think it's just like it's.

So like it's I don't think.

What.

Yeah.

I figured out we can just.

Yeah OK.

So you tell them.

Yeah, so.

OK.

It's on it all that.

I love it.

I like that.

Yeah, I can show you how to make it and

you can ask someone on the talks.

It goes in the in the break so I can

be so good at it if that's.

11:30 you can do whatever we're fighting to be like,

oh, that's when the next term starts and that's the

beginning of um sorry I need to prepare this so

um.

I think we definitely need to and then also I

don't know.

I love stuff like that so like super excited.

And I have the whole thing is to like just

try something for sure, and then you know after like

a legitimate very stressful part like I was talking to

my dad.

No it was like it can be so bad I

don't know.

Yeah, oh.

You know, you know that's the first I will not

meet the I I'm out.

I think that's what you're supposed to do.

Yeah.

No, I like that.

What is my 15.

I I I don't know.

I just.

That's like that.

No, no, but like I I.

12.

we have 2 hours there's anything I don't have but

we have we have.

All right, so let's continue.

And we have like 4 45 minutes or something like

this, um, and this is a lot of exciting content

I think now because it puts now we're at the

stage of being able to to combine a lot of

things we've done.

So the first thing is that's, you know, interesting, but

mostly kind of um definition work here is what's the

difference between this in context learning and fine tuning because

um you can also Um, you know, fine tune your

own, um, LLMs, um, either locally if you have enough

compute and they are small enough, or via APIs from,

for example, Open AI people then have their own GPTs

or something like this, which they have fine tuned.

OK.

So, first of all, one thing is also on what

data do you fine tune it, you might use a

more powerful model and let it give you examples to

automate these things and then fine tune of them.

And that's essentially what these, um, these papers are doing

here.

Um, the problem is it's still a human.

Process, right?

So curate these data sets and so on is you

can't fully automate this at this stage, um, but there's

increasingly, um, increasing use of synthetic data in that domain.

Whatever data you use, you can find in your own

model and in my research or also I've seen this

study in like.

sector work and so on is often the question is,

do I want to find you a model to make

it better at a specific task.

What I'm essentially continuing with this is this SFT that

they did before.

If I find you a model, maybe I don't change

all the parameters, they probably only let me change a

subset of the parameters.

They're not very transparent about this, um, but essentially I'm

just predicting the next to on data I'm uploading.

OK, they, they might do other stuff on top of

this, like, you know, hidden in the APIs like some

reinforcement learning or so.

Mm, good.

But the question is when do I fine-tune the model

and what's the alternative to it?

And there's always there are these two things.

Either I supply a lot of, you know, examples or

content in the prompt, and that's in context learning because

it's just a glorified prompt.

It's like more context, you know, more context, more info

in the prompt, or I do supervised fine tuning, that

means I change the parameters.

So these are different things.

If someone says, you know, um, uh.

Kind of like we're doing in context learning, that's a

different thing to supervised fine tuning.

OK.

Supervised fine tuning is um is changing the parameters in

context learning it's just even longer prompts.

So we can, we can look at like different examples

of intext, uh, in context learning, and, um, the, you

know, names given to these approaches are usually what we

already saw before in this OpenAI.

Um, paper on the GPT 0 shot, one shot or

few shot prompting.

OK, that means zero shot is what is the transformer

architecture about?

Um, one shot prompting what's the transformer architecture about?

Oh, by the way, this is what I want you

to respond.

Uh, this is me explaining, um, a CNN or something

like this.

And then Fuho is giving a few more examples, um,

translate that sentence into German, and then I give you

a few examples of, uh, sentences translated into different languages.

Um, Usually the the most powerful models, they are so

good at so many domains, um, that I don't need

to give them, of course, an example of how to

translate things into languages and so on.

But I might want to, uh, them to format the

output in a specific way or pay attention to certain

things in like the texts that I'm sending them, and

then I give them more information.

So to give you a research example, I was processing

parts of the newspaper archive and I was sending them

to these models and asking the models to give me

information that was in the text, text, and doing this

for hundreds of thousands of texts.

And then I had brought the two ways how to

approach it either or 3.

I either I took the unchanged model and I just

sent a text to the model API with a little

prompt saying, Tell me is this about economic policy, and

then the model would tell me, yeah, um, similarly to

what we did in the last seminar.

Um, OK, the other option would be.

To give examples in my prompt, by the way, these

are the economic policies I care about, and I care

about in particular about monetary policy or fiscal policy or

whatever as part of the prompts, that's in context learning.

I give it more information as part of the prompt

in context.

Um, I mean like it's, it's really prompting but like

abstractly or what I do is I write questions and

answers.

question would be my question to the model plus the

newspaper text.

That would be the question right, so my, my instruction

to the model plus the newspaper text.

And I would write answers what I would want to

have.

This is about fiscal policy because this is not about

fiscal policy because then I have these things and I

fine tune on them.

If I fine tune on them, I have a model

with my own parameter values that is tailored to my

task, but it's not obvious that this model is a

lot better than if I just gave a bunch of

examples in the prompt.

You see, that's kind of the tension here.

I can either change the parameter values, or I can

give more examples in the prompt.

If I change the parameter values, I might achieve something

quite similar to giving all the examples without me having

to also pay for all the examples in every prompt,

right?

Because I pay per token, and if I send a

huge list of examples as part of the prompts, I

might as well just fine-tune the model, but This is

kind of a decision that is very commonly, um, very

common to face in these kinds of symptoms.

OK.

What is better, it depends.

I would like my usual approach is take the strongest

model, um, uh, try if it's able to do that

task and try successively cheaper, um, faster models, but take

the strongest model, try to see if it can do

the task.

Um, with a very simple problem just telling it, you

know, tell me is this about fiscal policy, nothing more,

um, information, and then always tell the model to give

you, uh, if it's standard LM to give you a

reason for why it makes that decision, and then, uh,

the decision because then it's going to be better than

what we discussed in class.

Um, and I try this with a very simple prompt.

If this thing doesn't work, then the next thing would

be to try a more complex prompt with examples, and

then the next thing would be fine tuning, and then

I would compare these and see, you know, what's worth

my effort in this specification.

To give you an idea of this, um, let's revisit

this, uh, LA-based model, OK, that we had last time,

um, and this thing is just this all um complete

engine for the internet, right?

No, no alignment happens here.

Um, now, if I ask questions like why is the

sky blue?

What is the sensible autocomplete?

I mean, when I tried this yesterday, uh, it was

saying why is grass green or something like this, right?

So that's a sensible autocomplete now I don't know what

it will come up with, might answer me, but usually

it's just like, yeah.

It's probably us as a child, etc.

OK, so it's just like making up stuff, um, let's

make this a bit shorter.

So why is this sky blue?

More questions, you know, it's not answering.

OK.

Good.

So usually what I would do is I would align

this thing with supervised fine tuning examples just how we

did.

But what is quite interesting, I can leverage in context

learning to already elicit all the stuff it knows, just

that it doesn't know how to answer the question.

And what I'm doing here is I'm giving it things

to autocomplete in such a way that it's actually then

answering questions.

So I'm writing up the prompt in a way that

the sensible autocomplete is not answerable question.

So I just, um, you know, um, it's a bit

buggy here so I have to kind of get rid

of the new lines, um.

So you see what I did here, I said, you

know, what's the, what's the short form for the London

School of Economics, it's LC and I just say what

it means to answer questions, but I tried this in

a completely different domain to why is the sky blue,

right, and to see if it still works.

Um, OK, so here we go, almost done.

And then I tell you you are a helpful answering

questions, you know, um, and then I say, why is

this guy do?

and you answer, OK?

So let's see, maybe put this to 50.

There we go, perfect answer.

Yeah, um, but then, uh, it just all completes you

see here in a way that it's just like it

has seen these examples and it's now just making up

stuff what's the capital of France, Paris, and so on,

right?

So it doesn't know where to stop, it doesn't have

an end of statement token, but if I truncate this

at a, at a time, you know, where it's still

like going on uh answering the original question.

Um, It will again likely give me a a decent

answer.

Yeah, you see, and that's just driven by the, by

the context and why does this work?

because this thing is an autocompleter and now I'm giving

a sensible thing to autocomplete in a way that answers

my questions without any, did I change any parameter values?

Nothing.

I just gave it information in context.

OK, cool.

So that's just do, and this is with a base

model.

I usually wouldn't use the base model that is interesting

for teaching and and learning about these things, but I

might give information in context also if I from GPT,

and I might give it examples of what economic policy

that I want.

The only dangers, if you give examples you might overfit

on them.

So if I give you three examples, it might pay

extra attention to these particular examples and so on.

Um, right, so it's, that's all a process of trial

and error.

Great.

So this thing can also help us to reduce the

so-called hallucinations of these models.

And by now it should be quite clear why hallucinations

of these models are expected to be such a big

problem in the base versions because they are just these

auto completes.

Last time when we added into the 2023 cutoff model

to autocomplete the Paris Olympics, it was just making up

the Paris Olympics, right?

There is like that's just it its objective.

Um, there's nothing, you know, no grammar rules, like it's

just predicting, it's just predicting the next, um, predicting the

next, um.

A token.

Now, how can we make this thing more accurate?

One way is to use the context of the model.

We can actually put information to the context and basically

show it, refresh its memory of certain things, and then

it's, it's responses are likely more accurate.

You can think of the parameter values like it's abstract

knowledge, like what you learned, and then the context is

a bit like about.

Documents in front of you and now you can look

into these documents and recite them more easily rather than

just from memory.

This thing has amazing memory.

It can recite things, many things perfectly just from memory,

but not, um, like at some point it, it can't,

and then it has this tendency to make stuff up

by auto completing.

Um, now we can give it information as part of

its context and that is basing its autocomplete based on

that information.

Now, I'll add an optional exercise on on rack today

in the class.

Um, this is also ultimately what these rack systems are.

I added this here because some of you asked me

about them.

Um, you find relevant texts in a database.

You add them to a context of a model, retrieve

or augmented.

You find things in the database, you retrieve them, augment

generation, the context, you augment, you put this into the

context of a model, and then you generate based on

the stuff you put into the context.

So someone asks a question, you find related things in

the database, you put the text into the context of

the model, and all the model answers more accurately.

OK, and particularly for small models or very large data

sets, that's, that's a good way forward.

And that's one way to reduce these hallucinations, which was

another thing that some people asked to, to cover.

OK.

What is very interesting is this year, that's more subtle,

OK.

So if I ask the model.

Um, and that's a different approach to pro factual accuracy.

It doesn't require adding stuff to context.

Um, if I ask the model about the Paris Olympics,

for example, probably in its huge, you know, space of

neurons, there will be some neurons lighting up, signalling it

doesn't know.

Probably there is something there, but it has been trained

to just autocomplete the internet, so it will autocomplete the

Paris Olympics.

Interestingly, similarly, after supervised fine tuning, it has been trained

by questions and answer pairs or very confident answers.

It has been trained on people writing super confident answers

about a cooking recipe about something, you know.

And now I ask it something, it will give me

a confident answer.

It will just simulate these two, these, these, these annotators

now, um, and by doing this, it might not necessarily

be factually accurate, OK.

Um, so how can I teach it to be factually

more accurate?

I first need to figure out what it doesn't know,

and that's a tricky one, and that depends on the

model.

So usually, um, one approaches.

To, to go to here, I picked a small model

because the chances are, are higher that it doesn't hold

stuff, OK.

So now let's say it's a good example I also

gave yesterday.

So here, um, basically, um, you, uh, you, uh, that's

a, that's a website of a, of a super important

mathematician.

Um, and she did her dissertation under Paul Gordon, OK,

and the dissertation was named, um, on Complete Systems of,

of some, uh, um, of some invariants.

OK, good.

So now, um, I'll ask this model.

What was the title of Emmi Newton's dissertation, and sometimes

it might get it, um, sometimes it might not, um,

depends on the model, of course.

So let's see.

That's in Spark model, so, so it should answer my

questions.

OK, so just invariants very uh variation or invariant variation

problems, um, if you look at the thing here, um,

it's a different one.

So it's just fantasise what the dissertation was.

So now I know this model doesn't know.

OK.

I repeat this a couple of times because if I

ask that question again, it would give me a different

one.

Ideally, I want to find questions where it really doesn't

know the answer or very rarely knows the answer.

And now I have found like some boundary in the

knowledge of that model, and I saw that.

I saw that question.

And now I write an own answer saying, I don't

know.

Um, and then I try to sample other questions from,

you know, Wikipedia asking random factual things about random articles,

uh, making up, you know, I can make that I

hate it, I can ask language models to make up

questions based on text for me.

Now I have a set of potential questions.

I know the correct answers.

I see why the model doesn't get it.

I tend to my data set of everything the model

doesn't know.

And now for all these questions, I create own answers

saying, I'm sorry, I don't know, or other things like

to use.

I'll show you in a second, OK?

And now I find you the model on these questions.

And now I've told the model, like, basically, I know

what it doesn't know, and I've shown it that in

these instances, it's good to answer it doesn't know.

And now it will generalise this to lots of other

cases, hopefully, where it also doesn't know, but now, now

it's also where that a legit answer is that it

doesn't know.

And not that the desired answers to just mimic a

confident, um, uh, confident annotator who created the answer.

OK.

And there is a, do you think that there is

a risk that you will say, I don't know, but

it will know the answer.

Yeah, I mean there are these dangers, that's right, but

that's 11 approach that increase uh factual accuracy or decrease

hallucinations a lot.

Um, well, it's linked also to to use which we're,

which we're discussing, uh, soon, OK?

But this is, I think this is.

It's such an interesting way to approach this.

Try to find where the knowledge boundaries, be aware that

the model probably knows when it doesn't know, but it

just doesn't know how to respond in these cases.

You need to find the knowledge of boundary, and then

you tell it how to respond or you show it

how it could respond.

You supervise fine tune on this, and then it will,

in other cases that it doesn't know, more likely say

it doesn't, rather than making up stuff and just trying

to simulate the end of day which sounded so confident,

right?

Um.

OK, um, that's, that's, that's one approach.

Great, however, Um, You can go further and we can,

for example, if we ask a language model, it's like

a small model to do a very complicated multiplication.

It's not great at this, right?

So it's just great in the next to there's nothing

in it that we should expect it to be fantastic

and doing calculator stuff.

So multiply.

Um So, OK.

Um, great.

Let's see what happens.

Here you go, analysing.

So what it's actually doing it, it's cheating and it's

multiplying this in Python.

OK.

So that's what happened here.

Good.

For this to happen, it needs to know that that's

something it doesn't know and then trigger Python, basically.

OK.

So it's quite linked to what we did before.

Um, also, I can tell it not to use tools.

And now it just, you know, tells me something now.

And I was using this again.

And I would, it's very easy, of course, for it

to give me the right answer, um, and that's pretty

close to 24, OK.

Um, and I did that with, with using some tool,

and here I can check what it did.

Um, So It's approximately this, OK?

Cool.

Um, good.

So how does this work?

First of all, it needs to know what it doesn't

know in some way, uh, to get to use.

To use could be a calculator, could be a Python

interpreter.

It's amazing at writing code.

The next token prediction is a lot more suited to

writing code than multiplying two numbers.

Um, and then, uh, it needs to know, uh, when

to use the code it, um, or when to use

other tools, and tools can be searching the web using

a calculator or usually use Python code and instead of

the calculator would be my guess.

And how does it do this?

Um, we in post training, we can show it specific

tokens.

So we introduced these new tokens that we then learn

on a couple of examples.

And these could, for example, be something like, you know,

uh, web search or something like this.

Right?

So if something is, if it doesn't know something, Paris

Olympics, because it's beyond its knowledge cut off, you might

these days see that it starts searching the web.

How does it do this?

It predicts it doesn't know, but it doesn't say I

don't know.

It then predicts a like it predicts a web search

token, then gives the Europe.

for the web search token, then closes this and then

the system behind this that now makes up chatPT2 sees,

well, the language model emitted this.

Now let's run this through a search engine.

That system runs this through a search engine and gives

chat GPT back into context the answer of that search

engine.

OK, so maybe the HTML content or something like this

off the top hits in that search engine.

Python code, it would go, you know, say Python.

It would put some Python code here.

And then we would run this to a high top.

And then we would put the answer of the Python

interpreter back into the context model would answer.

Um, that's to you, OK?

How do we, uh, like we introduce these tokens, we

showed a couple of examples with supervised fine tuning again,

we tune on these things now it starts using tools.

Any questions?

Cool.

And it's linked to this of being aware when to

use tools and when not, right?

You don't always want to Google on the internet if

you just know everything you're probably it's just a waste

of time, um, and resources.

So this is a link to the previous discussion.

Great.

And then my last step on SFT, um, is, um,

linking into the reinforcement learning, um, and that's on reasoning.

So think of this, like, you know, so first is

we can, we can, uh, use SFT to improve the

reasoning capability of models and their answers.

And that's a really interesting feature of these models.

So try to, like, have a look at this, um,

so I master's degree is 4 units.

One for unit is taken up by a dissertation.

How many have unit courses, um, could a student take

if there were no restrictions on the course composition.

OK.

And now, what is the answer?

And now I want to supervise, fine tune this model,

um, it's say a base model, and I have two

annotators gave me these two answers.

Which one is that, you think?

Or are they kind of the same?

They both have the correct answer.

You don't have to look for this, uh, and.

So why is it better?

And is it much better, a little bit better, what

do you think?

Uh, is way better because it gives time to process.

So it can reason otherwise here he has to predict.

Uh, 6 before reasoning, so it, it's only 14.

That's right.

So if you want your model to um give you,

um, uh, like, you know, to analyse texts for you,

the newspaper text say if they are about economic policy.

I't ask you to make the decision before the reason.

Because it will then need to compute the decision without

having the tokens to distribute its reasoning over.

Um, and like taking a step back here, this model

is this forward path through this all these layers, um,

and then predicting the probabilities of the next token, that's

one step, right?

Regardless of how deep this model is, even if it

has a trillion parameters like these frontier models at the

moment, roughly, um, and likely, then this is a fixed

amount of compute it has, right?

It only has so many things it can do through

one pass next token, and there's no more because it

has only these parameters.

If I ask you this question and it's a good

model, it can do 6, no problem.

That's super easy.

But if it's a complicated question, to do the entire

calculation, it's actually very much resembling us asking us to

do a long multiplication in our head and just produce

the outcome.

We would also want to write down the individual parts

and then do some algorithm on it and distribute our

computation across multiple steps.

Whereas if we ask you to just produce the answer,

these are also these common failure modes.

It will be much, you know, less capable than if

we give it a time to think about stuff.

And we can see this in, in, um.

In action.

So let's try this out with a small model.

Um, So that was the Aminuta example.

Let's let's do another example here.

OK, so please reply with a single number.

Let's see if it gets it right.

Maybe.

All right, so it's, it's it's just disregarding my, OK,

OK.

Please only.

See a number and no other text next to it.

They were wrong.

OK.

Cool.

So I'll check your answer.

Makes it.

That's exactly this, right?

We forced it to do the entire task and one

forward pass wasn't able to, but it's not that the

model can't do it.

It's the model had too little compute to do it.

So if we distributed this over many tokens, it would

do it actually.

And that's really what these reasoning models these days are

based on, and that's what we can all segue basically

tokens, OK?

Um, any questions about this?

This is quite fascinating, um, but the underlying logic is,

has it fixed the model of compute.

The next token is based on that fixed amount of

compute.

If the computer is not enough to pull this off,

it can't do it, but maybe it can do it

by distributing it over many tokens.

That's why if you want to, um, uh, you know,

create SFT examples exactly like you were saying, at the,

uh, the output at the end, not at the start,

otherwise they're kind of useless.

Um, and that.

Leads to this, so it's key to distribute the required

contribution, uh, computation, um, over several tokens, OK?

And then good prompt answer here, um, are exactly, uh,

um, are, are doing exactly this.

And that's also a nice piece of trivia.

This is why.

GPT gives you lists of things all the time.

It's not, and people are saying this on, on like

some online or is giving me these lengthy responses and

so on.

It's not just to help you, it's also to help

the model.

That's why they're doing this.

So the supervised fine tuning examples have been long reasoning.

And that's because the model gets more capable with this,

and the model is mimicking the fine tuners.

Sure, for us it's also easier to understand its thinking,

but sometimes we don't want these long responses.

However, it's, it's there to make the moral um think

better and the model is mimicking the labs, right, or

the, the annotators.

Good.

Now why is this a nice, um, why is this

nicely leading into the reinforcement learning part?

Um, because what are the right reasoning steps?

We just gave them all of the reasoning steps for

the simple problem, kind of obvious, um, but what for

much more complex problems that you can attack in so

many different ways and so on, like, like math proofs

and things like this.

Do we really want like also it would take us

light years to, to, um, you know, to, to develop

a bunch of super fancy methods prove them in many

different ways and so on.

It might not be very time efficient to do this.

So what, what are different ways to do this?

And that's it needs to reinforce that.

OK, good.

So we did this a bunch in the last weeks.

There are all these applications, we discussed some of them.

Um, this is this environment, OK.

My state is my screen, my actions are left, right,

and I maximise my reward, the use of the usual

thing.

This is the environment uh from the Bardo book and

now it turns out that actually language generation is exactly

this.

um, if we, we can conceptualise it exactly like this

and how do we do this?

We think of the pre-trained model as a policy function,

right?

We think of this thing as a policy function and

we say the state is the sequence of tokens.

And the next uh token is my action.

So now I'm just relabeling things for now.

I take this policy like I take this LLM and

I say, I think of it for now as a

policy function, and I've initialised it with supervised fine tuning.

And by the way, that's how they initialised AlphaGo's reinforce

policy function.

They were predicting go moves onboards, like very, very similar

to this.

OK, so now I have initialised this and I'm thinking

about this differently now.

I'm saying this thing gets a prompt and it picks

the it's action is not a move on the draw

board, but it's action is the next token.

Now the next state is the state and the token

combined, and it's action is the next token.

We also see this full reinforcement learning problem, like a,

like an MDP my actions changed the future states.

OK.

Um, observe a sequence of tokens, pick the next token,

observe the next sequence of tokens.

Well, that's easy to observe because I knew the initial

one and I just added an action, and I'll pick

the next token, and so on.

That's exactly the same structure, um, in some ways, like

more complex, in some ways simpler because I know what

the next step will be perfectly by adding my answer.

That would be the last step of my answer.

Uh, there's no Atari game throwing a spaceship at me

or so, um, um, but then the dynamics here are

in other ways very complex, um, but mathematically, it's the

same thing.

And so far we've just relabeled this, and I hope

through the last weeks this makes, uh, like a lot

of sense to you how this is the same thing,

yeah.

So it's creating an an MDP or making language generation

an MDP just thinking of it this way.

The only thing missing for an MDP, you know, arguably

discount rate, fair enough, but like what are the rewards,

right?

That's the key thing.

So what are the rewards in language generation?

And there are broadly two options.

One are these fuzzy rewards that people started working with

actually in the in this RLA chef reinforcement learning from

human feedback, you might have heard of this.

And that was pioneered in this paper and this instructionBT

paper.

And these are so like fuzzy rewards for things where

it's not obvious what the right reward is.

Um, and they train a reward model to predict this.

OK, so think of summarization.

What is the reward for summarization?

What's a good summary?

Unclear, right?

And these days, what deepsea can do is they look

at problems where the right answer is clear, and that's

the trick.

And then the reward isn't fuzzy, OK?

Um, and we, we can do, um, lots of interesting

things.

So let's first quickly look at the option one for

the start.

So I showed you this before for supervised fine tuning,

but really the paper had two more components, and this

was estimating a reward mark.

So what they did is they looked at a lot

of question answer pairs, so they took questions.

Um, in the, in, you know, the API and then

they or like other questions, and they had problems and

then they settled.

A bunch of, um, a bunch of answers, possible answers

to it.

And now they asked humans how much they liked different,

so that's the same prompt.

But answer J, answer K or whatever, a bunch of

different answers, OK?

And then they they asked humans how much they like

this, and humans were ranking these answers.

They said, I like that answer more than that answer

to the problem more than that answer.

Either way, that's why Che GBT asks you to pick

between two options because you're doing this for them, you're

labelling data for them.

In the end, they try to predict with the reward

model, say they have a specific loss function how to

transform ranks into numbers, and they predict a number how

much humans like this, OK?

So some something in that interval.

So maybe you like this more explicit, maybe this year

gets a 0.7, this year it gets a 0.1, and

that's getting a 0.3.

OK, and um all of these are in the interval

of 0 and 1.

This is not a probability.

This is just a normed uh interval here or like

a range for giving me an estimate how much humans

like that answer.

So that could now be lots of prompts, right about

a a poem about the LSE poem, how much I

like the poem 0.7, I didn't like that poem, right?

It could be a summary, it could be lots of

things, cooking recipe, whatever we want.

Now we have a reward model giving us these estimates.

We initialise the reward model maybe with the transformer from

before, before we changed the final hat, and then we,

we make, we predict these scores that humans gave for

lots of human scores.

Um, we don't quite have human scores.

We have them ranked documents, but we can transform the

ranks into scores, OK?

And then we have a model in which we could

put any text prompt answer, and it will give us

a number between 0 and 1, telling us how much,

how much humans probably like this.

And now we have reinforcement learning going.

We, we can generate an answer with reinforcement learning, we

can sample the next token, the next token, arrive at

the end.

Now we have created an answer for a question, we

run it through the reward model, we know it likes

it or it doesn't like it, we reinforce things that

gave high rewards.

OK, so we train a separate reward model to give

us rewards based on prompt answer pe say on the

interval, um, and now we can in principle predict a

reward for every answer that an, uh, that an RA

policy, that's just the LLM is giving, um, and now

we can do reinforcement learning on this.

We start with the prompts, we sample the next token,

sample the next token.

At some point we do end of statement token.

We have completed the episode, the game.

And now we run this thing through a reward model

that's separate.

You see, that's a bit hacky, right?

We run this through a separate model that gives me

40, not 42, but say 0.8.

And now we reinforce this, gives me 0.1, we lower

the probability of that action.

What are issues with this?

This is just an approximation, this model.

This is just an approximation of how much humans like

this, and we are using this function approximation logic of

regression to try to estimate for new problems and answers,

how much humans will like this.

But this will have some failure modes, and we will

have some weird texts that are for, I don't know,

hello, hello, hello, hello, hello, 57 times that for some

reason gives me a 0.9 prediction or something like this.

And there will always be these failure modes and some

obscure parts of possible problem answer pairs, and reinforcement learning

will find them.

So reinforcement learning, you know, we find these things, maximise,

you know, everything out of this model, and then just

have it, um, answer with hello, hello, hello, hello, hello

at some point, because the reward model told you that's

high rewards.

And then you derail your policy and your LLM doesn't

work anymore.

So what then people do is they use it these

days and it, it works pretty well to align models

um and make them answer better, but they run it

for a couple of episodes and then stop before this

thing can, um, you know, find vulnerabilities in the reward

model and destroy the policy.

Um, but it's still a, a part of a lot

of, um, pipelines of, of model alignment in addition to

supervised fine tuning.

You create these reward models, what humans like, you run

reinforcement learning, but you can't run it for too long

because at some point reinforcement learning will find some obscurities

in your reward function and then only maximise these rewards,

get really high episode rewards, but will have no language

anymore, uh, out of, or like not read your language,

uh, more will be created from these, from these models.

Great, so now we're heading to kind of The close

and that's what if we only look at problems with

known rewards, OK?

And we don't have this reward model to bang, but

we really just um look at things where we know

the answers to or false.

Let's look at coding or mathematics, OK?

And then The LLM is just exploring answers.

At some point gives me an answer.

I extract that answer in some way from it, like,

you know, it gives me a 6 and for example

4, and I verify it.

If it arrived at the right answer, I give it

a high reward.

If it didn't, um, I give it a low reward.

And this is really broadly the secret behind things.

I mean, lots of other things simplifying the architectures and

so on, but behind these reasoning models and the open

source, the bigger open source reasoning model is the deep

sea one.

the old one models and so on, the architecture is

not publicly known.

You know, it might be similar, um.

Good.

So, um, these recent models, they likely have a flavour

of this.

Deepse you can look up what the flavour is.

And what the really cool thing about this is, is

that it's basically exploring different answers to problems, to, to

questions, complicated problems in mathematics.

It's using, using like this RL logic of exploring moves

in the game, and here it's just exploring next tokens.

And at some point it gives us an answer because

we asked a question about a problem and we know

whether that answer is true or false, and then we

can reinforce the cases where it gave us the right

answers.

Um, and that's from the website.

You see when you look at reasoning models, they give

you this example of how you ask the model something.

It has reasoning tokens and then it gives you an

output and then you are next input is all of

that combined, um, and then, uh, you know, with again,

um, another question, and then reasoning and output and so

on at some point you might hit token limits because

now we have these reasoning tokens.

The model is just emitting a bunch of tokens on

the way to getting you an answer, and these tokens,

they are often hidden from the user, um, but that's

like a bunch of tokens that the policy function is

emitting to arrive at the answer.

And see how this is extremely similar to, to how

you would master a game, right?

So if you look at these, um, these cases of,

of, you know, say these, the alphago or so like

in this um in this uh documentary that you can

see on, on YouTube, there is this Move 37 where

it just.

an obscure move that thought, like, people thought it was

a bug.

And then it was, uh, beating the best player at

the, at the time, like through that move in that

game quite spectacularly.

If you look at Alpha Zero, this reinforcement learning algorithm

that from scratch learns chess just to self-play of other

versions of itself.

It's very similar to what we did, trying out lots

of things, um, playing as earlier versions in a clever

way, such that it's reinforcing, um, strategies that works well

and work well, and then mastering this game.

There's a lot of discussion, uh, in the or like

has been at the time a few years back in

the chess world about this alpha zero for chess because

it was playing some strategies and doing some sacrifices that

were really out of the playbook of what humans were

playing because it didn't without any human knowledge around the

game.

It wasn't even initialised, which for language is probably too

complicated for like for now for sure, but it was

just figuring out stuff on the way.

Now here we initialise this with like the next token

prediction to have some world knowledge about how things work.

But then we also let it explore things and we

don't really care what it does to arrive at an

answer.

We want the right answer.

Um, and we wanted to explore ways in which it

arrives at the right answer itself, OK.

And when you look at Um, Like, for example, the

DC paper.

They, they have these cases here where, you know, like

they have this one case that they show.

This is just a token for nowcom reasoning tokens.

That's a special token.

And that's just a bunch of tokens as well, but

they might be hidden from the user.

And in these reasoning tokens, that's what's on the math

problem that they trade us on, it has these epiphanies.

It says, wait, wait, wait, that's an aha moment.

I can flag here.

And then it's like attacking the problem from a different

angle, right?

And you find these things in these reasoning traces, and

we can actually do this ourselves.

So I have an example that I Kind of cooked

up here.

It's a very kind of common, yeah we have like

enough time.

It's a very common example from like Asian statistics.

I recently saw it and thought that's a nice, uh,

kind of thing to add here.

Um, they asked us a bunch of doctors and lots

of people get this wrong.

It's not very intuitive, um, because you need a base

theorem for this.

OK.

So imagine, um, imagine 1 in 1000 people has a

disease, so pretty rare, OK.

There's a test to detect that disease with 100% of

curious.

For these people, so everyone who really has this, you

know they have it, OK, so that's a given.

However, lots of people don't have that.

For the people that don't have it, the test tells

you with 95% accuracy that they don't have it.

But there's this 5% error rate where it reflects someone

as having the disease that doesn't have it.

So now if a doctor just in their office observes

someone who has a positive test, what's the likelihood that

they actually have the disease?

What do you think, like just intuitively.

So now you're a doctor, you're seeing someone here who

has that disease or like who who comes with a

positive test.

With that structure, what's the likelihood that they're really sick?

Or what's the probability that they're using?

High low 50%.

It's extremely low.

It's like it's, it's 2% roughly.

And why intuitively?

Because I get the person that has the disease, but

out of the roughly 1000 people, 999 people that don't

have it, I wrongly flag 50 people.

So out of every 50 people that enters my office,

only one person has the disease.

That's where the 2% comes from.

OK.

So if I ask one to do this, uh, let's

go.

Here you go, think, you see that token, and now

it's going on, basically, yeah, so I mean it's, it's

very close already like in now using SCM this would

be plug in the numbers, etc.

plug it in.

OK, let me do this let's see, now it's another

angle.

OK, this is, these are all these reasoning tokens, and

this is, it came up with this approach itself.

It, it was, you know, developing this through during training

trying to arrive at the correct answers.

It's quite worthy, but it's already at the right answer

here.

OK, sure.

So now it has it already, we know that's correct.

Alternative if we keep the original numbers converted to decimals,

I was continuing.

Oh, I, I hit the max token probably.

Yes, I hit my max token limit.

Um, OK.

At some point, they will check this from many, many

angles and then give me the uh the, the, the

user as in like a few steps telling me these

are the formulas here you go.

But it checks this from many, many different um angles,

OK, um, uh, through its through its reasoning capabilities.

OK, in the paper, you see actually that during reinforcement

learning training, these are the steps very similar to the

steps we did in our reinforcement training.

The average length per responses increased, so it was emitting

ever more reasoning tokens because it was getting more powerful.

And by the way, that's why if you go to

um JGBT or so and you ask all three, you

have a high compute setting or low compute setting, and

that means how many reasoning tokens on the way.

Um, that's linking to uh like more, uh, maybe others

linking to this.

Um, OK, cool.

So they, we initialise these LLMs with pre-training and SFT,

um, we then refined them through reinforcement learning to find

own answers to solutions.

Um, and, um, yeah, by this we leverage basically RL

as a kind of advanced search algorithm, OK, searching for

solutions with, with great interscent, um, very similar, uh, to

what I did in games, um, and it's just not

choosing the next token to arrive at the solution not

to win and one game.

OK, um, yeah, and, um, many, many, uh, open questions

remain, such for example, these are these verifiable domains, right?

We have math or coding and you might have tried

this.

They are really, really good now at math or coding.

I mean, that really shows in these reasoning models, but

they are maybe not so good at finding summaries, uh,

like, you know, finding occurrences of certain concepts and text,

because they have not been trained on this.

So one question is, if it's super capable now in

math encoding, how much does this transfer to other domains?

Is it also getting better at other domains now?

Um, and then one massive area of current work is

try to get high quality data data sets with verifiable

answers.

If you can get this in chemistry and and all

the other stuff, you can do the same logic on

this.

Um, and, um, and then, yeah, there are, there are

lots of other, there are also lots of other fields.

We can also, you know, discuss this at the, at

the start of the seminar in like 445 more minutes

or so and you can ask questions if you have

any.

Um, cool.

But yeah, I hope this makes more sense because of

the RL stuff we did.

And that's really the current frontier and that's how these

reasoning models work, um, by trying to figure out tokens

that lead to the right answers and figuring out these

tokens with with RL, um, cool, so I'll see you

in the, in the classes and I'll save you if

you have any questions.

Absolutely yeah.

Yes, yes.

So I have like I put them up from 11:30

to 12 o'clock so that's.

I think that's.

and you can tell me.

I don't like that.

How are you doing, so I don't know.

Um, yeah, yeah, they have like a small like uh

how much it's yeah.

models which you can do so I think the models

that you find yourself depends on the model, but you,

you would probably a lot less I I I have

to start to work on the.

Um, you can also create some of these examples with

the AI, um, so you can have the same help

you, uh, to, uh, uh.

Yeah, OK, cool, and that can help you label things.

you could be like relatively.

OK Sure.

Yeah, yeah, and we don't really have any drinks or

so.

Yeah Yeah Yeah.

Oh my, I'm a bit some favourite I might say.

You're sick I'm phobic.

I used to sit standing behind trees.

I'm like, Oh yeah, I I've covered it a bit.

Hence I wear a hat when it gets really sunny.

I feel like it's not hot weather yet.

I'm not used to the sun like a vampire.

I'm the same, but I would have refused to wear

sunscreen.

I would refuse until I got like.

I, I invested in like sunscreen that's like good for

your skin just greasy, they have like non-oil's.

