TopicGPT: A Prompt-based Topic Modeling Framework
Chau Minh Pham1 Alexander Hoyle2 Simeng Sun1 Mohit Iyyer1
1University of Massachusetts Amherst 2University of Maryland {ctpham, simengsun, miyyer}@umass.edu,
hoyle@umd.edu
Abstract
Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require “reading the tea leaves” to interpret; additionally, they offer users minimal semantic control over topics. To tackle these issues, we introduce TopicGPT, a promptbased framework that uses large language models (LLMs) to uncover latent topics within a provided text collection. TopicGPT produces topics that align better with human categorizations compared to competing methods: for example, it achieves a harmonic mean purity of 0.74 against human-annotated Wikipedia topics compared to 0.64 for the strongest baseline. Its topics are also more interpretable, dispensing with ambiguous bags of words in favor of topics with natural language labels and associated free-form descriptions. Moreover, the framework is highly adaptable, allowing users to specify constraints and modify topics without the need for model retraining. TopicGPT can be further extended to hierarchical topical modeling, enabling users to explore topics at various levels of granularity. By streamlining access to high-quality and interpretable topics, TopicGPT represents a compelling, humancentered approach to topic modeling.1
1 Introduction
Topic modeling is a commonly used technique for discovering latent thematic structures in extensive collections of text documents. Traditional topic models such as latent Dirichlet allocation (Blei et al., 2003, LDA) represent documents as mixtures of topics, where each topic is a distribution over words.2 Topics are often represented with their most probable words, but this representation can contain incoherent or unrelated words that make topics difficult for users to interpret (Chang
1Code at https://github.com/chtmp223/topicGPT 2“Word” may also refer to n-grams or other lexical items.
et al., 2009; Newman et al., 2010). Although some models enable users to interactively guide topics based on needs and domain knowledge (Hu et al., 2014; Nikolenko et al., 2017), their usability is constrained by the bag-of-words topic format.
To address these limitations, we introduce TopicGPT (Figure 1), a human-centric approach to topic modeling that relies on prompting large language models to perform in-context topic generation (§3.1) and assignment (§3.2). First, we iteratively prompt an LLM to generate new topics given a sample of documents from an input dataset and a list of previously generated topics. The resulting set of topics is then further refined to consolidate redundant topics and eliminate infrequent ones. Finally, given a new document, an LLM assigns it to one or more of the generated topics, also providing a quotation from the document to support its assignment. These quotations make the method easily verifiable, addressing some of the validity concerns that plague traditional topic models.
TopicGPT produces higher-quality topics than competing approaches. TopicGPT’s topics and assignments align significantly more closely with human-annotated ground truth topics than those from LDA and BERTopic (Grootendorst, 2022) on two datasets: Wikipedia articles (Merity et al., 2018) and Congressional bills (as processed by Hoyle et al., 2022). We measure topical alignment using three external clustering metrics (harmonic mean purity, normalized mutual information, and adjusted Rand index) and find that TopicGPT improves substantially over baselines (e.g., absolute purity improves by 10 points over LDA on Wikipedia); furthermore, its topics are more semantically aligned with human-labeled topics (30.3% of TopicGPT’s topics are misaligned compared to 62.4% for LDA). Further analysis demonstrates the robustness of TopicGPT’s topic quality across various prompt and data settings.
1
arXiv:2311.01449v1 [cs.CL] 2 Nov 2023


...
Corpus
Seed topics
- Trade - Agriculture
[Seed topics]
[Document]
[Demonstrations]
Instruction: Identify generalizable topics within the document.
Generation Prompt
1. Topic Generation
- Trade: Mentions the exchange of capital, goods, and services. - Agriculture: Discusses policies relating to agricultural practices and products. ...
Generated topics
[Refined topics]
[Document]
[Demonstrations]
Instruction: Assign generated topics to the provided document.
Assignment Prompt
Document
2. Topic Assignment
- Agriculture: Mentions changes in agricultural export requirements ("...repeal of the agricultural export requirements...")
Assigned topic
[Generated topics]
[Demonstrations]
Instruction: Merge topic pairs that are near duplicates.
Remove infrequent topics
Refinement Prompt
Figure 1: Overview of TopicGPT. 1) Topic Generation: Given a corpus and a list of manually-curated seed topics, TopicGPT identifies topics within each corpus document. The framework then refines the topic list by merging repeated topics and removing infrequent topics. 2) Topic Assignment: Given the generated topic list and a document, TopicGPT assigns the most relevant topic to the given document and provides a quote that supports this assignment.
TopicGPT produces more interpretable topics. Topics generated by TopicGPT include natural language labels and descriptions that make them immediately interpretable without needing a separate labeling step. Furthermore, the framework provides informative document-topic associations along with contextual quoted evidence. By creating intuitive topic structures and understandable document-topic assignments, TopicGPT aims to make the overall topic modeling process more informative and interpretable.
TopicGPT is customizable to fit user needs: Topic models should suit their application settings (Doogan and Buntine, 2021). In addition to interpretability, TopicGPT offers users the ability to guide the generated topics to their specific needs. Users initially provide seed topics to steer the scope and focus of the generated topics. After reviewing the initial results, users can then manually edit or remove any topics to curate a coherent and goaloriented topic list. Notably, our approach distinguishes itself from a previous attempt in promptbased topic mining (Wang et al., 2023) by using topic semantics to refine the resulting topic list.
Open-source LLMs are competent topic assigners but bad topic generators. Most of our ex
periments implement topic generation with GPT-4 (OpenAI, 2023) and topic assignment with GPT3.5-turbo, which costs around $100 per dataset (Table 6). Ideally, both stages could be implemented with open-source LLMs to remove the dependence on expensive APIs. To this end, we also experimented with the open-source Mistral-7B-Instruct model (Jiang et al., 2023), which performs fairly well for topic assignment but cannot competently follow instructions for generating topics. Improving topic generation capabilities for open-source LLMs is thus an exciting direction for future work.
2 Related Work
Automated content analysis is the primary use case for our framework, as this is a dominant application of topic models (Hoyle et al., 2022). Inductive content analysis is a popular qualitative research method that involves deriving labels directly from the dataset without relying on preconceived categories or codes (Hsieh and Shannon, 2005; Kyngäs, 2020; Vears and Gillam, 2022). In this methodology, which guided TopicGPT’s design, practitioners first engage in open coding of individual documents to establish initial labels. These initial codes are then carefully examined, reconfigured, and organized into a cohesive coding system.
2


Topic modeling for content analysis: Traditional approaches to topic modeling, such as Latent Dirichlet Allocation (Blei et al., 2003, LDA), are parameterized by topic-word distributions (φk) and document-topic distributions (θd). Their inferred estimates can reveal latent thematic structures in a corpus. However, these representations are not straightforward to interpret (Mei et al., 2007; Chang et al., 2009; Ramage et al., 2011), requiring subjective manual effort that leads to issues of reliability and validity (Baden et al., 2021). Our work follows previous research that aims to produce more interpretable topics in natural language (Mei et al., 2007; Lau et al., 2011; Wan and Wang, 2016). Additionally, our design builds on both hierarchical topic models (Griffiths et al., 2003; Teh et al., 2006; Mimno et al., 2007; Paisley et al., 2014), as well as those that impose constraints on topics (Wallach et al., 2009; Hu et al., 2014).
LLM-based content analysis: LLMs such as ChatGPT have enabled new prompting and embedding-based approaches to analyzing text. Researchers have used prompting techniques on these LLMs for related content analysis tasks including text clustering (Viswanathan et al., 2023; Zhang et al., 2023; Hoyle et al., 2023), abstractive summarization (Liu and Healey, 2023), and deductive qualitative coding (Tai et al., 2023; Chew et al., 2023). Prior work has also explored topic modeling using contextualized embeddings from LLMs (Sia et al., 2020; Thompson and Mimno, 2020; Bianchi et al., 2021; Grootendorst, 2022).
Comparison to GoalEx: Our framework most closely resembles GoalEx (Wang et al., 2023), a goal-driven clustering approach, but is tailored specifically for topic modeling. GoalEx is focused on partitioning the corpus, and is not attuned to the overall coherence and organization of the set of topics, a fundamental need in content analysis settings. Rather than assigning each topic to individual documents, which becomes costlier as the number of clusters grows, TopicGPT’s topic assignment process is adaptable to the mixed membership assumption and more cost-effective by providing all topics in the prompt simultaneously. Additionally, while GoalEx was evaluated only on cluster recovery, TopicGPT is benchmarked on both stability and alignment with ground-truth topics, demonstrating its usefulness as a content analysis tool beyond just text clustering.
3 Methodology
TopicGPT consists of two main stages: topic generation (§3.1) and topic assignment (§3.2). Figure 1 provides an illustrative overview of our framework.
3.1 Stage 1: Topic Generation
Broadly, we prompt an LLM to generate a set of topics given an input dataset, and then we further refine these topics to remove infrequently used ones and merge duplicates. The output of this step can optionally be fed to a hierarchical extension of TopicGPT that prompts the model to generate more fine-grained subtopics.
Generating new topics: In the first stage, we iteratively prompt a large language model (LLM) to generate descriptive topics. Given a document d from the corpus and a set of seed topics S, the model is instructed to either assign d to an existing topic in S or generate a new topic that better describes d and add it to S. We define a topic to be a concise label paired with a broad one-sentence description, as in
Trade: Mentions the exchange of capital, goods, and services
where “Trade” serves as the topic label. The initial seed set consists of a small number of humanwritten topics (our experiments use two seed topics) and does not need to be dataset-specific as we show in Appendix B (refer to Appendix A.1 for more instructions on creating seed topics). The intuition behind this iterative process is that it encourages newly generated topics to be distinctive and also match the specificity seen in other topics. Notably, instead of running topic generation over the entire corpus, which can be extremely costly, we apply the process to a carefully constructed sample from the dataset (see Section 4.3 for an in-depth discussion on the formation of this subset).
Refining generated topics: We further refine the generated topics to ensure that the final topic list is coherent and non-redundant.3 We first use Sentence-Transformer embeddings (Reimers and Gurevych, 2019) to identify pairs of topics with cosine similarity ≥ 0.5. We then prompt the LLM with five such topic pairs, instructing it to merge relevant or near-duplicate pairs where appropriate. This merging step consolidates redundant topics
3It is important to note that this step is optional, and is intended to enhance the appearance of the resulting list.
3


and aligns the specificity across topics, returning a coherent final list. To address any minor topics that may have been overlooked in the prior step, we eliminate topics with low frequency of occurrence. To do this, we keep track of how frequently each topic gets generated. If a topic occurs below a “removal” threshold frequency,4 we consider that topic to be minor and remove it from the final list.
Generating a topic hierarchy: We further extend TopicGPT to construct a multi-level topic hierarchy. Specifically, we treat the generated topics that remain after the refinement stage as toplevel topics and prompt the LLM for more specific subtopics at subsequent levels. We then provide the model with a topic branch that contains a toplevel topic t, a list of seed subtopics S′, and the documents dt associated with the top-level topic t. With these inputs, we instruct the LLM to generate subtopics t′ that capture common themes among the provided documents. To ensure subtopics are grounded in the documents rather than hallucinated, the model must also return specific documents supporting each subtopic. If the documents cannot fit into a single prompt, we divide them into different prompts and include subtopics generated by earlier prompts in subsequent ones. We qualitatively evaluate our hierarchical TopicGPT in Section 5.5.
3.2 Stage 2: Topic Assignment
In the assignment stage, we aim to establish a valid and interpretable association between the generated topic list and the documents in our datasets. To achieve this, we provide the LLM with our generated topic list, 2-3 examples and a document, the topic(s) of which we are interested in obtaining. We then instruct the model to assign one or more topics to the given document. The final output contains the assigned topic label, a document-specific topic description, and a quote extracted from the document to support this assignment. The quoted text improves the verifiability of TopicGPT’s assignments, which has been a long-standing concern with traditional methods such as LDA. A sample topic assignment is
Agriculture: Mentions changes in agricultural export requirements (“...repeal of the agricultural export requirements...”)
4We recommend trying different thresholds to make sure that important topics are not removed from the final list.
where “Agriculture” is the assigned topic label, followed by the topic description and a quote from the document enclosed within parentheses.
Self-correction: To address topic assignments with incorrect formatting or low quality, we incorporate a self-correction step (Shinn et al., 2023; Sun et al., 2023). Specifically, we implement a parser to identify instances of hallucinated topic assignments or invalid responses (e.g. “None”/“Error”). Subsequently, we provide the LLM with the identified documents along with the error type, shuffle the topic list to add randomness, and prompt the model to reassign a valid topic.
4 Experiments
We compare TopicGPT to two popular topic models using two labeled English-language datasets. Our goal is to assess whether TopicGPT’s outputs align with human-coded ground truth topics, as well as to test its robustness to various settings. Here, we describe our datasets, baseline methods, model configurations, and evaluation metrics.
4.1 Datasets
We used two English-language datasets for evaluation: Wiki and Bills. We adopt Hoyle et al.’s processing pipeline for both datasets. Table 1 shows high-level statistics for these datasets.
Wiki (Merity et al., 2018) is a corpus of 14,290 Wikipedia articles that meet a core set of editorial standards (also known as “good” articles). This dataset comes with 15 high-level, 45 mid-level, and 279 low-level labels that were annotated by humans. Since the Wiki text-label mapping might have appeared in the pre-training corpus of LLMs, TopicGPT’s performance on this dataset reflects a best-case scenario for our algorithm (Thompson and Mimno, 2020).
Bills (Adler and Wilkerson 2018, compiled by Hoyle et al. 2022) contains 32,661 bill summaries from the 110 − 114th U.S. congresses. This dataset comes with 21 high-level and 114 low-level humanannotated labels. Bills is far less likely to appear in LLMs’ training data because the label text is not directly associated with the corresponding documents.5
5Specifically, the zipped data on www.congressionalbills.org/download.html that connects bills to labels contains numerical codes for the labels and URLs pointing to the bills. 6https://github.com/openai/tiktoken
4


Bills Wiki
# docs 32,661 14,290 avg. # tokens / doc 261 3,412 # test examples 15,242 8,024 # topic generation docs 1,000 1,100
Table 1: Dataset statistics of Bills and Wiki. The average number of tokens per document was calculated using tiktoken BPE tokenizer.6
4.2 Baselines
We considered two popular topic models that follow different paradigms: LDA and BERTopic.
LDA: We use the MALLET (McCallum, 2002) implementation of LDA with Gibbs sampling (Griffiths, 2002). LDA is a well-established topic model that boasts strong alignment with human codes (Srivastava and Sutton, 2017; Hoyle et al., 2021) and high stability compared to neural topic models (Hoyle et al., 2022). We set |V | = 15,000, α = 1.0, β = 0.1, and run LDA for 2,000 iterations with optimization at every 10 intervals. For fair comparison, we control the number of topics k to be equal to the number of topics generated by TopicGPT.
BERTopic: BERTopic is a popular neural topic model that obtains topics by performing clustering on Sentence-Transformer embeddings of documents (Grootendorst, 2022). We maintained all default hyperparameters except for the number of topics k, which were controlled to be equal to the number of topics generated by TopicGPT.
4.3 Sampling documents for TopicGPT
The number of documents used during the topic generation phase is a critical parameter of TopicGPT. Given enough time and money (if using closed-source LLM APIs), we could certainly feed the entire training corpus into the LLM for topic generation. However, this is impractical given our current setup, which implements both topic generation and assignment with OpenAI’s GPT models, and it is also unnecessary as we confirm both in theory and practice below. We thus sample a subset of documents uniformly at random for topic generation, which results in 1,000 documents from Bills and 1,100 documents from Wiki.
How many documents should we sample? We recommend that users of TopicGPT either choose a sample size that fits their budget or run topic generation incrementally and stop when no new topics are
generated for some threshold (e.g., 100 documents) — a “topic drought.” To assess the effectiveness of this approach in achieving comprehensive topic coverage, we examine the topics generated after reaching this “topic drought” threshold and check whether our refinement process removed any of them. Figure 2 shows that on both datasets, the number of new topics that remain after refinement plateaus after around 500 documents have been generated.
A probabilistic justification: We can also proceed probabilistically: the user sets a lower bound on the number of documents that should be assigned to the least-prevalent topic, nd, which induces an upper bound on the number of topics, Ku = ⌊N/nd⌋. We can then model the sample of size ns as a draw from a uniform7 c ∼ Multi(ns, Ku), where the expected number of ze
ros is then (Ku−1)ns
K ns −1
u
. A straightforward simula
tion can be used to find the ns that minimizes p∗ = |P (mink ck = 0) − ε|, where ε is a userdefined acceptable minimum probability of failing to find the least-prevalent topic. Taking the Wiki dataset, if we set nd = 140 (1% of the corpus) then ns = 1, 100 leads to p∗ ≈ 0.005.
4.4 TopicGPT implementation details
Our default TopicGPT setting uses OpenAI’s GPT4 to generate topics and GPT-3.5-turbo to assign topics to documents. To encourage deterministic outputs,8 we set max_tokens to 300 and temperature and top_p to 0. We truncate input documents that are too long to fit within the context window size of LLMs.9 Since Bills and Wiki have one-to-one mappings between documents and labels, we modify the assigner prompt to assign only one topic per document, although we emphasize that the method does not require a single assignment. To retain important topics in the final list, we set the removal frequency threshold to 10 and 5 for Bills and Wiki, respectively. We enable self-correction with a retry limit of 10 and end up resolving all topic hallucinations and formatting issues within this limit. For evaluation purposes, we sample 8,024 documents from Wiki
7If it were non-uniform, then we would have a topic with fewer than nd documents. 8We note that OpenAI’s LLMs possess some degree of nondeterminism even after fixing the decoding hyperparameters to perform greedy decoding. 9The context window size for GPT-4 is 8,192 tokens, whereas GPT-3.5-turbo has a context window of 4,096 tokens.
5


0 100 200 300 400 500 600 700 800 900 Number of Documents Processed
0
20
40
60
80
Number of Topics
Number of Topics Over Documents - Bills
0 100 200 300 400 500 600 700 800 900 1000 Number of Documents Processed
0
10
20
30
Number of Topics
Number of Topics Over Documents - Wiki
Unrefined Topics Refined Topics Expected Number of Topics Topic Drought
Figure 2: The number of topics generated over documents processed in the Bills and Wiki corpus. The grey line indicates the number of expected topics, simulated using the empirical distribution of ground-truth topics for the datasets. For both datasets, we see a similar pattern - after a "topic drought" period marked by the dashed red line, the number of initially generated topics (orange line) keeps increasing. However, the final refined topics (blue line) and expected number of topics (grey line) plateau, despite more documents being processed.
and 15,242 documents from Bills that are not included in the topic generation sample.
4.5 Evaluation Setup
To assess the usefulness of TopicGPT as an automated content analysis tool, we evaluate the topic alignment and stability of the generated topics, following (Hoyle et al., 2022).
4.5.1 Topical alignment
Since TopicGPT is not a probabilistic model like LDA, we compare it to baselines by assessing the alignment between predicted topic assignments and topic labels in the ground truth, as also done in prior work (Chuang et al., 2013; Poursabzi-Sangdeh et al., 2016; Korenˇci ́c et al., 2021; Hoyle et al., 2022). For LDA, we assign each document to its highest-probability topic. Concretely, given a set of ground-truth classes C = {c1, ..., cJ } and a set of predicted assignment clusters Ω = {ω1, ..., ωK}, we assessed the alignment between Ω and C using the external evaluation metrics for clustering detailed below.
Purity. Purity measures the degree to which predicted clusters contain data points predominantly from a single ground-truth class (Zhao, 2005). Purity is computed by assigning each predicted cluster to the ground-truth class that occurs most frequently within it, and then counting the number of correctly assigned documents and dividing by the number of clustered items N ,
Purity(Ω, C) =
X
k
|ωk |
N mjax Precision(ωk, cj) (1)
where the precision of a cluster ωk for a given ground-truth class Ci is calculated as:
Precision(ωk, cj) = |ωk ∩ cj|
|ωk| (2)
While high purity indicates low intra-cluster noise, it does not reward grouping items from the same ground-truth class. Purity reaches its maximum value of 1 when each cluster contains just one item. Inverse Purity, defined as Purity−1 = Purity(C, Ω), addresses this by rewarding clustering data points from the same class together. However, Inverse Purity fails to penalize mixing items from different ground-truth categories. Amigó et al. (2009) show that the harmonic mean of Purity and Inverse Purity balances these two objectives:
P1 =
X
k
|ck|
N mjax F (cj, ωk)
where
F (ck, ωj) = 2 ∗ Precision(ck, ωj) ∗ Recall(ck, ωj)
Precision(ck, ωj) + Recall(ck, ωj) (3)
and
Recall(C, Ω) = Precision(Ω, C) (4)
Adjusted Rand Index. The Rand Index measures the pairwise agreement between two sets of clusterings (Rand, 1971):
RI(Ω, C) = T P + T N
T P + F P + F N + T N (5)
where T P is the number of true positives, T N is the number of true negatives, F P is the number of
6


false positives, and F N is the number of false negatives. Adjusted Rand Index (ARI) further corrects for chance by comparing it to the expected value
ARI(Ω, C) = RI − E[RI]
max(RI) − E[RI] (6)
where E[RI] stands for expected rand index (Hubert and Arabie, 1985; Vinh et al., 2009). ARI yields a score close to 0 for random cluster assignments and near 1 for strongly consistent assignments.
Normalized Mutual Information. Mutual Information (MI) measures the amount of shared information between two sets of clusterings (Shannon, 1948). Normalized Mutual Information (NMI) normalizes the MI score to a value between 0 and 1, making MI less sensitive to a varying number of clusters (Strehl and Ghosh, 2002):
N M I(Ω, C) = I(Ω, C)
[H(Ω) + H(C)]/2 (7)
where I(Ω, C) is the mutual information between Ω and C, H(Ω) and H(C) are the entropy of Ω and C.
Comparison of metrics: P1, ARI, and NMI provide complementary perspectives through set matching, counting pairs, and variation of information (Meil ̆a, 2007). While each of these scores provides useful insights, relying on just one can be misleading. For instance, P1 places a strong emphasis on cluster purity while being less concerned with the distribution of ground-truth labels. ARI, unlike NMI, is adjusted for chance, but it does not account for class imbalance, which makes it more sensitive to the number of predicted clusters.
4.5.2 Stability
We also assess the robustness of TopicGPT to changes in prompts and also different corpus samples for topic generation. More specifically, we measure if TopicGPT maintains high topical alignment in these modified settings.
Out-of-domain prompts. In TopicGPT, users can change the seed topics and few-shot examples in the prompts to tailor the method to their dataset. We explore whether prompts written for one dataset can work on another by applying prompts for Wiki on the Bills dataset.
Additional seed topics. We assess the impacts of additional seed topics on TopicGPT’s performance
in Bills. Our original prompt for topic generation contains two seed topics. In this experiment, we expand the prompt by adding three more seed topics.
Shuffling sampled documents for topic generation. We shuffle documents in the original generation sample in Bills to understand the importance of the order in which LLM processes documents.
Using a different sample for topic generation. To evaluate TopicGPT’s robustness to data shift, we apply TopicGPT to a different generation sample from Bills and examine how much the resulting topic outputs varied.
5 Results
In this section, we present the findings of our experiments, demonstrating TopicGPT’s strong topical alignment with ground truth and robustness to variations in prompts and data. We also show through human experiments that the semantic content of TopicGPT’s generated topics (ignoring document assignment) is significantly more aligned with ground-truth topics on both datasets. Furthermore, we explore the impact of modifications within the framework, including the use of an opensource model for topic generation, and qualitatively analyze its hierarchical variant.
5.1 TopicGPT is strongly aligned to ground truth labels
Our experiments (Table 2) show that TopicGPT identifies topics that are substantially more aligned with human-annotated labels than baselines, and that this improvement holds across all datasets, settings, and evaluation metrics. Of the two baseline models, LDA is generally superior to BERTopic for all metrics, suggesting that LDA remains a strong baseline. However, neither baseline approaches the performance of TopicGPT: for example, TopicGPT achieves post-refinement harmonic purity scores P1 of 0.74 and 0.57 on Wiki and Bills, respectively, compared to 0.64 and 0.52 for LDA and 0.58 and 0.39 for BERTopic.
Where does TopicGPT disagree with the ground truth? To fully understand the disagreement between TopicGPT and human labels, we closely examine five assignments where the ground truth topics and TopicGPT’s assignments in the default setting do not match (Table 12), and we find that
7


Dataset Setting TopicGPT LDA BERTopic
P1 ARI NMI P1 ARI NMI P1 ARI NMI
Wiki Default setting (k = 31) 0.73 0.58 0.71 0.59 0.44 0.65 0.54 0.24 0.50
Refined topics (k = 22) 0.74 0.60 0.70 0.64 0.52 0.67 0.58 0.28 0.50
Bills Default setting (k = 79) 0.57 0.42 0.52 0.39 0.21 0.47 0.42 0.10 0.40
Refined topics (k = 24) 0.57 0.40 0.49 0.52 0.32 0.46 0.39 0.12 0.34
TopicGPT stability ablations, baselines controlled to have the same number of topics (k).
Bills
Different generation sample (k = 73) 0.57 0.40 0.51 0.41 0.23 0.47 0.38 0.08 0.38 Out-of-domain prompts (k = 147) 0.55 0.39 0.51 0.31 0.14 0.47 0.35 0.07 0.41 Additional seed topics (k = 123) 0.50 0.33 0.49 0.33 0.15 0.46 0.36 0.07 0.40 Shuffled generation sample (k = 118) 0.55 0.40 0.52 0.33 0.16 0.47 0.36 0.08 0.40 Assigning with Mistral (k = 79) 0.51 0.37 0.46 0.39 0.21 0.47 0.42 0.10 0.40
Table 2: Topical alignment between ground-truth labels and predicted assignments. Overall, TopicGPT achieves the best performance across all settings and metrics compared to LDA and BERTopic. The number of topics used in each setting is specified as k. The largest values in each metric and setting are bolded.
each sampled document could reasonably be assigned multiple topics. Therefore, TopicGPT’s assignments can still be valid even if they differ from the ground truth labels. For example, the second document in Table 12 could fit either the “Labor” or “Transportation Safety” topics, though “Labor” is more prominent. If we allow TopicGPT to assign multiple topics per document, can it successfully retrieve all appropriate topics? To check, we re-run topic assignment using a prompt that allows the assignment of multiple topics per document. With this updated prompt, three out of the five Wiki examples are assigned to the ground truth as well as the originally assigned topic (Table 12). Thus, we recommend that practitioners use multi-label assignment prompts to extract as many relevant topics as possible.
5.2 TopicGPT is stable
Broadly speaking, TopicGPT produces comparable topical alignment with the ground truth across all modified experimental settings for the Bills data (lower portion of Table 2). The setting with additional seed topics obtains the worst performance across all metrics, suggesting that simply adding more seed topics is not always helpful. We hypothesize that too many seed topics may overwhelm the model and lead to poorer coherence, as the model tries to fit diverse topics rather than consolidating around the most salient themes. We recommend keeping the seed topic list small (2-3 high-quality topics) for best results, rather than lengthening the list arbitrarily. More details on curating seed topics can be found in Appendix A.1.
Consistency between multiple settings of TopicGPT: To further evaluate the consistency of TopicGPT’s topic assignments under different settings, we computed alignment scores (P1, ARI, and NMI) between the default setting and each modified setting. As a benchmark, we ran LDA 10 times with k = 79 topics and calculated the average internal alignment between each pair of runs. Table 3 shows TopicGPT’s assignments were highly stable across settings, with all metrics within a tight 0.05 range. TopicGPT demonstrated greater stability than LDA in terms of P1 and ARI, while achieving comparable NMI. Interestingly, TopicGPT produces slightly different outputs between two runs with identical settings, a possible result of adding randomness to the self-correction process (see 3.2) as well as LLM API nondeterminism.
5.3 TopicGPT topics are semantically close to ground truth
The clustering metrics reported above do not capture whether the topics are semantically aligned with ground truth. To this end, we qualitatively compare LDA outputs and TopicGPT’s unrefined and refined topic generations. Specifically, we analyze the proportion of misaligned topics (defined below) generated by TopicGPT and LDA.
Manual topic matching process: Three annotators (the first author and two external annotators)10 went through the list of generated topics and assigned each topic to a ground truth class. If an
10Both external annotators are fluent English speakers with a high school diploma as their highest completed level of education. They generously provided annotations as an uncompensated contribution to support this research.
8


Method Setting P1 ARI NMI
LDA Default setting (k = 79) 0.64 0.55 0.71
TopicGPT
Different generation sample (k = 73) 0.67 0.61 0.69 Out-of-domain prompts (k = 147) 0.69 0.63 0.69 Additional seed topics (k = 123) 0.69 0.59 0.70 Shuffled generation sample (k = 118) 0.70 0.63 0.70 Assigning with Mistral (k = 79) 0.65 0.58 0.62 Refined topics (k = 24) 0.65 0.63 0.69 Running the pipeline twice (k = 79) 0.95 0.92 0.92
Table 3: Stability of topic assignments of TopicGPT and LDA in the Bills dataset, as measured by the topical alignment between topic assignments of each modified setting against the default setting (unlike Table 2 which reports alignment against ground-truth assignments). TopicGPT demonstrates overall stability across settings that is competitive with that of LDA.
exact match was not possible, the annotators labeled the generated topic as either "out-of-scope", "repeated", or "missing" (Chuang et al., 2013) according to the following definition:
1. Out-of-scope topics: topics that are too narrow or too broad compared to the associated ground truth topic.
2. Missing topics: topics present in the ground truth but not in the generated outputs.
3. Repeated topics: topics that are duplicates of other topics.
After completing the mapping, the percentage of out-of-scope, missing, and repeated topics was calculated for each annotator’s mappings (Table 4). Each person completed a total of 6 mappings, including the LDA outputs, and unrefined and refined topic hierarchies for both the Bills and Wiki datasets.11
TopicGPT contains far fewer misaligned topics than LDA, especially after refinement. Compared to LDA, both TopicGPT’s unrefined and refined topics are less likely to be misaligned overall (62.4% for LDA vs. 38.7% unrefined and 30.3% refined). Annotators also noted that TopicGPT’s outputs are much easier to work with compared to the ambiguous LDA outputs (see Table 5 for some examples of LDA and TopicGPT’s outputs). We notice that refinement consistently reduces the number of out-of-scope and repeated topics. On the other hand, refinement does increase the number of missing topics by 1 in the Bills dataset; upon closer examination, this missing topic was “Culture”, which appears infrequently in the Bills corpus (only 23 documents out of 32,661 documents).
11Mapping interface and instructions can be found here.
This might be acceptable depending on the use case, and we emphasize again that practitioners should try different refinement thresholds to avoid filtering out topics that are important to their research question.
5.4 Implementing TopicGPT with open-source LLMs
We examine alternate LLMs for both topic assignment and topic generation and discover that while topic assignment can be feasibly performed with open-source LLMs, topic generation is too complex for all LLMs we tried other than GPT-4.
Mistral-7B-Instruct for topic assignment: We experiment with Mistral-7B-Instruct (Jiang et al., 2023) for topic assignment to assess its feasibility as a lower-cost alternative to GPT-3.5-turbo. Mistral’s topic assignments align reasonably well with human ground truth, though not to the same degree as GPT-3.5-turbo; the bottom row of Table 2 shows an absolute purity decrease of about ∼ 6 points. However, the resulting assignments still outperform LDA and BERTopic on all clustering metrics.
Mistral-7B-Instruct for topic generation: We further test both Mistral and GPT-3.5-turbo as topic generation models, finding that both models struggle to follow formatting instructions for topic generation. In total, Mistral and GPT-3.5-turbo produced 1,418 and 151 topics, respectively. The large number of topics made it impossible to include all of them in a single-topic assignment prompt. Additionally, most of the generated topics are very finegrained with low frequency of occurrence, meaning they would likely be removed during refinement. Here are some overly specific topics produced by Mistral for the Bills data:
9


Dataset Setting Out-of-scope Missing Repeated Total
Wiki
LDA (k = 31) 46.3 4.3 11.9 62.4 Unrefined (k = 31) 38.7 0.0 1.1 39.8 Refined (k = 22) 30.3 0.0 0.0 30.3
Bills
LDA (k = 79) 56.1 2.1 22.0 80.2 Unrefined (k = 79) 65.0 1.3 3.8 70.1 Refined (k = 24) 27.8 4.2 0.0 31.9
Table 4: Comparison of misaligned topic proportions between LDA, unrefined TopicGPT, and refined TopicGPT outputs. Values are averaged over three annotations and rounded to one decimal place. Across both datasets, refined TopicGPT achieves the lowest proportion of misaligned topics compared to LDA. The best (lowest) misalignment proportion for each dataset is bolded.
Clinical Data Registries: Mentions policies and regulations related to clinical data registries. Quorum: Mentions the requirement for a quorum of at least five members on the board of directors. Lost Baggage: Mentions procedures for returning lost baggage.
We emphasize that the instructions for topic generation are complex with many criteria (see Table 8). As such, we recommend sticking with GPT4 or models with similar capabilities for topic generation.
5.5 Qualitatively inspecting topic hierarchies
It is simple to extend TopicGPT to generate a twolevel topic hierarchy, as discussed in Section 3.1. Overall, the generated subtopics are informative and well-grounded in the documents associated with their parent topics. These subtopics successfully capture more narrow and nuanced themes within the broader parent topic, allowing for richer exploration and analysis compared to a flat topic list. Figure 3 provides a closer look into a portion of the generated topic hierarchy for the Wiki dataset. We plan to more rigorously evaluate these hierarchies in follow-up experiments.
6 Limitations
Reliance on closed-source models. TopicGPT achieves optimal performance using GPT-4 for topic generation and GPT-3.5-turbo for assignment, both of which are closed-source LLMs that cost money to use (see Table 6 for our expenses). Furthermore, we have limited transparency into their pre-training and instruction tuning datasets, as well as their architectural details. Nevertheless, we observe promising results with open-source models like Mistral-7B-instruct for topic assignment. Our
[1] Architecture & Design
[2] Religious Architecture
[2] Parks and Public Spaces
[2] Historic Structures
[1] Animal Breeds & Husbandry
[2] Bird Species
[2] Horse & Cattle Breeds
[2] Zoo Animals
Wiki
Figure 3: Example topic hierarchy for Wiki, with "Architecture & Design" and "Animal Breeds & Husbandry" as the top-level topics generated by TopicGPT. This hierarchical topic structure, in which the upperlevel topics are broad enough to encompass more detailed subtopics, allows users to explore topics at different levels of specificity.
reliance on closed-source LLMs for topic generation reflects the current imbalance between the instruction-following abilities of closed and opensource models rather than a permanent limitation. We hope that this will eventually be addressed by the rapid advances in open-source LLMs.
Dealing with context limits. Another limitation of our current approach is the need to truncate documents to fit TopicGPT’s context length limit. By only providing partial documents, we lose potentially valuable context and risk misrepresenting the contents of full documents. While truncation was necessary in these initial experiments, we recognize it is not an ideal solution. We recommend future work to explore strategies to represent full documents within length limits, such as incrementally feeding in chunks of the document, sampling representative chunks, or providing a summarized version of the document.
10


Data Document Ground-truth topic TopicGPT assignment LDA assignment
Wiki Grant Park Music Festival = The Grant Park Music Festival ( formerly Grant Park Concerts ) is an annual ten-week classical music concert series held in Chicago, Illinois, USA. It features the Grant Park Symphony Orchestra and Grant Park Chorus along with featured guest performers and conductors. The Festival has earned non-profit organization status. It claims to be the nation’s only free, outdoor classical music series. The Grant Park Music Festival has been a Chicago tradition since 1931 when Chicago Mayor Anton Cermak suggested free concerts to lift the spirits of. . .
Music Music & Performing Arts: Discuss creation, production, and performance of music, as well as related arts and cultural aspects.
City infrastructure: city, building, area, new, park
Bills Perkins Fund for Equity and Excellence. This bill amends the Carl D. Perkins Career and Technical Education Act of 2006 to replace the existing Tech Prep program with a new competitive grant program to support career and technical education. Under the program, local educational agencies and their partners may apply for grant funding to support: career and technical education programs that are aligned with postsecondary education programs, dual or concurrent enrollment programs and early college programs, certain evidence-based strategies and delivery models related to career and technical education, teacher and leader experiential. . .
Education Education: Mentions policies and programs related to higher education and student loans.
Programs and grants: program, grants, grant, programs, state
Table 5: Example topic assignments from TopicGPT and LDA (showing top 5 topic words) on two documents. While TopicGPT’s topics closely align with the ground truth, LDA’s topics are influenced by frequently-occurring words, causing it to overlook the overarching theme of the document. TopicGPT’s topic labels and descriptions are both automatically generated, while LDA produces a bag of words that needs to be manually labeled.
Stages Bills Wiki
Topic generation $30 $90 Topic refinement $10 $5 Assignment + self-correction $48 $60
Total $88 $155
Table 6: Estimated cost (in US dollars) of running TopicGPT on the Bills and Wiki datasets. Though smaller in size, the Wiki dataset incurred a much higher cost to run TopicGPT than the Bills dataset. This is due to Wiki’s longer document length - on average 3,412 tokens per document compared to just 261 tokens per document in Bills.
Multilinguality. We have not yet evaluated TopicGPT on non-English datasets. However, OpenAI’s LLMs are pre-trained and instructiontuned primarily on English language data, and the instruction-following capabilities of LLMs in non-English languages are thus notably degraded (Huang et al., 2023; Li et al., 2023). We hope that future advances in multilingual LLMs will make TopicGPT more broadly accessible.
7 Future Work
Human evaluation. While our automated metrics provide a first look at the stability and topical alignment of topics produced by TopicGPT, human judgment is still critical for evaluating how meaningful and interpretable the topics are in real-life usage (Chang et al., 2009; Hoyle et al., 2021). To better understand the practical utility of TopicGPT, we plan to conduct semi-structured interviews with
domain experts to understand their experience using TopicGPT. The interviews will gather feedback on the intelligibility of topics, how well topics capture key semantic concepts, and whether the tool provided new insights or would be useful for realworld tasks. We will synthesize user comments to refine TopicGPT and gain insights into human perspectives on achievable standards of quality for topic modeling.
Hierarchy evaluation. As TopicGPT is capable of generating a multi-level topic hierarchy, our next objective is to evaluate its performance at more granular levels by determining whether TopicGPT’s subtopics are informative and relevant to their parent topics.
8 Conclusion
We introduce a prompt-based framework, TopicGPT, specifically designed for topic modeling. TopicGPT addresses the interpretability and adaptability limitations of traditional topic models by generating descriptive topics that are well aligned with human-annotated ground truth. Our results demonstrate that TopicGPT outperforms baseline topic models in terms of topic alignment with ground truth clusters, while also showing robustness across different prompts and data subsets. We release our prompt templates and pipeline so that interested researchers and practitioners can try our novel topic modeling framework.
11


Acknowledgement
We are grateful to Dzung Pham for the helpful discussion on evaluation metrics, Brendan O’Connor and Laure Thompson for their valuable insights, Chau-Anh for their assistance with the qualitative evaluation, and the UMass NLP community for helpful feedback throughout the project. This project was partially supported by awards IIS2202506 and IIS-2046248 from the National Science Foundation (NSF).
References
E Scott Adler and John Wilkerson. 2018. Congressional bills project: 1995-2018.
Enrique Amigó, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information retrieval, 12:461–486.
Christian Baden, Christian Pipal, Martijn Schoonvelde, and Mariken A.C.G. van der Velden. 2021. Three gaps in computational text analysis methods for social sciences: A research agenda. Communication Methods and Measures, 16:1 – 18.
Federico Bianchi, Silvia Terragni, and Dirk Hovy. 2021. Pre-training is a hot topic: Contextualized document embeddings improve topic coherence.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993–1022.
Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, and David Blei. 2009. Reading tea leaves: How humans interpret topic models. Advances in neural information processing systems, 22.
Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, and Annice Kim. 2023. Llm-assisted content analysis: Using large language models to support deductive coding.
Jason Chuang, Sonal Gupta, Christopher Manning, and Jeffrey Heer. 2013. Topic model diagnostics: Assessing domain relevance via topical alignment. In Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pages 612–620, Atlanta, Georgia, USA. PMLR.
Caitlin Doogan and Wray Buntine. 2021. Topic model or topic twaddle? re-evaluating semantic interpretability measures. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3824–3848, Online. Association for Computational Linguistics.
Thomas Griffiths, Michael Jordan, Joshua Tenenbaum, and David Blei. 2003. Hierarchical topic models and the nested chinese restaurant process. Advances in neural information processing systems, 16.
Tom Griffiths. 2002. Gibbs sampling in the generative model of latent dirichlet allocation. Standford University, 518(11):1–3.
Maarten Grootendorst. 2022. Bertopic: Neural topic modeling with a class-based tf-idf procedure.
Alexander Hoyle, Pranav Goel, Andrew Hian-Cheong, Denis Peskov, Jordan Boyd-Graber, and Philip Resnik. 2021. Is Automated Topic Model Evaluation Broken? The Incoherence of Coherence. In Advances in Neural Information Processing Systems, volume 34, pages 2018–2033. Curran Associates, Inc.
Alexander Hoyle, Rupak Sarkar, Pranav Goel, and Philip Resnik. 2023. Natural language decompositions of implicit content enable better text representations.
Alexander Miserlis Hoyle, Pranav Goel, Rupak Sarkar, and Philip Resnik. 2022. Are neural topic models broken? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5321–5344, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
Hsiu-Fang Hsieh and Sarah E Shannon. 2005. Three approaches to qualitative content analysis. Qualitative health research, 15(9):1277–1288.
Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, and Alison Smith. 2014. Interactive topic modeling. Machine learning, 95:423–469.
Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. 2023. Not all languages are created equal in llms: Improving multilingual capability by crosslingual-thought prompting. In Findings of Empirical Methods in Natural Language Processing.
Lawrence Hubert and Phipps Arabie. 1985. Comparing partitions. Journal of classification, 2:193–218.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b.
Damir Korenˇci ́c, Strahil Ristov, Jelena Repar, and Jan Šnajder. 2021. A Topic Coverage Approach to Evaluation of Topic Models. IEEE Access, 9:123280123312. ArXiv:2012.06274 [cs].
Helvi Kyngäs. 2020. Inductive content analysis. The application of content analysis in nursing science research, pages 13–21.
12


Jey Han Lau, Karl Grieser, David Newman, and Timothy Baldwin. 2011. Automatic labelling of topic models. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 1536–1545.
Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023. Bactrian-x : A multilingual replicable instruction-following model with low-rank adaptation.
Sengjie Liu and Christopher G Healey. 2023. Abstractive summarization of large document collections using gpt. arXiv preprint arXiv:2310.05690.
Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. Http://www.cs.umass.edu/ mccallum/mallet.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007. Automatic labeling of multinomial topic models. In Knowledge Discovery and Data Mining.
Marina Meil ̆a. 2007. Comparing clusterings—an information based distance. Journal of multivariate analysis, 98(5):873–895.
Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. Regularizing and optimizing LSTM language models. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net.
David Mimno, Wei Li, and Andrew McCallum. 2007. Mixtures of hierarchical topics with pachinko allocation. In Proceedings of the 24th international conference on Machine learning, pages 633–640.
David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics, pages 100–108.
Sergey I Nikolenko, Sergei Koltcov, and Olessia Koltsova. 2017. Topic modelling for qualitative studies. Journal of Information Science, 43(1):88–102.
OpenAI. 2023. Gpt-4 technical report.
John Paisley, Chong Wang, David M Blei, and Michael I Jordan. 2014. Nested hierarchical dirichlet processes. IEEE transactions on pattern analysis and machine intelligence, 37(2):256–270.
Forough Poursabzi-Sangdeh, Jordan Boyd-Graber, Leah Findlater, and Kevin Seppi. 2016. Alto: Active learning with topic overviews for speeding label induction and document labeling. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1158–1169.
Daniel Ramage, Christopher D. Manning, and Susan Dumais. 2011. Partially labeled topic models for interpretable text mining. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 457–465, San Diego California USA. ACM.
William M Rand. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association, 66(336):846–850.
Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982–3992, Hong Kong, China. Association for Computational Linguistics.
Claude Elwood Shannon. 1948. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423.
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.
Suzanna Sia, Ayush Dalmia, and Sabrina J. Mielke. 2020. Tired of topic models? clusters of pretrained word embeddings make for fast and good topics too! In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1728–1736, Online. Association for Computational Linguistics.
Akash Srivastava and Charles Sutton. 2017. Autoencoding variational inference for topic models. arXiv preprint arXiv:1703.01488.
Alexander Strehl and Joydeep Ghosh. 2002. Cluster ensembles—a knowledge reuse framework for combining multiple partitions. Journal of machine learning research, 3(Dec):583–617.
Simeng Sun, Yang Liu, Shuohang Wang, Chenguang Zhu, and Mohit Iyyer. 2023. Pearl: Prompting large language models to plan and execute actions over long documents.
Robert H Tai, Lillian R Bentley, Xin Xia, Jason M Sitt, Sarah C Fankhauser, Ana M Chicas-Mosier, and Barnas G Monteith. 2023. Use of large language models to aid analysis of textual data. bioRxiv, pages 2023–07.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical Dirichlet Processes. Journal of the American Statistical Association, 101(476):1566–1581.
Laure Thompson and David Mimno. 2020. Topic Modeling with Contextualized Word Representation Clusters. ArXiv:2010.12626 [cs].
13


Danya F Vears and Lynn Gillam. 2022. Inductive content analysis: A guide for beginning qualitative researchers. Focus on Health Professional Education: A Multi-disciplinary Journal, 23(1):111–127.
Nguyen Xuan Vinh, Julien Epps, and James Bailey. 2009. Information theoretic measures for clusterings comparison: is a correction for chance necessary? In Proceedings of the 26th annual international conference on machine learning, pages 1073–1080.
Vijay Viswanathan, Kiril Gashteovski, Carolin Lawrence, Tongshuang Wu, and Graham Neubig. 2023. Large language models enable few-shot clustering.
Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking lda: Why priors matter. Advances in neural information processing systems, 22.
Xiaojun Wan and Tianming Wang. 2016. Automatic labeling of topic models using text summaries. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2297–2305, Berlin, Germany. Association for Computational Linguistics.
Zihan Wang, Jingbo Shang, and Ruiqi Zhong. 2023. Goal-driven explainable clustering via language descriptions.
Yuwei Zhang, Zihan Wang, and Jingbo Shang. 2023. ClusterLLM: Large Language Models as a Guide for Text Clustering. ArXiv:2305.14871 [cs].
Ying Zhao. 2005. Criterion Functions for Document Clustering. Ph.D. thesis, University of Minnesota, USA. AAI3180039.
A Appendix
A.1 Seed Topics
In all levels of topic generation, we included a list of seed topics along with corresponding examples. These seed topics can be tailored to specific end use cases to achieve a descriptive and granular output topics. In this section, we offer suggestions on crafting seed topics and provide accompanying examples to illustrate the impacts of varying seed topics.
1. The format of the seed topics should resemble your desirable output format, which contains a correct topic level and a concise topic label.
Example 1: Using long seed topic formatting results in long topic labels. Seed topics: [1] Trade Policies [1] Agricultural Policies
Generated topics (using GPT-4): [1] Social Security Policies [1] Food Safety Policies
Example 2: Using short seed topic formatting. Seed topics: [1] Trade [1] Agriculture
Generated topics (using GPT-4): [1] Social Security [1] Food Safety
2. The specificity of the seed topics should match the degree of granularity you would expect in a given hierarchy level.
Example 1: Using abstract seed topics for level 2.
Seed topics: [1] Music & Performing Arts
[2] Music
[2] Performing Arts
Generated topics (using GPT-4): [1] Engineering & Technology
[2] Civil & Transportation Engineering
[2] Electrical Engineering
Example 2: Using detailed seed topic for level 2.
Seed topics: [1] Music & Performing Arts
[2] Albums
[2] Songs
Generated topics (using GPT-4): [1] Engineering & Technology
[2] Road & Highway Systems
[2] Microwave Technology
A.2 Semantic Matching between Ground Truth and topics generated by LDA and TopicGPT
Table 7 shows the proportion of misaligned topics determined by each annotator.
B Prompts
See below for the prompt templates that we used for generating, refining and assigning topics.
14


Dataset Setting Out-of-scope Missing Repeated Total
P1 P2 P3 P1 P2 P3 P1 P2 P3 P1 P2 P3
Wiki
LDA (k = 31) 74.2 19.4 45.2 3.2 6.5 3.2 9.7 6.5 19.4 87.1 32.4 67.8 Unrefined (k = 31) 67.7 25.8 22.7 0.0 0.0 0.0 3.2 0.0 0.0 70.9 25.8 22.7 Refined (k = 22) 59.1 9.1 22.7 0.0 0.0 0.0 0.0 0.0 0.0 59.1 9.1 22.7
Bills
LDA (k = 79) 64.6 50.6 53.2 2.5 2.5 1.3 16.5 22.8 26.6 83.6 75.9 81.0 Unrefined (k = 79) 72.2 62.0 60.8 1.3 2.5 0.0 6.3 2.5 2.5 79.8 67.1 63.3 Refined (k = 24) 45.8 4.2 33.3 4.2 4.2 4.2 0.0 0.0 0.0 50.0 8.3 37.5
Table 7: Proportion of misaligned topics in LDA’s outputs as well as TopicGPT’s unrefined and refined outputs (rounded to one decimal place). The lowest values in each column are bolded.
Prompt template for generating first-level/flat topics
You will receive a document and a set of top-level topics from a topic hierarchy. Your task is to identify generalizable topics within the document that can act as top-level topics in the hierarchy. If any relevant topics are missing from the provided set, please add them. Otherwise, output the existing top-level topics as identified in the document.
[Top-level topics] {Seed topics (containing "[1] Trade" in this example)}
[Examples] Example 1: Adding "[1] Agriculture" Document: Saving Essential American Sailors Act or SEAS Act - Amends the Moving Ahead for Progress in the 21st Century Act (MAP-21) to repeal the Act’s repeal of the agricultural export requirements that: (1) 25 of the gross tonnage of certain agricultural commodities or their products exported each fiscal year be transported on U.S. commercial vessels, and (2) the Secretary of Transportation (DOT) finance any increased ocean freight charges incurred in the transportation of such items. Revives and reinstates those repealed requirements to read as if they were never repealed.
Your response: [1] Agriculture: Mentions policies relating to agricultural practices and products.
Example 2: Duplicate "[1] Trade", returning the existing topic Document: Amends the Harmonized Tariff Schedule of the United States to suspend temporarily the duty on mixtures containing Fluopyram.
Your response: [1] Trade: Mentions the exchange of capital, goods, and services.
[Instructions] Step 1: Determine topics mentioned in the document. - The topic labels must be as GENERALIZABLE as possible. They must not be document-specific. - The topics must reflect a SINGLE topic instead of a combination of topics. - The new topics must have a level number, a short general label, and a topic description. - The topics must be broad enough to accommodate future subtopics. Step 2: Perform ONE of the following operations: 1. If there are already duplicates or relevant topics in the hierarchy, output those topics and stop here. 2. If the document contains no topic, return "None". 3. Otherwise, add your topic as a top-level topic. Stop here and output the added topic(s). DO NOT add any additional levels.
[Document] {Document}
Please ONLY return the relevant or modified topics at the top level in the hierarchy. [Your response]
Table 8: Prompt template for generating broad, high-level topics that can either serve as a flat list of standalone topics or as the first tier of a hierarchical topic taxonomy. The designation of ’first-level’ ensures these topics are sufficiently expansive to cover the topic distribution of the entire dataset. In practice, users need to modify the components highlighted in red (seed topic list and document) as well as tailor the examples to their specific dataset.
15


Prompt template for generating second-level subtopics
You will receive a branch from a topic hierarchy along with some documents assigned to the top-level topic of that branch. Your task is to identify generalizable second-level topics that can act as subtopics to the top-level topic in the provided branch. Add your topic(s) if they are missing from the provided branch. Otherwise, return the existing relevant or duplicate topics.
[Example] (Return "[2] Exports" (new) and "[2] Tariff" (existing) as the subtopics of "[1] Trade" (provided).) Topic branch: [1] Trade [2] Tariff [2] Foreign Investments
Document 1: Export Promotion Act of 2012 - Amends the Export Enhancement Act of 1988 to revise the duties of the Trade Promotion Coordinating Committee (TPCC). Requires the TPCC to: (1) make a recommendation for the annual unified federal trade promotion budget to the President; and (2) review the proposed fiscal year budget of each federal agency with responsibility for export promotion or export financing activities before it is submitted to the Office of Management and Budget (OMB) and the President, when (as required by current law) assessing the appropriate levels and allocation of resources among such agencies in support of such activities.
Document 2: Amends the Harmonized Tariff Schedule of the United States to suspend temporarily the duty on mixtures containing Fluopyram.
Document 3: Securing Exports Through Coordination and Technology Act - Amends the Foreign Relations Authorization Act, Fiscal Year 2003. Requires carriers obliged to file Shipper’s Export Declarations to file them through AES (either directly or through intermediaries) before items are exported from any U.S. port, unless the Secretary of Commerce grants an exception.
Your response: [1] Trade [2] Exports (Document: 1, 3): Mentions export policies on goods. [2] Tariff (Document: 2): Mentions tax policies on imports or exports of goods.
[Instructions] Step 1: Determine PRIMARY and GENERALIZABLE topics mentioned in the documents. - The topics must be generalizable among the provided documents. - Each topic must not be too specific so that it can accommodate future subtopics. - Each topic must reflect a SINGLE topic instead of a combination of topics. - Each top-level topic must have a level number and a short label. Second-level topics should also include the original documents associated with these topics (separated by commas) as well as a short description of the topic. - The number of topics proposed cannot exceed the number of documents provided. Step 2: Perform ONE of the following operations: 1. If the provided top-level topic is specific enough, DO NOT add any subtopics. Return the provided top-level topic. 2. If your topic is duplicate or relevant to the provided topics, DO NOT add any subtopics. Return the existing relevant topic. 3. If your topic is relevant to and more specific than the provided top-level topic, add your topic as a second-level topic. DO NOT add to the first or third level of the hierarchy.
[Topic branch] {Topic}
[Documents] {Documents}
DO NOT add first- or third-level topics. [Your response]
Table 9: Prompt template for generating second-level subtopics in a topic hierarchy. In practice, users need to modify the components highlighted in red (generated topic branch and associated documents) as well as tailor the examples to their specific dataset. This prompt can be further accommodate subtopic generation for lower-level by changing the topic level in the examples and instructions.
16


Prompt template for refining (merging) topics
You will receive a list of topics that belong to the same level of a topic hierarchy. Your task is to merge topics that are paraphrases or near duplicates of one another. Return "None" if no modification is needed.
[Examples] Example 1: Merging topics ("[1] Employer Taxes" and "[1] Employment Tax Reporting" into "[1] Employment Taxes") Topic List: [1] Employer Taxes: Mentions taxation policy for employer [1] Employment Tax Reporting: Mentions reporting requirements for employer [1] Immigration: Mentions policies and laws on the immigration process [1] Voting: Mentions rules and regulation for the voting process
Your response: [1] Employment Taxes: Mentions taxation report and requirement for employer ([1] Employer Taxes, [1] Employment Tax Reporting)
Example 2: Merging topics ([2] Digital Literacy and [2] Telecommunications into [2] Technology) [2] Mathematics: Discuss mathematical concepts, figures and breakthroughs. [2] Digital Literacy: Discuss the ability to use technology to find, evaluate, create, and communicate information. [2] Telecommunications: Mentions policies and regulations related to the telecommunications industry, including wireless service providers and consumer rights.
Your response [2] Technology: Discuss technology and its impact on society. ([2] Digital Literacy, [2] Telecommunications)
[Rules] - Each line represents a topic, with a level indicator and a topic label. - Perform the following operations as many times as needed: - Merge relevant topics into a single topic. - Do nothing and return "None" if no modification is needed. - When merging, the output format should contain a level indicator, the updated label and description, followed by the original topics.
[Topic List] {Topics}
Output the modification or "None" where appropriate. Do not output anything else. [Your response]
Table 10: Prompt template for generating second-level subtopics in a topic hierarchy. In practice, users need to modify the components highlighted in red (seed topic list and document) as well as tailor the examples to their specific dataset.
17


Prompt template for assigning topics
You will receive a document and a topic hierarchy. Assign the document to the most relevant topics the hierarchy. Then, output the topic labels, assignment reasoning and supporting quotes from the document. DO NOT make up new topics or quotes.
Here is the topic hierarchy: {tree}
[Examples] Example 1: Assign "[1] Agriculture" to the document Document: Saving Essential American Sailors Act or SEAS Act - Amends the Moving Ahead for Progress in the 21st Century Act (MAP-21) to repeal the Act’s repeal of the agricultural export requirements that: (1) 25% of the gross tonnage of certain agricultural commodities or their products exported each fiscal year be transported on U.S. commercial vessels, and (2) the Secretary of Transportation (DOT) finance any increased ocean freight charges incurred in the transportation of such items.
Your response: [1] Agriculture: Mentions changes in agricultural export requirements ("...repeal of the agricultural export requirements that...")
Example 2: Assign "[2] Tariff" to the document Document: Amends the Harmonized Tariff Schedule of the United States to suspend temporarily the duty on mixtures containing Fluopyram.
Your response: [1] Trade [2] Tariff: Mentions adjusting the taxation on mixtures containing Fluopyram ("...suspend temporarily the duty on mixtures containing Fluopyram.")
[Instructions] 1. Topic labels must be present in the provided topic hierarchy. You MUST NOT make up new topics. 2. The quote must be taken from the document. You MUST NOT make up quotes. 3. If the assigned topic is not on the top level, you must also output the path from the top-level topic to the assigned topic.
[Document] {Document}
Double check that your assignment exists in the hierarchy! [Your response]
Table 11: Prompt template for assigning topics to a given document in the corpus. In practice, users need to modify the components highlighted in red as well as tailor the examples to the specific dataset (can be reused from prompts in previous stages). Users can also modify the prompt to strictly assign to one topic.
18


Dataset Document Groundtruth label
Initial label Reassigned labels
Bills Securing Health for Ocean Resources and Environment Act or the SHORE Act - Requires the Under Secretary for Oceans and Atmosphere to: (1) review the National Oceanic and Atmospheric Administration’s (NOAA) capacity to respond to oil spills; (2) be responsible for developing and maintaining oil spill trajectory modeling capabilities...
Technology Environment Technology; Environment
Bills Driver Fatigue Prevention Act. This bill amends the Fair Labor Standards Act of 1938 to apply its maximum hours requirements to over-the-road bus drivers.
Labor Transportation Safety
Labor; Transportation Safety
Bills Defense Travel Simplification Act of 2007 - Requires the Secretary of Defense to: (1) redesignate the Defense Travel System as the Defense Travel Accounting and Voucher Processing System; and (2) establish an intra-agency task force to recommend measures to streamline and simplify the commercial travel system...
Domestic Commerce
State and Local Government
Technology; Military and Veterans Affairs
Bills Amends the Internal Revenue Code to allow until June 30, 2010: (1) a first-time homebuyer tax credit for all purchasers of a principal residence (not just first-time homebuyers); and (2) a refundable tax credit, up to $3,000, for the costs of refinancing a principal residence.
Domestic Commerce
Housing Housing
Bills Amends the Internal Revenue Code to extend through 2014 the equalization of the exclusion from gross income for employerprovided mass transit and parking benefits.
Labor Transportation Safety
Taxation; Transportation Safety
Wiki Colonel Cyrus Kurtz Holliday (April 3, 1826 – March 29, 1900) was one of the founders of the township of Topeka, Kansas, in the mid 19th century; and was Adjutant General of Kansas during the American Civil War. The title Colonel, however, was honorary. He was the first president of the Atchison, Topeka and Santa Fe Railway, as well as one of the railroad’s directors for nearly 40 years, up to 1900...
Engineering and Technology
History and Politics
History and Politics; Engineering and Technology
Wiki Jack Banham Coggins (July 10, 1911 – January 30, 2006) was an artist, author, and illustrator. He is known in the United States for his oil paintings, which focused predominantly on marine subjects. He is also known for his books on space travel, which were both authored and illustrated by Coggins. Besides his own works, Coggins also provided illustrations for advertisements and magazine covers and articles....
Language and literature
Art and Craftmanship
Art and Craftmanship; Literature and Writing
Wiki HMS Belfast is a museum ship, originally a Royal Navy light cruiser, permanently moored in London on the River Thames and operated by the Imperial War Museum. Construction of Belfast, the first Royal Navy ship to be named after the capital city of Northern Ireland, and one of ten Town-class cruisers, began in December 1936...
Warfare History and Politics
Military and Warfare
Wiki The Grand Street Bridge was a double-leaf deck-girder bascule bridge in Bridgeport, Connecticut, United States, that spanned the Pequonnock River and connected Grand Street and Artic Street. It was one of three movable bridges planned by the City of Bridgeport in 1916 at the request of the War Department during World War I.....
Art and architecture
Engineering and Technology
Engineering and Technology
Wiki Burger King Specialty Sandwiches = The Burger King Specialty Sandwiches are a line of sandwiches developed by the international fast-food restaurant chain Burger King in 1978 and introduced in 1979 as part of a new product line designed to expand Burger King ’s menu with more sophisticated, adult oriented fare beyond hamburgers....
Agriculture, food, and drink
Business and Finance
Food and Cooking; Business and Finance; Advertising and Marketing
Table 12: Error analysis on five examples from each of Bills and Wiki datasets. Documents are truncated for ease of viewing.
19