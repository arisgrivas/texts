Lecture 1:

she wouldn't be so hard to do.

I really did because I.

right.

we're going back.

Go.

I didn't know.

Yeah, so like I got to a point where I

just I would rather all day because I'm.

I'm like you look really.

The syphoning the cold air woke you up, you know.

This afternoon I am having survey I can't and I

think that it's.

Well, I would say is this, is this I like

more, you know, service for like both.

I and my dad I think.

It's just I was I've been that.

I see you first so I don't.

Yeah yeah I I know it took so much time

and then like I thought I know um.

I, I just opened.

I little bits.

So I saw my grandparents I was equipment was great,

right?

I, yeah, yeah.

The nights?

No, I didn't didn't look like a ski I like

it, yeah, but it was below so you just walk

outside and just yeah.

So no one, no one went to or just yeah,

but yeah, no the other people were OK, but mine

is.

Oh, you know, there's no sun lights.

I mean, I.

to do.

I actually I was tricky, you know, just to, you

know, figure out.

Catch the all the pigs.

Yeah.

OK.

No.

Reading and it's, you know, they correct it correct it

so many times, so I decided to I always like

to find the so I will have like a margins

and like I have a perfect margin.

And what is yours.

Oh Yeah well.

Just what we did last time when you can use

I buy so many things.

OK.

Hi, everybody.

Uh, I'm Ryan Hubert, and this is, uh, MY 459

and also MY 360 and also MY 559.

Um, so hopefully you're in the right class.

uh, if not, then like, you know.

You can stay.

It'll be fun.

I got a lot of stuff, interesting things to say

this, this term.

Uh, OK, so I'm gonna, this class is going to

be a little bit of an introduction to this course.

And then also, um, I'm going to cover some material

that I know many of you in the room have

already seen in another course, but it is important for

this course, which is, uh, uh, how texts are stored

in digital format.

So let's just jump in.

So so much of the interaction that humans have with

each other involves text.

So, you know, obviously I mean I see many of

you, all of you basically have phones and computers and

are probably reading text on those phones and computers.

So social media is like a big place where uh

text is really important.

Political speech is also important.

I'm talking to you right now and so while this

isn't written text like you could transcribe what I'm saying

and and it would then be written text.

But then of course there's other areas where text is

really important for human interaction, literature, artistic expression, laws, legal

opinions, regulations, government records, news articles, like all sorts of

places where human interactions are moderated by and influenced by

text.

Uh, so the starting point of this class is that

texts reveal important things about the world.

I hope you agree with me that probably is why

you're here.

You saw the course description and thought, hey, I want

to understand better the role that texts play in some

kind of thing that I care about.

But what exactly do texts reveal about important things in

the world?

And that's what this course is really about at some

level.

Now, of course, the classes.

Going to be kind of technical.

Um, it's a methods class after all you're in a

methodology department in this course anyway.

um, so you know sometimes maybe the big picture is

gonna get lost because we're gonna be spending a lot

of time talking about sort of technical things, but this

is the big picture.

You're here to learn about the ways that text, you're

here to learn ways to figure out how text affects

human interaction.

Now I want to start with a, a neat example.

This is uh a paper by King Pen and Roberts

from 2013, a very famous paper in the area of

quantitative text analysis.

So that's why I'm using it as an example.

It's probably one of the most famous papers actually.

It was published in.

Um, top political science journal.

uh, so, um, this is a screen grab from the

paper which depicts a whole bunch of, uh, Chinese social

media apps and websites.

Um, so what these authors did is they, uh, somehow

figured out how to.

Get like around 12 million social media posts from thousands

of Chinese social media websites and apps in the period,

obviously it was this paper was published in 2013, so

this was prior to that, so sort of the 2010

era, 2010, 2011 era.

Um, and so they collected these social media posts for

a larger project they were working on, but when they

started looking at the data, they saw something really interesting.

Uh, which were pretty clear patterns of censorship in the

data.

So this isn't what they initially meant to study, but

studying censorship is very hard because oftentimes when governments censor

things online, you don't actually see them, right?

So studying censorship is hard just like studying corruption is

hard.

These are things that people try very hard to hide.

governments try to hide them.

People who are engaged in corruption try to hide corruption.

So these are topics that social scientists often struggle to

study because there's no data on it.

But when they collected these social media posts for some

other project, they noticed these interesting patterns in the in

the data because they were able to collect social media

posts before they were censored, so they got to see

when social media posts ended up disappearing.

So they see they come into the data set and

then they see they disappear.

So, so they thought to themselves, OK, cool, we have

this interesting textual data.

So what we can do, we can see which posts

were censored, right?

So this is like, you know, just a pattern that

appears in their data, but they also have the text

of the post so they could dive into the content

of these posts to figure out what kinds of things

are being censored and what kinds of things aren't being

censored.

So it's a very rare opportunity to study something that

political scientists think a lot about.

Censorship but don't have data on, right?

And so what they were able to find out when

they looked at the content of the posts that were

censored is that they were disproportionately posts that were about

collective action type events, right?

So the the the the state was intervening to censor

posts when they potentially could have led to.

Collective action, protests, large social movements and stuff like that.

But that I mean that may seem kind of obvious,

but one thing that was sort of less obvious is

that actually there was not that much censorship of criticism

of government policies, which you might think like, OK, that

the state is engaged in censorship of social media posts.

They might want to censor posts that would criticise the

policies that they that they've put into place, but that's

actually not what they found in the data.

That there was actually not that much censorship of criticism

of state policies of government policies, but there was a

lot of censorship of um posts that were about things

that could have led to collective action, protests, stuff like

that.

So it's sort of a neat project because they, well

they didn't actually intend to study this, right?

They had collected this data for other reasons, but then

they found this interesting pattern in the data and they

could look at the content of what these posts were

about using quantitative text analysis tools to be able to

figure out what kinds of things were censored and what

kinds of things were not censored.

So that's kind of neat.

It's a neat application of quantitative text analysis.

So that gives you a little taste of the kinds

of things you can do with with quantitative text analysis.

So what I want to do for this lecture is

I want to first go through the course logistics, the

boring stuff about what we're going to, you know, how

this course works, and then I want to talk big

picture like why would you want to do quantitative text

analysis in a research setting or in a job setting,

so on and so forth, and then I'm going to

talk about some kind of I'm going to call foundations,

so character encoding, digitally storing texts, and working with text

in R.

So this is like background stuff that you need to

know in order to start doing quantitative text analysis, which

is basically amounts to, you know, how do we digitally

store texts, how do computers read texts, how do you

work with texts in R using the packages that we're

going to use throughout the, the, the course.

OK, cool.

So let's talk about the logistics.

So lectures are going to be from our Mondays obviously

from 10 to 12.

You know that if you're here in this room, of

course we're going to have classes or seminars.

I tend to call these seminars.

I think officially they're called classes in the timetable.

Friedrich, the other instructor, calls them classes, but these are,

these are the things that are not the lecture that

are things you have to go to.

They're only every other week, so week 2479, and 11,

and they're 2 hours.

You should look at the timetable to figure out where

you need to go and when you need to go

to your, uh, seminar.

Presumably you've signed up for one already.

There's going to be no lectures or classes during reading

week, week 6, which is the case for every term

at LSE.

If you want to audit the class, you have to

submit a request to the department.

If you audit, you can't attend classes slash seminars.

That's just department policy.

I think if you're a PhD student.

You might be able to get an exception to that,

but we can talk about that afterwards.

OK, so this class is co-taught between me and Friedrich

Gecka.

Um, so I'm Ryan Hubert again, this is my email

address.

I'm the course convener for MY 47459 and 559.

Um, so I'm going to lead all the lectures and

then all the seminars for weeks 1 through 7, and

then Friedrich is the course convenor for MY 360, and

he's going to lead all the lectures and classes slash

seminars during weeks 8 through 11.

So he'll do the latter part of the class.

For both of us, you can book office hours through

Student hub.

I hold my office hours on Thursdays.

Um, so I'm gonna, in a minute I'm gonna tell

you who to contact for what because it's code talk

class so obviously we're sharing responsibilities, but before we get

there, I want to talk about the pre-recs for the

class.

So, um, we're going to assume in this class that

you have access to a laptop that can run the

required software for the course, OK?

Um, and you want, and we want you to bring

it to lectures in class because we're gonna, we're gonna

be working with code a lot in this class.

Um, we're also going to assume you're comfortable with our

in our studio and know how to work with our

markdown and Quarto documents.

Now, um, we're going to be using our markdown slash

Quarto documents in this class.

If you've not experienced this before, I mean, it's not

the end of the world.

We'll I'll kind of show you how they work as

we go.

So, um.

Don't worry too much, but about these particular types of

documents, but you need to be comfortable with R.

We're just going to assume that you've done the preparatory

coursework in R and we're just going to, you know,

go forward with that assumption, and we're going to be

working in our studio mostly, which is the app on

your computer that runs R, um, or it's an app

on one's computer that can run R.

You don't have to use our studio, but we are

going to in this class, um, for those of you

who are.

Familiar with our markdown, but have never seen this word

udo.

Quordo is just like the new R markdown that's fancier,

has more things in it, but it's, it's basically exactly

the same.

You can copy and paste our markdown into a Quarto

document, but it does more things.

And so slowly I'm trying to actually move towards Quordo

documents, um, but it's basically the same thing.

Um, we're going to assume you're capable of using Git

and GitHubb again, if you've never used it before, not

the end of the world.

We'll kind of talk you through it.

We're not actually going to use it for a tonne

of stuff in this class, as you'll see, but we're

going to assume you're capable of using it in the

sense that whatever we tell you to do with GitHub,

you'll be able to do, and it's not going to

be super complicated stuff.

This is more important.

We're going to assume you have some exposure to elementary

linear algebra and have taken quantitative methods courses up to

including the equivalent of MY 452, which is a course

on regression.

Now if you look at this and you say elementary

linear algebra, what don't worry too much about that.

We just want you to know what a matrix is,

what a vector is, and how to conceive of multidimensional

spaces in the classic linear algebra sense.

The end of the class is going to rely more

heavily on linear algebra topics, but there's also going to

be a refresher in week something we.

9, I think um that's a kind of math refresher

that will give you basically what you need there.

Um, we're gonna, but on that note, we're going to

just assume you're, you're in a methodology class, we're going

to assume you're comfortable engaging with some amount of mathematical

notation and mathematical concepts, and that's just how it is,

right?

So, um, if, if that, if, if that really throws

you off, then uh we should talk because that's just

part of this class.

And then we're also going to assume we have computing

habits that minimise the risk of problems.

And so I mean like, you know, you, you're relatively

organised and so you're not going to have code that's

corrupted and you're not going to lose all your stuff.

You back things up.

We, you know, like, please, if you don't do it

already, get in the habit of backing things up and

making sure things are organised and that you're um on

top of your computer hygiene.

OK.

Now, so the course resources are as follows.

We have a course website which is available at this

link here.

We also have Moodle pages.

Um, the Moodle pages are a mess.

I'm sorry.

I don't like Moodle.

It's weird.

It doesn't function very well.

This, the website is where we're basically putting everything, OK,

but we have these Moodle pages because we're supposed to.

Um, and so the Moodle page, I'll just get, I'll

tell you really quickly what happens on Moodle.

We send course announcements through Moodle, so make sure you

have your notifications on to receive those announcements, and then

we'll put some supporting materials on Moodle, but only stuff

that we can't put publicly.

Like there's some, you know, like there's data that's proprietary,

we can't put it on the internet, we'll put it

on Moodle, but we'll link to it usually.

Everything else is going to be on the course website,

the course schedule.

Materials for the lectures and seminars, so that would be

slides like this and slides that could be for lecture

or code that you would work through a lecture.

There are reading lists for each topic.

I'll show you the website in a second, but their

reading list for each topic.

They're divided up into primary and further, so primary readings

and further readings.

I just want to say that like the course is

going to drop very, very heavily from this textbook by

Grimmer Roberts and Stewart.

That's awesome.

It's just going to be abbreviated GSR everywhere, so you

might want to get a copy if you don't have

one, especially if you want to do this stuff more

seriously going forward.

Um, so if you want to think that if you

want to, if your question is, do we have a

course textbook?

I guess this.

But here's my philosophy about readings like, uh, you know,

you can do the readings, you should do the readings,

but your job mainly is to learn the material we're

teaching you and so.

You know, I'm, you know, in the, in the mode

of being mostly a graduate instructor.

My, my philosophy is like it's kind of like everyone

has different learning styles and if you're the kind of

person who really, really likes readings as a way to

learn material, great, we have a whole bunch of readings

available to you, but we're not like, you know, in

the assessments, we're not gonna make sure you did all

the readings like that's not the point.

We're gonna make sure you understand the concepts, can use

the tools, um.

Really like at a deep level like get quantitative text

analysis.

It's not a course in regurgitating things that you got

in readings, OK?

So I don't want to say you don't have to

do the readings, but I don't want to say you

have to do the readings either, but that textbook will

be really useful for you if you do find readings

as a nice way to complement your, your learning journey.

Um, and then there's also going to be links to

sample code exercise in various data sets on the website,

um, when we can post them publicly, otherwise they'll be

in Moodle when they're, we don't feel comfortable putting them

on the internet.

Yeah.

Oh, OK, probably not.

I will, I will, I'll get to that, um.

I don't love the fact that there are 2 pages

because announcements we should just send everybody doesn't make, you

know, like doesn't make sense to kind of silo it

off that way.

But can you send me an email and ask me

to figure that out, um.

OK, so assessments, there's two main assessments.

The formative is there's going to be a problem set

later in the term.

We're going to give you solutions for that.

Um, so, uh, more information will be coming later on.

And then the thing that you probably care about is

there's one summit of assessment, which is an exam that

happens in the spring term and it's worth 100% of

your mark.

There will be different exams for 360 and 459, 360

is an undergraduate class.

459 is a graduate class, um.

So we will be different exams for those two.

please review the department and school policies about assessments.

Keep in mind though, distinctions for graduate, first class honours

for undergraduate, that's 70 to 100 marks.

Typically only go to the students who perform exceptionally well.

In principle, it's possible for everybody to get in a

distinction.

Um, but especially if you're not familiar with the British

system, yeah, well, you should be by now, but if

you, if you still aren't, um, 70 to 100 is

like, you know, only a few students usually get that,

um.

I felt like it was important to put this in

here because I come from the US system where that's

like that's nuts, like, you know, in, in the context

I was in before, like most students got between 90%

or not most, but like, you know, 40% of students

got between 90 and 100%.

That's not how it works here.

So in case you're not aware of that yet, uh

please do keep that in mind.

Um, the grade scale is a little bit different than

the US context specifically, yeah.

Sorry, yes, it's going to be administered by the university

using the university's processes.

I don't know anything about that.

All I know is I have to provide them with

an exam, um, but then you will just figure out,

however you figure out exams for other classes that are

administered by the school, it's going to be in the

same process.

But sorry, it's my first methodology course, but is it

going to be like solving a math problem or writing

paragraph or writing code?

It to be a written exam, so it's going to

be an exam in the format that, uh, OK, let

me back up here and just say the school only

allows us to do this kind of written exam.

So this is not a, this is, if I had

my way, I would not do it this way, but

this is what the school requires.

So it's going to be a pen and paper exam.

That is weird for a class where there is a

lot of coding, especially a class where a lot of

students want to learn how to do coding so that

they can do quantitative text analysis, uh.

Take it up with the school because it is, it

is a point of frustration for me as well that

this is how it has to be, but you're going

to get a pen and paper exam.

So what it's going to look like is some amount

of calculating things, so doing math, although it's not like

doing formal proofs, it's going to be more like, you

know, calculate averages and stuff by hand with little tiny

small data sets that you could do by hand.

The other thing is you're gonna get code and you're

gonna have to like, you know, identify problems in the

code and stuff like that.

Um, so that's the kind of exam that it's gonna

be paper exam, pen and paper, yeah, and the formative

is supposed to be like something similar to the summative

or it's like, so the formative is actually more like,

you know, practising coding, learning the skills that presumably you

want to learn in this class.

There will at some point later in the term we're

going to give some because this is so this changed

prior to me getting assigned to this class, the assessment

format has changed.

This is new this year.

There isn't there aren't past exams available, so we're going

to provide some example problems so that you'll know what

the exam will look like.

This is a particular problem to this year, but next

year there will be a last year's exam that people

can look at, but you will get some additional like

practise problems so that you'll know what the exam will

look like.

So you get that.

Yeah.

Will there also be like written questions about conceptual understanding.

Yeah, yeah, for sure.

The mix of like doing things, conceptual things, finding problems

in code, so on maybe completing some code.

It's awkward.

You got to do it with pen and paper, but

hey, you know, Again, that's how they do it here.

So I, we're actually, um, I'm trying actually to get

them to allow us to do a different kind of

exam, but it takes time to figure that out.

OK, so generative AI, it's not super important, but of

course we're going to allow the use of GitHub copilot,

chat, GPT, other generative AI tools in this class.

It's sort of weird to say that we allow this

because it's not allowed on the final exam.

So like what does it matter for your assessments, right?

But Again, we think it's a really important in this

department we're adopting an approach that like this is the

this is the world now and and students need to

learn how to use these tools in their own learning

process and in their own work and so um of

course we don't have any bars against using it in

this class, but, uh, beware you need to know it

tells you wrong things all the time.

It gives bad code a lot.

I mean, I know from personal experience they get bad

code from JCBT all the time.

So you gotta have to be able to check the

outputs that it's giving you.

So it requires some level of knowledge of your own

that you're here to learn, um.

And you're sharing your, your, your thoughts and your ideas

and stuff with a company, maybe multiple companies who are

going to take your data and use it for their

own purposes.

So I don't know if that kind of weirds you

out.

Maybe you don't use it or pay for it, I

guess if you pay for it, there are options for

having your data protected.

You can use it on the final exam, so don't

rely on it too much in this class because then

you're going to get to the final exam and you're

going to not be able to do it.

The other thing I want to say is that.

You this is in general across all courses using generative

AI to do work does not free you from your

obligations to act with integrity in your academic work.

And so there are ways that you can use these

tools that would constitute academic misconduct in the same way

that it would be academic misconduct to just have somebody

else write something for you.

It's academic misconduct to have Chad GBT just write something

for you.

So you know, to be clear, when you're allowed to

use these tools, you're still supposed to use them within

the boundaries of what's acceptable in terms of academic conduct,

um, but we as a department are trying to move

in that direction where we incorporate this into our, into

our, uh our courses.

So it's a little bit of an experiment too.

So feel free to let us know how, you know,

if you learn lessons about how best to use chat

GBT and other Gen AI tools that that would be

great.

It's my impression that people substantially younger than me, I

use it instead of Googling things now.

Um, I, that's cool, but also it says wrong things

a lot, so just be careful, you know.

OK, so for administrative support, auditing registration extensions, late assignments,

department school policy questions, that's the email address you want

to contact.

For questions about the course platforms including Moodle, website, or

GitHub classrooms, including why is 360's Moodle not active, email

me.

For administrative issues related specifically to 459 or 4 or

559 that don't fall into these two categories, email me,

and then for administrative issues related to 360 that don't

fall into these categories, email Friedrich and then questions about

course material like substance come to office hours and you

can schedule those on student.

OK, so course outline, see the website, but I do

want to say I'm going to show you the website

in a second and I'm going to briefly tell you

the outline, but there's going to be overlap with some

other courses here, specifically 472 and 474.

That's, that's a feature, not a bug.

It's kind of on purpose.

And the thing is this class is about text and

so even though there's a course on machine learning 474,

and we're going to do some machine learning in this

class, we are going to be doing machine learning and

service and doing text analysis.

Right, so it's different and we're going to focus on

the peculiarities of applying these kind of more general methods

to the context of text, right?

So everything's gonna come back to, OK, so here we're

dealing with text specifically and what kinds of important things

we have to consider when dealing with text, but there

is going to be some overlap with things that you

may have learned in 472 or will be learning in

474, and that's just like the nature of these methods

is they're all actually kind of related.

I mean they're all sort of drawing from each other.

And that's just how it is.

Um.

And also a lot of people in the didn't take

472 or 474.

So you know, like we got to, you know, cover

some some basic territory.

I think for those of you who are at 472,

you're going to find the second half of this class

very frustrating because I'm going to talk about character and

coding and digital storing of texts and like I saw

that already, but you know what?

I was so long ago in my brief part of

my graduate training where they actually told us how to

Teach, which is way too brief for the amount of

teaching you actually do in your career, but repetition apparently

is how people learn.

So hey, you know what, I'm repeating some of the

stuff you've already learned in 472, but repetition will be

good for your learning.

Take my word for it or take somebody else's word

for it, and I'm just telling, I'm like the middle

person telling you what they told me.

OK, great.

So, oh, let me show you the outline.

Of the, of the course.

No.

Cool.

All right.

So this is the course website.

There's a lot of stuff on here.

Let me just tell you quickly how this works.

So if you, at the very top of it, there's

like the outline of the course.

So this week, introduction of foundations.

Next week, quantifying text.

That's basically like how do we get text into a

quantitative format.

Third week exploiting word meanings to do some cool things.

Um, words have meanings usually and so like what can

we use those meanings to measure in text?

4th week classifying text into categories, right?

Um, so typically I mean in the case of week

4 we classifying text in.

Known categories.

We have a set of predetermined categories and let's classify

the text into those categories.

Week 5, scaling latent traits using text.

Well, so the classic example of this is trying to

figure out the ideological content of texts, right?

So where does a text fall on an ideological spectrum

from left to right?

That's like the classic example of scaling.

Then we're going to do text similarity and clustering.

So this is, uh, you have a bunch of texts.

Can we cluster them into, into groups of texts that

are similar to one another?

And how do we measure how similar texts are to

each other?

Weg probabilistic topic models.

How do we take a bunch of texts and figure

out what those texts are about using quantitative tools?

So what topics are those texts about?

Week 9, 10, and 11.

These are all, this is kind of like new stuff

sort of developed last year, but it's going to be

further developed this year.

So you can't teach a class without like talking about

the elephant in the you can't teach a class on

quantitative text analysis without talking about the elephant in the

room, which is large language models and chat GPT and

the.

of generative AI for text-based communication.

And so weeks 9, 10, and 11 are really about,

you know, the culmination is week 11 where we're going

to give you a sort of a high level overview

to like, like what's like what's going on with chat

GPT, how does it work?

Um, but this is all sort of background material to

get to that.

This background material is also important in its own right,

so you'll see when we get there, um, but sort

of weeks 9, 1011 are kind of like at the

boundary of where we are now with quantitative text analysis.

So it's a little more experimental.

And also, let me just say the social science of

these 3 weeks is a little bit less clear because

we are still figuring out how to use this stuff

in social science research.

Um, but of course we cannot teach a class on

this without talking about large language models, right?

Do you agree with me?

Yes, I mean that would be ridiculous.

Um, OK, cool.

So the way the website works, and I might change

this to make it a little bit easier at some

point, but for week one, for example, Say you're in

week one, you click on it, it takes you down

to week one in the, in the description.

Here's a quick description of week one.

Any resources and readings and stuff here.

And then if you click on the link there, it'll

take you to a folder on GitHub, uh, where you

should see materials for the week.

And so the slides are the only materials that I

have posted here.

So, um, you can get the slide there.

Cool.

Now Look at that new version of Chat GPT.

Listening to me.

OK, so, uh, preparing for lectures and seminars, make sure

that you have both our and our studio installed on

your computer and you know how to use them.

They work fine.

Make sure you have a GitHub account and you know

how to use Git GitHub.

Again, we're not going to use it a tonne in

this class just because the primary assessment is this paper

exam, which we're not going to be using GitHub, but

you will use it for the formative problem set and

we will be using it for various coding activities, um.

We're not going to teach G and GitHub in this

class, but we will provide support for it.

So you know, we're kind of kind of awkwardly give

you a little bit of information about how to use

Get GitHub just to get you through the class.

But this isn't a class on getting GitHub, so, um,

you know, just do a little extra reading if you

need to, um, when the moments when the moment is

right and you you actually need it.

Um, so during the course we're going to expect that

you're constantly reviewing the course materials and other sources as

needed to learn the concepts.

I want to say quantitative text analysis at this point

is a massive topic, so much we can cover, like

multiple textbooks' worth of stuff.

So we're going to move fast and we're not going

to cover everything.

We're just going to cover the, as I'll talk a

little bit more in detail soon.

We're going to cover the basics of um.

Of how quantitative text analysis is being used in the

context of social science research.

I mean, this is LLC, it's a social science university,

so that's our primary, our primary orientation.

So a lot of things that that you might consider

quantitative text analysis in some contexts we're not going to

cover here because they don't have like clear social science

applications or they haven't yet and it's only 10 weeks

we got, so you know you got to pick and

choose.

OK.

So now that I've done, I'm done talking about course

logistics, I'll pause for any brief questions.

OK, cool.

So what I want to do now is I want

to ask a big question, which is why quantitative text

analysis, why should you take this class?

Why should you want to use quantitative text analysis?

Um, So let me jump into it.

I'm going to tell you what I think.

Oh well, actually let me tell you about myself a

little bit because I don't think I did.

So I'm a professor in the methodology department.

I just started this year.

Previously I was a professor at the University of California

at Davis in political science.

So I'm a political scientist.

Um, my research is on political institutions, mostly in the

past I've studied US political institutions, but I share my

research or my research focus is sort of in two

buckets.

I do a lot of game theory, which is purely

theoretical work, and then I do some like data science

type stuff.

Now my data science type research is why I'm here

in this department, why I'm teaching this class, mostly focused

on American courts, because I have like a lot of

data there.

I spent a lot of time collecting data there, gotten

very familiar with it, um.

So I can talk to you for days about that

if you care about that.

Um, the game theory side of me is more focused

generally on political institutions, so it's not like US specific,

although oftentimes it's framed as US specific for practical professional

reasons, um.

I have done some text analysis in the past.

I have not done a tonne of it.

I of course have been trained in text analysis.

I have thoughts about text analysis that I will share

with you over the course of the term.

And I think it's going to be interesting because you're

going to find you're learning about text analysis from a

sceptic, and that's going to be useful because it means

that I am going to be kind of constantly poking

at stuff, which I think is pedagogically useful because you

can see what a critic might say about various things.

Now, I'm not like so much of a critic of

text analysis that I can't teach this class or I'm

fundamentally, you know, I object to any of this.

I just, I, I have a particular perspective that I

think is kind of interesting for a class like this,

and it's probably the first time in a long time

this class has been taught by somebody who has a

little bit of a sceptical orientation.

So, um, I hope you, you like that, uh, and

find that a little bit, uh.

Uh, intellectually rewarding.

We'll see.

OK, so let's jump in.

Why quantitative text analysis?

So let me make the case here.

So I think it should be obvious why we would

want to study text.

I mean, I already mentioned at the beginning of this

class, so much of human interaction is structured by textual

data, right?

People, you know, you're writing text to people all the

time.

You're reading stuff.

You're all like on your computers reading things, presumably hopefully

you're not watching YouTube videos right now and you're actually

reading stuff online, in which case you're interacting with texts.

OK, so.

I think it's obvious why we would want to study

text, but why would you want to do it quantitatively?

This is not usually people's starting point when they think

about engaging with text.

They think, oh, I'm just going to read it and

I'm going to think about it and I'm going to

have a more qualitative engagement with it.

So why might you want to study quantitatively?

So Justin Grimmer, who is a political scientist at Stanford

and kind of one of the like the sort of

Lead people in text analysis in political science anyway.

He has a Texas haystack metaphor that is actually quite

useful.

So if you have a big haystack, right, so a

lot of pieces of hay, looking at an individual piece

of hay is easy as a person, right?

So if you need to find something in a stack

of of hay, you can look at each of the,

you know, components of this stack of hay, and you

can make sense of whether it fits what you're looking

for or not.

Um, so humans are really, so if you think about

texts as like a big stack of hay, humans are

really good at looking at a specific straw of hay

that is a specific text and making meaning of it.

They can read it, they can make, they know, they

can understand something that's coming from it.

But and computers actually struggle to do this, right?

Computers struggle to like look at a specific text and

like know what it means, right?

I mean, computers are not, they're not like creative beings

like we are, OK.

But Sometimes you want to organise a haystack or you

want to find something in a large haystack.

And so the analogy when you apply it to text

is like you have a whole bunch of texts and

you need to find the needle in the haystack or

you need to characterise this large haystack in some way.

So if you have a lot of texts, you want

to find some kind of specific element of text or

a specific kind of text or you want to look

to see a common theme across these texts, and you

could have millions.

Text.

So if you're doing quantitative text analysis on social media

like the King Pen and Robert's paper I mentioned at

the front, they have like 11, 11+ million social media

posts.

They cannot read them individually.

OK?

They would be very good at reading any one of

them and understanding what they mean, but they can't read

them all.

So computers are really good at taking a large amount

of, uh, a large amount of data and sort of

summarising it in some way that a human could better

understand.

So this course is really about using Compute computational tools

to organise the haystack to make sense of a large

amount of text.

If you were, if you have a project in mind

as a research project that involves 5 texts, let me

tell you, don't use quantitative text analysis.

Just read the documents and do your research based on

your own reading of the documents.

But if you have a project in mind that involves

thousands, millions of texts, this is where this is like

what the tools in this course are useful for.

Now, um, the basic way to summarise all this is

quantitative text analysis improves reading.

So it allows humans to focus on key things by

pointing attention to ideas, concepts, organisations of a large amount

of text, OK.

I think this is actually sort of an interesting side

point.

There are a lot of parallels to how we're starting

to think about generative AI, right?

Like we all know that like, you know, chat GPT,

you can ask it to do something, and I mean

you've probably had this experience.

I've had this experience a lot of times where like

I go to chat GPT, ask it something, it gives

me a really bad answer and then I'm like, oh,

the problem is me.

Like it gave me the answer that like I guess

in some sense makes sense to a computer.

But it was like I asked a bad question.

Like my prompts wasn't, wasn't good enough to get the

answer I wanted.

And I think there's parallels to quantitative text analysis, right?

Like you, like, it's really like quantitative text analysis is

really good at like organising large amounts of data, but

it, it's only as good as like the, the sort

of the intellectual power you put into it on the

kind of analyst side.

And that's going to be a constant theme in this

class.

We're like powerful set of tools for organising a haystack,

organising a large amount of documents, but only as good

as the sort of intellectual power you're willing to put

into, into using those tools.

Now, um, there are, I would say, I think 3

like cultures of quantitative text analysis that they're worth mentioning

right off the bat.

So quantitative text analysis, I'm going to, I'm just going

to start saying QTA.

It's kind of awkward because there's qualitative text analysis and

quantitative text analysis.

They both have a Q in them.

But when I say QTA here, I mean quantitative text

analysis because that's what this class is about.

If I mean qualitative, I will just say qualitative.

OK, but from now on when I say QTA, I

mean quantitative text analysis.

It's built on a foundation largely from computer science, OK,

from the field of computer science.

That culture of quantitative text analysis is usually referred to

as natural language processing or NLP, and it's typically the

work from this culture of QTA is focused on more

technical issues with various industry and and kind of linguistic

applications.

A lot of the work behind large language models, chatGBT,

etc.

that's really going on in the computer science culture of

quantitative text analysis and is being developed for industry applications.

However, social scientists are starting to think about like bringing

into the social science culture, but you'll hear more about

that at the end of the class.

Jarosky and Martin have this online textbook, so 2000 XX

because they just update it all the time, so it's

not like a standard book that has like a specific

publication date, so I just put XX, presuming they won't

keep publishing it into the 2100s.

It's the canonical textbook for natural language processing.

So if you have some sort of like sort of

basic NLP type thing you want to do with text,

grab that textbook.

It's very big.

It's got a lot of stuff in it.

Now quantitative text analysis come to the liberal arts, so

like the non-computer science, non hard science world, relatively recently.

So there's like two cultures there.

So one is called digital digital humanities.

So this is often people in the humanities who want

to use quantitative text analysis to study texts in depth,

right?

So think of somebody in in an English.

at a university who wants this, who has a particular

focus on a particular kind of literature and wants to

use QTA tools to analyse a novel that they're really

interested in, right?

But they use the QTA tools to really understand features

of the individual texts.

Then there's this second culture within the liberal arts that

is really based in social science and social scientists use

QTA to learn about bigger things in the social world.

And this is, I think, often referred to as the

text as data approach.

The reason why it's called that is it's sort of

thinking of text as one type of data for studying

bigger things.

People in the social science culture of QTA, the culture

we're in in this class, don't necessarily, are not interested.

Text per se.

They don't want to, they're not like people in the

digital humanities that want to use QTA to study in

depth one particular text.

They think of text as a means to an end,

the end being learning something about the social world, learning

something bigger about how things work in the world.

And the texts are just one way to get at

that, OK?

So we often people have been referring to this as

like the text as data approach.

This class, so, so these are three distinct cultures, the

kind of NLP culture, the digital humanities culture, and the

social science like text is data culture.

And um they're intertwined with each other and they learn

from each other.

So I don't want to, I don't want to draw

the barriers to, to, um.

To strictly because obviously people who do social science with

QTA have learned a lot from the NLP.

I mean this NLP is basically informed all of this,

right?

And so, and there's a lot of back and forth

and a lot of computer scientists are now taking lessons

from social science.

Adapting them for NLP.

So there's a lot of intertwining here and actually what

makes QTA interesting is that there's a lot of this

active research going on where there are sort of different

cultures talking to each other, critiquing each other, trying to

improve, and that's always like a good thing for advancing

knowledge.

But you should be aware there are these three cultures.

The unapologetic, unapologetic starting point for this course is that

we're here to do social science using QTA methods, OK?

So we're very clearly in the social science culture within

this class.

That's the word LLC.

That's just how, I mean, this is a social science

university that's, this is our job here is to train

social scientists.

And there's this book, Grimmer, Stewarts, and Robert that we'll

be using a lot in this class that was built,

that was written for the social science culture.

In fact, it's as far as I know, it's the

first.

Textbook written for that culture specifically.

It's a social science textbook about using quantitative text analysis.

So it's very, very useful, but we're not, we can't

ignore the elephant in the room.

I've already mentioned this larger language models or something here

that's going to change the world, and we, it's still

largely being developed in the CS culture and computer science

culture.

It's expanding rapidly within the social science culture.

I can tell you like half of my colleagues are

trying to think about how to incorporate.

Uh, Generative AI, chat GBT, etc.

into the research as we speak right now.

Um, so at the end of this course we're going

to talk about this, and Friedrich, who's actually one of

the people trying to work on pulling large language models

and chat GBT into his own research, is gonna teach

you about, uh, these large language models and and the

kind of methodological underpinnings of them.

OK, so let me briefly tell you the, the, what

are the three core assumptions of QTA and the social

sciences, OK?

So the 3 assumptions that are underneath everything we're gonna

talk about in this class.

So one is that texts represent observable implications of some

underlying thing of interest in the social world, OK.

So for example, texts represent attributes of the author, right?

So texts often reveals something about the author, their ideology

or whatever, you know, like their, their preferences about the

world, etc.

They reveal sentiments, emotions, right?

You've obviously heard people get very emotional speeches in the

past and it reveals emotion, right?

Also, the salience of political issues is revealed through text,

right?

So this is the first core assumption here is that

texts are just observable implications of some larger social things

going on, OK?

And so hopefully we can then learn about these larger

social things by examining the text that gives us to

gets us to assumption 2.

Which is that text can be represented by extracting features

from them and then using those features to learn about

the world.

But we're going, as assumption number 2, we're going to

assume that we can extract features from text.

OK.

So the most common approach for doing this is called

the bag of words approach that I'm going to talk

about more in the next lecture.

Basically what in that.

In that framework, the features of a text are the

words that are used in that text.

And then those words are the things that we can

then use to learn about the social world, OK?

But there are many other possible definitions of features of

text.

Does it just have to be the words in them.

So one example is word embeddings, which we'll cover at

the very end of the class.

But there are other examples like you might think of

other features of text like, you know, parts of speech,

you know, like how many verbs are used, you might

think of like, um, like how many names are mentioned

in a speech, right?

So rather than focusing on individual words of, of, of

text and trying to extract lessons from them, you might

have other features of those texts.

You want to extract, but the core assumption is that

you can extract features from a text which you then

learn to use about.

Then you use to learn about the text.

That gives us to assumption number 3, which is that

conditional on accepting number 1 and number 2 that texts

are observable implications of bigger things in the world and

texts have features you can extract from them, would be

useful for learning.

You can then create quantitative quantitative data sets after you've

extracted those features to then learn about the underlying thing

of interest.

And the primary way we'll do this, we'll talk about

this again in more detail next week, is the document

feature matrix, which is a matrix, a spreadsheet, a table.

Where every document is represented as a row, that's a

unit of observation.

Every column is a feature of the document, and because

we're going to mostly be working with the bag of

words framework, each the features of the documents will be

the words used in them.

And so each column, each variable is a word.

And so for each document you have a, you have

a row that says for each of the possible words

that could have been used, how many times those words

were used in that document.

The word counts.

And then you can get this, once you get this

matrix, this matrix of word counts.

You can then do some statistics, right, because you can

do statistics with matrices.

Cool.

Great.

Now, There are criticisms of QTA, all right?

And I'm a good person to represent these criticisms because

I find them sympathetic.

OK.

So I like this quote a lot from 2019.

So if you click on, so these red, red here

usually means a link so you can click on them.

So this will take you to the, to the full,

to the, the actual paper.

But 2019 argues that in quantitative text analysis, quote, What

is robust is obvious in the empirical sense, and what

is not obvious is not robust.

OK, I really like this very simple way of framing

the what I think is the largest critique of quantitative

text analysis, but let me try to say it in

my own words, so.

In my words, this critique is saying, when we learn

something of interest from quantitative text analysis, it's often not

replicable, meaning that other people can't find it in other

contexts in other texts.

Um, or it doesn't really capture something general or something

real, right?

And when it does capture something general, it's really obvious.

So the critique is like, OK, so we can use

these quantitative text analysis tools, sure.

When we learn something that is real, that's actually like

we feel confident that we learn something about the world,

it's really obvious what we learned.

So, OK, boring.

And then when we learn things that are interesting and

maybe not very obvious.

Other people can't get the same results.

So it's not, maybe it's not even real.

Maybe it's, it's, it's just specific to the data set

that's being looked at.

A more cynical way to put this is that quantitative

analysis involves too many researcher degrees of freedom.

That's the kind of euphemism if you hang around social

scientists a lot, you'll you'll hear people say this, but

it's meant in a really negative way.

It's meant that like, oh, OK, like this project, there's

a lot of degrees of researcher degrees of freedom here,

meaning that the researcher had a lot of discretion to

choose which direction to go and so you're therefore a

little bit sceptical about what they've told you because there

were all these choices they made along the way that

they maybe weren't very transparent about, um, and so I

don't know, do we believe what they're telling us?

On some backing up here, like this is a this

kind of critique is a really big problem for social

scientists because the whole point of social science is to

learn.

General knowledge about the world.

Like all of social science is looking at this very

nuanced, complicated, you know, complex world and trying to distil

from it like big lessons, big lessons that apply across

a wide range of contexts.

And so if you're doing quantitative text analysis and you're

getting these very bespoke results depending on which data set

you're, you're working with, then are we learning general things

about the world?

I don't know.

Now digital humanities folks might say, oh, that's fine, we're

just, you know, I want to just learn about what

the text is saying.

Cool.

The social scientists want to do more than that.

They're not learning about text per se, they're learning about

the bigger social processes that are happening out, you know,

in the, I always point to the sky, but like,

you know, happening above us.

And we want to know that the things we're learning

are like they hold across a wide range of contexts,

yeah, but what is the reason why you can't really

access this higher reality or social order?

Is it because of the Uh, too many degrees of

freedom of the researcher or because of the data itself,

the text that has some characteristics that doesn't let you

do that.

Yeah, that's a good question, and that's a question that

I don't have like a clear answer for.

OK, so first of all, just backing up, stepping away

from quantitative text analysis for a second, like the core

problem of all social science is that we, we care

about these sort of bigger realities, but what we have

to work with is like Like disparate data sets and

disparate empirical observations that we have to try to piece

together to learn about the bigger processes.

The presumption of social scientists usually is that these bigger

processes aren't directly observable, and what we see is like

little things happening in the world that might speak to

those bigger processes.

And so trying to get from the things we're observing

in the world to the bigger processes, that's like all

of statistics even like that's the whole point of like

sampling and statistics, right?

So.

Um, to put to make it a little bit more

technical, um, so this is a bigger problem than just

QTA, OK, but there are among social scientists who care

about methodology and talk about methodology, there are, there are,

for lack of a better word, like better and worse

ways of trying to learn about the bigger processes using

observable implications in the world.

And I think a lot of critics of QTA are

saying QTA is itself not a great way to learn

about the bigger processes because of something about the nature

of either the data, the texts themselves, or the set

of tools that have been developed to work with those,

with those data sets.

And actually that's my next point here, which is like

this is a famous diagram from a paper by Grimmer

and Stewart from 2013.

It's like a It's a paper that's kind of an

overview of Texas's data methods.

It predates this textbook by 10 years, as you can

see, but this was kind of the back in the

day before the textbook existed, this is the paper that

people would assigned, and this is the diagram that's in

the paper.

Now I look at this and think, OK, we have

a flow chart here, but this, this is like partly

the criticism of QTA is about this, about the fact

that quantitative text analysis at some level is just a

whole bunch of stuff like different tools.

It Mean one thing.

It's like you could be doing ideological scaling or you

could be supervised or unsupervised, or you could be doing

finding known categories.

You can either do it in a supervised way or

in a dictionary way and oh maybe if you want

to do individual classification use this and oh maybe you

want to do clustering and then so this is like

an impression a lot of people got to get of

QTA is like it's just like a.

Massive stuff partly because it comes from the natural language

processing world where a lot of computer scientists are focused

on discrete, trying to do discrete things with text and

so it's kind of come over to social science in

that way as well.

There isn't like a unifying structure to quantitative text analysis

that makes people say, ah, like this is a useful

set of tools for learning about the world.

Contrast this with like causal inference, which I think social

scientists generally feel is like a very powerful way to

learn about the world and it's very constrained, right?

It's like very, I mean there's lots of different ways

to implement it, but like the underlying the underlying like

philosophical structure is the same.

Here, as you'll see, there's like there's a little bit

of this and a little bit of that and and

each person who does a QTA project can like pick

and choose and decide which thing they want to do,

OK.

Is this it's a long way of answering your question,

but I was responding to that like how is it

different from economics or doing causal?

I mean, it's true that you have a philosophical model,

and, and I'm going to ask you like, is there

any like no philosophical model for QTA exactly like philosophical

support for any of this.

So I'm gonna, I'm gonna talk a little bit about

various ways that people, so various underlying um models of

language and stuff, but this isn't like agreed upon.

There's like.

It's, yeah, we'll, you'll see.

I mean, you'll see more detail next week for sure.

Now, um, uh, GSR, Grimmer Stewart Grimer Roberts and Stewart,

this this textbook that we're going to use here, they

have a response to this criticism and they call it

radical agnosticism.

All right.

So what they say is that Like when it comes

to using quantitative text analysis for social science, we can

get around this critique of QTA by adopting radical agnosticism

that has 6 principles.

The first principle is social science theories and substantive knowledge

are essential for QTA research design.

OK.

Another way of saying that is any quantitative text analysis

project has to make sense as a work of substantive

social science.

You can, you should not just do quantitative text analysis

because you have access to data and you can just

fire up your computer and run some stuff.

That's not like you have to have a social science

grounding.

There has to be like a big social process you

want to study, and then your quantitative text analysis needs

to make sense as an outgrowth of that bigger social

science question and and social process.

Principle number 2, text does not replace humans.

It augments them.

OK.

So even with quantitative text analysis, you can't avoid the

old fashioned work of reading and thinking a lot.

And in fact, that is something I'm going to be

a jerk about in this class and uh, you know,

we as a department increasingly are going to be a

jerk about when it comes to evaluating quantitative text analysis

projects.

You cannot just avoid looking at the documents you're studying.

You've got to read a lot of them.

Like that's just how it is.

The computer can help.

It can help you read more than you can read

on your own.

That's great.

That's a great way to use quantitative text analysis, but

you still got to read and think on your own.

Um, you're not, you can't just like, you can't just

like pick up these tools because you can.

Third principle building, refining, and testing social science theories requires

iteration and cumulation.

This is a little bit in the weeds of social

science methodology, so I'm not going to dwell on it

too much, but um this amounts to the principle that

you can take an inductive approach to studying social science.

Phenomenon when you're using quantitative text analysis as opposed to

a deductive approach which is really common in the social

sciences.

OK.

Have you ever had a social science research methodology class

where they say, come up with a theory, derive testable

implications, set up your hypotheses and test your hypotheses.

Have you heard this before?

This is a Productive approach to social science and what

they're saying is that when it comes to using quantitative

text analysis, you don't have to do that.

You can start, you should start with some interesting social

science concepts that you want to study.

You do some work with your texts and then you

realise, oh, I learned something new.

I, I have now learned that maybe my original theory

was a little bit off, so I'm going to go

back and revisit the original theory, and then I'm going

to do some more work and then I'm going to

go back and revisit the original theory.

That's called an inductive approach to doing social science, and

they're saying, yeah, you can do that in with quantitative

text analysis.

Again, a little in the weeds, but important nonetheless.

The fourth principle is that text analysis methods distil generalisation

from language, um.

So the whole point of QTA is to take these

texts that are very complicated and nuanced and distil from

them more general lessons about the social world.

All, I mean, literally all of learning like what we

think of as learning involves taking a complicated reality and

distilling generalisations from it.

Um, so you know, QTA is no different than than

any kind of learning that you would do where you

sort of take a complicated reality and like draw lessons

from it.

Principle number 5 is that the best method depends on

the task.

So before using a method to do quantitative text analysis,

it should be clear what you're trying to do in

social scientific terms, right?

So if you come to, for example, and you have

a QTA project you want to do for your dissertation

or your capstone and it's not obvious what the social

science is, I'm going to say, OK, so why are

you like, what, what about this method you want to

use is speaking to a social science question.

Like what am I learning?

What will we learn about the bigger world if you

use this method on these texts?

It's very important that the best method depends on the

task.

You have to define the task first, right?

And then 6, this is the most important.

You don't remember any of these, remember this one.

Validations are essential and they depend on both the social

science theory that you're working with and also the tasks

that you're the QTA tasks you're using.

So what this means in practical terms is that when

you start doing some quantitative text analysis, you get some

texts, you fire up your computer, you run some lines

of code in R, you get some outputs.

Look at the outputs, look at the outputs, compare them

to the text, read some of the texts, say, does

this make sense?

Does it actually like when I got this output from

the statistical model, did I, did it like capture something?

Do I, do I think it captures something real based

on my own reading of a large number of the

texts that I'm studying?

And you should be able to explain why the text,

the output of the quantitative text analysis that you've gotten

actually relates to social science concepts and also accurately reflects

the text, the text that you're working with.

That we call this validation, validation.

You're validating the, the outputs of the quantitative text analysis

tools that you are using.

There's a whole wide range of techniques for doing this,

but I want to pause and say if you've taken

a machine learning class or more methodsy classes and you've

heard about validation in that context, here we're talking about

something bigger, like we're talking about sort of conceptual validation,

we're talking about.

You know, conditional on getting the output of quantitative text

analysis task, you're validating it in the sense that you're

comparing it to your own qualitative reading of the text

as well as asking yourself how does it speak to

a social science theory, um.

Sometimes that involves narrow validation like in the sense of

machine learning when you validate you validate a model using

a test set, and we're going to talk about that

in more detail in week 4 I think.

But that kind of narrow kind of like statistical validation

is, is not strictly what we mean here, we mean

something bigger.

Although those narrow forms of validation can also be useful

for the bigger kind of validation, OK.

Any questions?

Now, I'm gonna come back to this diagram again because

I, I first showed you this diagram in the, in

the context of saying, oh.

This kind of proves the point of the critics, but

now let me come back to it and say actually

this diagram is quite nice if you take a radically

agnostic approach because you can come over here and you

can use this diagram to figure out, OK, well, Once

I have my social science concepts and theories in my

mind, and so I know I'm going to focus on

some social science ideas and I have an idea of

the text that I'm going to use, then I can

go to this chart and say, OK, well what, what

exactly was it that I wanted to do?

And then this chart will help me figure out which

of the tools I can use for the particular thing

I'm trying to do.

But you shouldn't look at this chart until you already

have an idea in your head of like what social

science concepts you want to study and, and potentially what

text you want to, you want to use for that,

for that um research as well.

Although finding the text is often, sometimes it makes sense

to come up with the social science ideas, come here

to see how you would put those ideas into practise,

and then go find text that would help, would help

you do that, that can make sense in a particular

context as well.

OK, so let me give you the big overview of

quantitative text analysis in the social science.

Uh, the, the book that I'm going to primarily be

drawing from, we are going to be primarily drawing from

in this class is kind of organised in this way,

and I really like it.

So this is like the big overview of quantitative text

analysis in the social science.

4 parts.

One, selection and representation.

So that is deciding what text to use.

to do your analysis, digitising or converting those texts into

a format that is useful for quantitative work, and then

quantifying it, right?

So taking texts, digital texts, and then turning them into

a data set of data, a numerical data set of

some kind that you can do statistics with.

So quantifying the text.

So that's the first part.

The second part is discovery.

So G GRS.

GRS defines this as the process of creating new conceptualizations

or ways to organise the world, so really discovering things

from text, discovering new ideas, new social science concepts, concepts

from text.

So there are a set of methods that are more

focused on discovering on sort of like we have some

text here.

Let's like, let's like see what we can learn.

Like maybe there are new conceptualizations we had thought of,

you know, from, from looking at these texts.

So that's gonna be roughly weeks 7 and 80, selection

representation will be weeks 1 and 2.

And then measurement is the process where concepts are connected

to data allowing us to describe the prevalence of these

concepts in the real world.

That's going to be roughly weeks 3 and 5.

Now you'll notice that in the GRS ordering it goes

selection representation, discovery measurement.

That's how the book is structured, but we're reversing it

in this class.

We're going to do measurement, then discovery.

Pros and cons of, of either way, um, but that's

how we're gonna do this class.

And then the final I'll get to you in a

second.

The final portion of the book is devoted to inference.

So that is like, OK, now like what can we

infer about the social world using the quantitative text analysis

tools we've developed.

So there's two basic things here.

There's prediction, so using text to predict things in the

real world, that's going to be what we're going to

cover in weeks 9 and 9 through 11.

That's kind of like Gen AILM world, right?

So all about prediction.

And then causal inference is using text to learn about

what causes what in the real world.

Sadly not enough time to cover that in this class.

But let me just say that it's OK because because

causal inference has an underlying set of like a big

organising framework that's based on a clear philosophical approach.

If you take a causal inference class, all the texts

now I mean you can, you can easily figure out

how to use text for causal inference just based on

what you learned in inference class.

So it's sad that we don't get it covered in

this class.

Personally, I'm sad about that, but we're not going to

cover that.

We're going to, for lack of time, we're going to

cover these sections.

OK, what was your question?

What's the pros and cons of learning discovery first and

or measurement first or review.

So, all right, the way the book frames it, that

makes sense to me, as they say, well, You want

to start with discovery because you want to learn new

concepts, right?

Discovery is really about learning new social science concepts and

then measurement is about measuring those concepts.

I'm, we're reversing in this class because a lot of

the measurement that happens uh in quantitative text analysis is

actually measuring concepts that we all already kind of know

about, right?

So for example, scaling texts, finding the like how texts

fall on an ideological spectrum like the idea of there

being a left right ideological spectrum is like not something

we need to discover.

Like this has been a big part of political science

for decades and so we can just cut to the

chase and say, OK, well, suppose that we all agree

that there's this left right ideological dimension.

How can we use text to measure ideology in this

particular data set?

So, so that's why we can do measurement first is

because a lot of the things we're measuring are things

that, that we all kind of understand anyway.

I think they have a preference for discovery first because

I think they have a more idealistic, uh, view that

I don't, I don't want to say more idealistic view,

but they have a quite idealistic view that like actually

text can reveal a lot of new concepts we don't

know about yet.

Um, and in this class, there's so much to cover

of measuring things we already know about that we're just

going to cut to the chase and start there.

OK, um, now.

I'm going to pause here.

Any Other questions?

Where we get to the really fun stuff.

encoding, computer science stuff.

OK.

So in the past years of this course, there wasn't

really like a part of any lecture devoted to this

stuff that we're about to talk about character encoding, digitally

storing texts, which is very odd because this is a

class on quantitative text analysis and like you got to

know some basic stuff about like how quant how texts

are stored in a digital format.

So I'm going to teach you some of that stuff

here.

For the MY 472 people, I'm sorry, it's, it's repetition,

but it'll help you learn better.

Um, but it's important stuff.

Anyone who wants to do quantitative text analysis needs to

have in the back of their mind some of this

knowledge because it's like the practical stuff that comes up

when you're struggling with your QTA project.

OK, so let's go.

So there's some useful backgrounds, basic units of data storage

on a computer.

There's bits.

That's the smallest unit of storage.

It's either a 0 or 1.

With n bits you can store 2 tonne patterns.

OK.

So for example, if you have 2 bits of storage

space available, you can store 4 patterns 00, 01, 1011.

Cool.

Bytes.

A byte is 8 bits.

OK, so a byte is just a, it's an aggregation

of bits.

So one byte can store 256 patterns.

Why?

Because if one byte has 8 bits, it's 28 patterns

that one byte can store.

28 is 256.

Now, Uh, kilobytes, megabytes, gigabytes, words you may have encountered

in other contexts.

They're a kind of metric aggregation of bytes, roughly there

are some complications, some computer science complications here.

You can read about them in this Wikipedia page.

Not important for our purposes.

It's important for our purposes is understanding that computers store

data in bits, bits are 1s or zeros, and then

bytes are strings of 8 bits.

Cool.

Great.

Now, a character is the smallest component of written language

that has semantic value.

See this link for the definition.

A character set is a list of characters with ass

associated numerical representations and so code points are the unique

numbers that are associated with characters, right?

So if you think of characters in, in, in a

Latin language like English, there's a character called A, it's

like this, right?

Um that character uh uh is part of a character

set.

There are code points associated with that character, OK.

And they can code points can be associated, it can

be expressed in multiple formats, decimal, heck, hex, I don't

know actually what hex means full, but people just say

hex.

There are different ways to express, to represent these code

points, but the mapping between a character and a code

point is called an encoding.

And encodings use different numbers of bits to represent characters.

So you have 7 bit encodings, 8 bit encodings, 16

bit encodings, so on and so forth.

OK, so the original.

Encoding what's called AI it just uses 7 bits.

What that means, so each character is stored as a

sequence of 7 bits.

What that means is that there are 128 combinations.

So therefore, the ASCI encoding scheme could accommodate 128 total

characters because we have 7 bits.

There's 1 128 combinations you can create.

So that means this encoding can store 128 different characters.

Not enough.

There are more characters in the world than 128.

Think beyond Latin languages, think Russian, think, uh, languages from

Ethiopia.

Like think about all these other languages that have different

characters than the, the Greek, you know, different characters than

the Latin, Latin characters, plus all the punctuation and all

that kind of stuff, right?

All right.

So Aki was later extended to.

Bits, so now you can accommodate double the number of

characters, so 256 characters.

Still not enough.

There's many more characters in the world.

Now, especially think about emojis.

Those are each characters themselves.

So think about all the different kinds of emojis there

are.

There are probably more than 256 emojis on their own.

So you need, if you want to have each character.

Accommodated in an encoding, you need to have more than

you need to have more slots than just 256.

OK, so just to give you an example of how

this works, the letter A, capital A, so each capital

and lowercase were are considered different characters.

And by the way, in computer world that matters.

Capitalization is, is, you know, capital and lowercase letters are

different letters, OK, so keep that in mind.

So A capital A was code.

65 in the decimal format.

OK.

And what did it look like in sort of underlying

bits?

A is capital A is 1000001 in the original 7

bit Ay, so that's 7 bits.

And then once it got extended to eight bits they

just Tacked to 0 in front of it, so 01

000 001.

So these are the bits that are used to represent

the capital letter A in the original ASCI and then

the extended AI encoding schemes.

OK, now as you can imagine, different languages, different characters,

different character sets, different encodings.

Back in the day before.

Now, uh, this is really complicated.

Like every language, every country, not, not every, but like,

uh, I would exaggerate a little bit, but there were

different encodings for different languages in different countries and you

know, even within a language there'd be multiple encodings.

OK?

So this is really, really, really messy.

All right.

You can read about the, the mess of this if

you go to this link.

So what happened is at some point people got together

and thought well let's let's unify all this.

Let's not have All these different encodings.

Let's just have one encoding, and it's called Unicode.

It's created by the Unicode Consortium.

And so Unicode comes in 3 encoding formats UTF 8,

UTF 16, and UTF 32, which I didn't list here,

but the commonly used ones are UTF 8 and UTF

16.

So that stands for Unicode Transformation format.

So, um, UTF 8, which is the primary one used

in the English speaking world and that we're going to

use in this class, is a variable width character encoding.

It's by far the most frequent encoding in the world

today and on the web, especially.

Let me give you examples of it in a minute,

but the key idea here is that variable amounts of

bits are used for each character with the first byte

8 bits corresponding to AI.

OK, so that's the basic framework of Unicode.

But Unicode, so back with AI, like at least the

extended AI, every character is stored as a combination of

8 bits, OK?

But Unicode now allows for variable width character encodings.

So some characters are encoded with 8 bits, some are

encoded with 16 bits, some are encoded with 32 bits.

Um, and so this, as you can imagine, allows for

many more options, um.

So I'm going to show you, OK, so the nice

thing about it being variable width is that the most

common characters used on the web, in writing, whatever, they're

the smallest, so they're usually 8 bits.

So that means that when you're storing, because every time

you increase the amount of bits used to store a

character, you're taking up more space, right, because each bit

takes up space on your computer, like storage space.

So the most common characters are stored in 8 bits,

the smallest amount of bits you can.

So it kind of, it's like the most efficient way

to store characters because the common characters are being stored

with fewer bits and then it still does allow you

to accommodate less common characters.

You just tack on more bits, but it takes more

storage.

OK.

Now, so let me show you an example, um, that's

where you can see the uh I don't like that.

You can see the, uh, you see this?

It's like the air coming out of the vent is

like visible in the projector.

I don't know why I like that, but OK, so,

um, UTF 8 is variable width.

So let me show you an example of how it

works.

So this is the ampersand, so it's a character.

This is a code for it in Unicode.

So they have this common code point definition.

So it's U + 0026, and this is how it's

represented.

it in bytes, uh, bits.

So there's, uh, so it can Unicode, UTF 8 can

have, um, up to 4 bytes.

So you can encode a character using only 8 bits,

so 1 byte or you can encode a character using

up to 4 bit bytes, which is, you know, however

many bits that is, um.

4 x 8, 32, uh, and different characters will have

different numbers of bytes.

OK.

So this is a very common character, the ampersand.

So it's one of the ones that is stored with

8 bits.

In fact, all of the old Asy characters are stored

with 8 bits.

OK, so, um.

This is the Unicode bits for Ampersand.

U is also stored with 1, with 8 bits slash

1 byte because it's also a very common character and

this is the, this is, this is the code point

for it and this is the, the, the bit representation

or this is how the computer stores it.

Um, you with umlauts, which is in my name, this

is a less common character, so it takes two bites

and so it's all of these bits together.

Is how Unicode encodes you with um lowercase u with

um.

And then this is a Russian D D character?

Yeah.

OK.

So the character, uh, it's also encoded with 2 bytes

slash 16 bits.

I think all the standard Russian characters are encoded with,

uh, 2 bytes, um, but I could be wrong about

that.

Obviously, I have not memorised every single character and uh

how many bytes that they, uh, are encoded with.

This is an Ethiopian character.

It is encoded with 3 bytes, so that is 24

bits.

And then this is a melting face emoji, and it

is encoded with 4 bytes 32 bits.

All the emojis typically are encoded with 4.

bytes 32 bits, meaning that they take up a lot

more space in your computer.

So if you have emojis in your document, you're you're

using, you're using up more space on your computer.

Now modern computers have so much space on it this

is like not that big of a deal, but still,

you're taking up 4 times the space in your computer

to store that one emoji than you are to store

the ampersand in a text file.

And you can if you were in 472, we did

that, you can verify that if you put different characters

in your file, you'll see the file size will increase

depending on how many bytes it's taking up, and you

can make a text file with one emoji in it.

You'll see it, it's 4 bytes.

Make a text file with an ampersand in it and

it's only 1 bite.

It's kind of cool that you can see it actually

on your computer.

OK.

So now let's talk a little bit about storing the

text digitally now that we have some background about like

the, the mapping between characters, which we know, like we

understand what a character is and then the sort of

computer representation of them in bits.

So textual data, the data that we're going to be

focused on this class can be stored in a variety

of digital file formats.

This is new for the 472 people in the room,

but there are some, there are some ones, some that

you may encounter a lot.

So there are binary files.

Binary files are computer readable, meaning the text, if you

have a text stored in a binary file, the computer

can read it, but you can't read it, so you

wouldn't be able to open it and read it yourself.

So, uh BSO is a, is a, is a common

um binary file type.

These are usually more efficient and smaller.

And so that's why oftentimes people will want to store

a large amount of data in a binary file.

But You probably won't encounter that really at all in

this class and probably not much in your day to

day life.

Plain text files is what you'll encounter the most in

this class anyway, and in plain text files, text is

computer and human readable and it stores encoded characters but

no other information, right?

So you open up a plain text file, it'll show

you all the characters that are stored in that plain

text file, but there's nothing else, OK.

So this is a variety of types of files, simple

text files like files that end with TXT, plain text

file.

All it does is store text in it.

Markup files, they store all characters, so encoded characters, that's

the only thing that's in them, but there's tags written

into them that indicate what formatting should be applied if

you were to render the text file into some kind

of formatted format.

So for example, HTML is an example of this an

HTML file.

It is a plain text file in the sense that

if you open it up, it's just characters, but when

it goes into a web browser and it gets rendered

on a webpage, the web browser knows how to take

the tags out and then create a formatted page that

is more than just plain text.

It has colours and bold and italics and all this

stuff, and it's using the commands and the tags inside

the plain text file to render that.

So.

Markup files need to be rendered in order to appear

with the formatting, but you can inside the markup file

use various characters to indicate formatting.

Mark markdown files are marked down files.

That's those are plain text files that you can then

knit or render to a different format that will show

you formatting.

Because the plaintext file itself does not have any formatting

in it.

There's no bold, there's no italics, there's no underlining, there's

no tables, there's nothing like that.

It's just characters written in sequence on a, on a,

on a document.

So these files will have instructions about how to do

the formatting when it's rendered.

Programming scripts are often are always in plain text, so

our scripts, Python scripts, texts that's structured for data storage.

So this would be plain text that includes delimiters, so

dividing up information, and then containers, so containing information into

groups.

So CSV files, tab files, JSON files, XML files, these

are all plain text files that are structured in such

a way that they store data.

So CSV file, what it does is it stores what

you know of as a spreadsheet, right?

So like rows and columns, but the way it does,

it's plain text.

The way it does it is it is each row,

each of the cells.

row is separated by a comma, hence comma separated value.

That's what CSV stands for.

And then each line has a line break, and then

the next row all the data pieces are separated by

commas line break, and then when you import it into

Excel or into R or whatever, it renders it looking

like a table, but it's just using the commas to

figure out where to put the lines, OK.

And that's why sometimes you get errors when you try

to load a CSV into if you've ever experienced this,

I don't know, I have experienced this, but there's occasionally

times where you try to load a CSV into R

and you get an error because one of the lines

has too few commas in it and so they all

have to have the same number of commas so that

you get the same number of columns in every row.

But if you get one row that somebody deleted a

comma in, then it's like this row has fewer, it

has fewer cells in that row, and then, oh what

do I do?

How do I fill in the gap?

Like which, where is the missing comma, you know, supposed

to go.

Um, but those are stored in plain text files.

And then you have JSON and XML.

They're not, they're not, it's not table data, so it's

not storing in rows and columns, but it is still

another data storage format that stores chunks of data grouped

together like in sets or in arrays or something like

that.

OK, many word processor documents word processor documents in 2025,

they actually are plain text files.

Ooh, what do you mean?

Oh, Ryan, what are you talking about?

So back in the day, like old Microsoft Word, you

would, it would be some proprietary non-plain text file and

you would have to open it in Microsoft Word.

That has not been the case now for 2025 years

now.

Microsoft Microsoft Word documents are stored as DOOCX.

They're open source XML files zipped together into a package

and so you can actually like, you can, you don't

have to open Microsoft Microsoft Microsoft Word document in Microsoft

Word.

You can you can open the underlying plain text as

well in a plain text editor.

The thing is, The XML package that's stored as a

document as a Word document, if you open it in

Word, it renders very pretty on your screen.

It looks nice.

If you try to recreate that on your own, you

might not be.

I mean, this is Microsoft is making money off of

creating software that renders these files that look nice, um,

so, but they're still plain text, so you can get

to the plain text as well.

Now there are other types of ways to store text.

There's rich text files.

They are human readable.

They store encoded characters in some kind of alongside some

kind of formatting metadata.

So RTF file is an example of this.

So they're not just plain text.

I don't, this isn't really common these days because the

world has moved online primarily, we've kind of gotten rid

of text files that have all this formatting metadata with

them, and we've moved towards plain text files, but I

just, I.

I feel it's important to mention that there at least

there used to be, and you may encounter some kinds

of files that store text that are not just plain

text, in which case you've got to figure out how

to access the text from them.

There's also cloud-based documents, so texts that are available on

cloud-based editor like Google Documents, but they're usually plain text

under the hood and also you can export them.

You know, if you have a Google Doc, you can

usually export it as some kind of plain text file

onto your computer.

OK, cool.

Um, and then finally, very important, there are image like

files, so this is another way text is stored.

So I mean text in the in the broader sense.

I don't mean text in the like plain text sense.

Like text that we all read as humans is often

stored in PDFs or JPEGs.

If you see somebody taking a photo, somebody posting on

social media a photo of a protest where there are

signs that have text on them, that is a file

that has text in it, OK?

And so the and it's a JPEG file, it's an

image like file, but it has text in it.

It just happens to be text that's written on a

sign as being held up in the photo.

Um, so PDF, JPEG, PNG, TIF, etc.

these are image image-like files that store text.

Um, it actually turns out to be a little bit

more complicated than this, so.

Image-like files, they're often how analogue texts are stored.

OK, so what I mean by analogue text analogue is

non-digital paper, stuff on paper, that's analogue.

OK.

In the context of text, that would be analogue.

So scans of old paper documents would be a way

to digitise analogue text, analogue text being text on paper,

digitising it by taking a picture of it basically.

So often when that happens, those documents are stored in

PDFs most of the time, at least nowadays, PDFs come

in multiple varieties.

So some PDFs are, they're basically, they were digitally generated.

So if you had a word processor document and you

converted it into PDF, the PDF was generated from digital

text and so the PDF actually has the digital text

underneath it inside of it, so you can extract it

very easily.

Other types of PDFs are really based on images, right?

You take a picture of an old paper record and

then convert it into a PDF format, and so the

text is not stored underneath and you need to recognise

the text.

That's called optimal, uh, optimal character recognition or is it

optical?

I don't think it's optimal.

That's an optical, yeah, this is wrong.

Sorry.

This is supposed to be optical character recognition, optical meaning

eyes, right?

OK, so, um.

So, pictures, scans, so on and so forth, you'll often

have to use OCRing techniques, optical character recognition tech techniques

to, to identify the text inside the image.

They're not perfect, right?

They're often working with machine learning models to try to

identify like the probabilities of various, especially with written language,

like protest signs and people just write handwritten stuff.

You can use OCR to identify the text, but it's,

you know, it's different people have different handwriting and so

it's not always perfect.

Um, so, uh, to extract the text from image-like files

with nice PDFs that were digitally generated, you can usually

just export the text directly from the PDF in Adobe

Acrobat or Preview or whatever.

Otherwise, you got to use OCR and we're going to

show you techniques for OCR, like seminar.

So don't worry, we'll give you some sample code for

how to do that.

OK.

Now there are many potential encoding issues.

So the most common thing that happens is wrongly detected

encoding encoding.

So whenever a plain text file gets made, so whoever

created the plain text file long ago, it's encoded somehow.

So when they saved it on initially saved it onto

their computer before they loaded it up to the web

or whatever, it was encoded.

How that happened, who knows?

You don't have access to their computer.

You don't know what they were doing.

Their computer default encoding might be something that is unusual.

You don't know.

So the point is when a plaintext file was initially

saved, it had, it was saved in with a particular

encoding.

But that encoding is not stored in the document anywhere

and so a future person who downloads a plain text

file created elsewhere and tries to open it on their

computer, the computer is going to guess what the encoding

is.

It yeah, computers are pretty good at guessing, but sometimes

there will be documents encoded in very rare encodings.

So they were created on a computer that uses a

very rare encoding system, and then somebody else tries to

open it in the future and the computer cannot figure

out what the encoding was.

And to make matters worse, a lot of encodings have

similarities, so they'll use the same sequence of bits for

certain characters but not other characters, and so that confuses

the computer even more.

So, Whenever you open a document and the computer guesses

an encoding that's incorrect, you get what's called emojibake, and

that's like if you've ever opened a document and you

see it's just gobbledygook and it's like you can't understand

what it is, that's emoji Bake.

We call that emoji bake.

I'll show you an example in a second.

Yeah.

What is when you get the little like square with

a question mark in it, is that also emoji bake

or is that different?

Where it only happens for like one character every so

often.

It is, it is, it's technically emoji bake because um.

To be a purist, if your computer knew the full

encoding, it would know what that that character is, but

it doesn't.

Um, so it is emoji bake, but it's, there are

situations that are more narrow where it's like there's specific

characters and it could be that so in my experience

that happens mostly when you are trying to open files

that, um.

That contained characters that like they were like in beta

form, you know, like emojis or whatever, like back in

the day when the document was created and then Unicode

got around to creating an official, um, an official encoding

for it.

And then the encoding that Unicode.

Created for that is different than the sort of experimental

encoding that was used when the document was created.

But it's like a one-off, those would be like one-off

situations.

Emojiba, the, the worst kind of emojiba is like when

the entire coding is off and what you see is

like not, not intelligible at all.

OK, so the other problem you can encounter with encodings

is space constraints.

So each bit uses storage on your computer.

So if you're storing digital text using encodings that use

a lot of bits, you're using a lot of space

in your computer.

So UTF 8 encoding is variable with encoding, meaning that

common characters have are Encoded with 1 byte, less common

with 4 bytes.

That's quite nice because it kind of conserves space.

But there is something called UTF 32, which encodes every

single character with 32 bits, every character, even the really

common ones.

That's a fixed fixed length encoding that takes up so

much space because you're encoding the capital letter A with

32 bits.

Every single time you use it, which is 4 times

more space than you would encode it with if you

use UTF 8, which is variable with.

So, um, that can be a big deal, and especially

if you are by default using UTF 32, um, you're

going to use up a lot of space on your

computer to store text files.

Now it turns out, I don't know why I say

this.

Oh yeah, it turns out that Windows tends to use

UTF 16 by default, and, and Mac tends to use

UTF 8.

UTF 16 takes more space because it's, it, it starts

with 16 bits.

Um, so it's going to take, so a plaintext file

is going to take more space on a Windows machine

that is defaulting to UTF 16.

Um, so if you can change it, do change it.

I don't know if you can, um.

They actually probably don't because you messed something up.

Don't, don't do that.

Don't take my advice.

I actually, I don't want to take that back.

Don't change the, don't try to change the default encoding

in your computer because that they can create issues.

Um, so, yeah, OK.

Uh, things to watch out for.

Many text production applications might still use proprietary encodings, so

I think some Microsoft Office-based products use Windows 1252 encoding.

So even though Windows, sorry, even though Microsoft Office documents

are plain text documents, that doesn't mean that they can't

be encoded with a proprietary Windows encoding.

They're still plain text.

They're just the encoding used for the plain text is,

is not UTF 8.

I don't know if that's still true.

Um, you'd have to check, but for many Microsoft documents,

the default encoding will be the proprietary Windows encoding, which

is kind of annoying.

Um, again, Windows uses UTF 1816, Unix uses UTF 8,

Text editors can be misleading.

So if you open a plain text file in a

text editor, what you're seeing might be look like emoji

bake, but the encoding might still be intended.

Uh, and then generally there's no easy way to figure

out encoding issues, and this is just, but you, you're

gonna, you may deal with it when you, especially if

you're dealing with text analysis in other languages, you may

have, especially older text data in other languages, you may

have issues with encoding and you should read up on

it, um.

So there are some things you can try.

I don't want to, so this is things you can

do in terminal.

You can read that on your own, but what you

can do at R is you can open a file

using the reader package and you can, there's some functions

there where you can have the haver try to guess

the encoding and it gives you a list of the

if you have problem set in 472, you know this

because you did it.

Uh, but let me show you an example of Emoji

Bake.

I don't want to dwell too much on the R

code because we're going to do that in seminar, but,

but let me show you what happens.

So this is a document that was on the problem

set for 472 last year, and this is emoji Bake.

So this is, oh, I, so I made this file

and I encoded it in in an encoding that is

not commonly used on machines that are set to have

their default language in English, OK.

And so when you open it, as my machine as

the default language is English, when you open it, this

is what appears.

This is emoji Bake.

It assumed that encoding it was incorrect, so it's showing

you all of the bits translated via the encoding it

assumed that was incorrect.

And so it renders, I mean it renders it correctly

according to the encoding it assumed, but this is not

the intended encoding of this document.

So what you can do is you can open for

something like that you can open in a reader if

you happen to know the encoding, in this case it

was shift JIS.

You can read the file into R specifying that you

know which encoding it is, and then you can load

it into R and then write the file, save that

file again and for whatever reason, when you save file

under reader using the right file command, it defaults to

saving it is UTF 8.

Great.

That's what we all want UTF 8.

Everybody should have UTF 8.

UTF 8 it works across all languages.

It's universal.

We should all do UTF 8.

OK.

So if you do that, you, and then you, you,

you save in UTF 8 and you reopen the file

and text editor, you get the text as intended.

This is a, a Japanese text that um is from

a press release from the Prime Minister when Trump got

elected.

OK.

But this is how it was supposed to uh render,

but this was me opening the UTF version I saved

in R.

Now, of course, the problem here is I had to

know that that was encoded and shipped just to get

this to work, GIS to get this to work.

But I did it.

I mean, I made the documents so I knew that

it was encoded that way.

And so sometimes with encoding, you just got to do

some digging and figure out like if you got some

documents from Like an old like source of digital Russian

documents, you know, you know roughly the time frame you

got the documents from and you open it up and

it's emoji bank.

You might do some digging to see, OK, what was

the common encoding used in Russian speaking countries in the

time frame that this data was created, and then that

might help you figure out.

You can also have the computer help you try to

guess what the encoding is as well.

OK, great.

Some unsolicited advice.

Please get in the habit of working, storing your work

in and your data in plain text formats.

It's easier to work with.

It's safer for archiving.

It's more portable across context.

And when you and more than that, not only use

plain text formats, encoded UTF 8, so set your your

set your text editors and stuff to default encode in

UTF 8.

So when you save text files, they'll be UTF 8

encoded.

Um, if you like writing formatted texts, so you like

to write notes for class that you like to see

formatted as I do, there are some options.

You can use markdown our markdown, Quarto, or Lawtech, which

are all plain text files that you can then render

into a formatted document as as you like.

So then you can get the, you can get the

nice formatted version you can look at later, but you're,

you're giving instructions in a plain text format so that.

Uh, you, you don't have problems down the road, and

you can always re-render these over and over and over

again, right?

So you can just keep rendering them as, as you

like.

So I highly recommend reading more about encoding because it

may come up especially if you're non, if you're gonna

work in non-English languages.

Um, here's some Wikipedia pages that are really interesting and

this, I love this.

This is really nice, this is really good, uh.

Good discussion and coding.

OK.

So now let me briefly talk to you about working

with strings and characters in R specifically this is what

we'll be doing in this class.

So an object in R consisting of plain text characters

is often referred to as a string or a character

string.

So going forward, if I use the phrase string or

character string or character in the context of R or

programming, what you know I'm saying is that we're working

with objects in R that are plain text.

As opposed to numbers, um, dates, uh, uh, uh, what

other kinds of things you can have, right?

The strings, a particular kind of object in a coding

language that store plain text.

So in R, a vector of strings is called a

character vector.

So character and string are kind of like interchangeable in

the context of R.

and more generally, yeah, well, character and string are interchangeable.

So here's an example of in R1 creating an object

called my string that is just a string that says

hello world.

Here's a second example of creating a character vector which

is a vector of strings.

So you can imagine maybe like a set of documents

is like a vector of strings, right?

So it's like a collection of multiple texts, um, each

of which is itself stored as a string.

The strings can be very long, right?

They can be entire novel, it could be a string,

right?

Um, so with base R you can look at substrings

and paste things.

So for example, you could take my string that I

made up here and you can paste.

A second sentence to it, so it will say my,

it will say hello world, hello sun, or you can

uh take substrings so you can take my so so

that creates that, so this pace creates that string, or

you can take a substring, so you can take my

original string and I can get characters 1 through 41

through 4, and then it's just going to give, well,

that was unintentional.

I just, I did not mean it to say hell,

but that's 1 to 4, um.

OK, great.

Uh, so in this class, we're going to use a

powerful set of tools available in the Tidy Tidyverse suite

of our packages.

So Tidyverse is You load it as a package in

R, but technically it's a bunch of packages, and we're

going to use it over and over and over again.

So Reader, which you can use to deal with file

encoding that I already talked about, is part of Tidyverse.

So is Stringer.

So Stringer is a package that you use for working

with strings.

So for example, with Stringer you can find patterns using,

for example, string detect, SDR detect.

You can mutate strings so you can replace stuff in

strings.

You can turn.

Capital letters into the lowercase letters.

You can combine split strings, so this is combining strings,

this is splitting strings.

So what did I say?

What do I mean splitting strings.

So for example, if I wanted to split this string.

But based on spaces, I could, which is going to

be a thing we're going to be doing a lot

in this class, I could take this string and split

it on spaces, and then I would end up with

a character vector that has two objects in it, one

that just says hello and one that says world exclamation

point because I had just split the string based on

the space.

Um, I can clean strings, so string squish, string strim,

string pad.

These are all so these remove white like extra white

space.

These this one will add extra things that you want

to the front and back.

So if you want to add more spaces or you

want to add leading zeros or something, you can use

string pad.

You can do a lot of this stuff in base

R as well.

Tidyverse is a specific package we're going to load to

do string manipulations, but you don't have to actually load

a package.

There's some there are some ways to do this in

like the sort of simple R without loading packages, but

we're going to use Tiddiverse and in Python, the equivalent

would be the RE library.

OK, so now I need to tell you in the

remaining time we have left about regular expressions.

So searching and counting strings is going to be the

core of quantitative text analysis.

We're going to do that a lot over and over

and over again, OK?

So when you need to search, find things, you will

use regular expressions a lot of the time.

So this is often shortened to reg X, regular expressions,

reg X is just what people say.

So I'm just going to say reg x, OK.

So Reg X provides a powerful and flexible tool to

search and replace texts.

So this is, if you've heard of globs, this is

similar to globs, but we're not going to talk about

globs in this class.

It's not super important.

You can read the Wikipedia page, which is highlighted here,

but regular expressions is reg X is a set of

A patterns you can use to find and replace text.

Many texts that many editors that work with plain text

like R Studio VS Code, sublime text, so on and

so forth.

These are text editors.

They will allow you to do a find and replace

using reg X.

So instead of just like finding, replacing based on, you

know, you do, I'm sure you've done find and replace

in the past, right, you find a particular word, replace

it with something else.

Well, you can also find and replace based on regx

patterns.

And so, um.

Uh, you can also use this in many programming languages

to do various kinds of manipulations and in our both

Stringer and Quanta, which we'll use for text analysis later

in the course, they're going to use reg X searches.

We could talk about this for days, but I'm not

going to.

You're going to see examples in code, but I just

want to give you a little bit of a, of

a, an overview.

So regular expressions contain both literal characters and meta characters.

So literal characters are the usual texts.

So for example, if you are, you have a long

document and you want to find every time the word

the THE was used, you can make a reg X

search pattern that's THE and it will find all the

examples of THE in the document, OK.

But sometimes you want to find more than that, so

maybe you want to find all instances of the word

either this or these, those two words, they have common

meanings, one's plural, one's singular, but they're spelled differently.

So you can't do an exact search for, I mean

you could do one after the other, but sometimes that's,

you know, it takes too much time.

So you want to search for both at the same

time.

You can set up a reg X pattern that will

find T H, and then.

Some you'll use some meta characters.

They're like wild cards to find the rest of the

word.

OK, so the meta characters help you define those kind

of flexible searching parameters, and these are all the meta

characters and you're going to see examples of how these

use in sample code that we'll give you in seminars,

so don't worry too much, uh.

Just to give you for an example, the period that

is a meta character in reg x.

It means any character.

That's what it means.

So suppose you want to find all of the periods

in a text.

Well, if you're using reg X, you cannot search for

all of the periods using just the period because reg

X thinks that's a meta character that's supposed to mean

any character.

So in that situation, if you want to specifically find

the period, you've got to escape it, meaning you've got

to put a slash in front of it to say,

no, no, no, I don't want you to find any

character because I know Regix treats this as a meta

character.

I want you to literally find the periods, so I'm

going to escape it so that we literally find the

periods.

So this is the kind of flexibility reg x gives

you.

So if you wanted to find, if you wanted to

find.

Oh, do you have any markers here?

If you wanted to find this or these, you could

do, no markers, you could do T H, and then

you could do in square brackets i meaning find either

an I or E and then S.

And then E, because that comes at the end of

these with a question mark after it, meaning maybe there's

an E.

So that would be a search pattern you could design

to find either this or these, and I do this

off the top of my head because I have used

Drag X so much in my life, so don't worry,

like you're going to have practise doing it.

But, but the point I want to make conceptually is

that you can design a flexible search pattern that will

identify all the, the, the versions of a particular word

or string that you want to find.

OK.

So, again, with regular expressions, you can specify characters including

with wildcards, you know, like, match any character that takes

this particular form.

You can specify sequences of characters, you can use booleans

and capturing groups.

So on and so forth.

Again, lots of stuff that we'll see examples of when

we, when we are in seminar, and you can review

detailed discussion of strings and regular expressions and are here

and are marked down with many examples there.

Now there's also more resources you could use for reg

X, so, um.

I don't need to talk about them except I want

to tell you about these two websites are really great.

So these two websites allow you to play with reg

X.

So what you can do is you can put in

a string that you have and then you can, you

can work out a particular reg x that would work

to find things you want to find in that string.

And I'm telling you all this now, and you're, I

hope you're thinking, what, I'm just going to ask Chat

GBT to do it for me instead because Chat GBT

is very good at making regga patterns.

That's one of the things that you can count on

it to, to do well.

Um, but if you want to learn this stuff at

a more intuitive level, like if you're sitting on a

pen and paper exam in spring term, uh, you might

want to play around a little bit with the, with

the reg x 101 just to like get the intuition

for how to build reg x, uh, strings yourself.

And let Chat GBT help you because, uh, you know.

It knows a lot about this stuff.

OK, so next class, what we're going to cover is,

um, we're going to cover document selection.

So it's like how do you choose documents to work

with, and then we're going to talk about how you

quantify those documents in the context of a document feature

matrix, which is basically a data frame of word counts.

And then we're going to conclude by talking about some

of the analytical approaches people use conditional and.

Getting a document feature matrix, what do you do with

it?

Like, how do you approach it?

Do you think about it in terms of statistics?

Do you think about it in terms of vectors?

Like what's the next step forward?

Um, and yeah, so there's a lot to cover next

week.

It's very important week.

So it's, you know, one of the ones that I,

I would, I would say not to miss because it's

foundational for everything we're doing in this class.

It's like getting stuff into quantitative formats.

Um, so it's important, important material.

All right, I'll see you next week.

Oh, this is the textbook you're that we're going to

be using.

I brought it to show you.

If you want to get a copy of it, great,

um.

the library, but uh, you know.

Speak now you're gonna disappear at some point the library.

I don't know she Yeah.

Oh yeah, I didn't even think this time I know

I meant to turn it on and then I just

got distracted with teaching and I I I Oh yeah.

I have a lot of uh idioms that I use

that I'm trying to tone down because it's, you know,

they don't have universal meanings.

Yeah, it is, it is quite good, yeah, no, it's

um.

I'm a little more theatrical, I think that that's good.

OK.

OK, great.

OK, yeah, I think, yeah, with the auditing for PhD

students, I don't think you have to go to the

department, but you might see the admin people just, uh,

they like to know, um, so when you email me

can you seemethodology.

admin@ LLC.uk, um, because they just like classes that matters

for something, yeah, all the methodology classes are recorded, yeah,

so they'll be in the usual, yeah, they'll be in

the usual.

I'm a little bit.

Mhm.

Oh shoot, sorry, I gotta put this back in there.

Oh.

sorry.

Are there any markers?

I see.

Last year, that was, yeah, that was the cost of

winter stuff.

How did you find it?

Was it?

I liked it.

That's my favourite parts, really, yeah.

you not It just reminds me a bit too much

of what SP 111.

the way the way that the classes are structured, it's

it's about it's an hour and I don't mind that

part, but it's just the the content of the class

is so long and it's so it's like I love

that.

Like it's, it's did you do sociology at A level?

Oh really, like, at least what we've done so far.

Did you do one of the papers is on education,

like a third of the course, and this is exactly

half the half the people we look at we did

in A level most of the readings I've like heard

of them before.

OK, that's really good.

Yeah, maybe I just like it cuz it's string and

even like the topics like they are really interesting.

I feel like we don't Like they're a lot more

interesting in theory than the way that they're explored in

the lecture, and I find the lectures so boring and

I can't.

I'm only going to now because my timetable's changed.

I don't know because now I've got a, so I

just want to go home because I'm in 9 to

6.

I'm like I might as well just come and I'd

rather go home really.

Because then I've got a 5 till 6 class.

I can't now, like for me it has to be

at least a good 34 hours.

I can't do like a 1 hour nap.

Any any amount's good.

Do you have anything in between the class and the

lecture?

On a Tuesday and yeah, um, no, I've got that.

Oh wait, I'm in your class now are we the

same class, I put that and then that but not

that class.

I don't have the middle one so I'm gonna go

there.

I remember Ellie used to try doing that, but she'd

end up oversleeping and then not turning up for the

next thing.

Well like I've got a few things to do, so

I'm going to be in an accommodation.

View and accommodation for next year.

Yeah, it's too expensive.

They've put it up 2000 pounds.

Who do you think you are?

Over the year, it's gone up like 50 quid a

week like where do you think you are going in

a different one.

It Do I like this is really.

And a pharmacy too and I picked one that was.

You what I like.

picked all ages away because I got the address um

I don't no one.

Did

Lecture 2:

Oh like his had been like, I, I um I

was like.

that they would.

He was, oh no, he was like.

It just looks like the.

I was like actually it sounds like I just like

have to adjust it was like oh yeah I think.

Um, I like, you know, I think that it has

like a like a sit down.

And I think that those are like like ROI in

terms of like what I know.

Yeah, yeah, it's just like I don't think.

in my head, it's a bad they're just like.

And yesterday was hot.

Because.

Yeah, yeah, yeah, that that's kind of what that's how

that's when they go OK that's actually a very fair

threshold.

Yeah Yeah it was I can um yeah.

I do think that like.

Yeah I think yeah.

I wanted to but yeah you know if that makes

sense.

Oh right, right, yeah, because I submitted my proposal, it

was quite, yeah, no, I, I think I was because

I did I see you.

Oh.

Yeah Yeah, and I dropped a lot and it was

probably.

Yeah, she's like I I didn't go to the.

Yeah.

And oh, it was.

No, it's secondary.

Yeah I was like I can't, I can't be OK.

So the leading questions, the like the one he believes

in them.

It's um no, that's true.

It's just I just feel like it all it's too

short and that's like my fault that this is one

but I am doing.

OK It's just What about you?

I was here.

I I like you know what I mean.

I did.

My mom came.

This isn't a real place, yeah, like it's called and

how was it?

What was it like?

Um, I think, I think, I don't really hear about

that.

OK.

So.

Yeah yeah.

Um, no, at the same time, yeah, yeah, spring break,

exactly like I just like them.

Real, but I, I don't.

Yeah I managed to, so it's a great start and

there's like uh.

and that.

Not so I imagine that like I didn't say where

have you been, OK.

Um, I thought about going to Cooksville and then I

went to it's um Maryville on the metropolitan, very sweet.

We are, yeah.

I like that too even that wasn't I think like.

OK, uh, sorry, I'm a minute late today.

Uh, all right, so the slide should be up on

the website.

Um, so just so you know, the slides and the

materials for the course are gonna be loaded up to

GitHub, and then you should be able to link to

them from the website.

But you can also go directly to the GitHub, um,

repository where this material is gonna be.

It's, uh, called.

Uh, github.com/ LSC-MY459.

Um, and so if you go to that link, you'll

be able to find materials for the course.

Um, there's a folder there called lectures in that GitHub

repository.

That's where I'm gonna put the lectures sides, obviously, but

I think I'm also gonna put the seminar stuff in

there as well.

Um, I don't know why we need to have like

multiple folders different for lecture and seminars, so I'm just

gonna put.

Um, each week's materials in the relevant week folder.

So if you go to that GitHub site again, it's

github.com/LSE-MY459.

That website will take you to the GitHub repository where

there's a folder called lectures.

You go into that folder and it's week 1, week

2, and then whenever the future weeks come, they'll be

added as well.

Just go into the, if you, if you want to

directly access files, you can just bypass the website.

Um, you can just go to that and you can

look into the week where we're at.

So this week's lecture materials will be in there and

also this week's seminar materials will be in there as

well.

Um, they're not there yet.

They'll be there after this class, uh, scrambling to get

them done.

Um, but, uh, they will be there, uh, before your

seminar either today or, uh, later this week.

OK.

So, uh, we're now in week two, which is on

quantifying texts.

Um, so I just want to give you a few

reminders for seminar, bring your laptop, install R, install R

Studio desktop.

Um, uh, you're going to be doing coding activities in

seminars, so it doesn't really, if you don't come with

your computer.

Uh, I mean, I don't know, you can watch somebody

else's code if you want to, but, um, it's probably

more useful for you if you bring, bring your laptop

with you, um, and try to get this, this software

installed but if it isn't already installed, because you're going

to be using it.

Um, also, you know, make sure you have a GitHub

account and install GitHub Desktop.

I had to put this in here because I think

we'll use GitHub for some stuff later in the class,

but as you know, the assessment format changed this year

for this course.

So we have an exam.

Previous years, there were like a whole bunch of problem

sets, and this was all done through GitHub, so it

was like more urgent that you had a GitHub account.

You will, we will use GitHub for the formative problem

set, um, But, uh, that's not coming for a little

while, so this isn't super, super urgent, but at some

point, no, we're going to be using GitHub for it.

What we're going to be using it for is not

that difficult.

So, um, don't worry too much.

We can cross that bridge when we get there.

But I felt I should at least raise it once,

uh, at this point in the class.

All right.

So this week's lecture is about quantifying texts.

So we want to So the sort of core idea

of quantitative text analysis is that we want to use

quantitative, and that's another way of saying statistical methods.

So that's like why we're why this class is called

quantitative and so if we want to use statistical methods

to analyse text, we have to somehow conceptualise those texts

as tabular data, as you know, as a matrix, as

a data frame.

Um, so that's kind of what this class is about,

and here I have a little diagram here.

So these are like texts, you know, that you might

recognise, I think, what are these?

Oh, these are, um, speeches in.

I think the Irish parliament, although I'm not 100% sure,

but I think these are speeches from the Irish Parliament,

um, and these were the original texts.

Then you do some magic, you turn it into a

data frame.

It looks like this, we call this a document feature

matrix.

And then with that document feature matrix, which is the

core data set you would use for analysis, you can

do all.

Different kinds of analysis.

So this, this lecture is about going from here to

here and then the remainder of the class is like

a lot of this stuff, right?

Like how you then use this thing to do some

of these different types of analysis that you that we

want to do with text, OK.

So outline first we're going to talk about document selection.

So that's like the rows of a data frame, like

what are the individual documents that you're going to be

studying.

Then we're going to talk about feature selection.

That's the columns of the the document feature matrix.

So that's like in our case here in this class

is going to be words, but I'll talk more about

that later, um.

Then we're going to talk about various issues, problems that

arise when you do feature selection, meaning you figure out

what columns you want to have in your in your

data frame that you do analysis with.

Then we're going to talk about, so we're going to

get all of this is going to get us to

the point where we create a document feature matrix with

like one of these data sets.

Um, and then we're going to do a few things

with that document feature matrix that are kind of simple

things like describing text.

You can think of this if you sort of remember

from your stats class like descriptive statistics on your data

frame of textual data, and, and I'll go into some

detail there.

And then maybe probably almost certainly not, we'll do.

Number 5, which is two approaches to analysis with document

feature matrices.

Almost surely I will not get there, but I had

it in the slides already and then I looked at

how long the slides were and I was like, oh,

that's probably, we're probably not going to get there.

But I left it in these slides in case you

want to preview.

If we don't get to it, which we probably won't,

it'll just be kicked to next week because it sort

of naturally fits in next week as well.

OK, cool.

Now, So let's start by talking about document selection.

I don't have a tonne to say about this, but

I do want to establish some vocabulary first.

So a document again in the slides, the blue bold

text, you can think of these as like important vocabulary

words, a lot of terminology, important ideas.

Sometimes there's, I think red, red text, red texts are

links.

So if you see something that's red, you can usually

click on it and takes you somewhere.

So a lot of the sources in here.

Um, I'll have the citation, but then it'll be red

and click on it and it'll take you to the

website where the where the original source is.

So vocabulary, OK, so this is important terminology.

A document is the fundamental unit of analysis for quantitative

text analysis.

So it's like the row of the eventual data frame

that we're going to make.

Now you have to choose what you want your rows

of your text analysis data frame to be.

So you could choose n-word sequences.

So you could say like I want every row, every

document in my data set to be like sequence of

10 words.

You might have a reason to do that.

I don't know.

It could be sentences, so you might want to pull

out all the sentences in a whole bunch of texts,

and each sentence is considered its own document.

Could be pages.

So if you have like a novel, you might say

I'm going to take like a PDF like printed novel,

and you could say I'm going to take out every

page of this and each page is going to be

a separate document in my analysis.

You could do paragraphs from a document.

You could do you could use natural units.

So, so like for example, the whole text could be

a document, right?

When we use the phrase document, typically we think of

like an entire document, so we think of like a

novel.

Would be a document or a legal opinion would be

a document, but in the context of quantitative text analysis,

the word document is a is a is a little

bit different.

We use it a little bit differently.

We are choosing what the documents are in our document

feature matrix in our data set.

So that could be we, we break up what we

think of as documents like books or whatever into paragraphs

and so each paragraph is a separate document.

Um, so when we say natural units, that's like the

word I'm using for like the full document as we

know it in the colloquial sense.

Or you could aggregate even further and say I want

every document in my data frame to be all speeches

by everyone from a particular party in a particular year.

um, so like every row is that so you're like

actually aggregating documents together, you're pasting them all together and

treating a bunch of natural unit documents as one document

for the purposes of your QTA analysis.

So then we refer to a collection of documents as

a corpus.

So when I use the phrase corpus, I'm going to

constant, I'm going to mean like a set of documents.

When I say document, I mean like how is it

that we broke up the texts that we have into

rows that would go into the data frame.

You know how you define the documents, whether you choose

each document as each sentence as a document, each paragraph

is a document, so on and so forth, that is

dependent on your research design, a theme that you're going

to notice over and over and over again in this

class, and especially in these slides, you're, it's going to

become really tedious and repetitive at some point.

So I'm going to keep saying some of these choices,

a lot of these choices that you make in quantitative

text analysis about how you do things depends on what.

You're trying to do.

So how you define documents, whether you want to use

sentences as documents, paragraphs, whatever, it depends on what you're

trying to do, what kind of analysis you're trying to

do, and what kinds of issues are coming up in

your own analysis.

And I don't have a magic formula for this, right?

This is the hard work of you figure, I mean

if you're doing a QTA type project, you sitting down

and figuring out, OK, what makes the most sense as

a way to break up the text into rows that

go into my data frame.

And that's dependent on the task that you're trying to,

you're trying to do.

Now there's lots of rules of thumb about how to

do that for a variety of different kinds of analysis,

and it'll come up a few times in this class,

but ultimately you kind of have to make that decision

yourself based on what it is you're trying to do.

Now remember, first assumption of QTA from last, uh last

lecture, texts represent an observable implication of some underlying thing

of interest, right?

So So that's the first assumption of quantitative text analysis

in social science is that texts represent something larger in

the social world that we care about that we're trying

to study.

So when you choose what documents you want to have

in your data set, so that is like at the

big level like what texts you want to collect in

the first place and then how you want to break

them up into documents, this is all about what is

the social science question you're asking and what's the best

way to go about answering it.

So in this class I want to just do a

brief side note about terminology.

I've already said some of this before, but it's important.

A document is a chosen unit of analysis, so it

could be a full document in the colloquial sense like

legal opinions, laws, whatever, or it could be pieces of

it, paragraphs, for example, sections, chapters, whatever.

Sentences, um, but when we use in this class when

I talk about documents, you should think, oh, so this

is like the rows of the data frame and it

could be full documents in the colloquial sense or it

could be parts of the documents like paragraphs or whatever.

Um, a text used as accountable noun, so like a

text or the text.

That's what we often refer to as the document in

the colloquial sense.

So the texts that we start with are like the

legal opinions or the laws or the speeches or whatever.

Each one of those is a text.

We may then decide to break up each of those

texts into smaller subunits like paragraphs or sentences to create

our documents, but texts text, the text, texts, that will

be the phrase we're using for generally the sort of

like the thing, the original things we collect the PDFs,

the speeches, whatever.

And then text uses an uncountable noun.

So, so this is like referring to text as a

concept that's going to be a general word we use

to label a type of data, so it's equivalent to

string.

So when I say the text of this is blah

blah blah, that's I'm talking about string.

When I say a tax, I'm talking about a document,

OK?

All right, now, I keep coming back to this, so

I just, but I do want to keep reminding you

that about the vocabulary we're using because QTA people, like

all methodology people, have a particular set of words they're

using sometimes doesn't match our intuitions about what those words

would mean, and so I want to keep reminding you

of this terminology.

So we're gonna try, I'm going to try really hard

to be consistent and no promises about what Friedrich is

going to do, but I'm going to try to be

consistent, but the context will usually indicate what we're talking

about.

So if I slip up and I refer to a

collection of text files that you have on your computer

as the documents that you have collected from some website

that you then intend to use for QTA, you should

know, oh, he means the texts because I haven't yet

gotten to the point where I've figured out what rows.

I want my data frame, so he's using the word

document in the colloquial sense and not in the narrow

QTA sense.

So the context will also give clues about the meaning,

but I'm going to try to be consistent.

When I say document, I'd be like, what is the

unit of analysis for your QTA project?

When I say texts, I'm talking about like what are

the actual like texts that you originally collected the PDFs,

the text files, the web pages, whatever it is that

you want to use in your text analysis.

OK.

Now, stepping back, how do you know which text you

should collect and include for your QTA analysis?

Oh, this is a really complicated issue.

It starts, again, this is in the spirit of me

being repetitive.

Answering this question starts with a deep substantive knowledge about

what it is you want to study, OK?

That you can't get around this problem.

I think a lot of people come to QTA thinking,

oh, there's all these like things you can do and

it's kind of fun and like whatever.

In the purpose in the context of doing social science,

like you have to start with the substantive social science

question, and then that's gonna lead you down the path

of figuring out what documents you, what text, sorry, what

text you might want to collect.

Um, to use for your QTA analysis.

Um, now, as with most research, most things in research,

it's like part science, part art, so figuring out what

text you want to study to answer your social science

question is like, is like the original work you do

as a researcher in some sense, right?

It's not like, and there's some rules of thumb we

use.

You'll see lots of examples in this class that I

think will help give you an intuition about how to

do this, but some of this is, is, is about

you and like your, your background and intuitions about what

you think a good idea is.

And this is like, you know, not all research is

like very scientific.

Some of it is a little bit about the creativity

and the art part and that and that comes into

figuring out what text you want to study for your

QTA project.

But I do want to point out there is a

difference between a sample and a population.

This is kind of like an important distinction.

A population of texts would be like you have all

the texts.

Um, a sample would be you have some subset of

those texts.

You can get a random sample of.

Text you could get a non-random sample of texts, but

the key thing is you should be aware when you're,

when you have a collection of texts you want to

use in a QTA project, like do you have the

population of text, meaning do you have everything, all of

the texts.

So for example, if you're studying, uh, Prime Minister's questions

in the UK, like do you have all of the

transcripts of the Prime Minister's questions, uh, available to you?

Now, probably you don't have them all, right?

You probably have for a certain year range or whatever,

but you would have the population, if you have them

all for a particular year range, you would have the

population of them for the year range.

Sometimes it's not feasible, social media posts, there are like

millions, probably billions of social media posts every day.

Computationally, you probably couldn't have the population of them because

there's just too many.

So you would have to think about a sampling strategy.

Well, how am I going to choose a smaller number

of them that I'm going to use?

Am I going to choose it randomly?

Am I going to choose it based on features, you

know, I'm going to go to this particular website, I'm

gonna do this, that, the other.

Um, that's important.

The distinction between these two things is actually a little

bit more philosophical than when I'm leading, leading you to,

to think about here.

That's beyond the, the, this course.

But I want you to keep in mind that when

you, when you decide what text you want to collect,

whether that be you're collecting texts, you know, entire population

of texts or you're collecting a sample of texts, you

should think in the back of your mind like how,

how is the collection of texts that I'm getting.

Going to help me learn about the social science thing

I want to learn about and what potential biases might

be introduced in my statistical analysis, and I mean biassed

in the, in the standard statistical sense.

By way of my choice of how I chose my

text, how I sampled, how, you know, what websites I

went to, and so on and so forth.

Now, I'm not going to cover this in a lot

of detail in this class because this is also a,

you know, this is something you got to think about

for your particular uh research task, but I would recommend

reading chapters 3 and 4 of the, the, the book

because that actually goes into this in a little bit

more detail, um, and I think it's kind of interesting.

All right, so where do you obtain textual data?

There is a bunch of existing data sets, OK.

So you can exploit those, like by existing data sets,

what I mean is like out there in the world,

there's like websites and uh Uh, archives and stuff that

have collections of texts that you can then use for

quantitative text analysis.

You could collect your own data so you could get

it from social media websites or blogs.

You could scrape from websites, right?

So you could build a little web scraper that goes

across a bunch of websites and collects texts from it.

You could also digitise your own text using OCR or

some other kind of digitization method.

We talked about that briefly last week.

So if you have a collection of PDFs, for example,

you could digitise them, uh, and extract the text with

OCR, um, you could go to the, you like go

to physical archives and take pictures of pages and books

and then Turn them into PDFs and then do OCRing,

right?

So these are ways you can get textual data, but

I want to give you a warning on like, please,

please listen carefully.

Always before you start collecting texts, especially from the internet,

from databases, whatever, always look at the terms of service

and do not violate the terms of service.

Do not scrape a website that you're not supposed to

scrape.

And never, and I mean never scrape stuff from the

library.

Just don't do it because if you do, you're going

to get the whole school cut off from the, from

various resources.

So I, I, I feel like this is really important

because there's a lot of news articles and text form

that are available.

In the library's website, do not scrape them.

If you want to use them in a text analysis

project, you've got to go manually down them one by

one.

If you don't, you will get the, you will get

all everyone else cut off because they will shut us

down.

OK, so this is really important because I don't think

a lot of people are aware, but the library's terms

of services are Such that you should never scrape things

from the library's paid databases.

You should never do it.

Cool.

We have everyone hear me?

OK, good.

Don't do it.

All right.

Um, but also we're not going to cover data collection

in a lot of detail in this class.

There are other courses where you can do that MY

472.

So this class is primarily like presuming we have some

textual data available and what kind of analysis can we

do with it.

That's just like, you know, we have limited time and

we want to focus more of our attention on the

analysis part and not on the kind of data collection

part.

OK, so the example I'm going to use to build

ideas in this lecture, probably other lectures as well, a

seminar is, uh, there's a corpus of US President Trump's

tweets from 2017.5 of 2018.

So that's going to be available on the course website.

It's actually in the week 2 folder, so you can

find it on the GitHub where these slides were.

It's in that folder.

The data is stored as a JSON file that's, it

came, it came from Twitter, so whoever originally collected this

used the Twitter API that doesn't really exist anymore.

Um, so it's not formatted as a standard JSON.

This is kind of in the weed stuff, so you

can't use like typical packages one would use to read

a JSON file.

That's OK.

There's this package called streamer that is designed to deal

with Twitter API data.

So we're going to use that package to load in

the corpus of tweets from this JSON file and from

using that, um, loaded in corpus of tweets, we'll be

able to do all of the stuff we want to

do.

Cool.

All right, we'll do that seminar.

So I'm not going to show you how to do

that here, boring way to spend my lecture time, but

we will do it in seminar.

All right, so this is a tweet that I'm going

to use as a, as a motivating example to do

some stuff, to show you some stuff.

So this is the, the first tweet in this corpus,

which is just a happy New Year's tweet from Trump

in On January 1st, 2017, um, and it says to

all Americans, Happy New Year, many blessings to you all

looking forward to a wonder and prosperous 2017 as we

work together to MAA, which means Make America Great Again.

OK, cool.

And then there's an image in it as well.

So this, so obviously you see this image here, it

has text in it as well.

We're not really gonna like, I'm just going to ignore

this.

We're going to focus on the actual plain text here.

You could in theory also use tools to extract the

text from images we're not going to do that here.

I mean, like I just so you know, you can

do it.

It's not really like it's not important for us to

do it here because all the text analysis stuff we

can do, we can just do it with this plain

text, but if you wanted to add additional text to

your corpus, you might want to go through all the

images and extract out all the text as well.

This is common with like Instagram posts and stuff.

You'll see like Instagram.

Pictures with text in them.

You might want to extract the text, but we're just

going to work with the plain, the plain text that's

in the original tweet.

Cool.

Now this is what it looks like when you open

the file.

The JSON file, it has a lot of data about

each tweet, so it's like what was the date the

tweet was posted, what is the ID number for the

tweet?

It's like a Twitter ID number and then this is

like a long version of the ID number, the full

plain text, um.

The, uh, yeah, the full plain text and then other

like Twitter stuff.

OK, so for example under entities in the Twitter JSON

file it tracks what hashtags were being used, right?

But you can obviously also see the hashtags in the

plain text here, but the the structured file also has

all this stuff as other columns in the data.

It's fine, whatever.

This is, this is what it looks like underneath the

hood.

When you use that streamer package to load in the

the tweets, they will come in as a nice data

frame.

So it'll be like every row is a different document.

So every row is a different tweet, and then every

column is like one of the variables here.

So there'll be one column for what date the tweet

was posted, one column with the ID number, one column

with the longer ID number, one column with the full

text of the tweet, and so on and so forth.

Now Let's do feature selection.

Let's, this is the, this is actually the like this

is like the more important thing.

Like document selection is really important for you as an

individual coming up with your project, um, but there's not

as much I can say about that for all of

you, sort of like there's not like universal lessons I

can give you, but with feature selection that is figuring

out what columns you want in your data frame, like

what the sort of core thing you're analysing with quantitative

text analysis, that's, we've got a lot to say about

that and I'm gonna, I'm gonna say a lot about

it now.

OK.

So I'm going to start by pointing you to the

second assumption of QTA from last lecture, which is that

text can be represented by extracting their feature.

Assumption one was Texts are an observable implication of real

world social science things we care about.

Assumption two is texts can be represented by extracting features

from them, and those features are informative about the things

we want to know about.

OK, so what do we mean by features?

So again, this is vocabulary, very commonly used in the

context of QTA.

It could be individual characters.

So you have like in English there are 26 characters,

right?

So A through Z, well I guess 26 times 2

because you have the lowercase ones as well.

Well then you have symbols and punctuations, OK, so a

lot more than 26, but you could take each individual

character as a unit of as a feature.

So you know, your columns and your your data frame

would be like the letter A, and the next one

is like the letter lowercase a, and the next one

is like the letter B, so on and so forth.

You could have individual words be your features of interest,

right?

That's uh Uh, that's these two things are what we

are going to do here.

So words and then words stems or lemmas.

I'll talk about this more later, but this is just

a way to like standardise words into their root form.

But so, so one thing you do is you can

focus on the individual characters as the features of the

document.

Another thing you can do is focus on the individual

words or kind of Uh, formatted versions of the words

stems and lemmas, which I will talk about in more

detail.

You can also focus on word segments.

So some languages like German have like these very long

words that have multiple words inside of them.

So you might actually want to break up those sort

of multi-word words and focus on the segments in them.

So I guess apparently, apparently I googled this.

This is a, this is a sort of mild insult

in German.

It means a person who sits at the bottom of

the sauna.

I don't know why that's an insult, but It has

multiple words in it, sauna, uh, under, sit.

So you might want to break that up into three

separate words, right, and not consider it as one word.

OK, cool.

Then you, you might want to have word sequences be

your, your feature of interest, right?

So, um, That this is more common in context where

there aren't interword delimiters like white space.

So in English words are divided with whitespace, right?

So every word is separated by a space or multiple

spaces or a line break or something.

But in other languages like in Chinese, that's not the

case, right?

And so when you're figuring out.

Words, you want to, you say you want to use

words as your main feature of interest, but you are

working with a language that does not use whitespace to

divide up words.

Well, then you might, you might have to come up

with some complicated way to figure out sequences of characters

that count as words, right?

Um.

So, so there's that.

Um, you might want to have as your features of

interest parts of speech, right?

So like nouns, verbs, so on and so forth.

So that, I don't know why you might be interested

in that, but you, you might be interested in that,

right, as like the, the, the feature of the document

that you think teaches you about your social science topic.

Um, and then the final thing to mention here is

word embeddings.

Um, and that second to last one, I'm not really

going to mention it, but word embeddings, I'm going to

talk about more later.

That could be a feature of a document that you,

you actually want to study.

OK.

The most common approach used in quantitative text analysis in

the social sciences is what we call the bag of

words model.

OK, so I like this.

It's a very evocative phrase because it literally like makes

you think about a bag full of words and the

words are just all in the bag, all mixed together,

right?

Like that's that's like where it comes from.

What this model is is a model that treats single

words as the feature that you care about in your,

in your quantitative text analysis analysis, um, and so we're

gonna, so bag of words assumes words are the relevant

features of interest for you, OK.

So documents are quantified in the bag of words world

based on how many times individual words show up.

So if you think about the row of your, your

eventual data frame that you'll get to, a document is

going to be represented by a vector, like a bunch

of numbers, and each number corresponds to how many times

a particular word shows up in that document.

Now a key thing, a key thing of this approach

is it completely discards grammar and syntax.

Word order doesn't matter.

So if you start with the bag of words assumption,

you're going to use the bag of words model.

What you're saying is, I think that the relevant feature

of interest that I'm going to use in my analysis

are individual words in a document, and I don't care

about what order they're in.

I'm going to treat them as a bag of words

and I'm just going to, for every document count how

many times each of the individual possible words that could

arise arose in that document.

Now you might ask yourself what we all speak languages

where the word order matters, the grammar matters, the syntax

matters.

It doesn't matter maybe as much as we think.

People usually can understand what you mean even if you

use incorrect grammar, but we all have a sense that

word order especially matters, right?

Like, obviously if I were to mix up the order

in which I'm speaking right now, you would probably not

understand what I'm saying, right?

OK.

So why do we do this?

Why do we do use this fiction of the bag

of words and we do quantitative text analysis where we

just say, you know what, all we care about is

like counting up the number of times individual words appeared

in the document and we don't really care what order

they appeared.

We just want to know how many times they appeared.

Well, obvious reason we do this is it's really simple

to do that.

Like it makes all of this stuff so much easier

than having to like think about word order for reasons

that we don't want you to get into, but it's

a lot more complicated if you want to start thinking

about word order and and whether that matters.

OK, cool.

But that's a, that's a, that's a dumb reason to

give you, right?

Obviously like we like to do simple stuff, but if

simple stuff is not a good thing to do, we

shouldn't do it, right?

Well, it turns out, it turns out that this just

works really well.

Context that is word order, grammatical information, stuff like that,

often is uninformative and also it's conditioned on the presence

of words.

So what do I mean by that?

Like it's true that word order matters and grammatical information

matters and syntax matters and so on and so forth,

but there's a lot, there's a high correlation between the

presence of particular words.

And context that would be meaningful to a person.

That's another way of saying that like, The having a

large number of particular words in a document is itself

informative about word order, context, meaning.

How do we know that?

Decades and decades and decades of experience with research projects

where people use bag of words and it turns out

you learn something that people find appealing and that they

have validated.

So, it just turns out that this works really well,

that you don't actually need to think about word order.

In order to use QTA tools that then teach you

something that you intuitively understand to be like correct when

you look at the output of your QTA analysis and

then you read the documents, OK, yeah, that gave me

a good description of this document, even though it starts

with a model where we didn't, we ignored the word

order.

OK, So the other thing to say about this is

We're going to be using unigrams.

Meaning that we are going to treat individual words as

the features of interest.

Some we are going to talk more about this later,

but you could use bigrams or trigrams or n-grams, meaning

what you do is you, your feature of interest is

like, so in the case of bigrams, is collections of

two words next to each other.

So you could do that instead, but it turns out

that uh words that co-locate, meaning they appear together often,

are pretty rare.

Well, no, they're common.

It's common, but actually most of the time when it

happens it doesn't mean anything.

For example, of the occurs together a lot in English.

That doesn't mean anything, right?

The fact that of and the are together doesn't give

you any substantive meaning about what that two words means.

So you don't need to treat them as being next

to each other because they don't have meaning next to

each other.

You know, a phrase that does have meaning with two

words are next to each other.

So here's an exception to the rule New York.

New York is the name of a city.

If you split it up and you treat new and

New York as separate words, that's kind of weird.

You want New York to be one word, but those

examples are actually pretty rare in English.

So most of the time you can just get away

with.

Treating individual words as the features you care about, not

really caring about the word order, and then making a

choice on the fly about individual co-located words New York,

United Kingdom, United States that you might want to merge

together to keep as one word.

But that's a case by case determination that we'll talk

about later in this in this class.

The point is there's been decades of research using the

bag of words model and it's given we've like learned

a lot from it and it's fine.

I mean that's the best thing I can say for

you is like this works over and over and over

again.

So like let's just do it.

It's simple and it works.

Now there are times where word art is really important.

Let me give you a few of them.

Text reuse.

Some projects involve using plagiarism detection software to study things.

So for example, quarterly studies uses plagiarism detection software to

study how much language from, um, she read a lot

of papers, so I'm not exactly sure which paper this

is, but it's what she does is she looks to

see how much language in legal briefs is reflected in

the opinion that courts, the opinions that courts issue, so

like the question.

Of interest might be like how often do courts just

take legal briefs submitted by parties and just like incorporate

the text into their opinions like sort of do a

shortcut and like you know so that's a a kind

of question you might want to use plagiarism detection software,

but in order to study something like that that is

text reuse.

You, the word order matters.

Like the order in which the words appear is what

makes something plagiarism, right?

Like the fact that you use the same number of

words in one text versus another text does not make

it plagiarism.

They have to appear in the same order.

So, so plagiarism is, is like, I mean, obviously you're

in an academic context, you know what plagiarism is, so

I'm using that phrase, but the more general phrase is

just text reuse.

It's not always plagiarism, right, like.

Judges will reuse text that they see in legal briefs

and nobody calls it plagiarism.

It's just, I don't know, it's how that works.

It's how it works for courses.

Plagiarism is a particular academic context, uh, a concept, but

this software is usually is built for educational context, but

it's actually kind of cool.

Social scientists have used it, like, you know, turn it

in and stuff.

They've like used this to to do like social science

research.

So it's kind of neat.

Grimmer did the same thing with um press releases issued

by members of Congress to see how much they.

They're kind of like echoing each other and like and

putting out the same message.

OK, cool.

Another thing you might be interested in is parts of

speech tagging, so that's like looking at text and then

figuring out what part of speech each word is.

Is it a noun?

Is it a verb, is it an adjective, so on

and so forth?

Turns out that in order to determine that, the order

of the words matters.

So there's some.

Applicationss mentioned here you can look at.

So Batman and Smith and Handler at all.

Um, but this is where you're, you're trying to get

grammatical information from the words and, and, and it turns

out that the order in which the words show up

determine whether some words are a noun or a verb,

for example, um, so it does not matter, order matters

there.

And then named entity recognition is another example.

So I already kind of hinted at this before, like

New York is a named entity, meaning it is a,

it's like a, it's a thing that has a name.

New York has two words in it and so, and

the word order matters.

York new is not the same thing as New York.

And so for situations where it's really important that you,

uh, extract out named entities, names of cities, names of

company, names of people, the word order matters, right?

So, yeah, like York new is not the same thing

as New York.

There are tools for doing this.

It's called NER.

So an example is actually from a paper I wrote

where we had a bunch of legal records that listed

the plaintiffs that filed civil law, civil rights lawsuits in

US court, um, and it had all the plaintiffs' names

in it.

So what we did is we used the NER algorithm

to identify which Plaintiffs that were bringing lawsuits were individual

people like humans and which ones were companies or nonprofits

or something.

Turned out it we don't need to talk about why

it mattered, but it's a very minor part of the

paper actually, but we did use this to try to

identify who were the humans bringing lawsuits and who were

the like organisations or or companies or whatever.

Now, as usual, whether word order matters depends on the

QTA task.

90% of the projects you will be interested in will,

will not require you to think about word order, OK?

So we're not covering it in a lot of detail

here.

This is about all you're going to get on, on

word order.

Now you can read more about it in the textbook.

There's a, there's a whole chapter on it that has

even more examples.

So if you want to see examples of where people

have done text reuse, parts of speech tagging and named

entity recognition to get an idea of like how these

things work, you can look at the additional sources that

are in the, in the textbook, but here are 5

you can look at, you know, by clicking on these

links that are red.

OK, cool.

So now how do we implement the bag of words?

We already talked about what bag of words is.

I hopefully gave you a justification for why we use

it.

It's fine.

We can use it.

It seems to perform well really across a wide range

of applications and also it's simple.

And also there's some, I mean, I'm going to repeat

what I said before because I think I maybe mangled

it when I said it the first time.

It turns out that There's a correlation between the presence

of words and then sort of context type stuff, right?

So you learn about word order and grammar and stuff

simply by counting the number of words that appear.

That's just like a it's like a linguistic fact.

OK, so this is another kind of more principled justification

for a bag of words that turns out that there's

a correlation between just like counting the number of words

and learning about context and grammar and stuff.

Not like a perfect correlation, OK, but there is a

correlation and so this is like a linguistic justification for

a bag of words that that's a little bit more

principled than like, hey, it just works all the time,

so it's fine.

OK, but now how do we implement it?

There's a four-step process.

First, you choose your unit of analysis.

We already talked about that.

That's like you're choosing your rows for the data frame.

So what are your documents?

So we've covered that.

Then we're getting to the point where we had to

figure out what our features are.

Of course, of course, if we're using the bag of

words model, our features are going to be words.

So we've got to tokenize.

That means we take a text and we split it

up into the features of interest, tokenize it means turning

it into tokens.

So what that is, you have a string and then

you split it up into a vector and the vector

contains every distinct token that's in the document.

I'll show you an example in a minute.

And how do you split it up?

Well, in English it's using whitespace, but I'll talk about

that in a second.

Um, then you've got to reduce complexity.

Oh my God, this is, this is a big topic,

lots of things to say here, but I'm just going

to summarise it as reduce complexity, and I'll explain what

that means in a minute.

And then you create the document feature matrix, which is

the data set you will use.

So that's every row is a document, every column is

a feature, and then each cell is how many times

that feature appeared in that document.

Easy.

Now people usually refer to this whole thing as preprocessing,

getting from a set of texts.

Oh why did I do this?

This is a dumb idea.

OK.

This, getting from here to here, this stuff is usually

called pre-processing.

That's just like the general term people use.

All right.

Now, and it encompasses encompasses all of these things.

The reducing complexity part, this is where like all the,

this is where all the discretion is, you know, this

is like where the analyst has a lot of leeway

to make things go in one direction or another.

Um, so I want to tell you something right now

and I'm going to keep coming back to it in

seminar and whatever.

When you do QTA, you got to justify why you

did the preprocessing steps you did.

You can, there's a lot of things you can choose

to do or not to do, and you're going to

see them in a second.

Your QTA application has to justify why you did the

preprocessing in the way you did.

Why did you include some words and not other words,

so on and so forth.

But that depends on the research question, and each research

question will require different kinds of pre-processing.

OK, cool.

So let's talk about the steps of the preprocessing.

So first, tokenizing.

Once we have our documents, we tokenize each document.

So what does that mean?

We split that document into an array, meaning a vector.

An array of words.

Each of the words inside the vector is called a

token.

That's why it's called tokenizing.

Tokenizing is the verb form of it.

In English, you tokenize using whitespace because words are separated

with white space, spaces, line breaks, so on and so

forth.

Those are all called white space.

Um, it's trickier for logographic languages like Chinese that don't

use whitespace to split up words, but there are a

set of like out of the box tools you can

use for languages that are logographic that have algorithms that

decide where to split.

So it's, it's kind of boring.

You just have to do an extra with English, it's

easy.

You just like find all the white space.

Split up the text based on that and in any

language that uses whitespace to separate words with with logographic

languages, you've got to like run it through an algorithm

to figure out where to do the splits, but it,

you know, at this point it's so well developed, it's

not that difficult.

I'm not going to cover it in detail, but I

promise you you Google it, you'll find out very quickly

how to do it in R.

Now, um, tokens that then result from tokenizing are mostly

words, words that we would recognise as words, but they're

not always words, right?

For example, tweets have URLs in them.

Those aren't really words in the like sense of like

English language words, but they would still be a token.

Punctuation often appears as a token as well.

So like if you had, uh, you ampersand, meaning and

I, you tokenize it based on the white space, you're

going to get ampersand as one of the tokens, Ampersand

being the symbol for and.

Not really a word.

I mean, I guess it represents and, but it's not

really a word, OK.

So you're gonna get mostly words, but not always.

Um, but the list of all the distinct tokens used

in an entire corpus is called a vocabulary.

So all the features that appear, like all the columns

in your eventual document feature matrix, we call that the

vocabulary.

How many unique tokens are there in your, um your

Your, uh, corpus, and then each element of the vocabulary

is called a type.

So just so you know, when I'm talking about tokens,

I'm talking about the words appearing in a document.

There's a lot of repetition.

The word the is going to appear in many documents

multiple times, so there's multiple tokens of the word the

in any given document.

When I when I talk about the vocabulary, I'm talking

about just like the list of words that appear at

least once in any document.

That's the vocabulary of the corpus, and each of the

words in the vocabulary is called a type.

This is just jargon that QTA people use and I'm

just trying to get you familiar with it.

Now a vocabulary obviously is decided by your QTA project.

So when you do all the pre-processing steps, you'll end

up with a vocabulary, and that vocabulary will be specific

to your corpus and your project.

Um, so we're using the word vocabulary here in a

more narrow sense than you do colloquially, where you talk

about English vocabulary, meaning like all the possible words you

could say in English.

In a QTA project, the vocabulary refers to all the

words that appear in the corpus.

And each one of those is referred to as a

type.

So there are a vocabulary consists of many types.

OK, so let's show you what this looks like tokenizing

for a real world example.

Here's the original plain text of the Trump tweet.

A couple things to point out here just in the,

you know, going back to digitising text stuff.

That symbol slash N that's a line break, that's a

plain text.

Way of rendering a line break.

So if you, if you like printed this out in

a more formatted way, this would actually be another line.

But when you print it out as plain text, it

just shows you that's supposed to be a line break.

It's quite nice actually to print it out in this

format because you can see all of the characters in

here.

Without formatting it and then destroying information because if you

just saw the line break, you wouldn't, you you might

not know if that was a line break or if

it was like two separate strings that were printed, right?

Like so it's nice you can see explicitly there's a

line break in there.

Also this is HTML code for Ampersand, the symbol for

and.

In HTML you, a lot of symbols like that, you

don't, they don't appear explicitly.

They have, you have to put a code for them.

And a little bit ironically, I guess, uh, in this

case, the ampersand.

Code contains an ampersand in it.

That's kind of funny, but all HTML code for symbols

contains an ampersand.

But the one for Ampersand is Ampersand Amp, uh, that

the one for, um, You know, I just don't know

off the top of my head there's one for space

that's like ampersand and then NSBP or something.

You wouldn't know off the top of their head and

then semicolon, but each symbol in HTML has its own

code.

So this is actually supposed to be an ampersand, but

this is from an HTML document, so it's rendering as

HTML code.

OK.

The other thing to notice is there's these things here.

Do you see this tweet has a hashtag that has

an emoji next to it, an American flag emoji?

This is The way the American flag emoji renders in

kind of renders in plain text.

I mean, I had to do a little bit of

formatting here to, to make it explicit, but these are

Unicode characters.

This is a Unicode code point.

It doesn't print here because it Doesn't print in this

format like in PDF, the kind of PDF I created.

So instead it just shows you the code points for

it, but these two code points together create the American

flag emoji.

So if I were using software that knew how to

render Unicode Unicode code points for, for uh emojis, it

would just show you the emoji.

But the software I use to create this slide doesn't

know how to do that, and so it just appears

as the the code points.

Yeah, no big deal, but you, you know that these

are things that you should be aware of when you're

looking at.

And then there's, of course, a link.

Now, if we tokenize, um, OK, first of all, I

got rid of the emoji.

I had to because of issues.

You might not want to in your text analysis project,

but I had to because of Just this PDF formatting

didn't work well.

So I, so just so you know, one thing I

did like behind the scenes is I just got rid

of the emoji because it's, it's not important for our

purposes.

So if we tokenize by using the white space, what

happens is every space or line break.

is the divider between words and then we get a

vector.

So this is our our syntax for vector C open

parenthesis, all the elements in the vector closed parentheses.

OK, so I'm just going to use the r syntax

here, but when we tokenize this is what we this

is what results.

So we have a vector of a bunch of different

individual tokens.

Notice issues here, useless punctuation.

The word Americans has a random uh thing after it,

but we might not want to keep that because As

far as this is concerned, that's going to be a

separate word from the word Americans because it has that.

I mean, computers are very literal.

It has that that there so it's not the same

word as the word Americans without that thing there, the

hyphen.

There's also some nonwords included.

There's a link here.

There is a year, there is the ampersand, um, and

then also notice the word to here and to here.

Same word.

The computer's going to think they're different words.

This is capitalised.

That's not.

They're different words.

One thing I want to say in the context of

coding and computer stuff, capitalization is really important.

You just got to get used to that.

Capitalization matters.

Computers are very literal.

They treat capital letters differently than lowercase letters.

We as as like more intuitive, smarter beings don't.

I mean, we kind of see them as the same

thing.

Computers see them literally as different things.

So capitalization is important.

Now if we start, if we tokenize and then create

a vocabulary from these tokens, we're going to have too

many words.

We're going to have one word for capital 21 word

for lowercase 2, but we think that's the same word.

So I don't want to have a vocabulary that has

a 2 and lowercase 2.

I don't want to have a vocabulary that has this

word in it with a hyphen at the end.

I don't want to have a vocabulary that has a

link in it.

It's not a real word.

It's a link.

So if we just tokenize and then create our vocabulary

from that and then create a document feature matrix, we're

going to have way too many words that we don't

want URLs, years, two separate versions of the same word.

Yeah.

There's another 2.

Um, why is that?

Yeah, there's 3, yeah.

So if we did, if we use the, if we

didn't do any additional pre-processing and created our document feature

matrix, we would have 1, so in the column for

lowercase 2, it would be the number 123, because there's

3, it appears 3 times in here.

And then in the column capital 2 it would be

1 time.

But what, but actually the word 2 appears 4 times

and we want to keep it as 4 times in

one column.

Does that make sense?

Yeah yeah.

OK, so to deal with those problems that I just

brought up, we can remove nonwords sort of on a

case by case basis.

So and then we can remove punctuation and we can

make the text lower case.

So I made all the text lower case.

I removed all the nonwords, so links, the ampersand, so

on and so forth, and then I removed punctuation, so

I got rid of the dash that was after Americans.

I did something though.

I made an exception to the rule, which is I

left in the the hash this hash thing, because that's

important in Twitter data.

Those are hashtags.

They have a particular, they play a particular role in

Twitter data.

You might also want to keep any word that starts

with an at symbol and keep the at symbol at

the beginning of it because what is that?

It's a username, yeah, so you might want to keep

that the at symbol and the hash when you're doing

when you're working with Twitter data because those are important

concepts in Twitter data hashtags and usernames because for example,

you might want to study how much people are talking

to each other through tweets.

So you would want to know, are usernames being referenced

in the tweets to To, to speak to Other people.

OK, so I got rid, and also I got rid

of the ampersand using a reg X pattern.

That's the reg X pattern.

You'll see reg X in seminar.

Um, and of course, I decided to keep the hashtags

intact.

I keep saying Twitter because this was actually from Twitter,

but of course we all know it's called X now,

so whatever.

I'm going to keep saying Twitter because this is from

when it was Twitter.

OK, so now we have this little bit cleaned up

set of tokens for this document.

What's nice is now the word 21234 occurs 4 times.

It will appear as one column in the DFM, right.

But there's a lot of words in there that are

considered stop words.

So those are words that are used extremely frequently in

language, but they don't have much meaning.

They don't convey much information.

So here is a list of very common English stop

words.

Different people have different lists of stop words, but this

is a pretty short list.

But like the word up.

You know, like, uh, texts, the word the, the text,

pronouns, you, he, she, it, like those are all considered,

I mean, the words, but they don't convey a lot

of substantive meaning.

So they're called stop words and typically people just remove

the stop words from their documents because they just, you

know.

It's a waste.

You don't get much information out of them.

Why include them in your analysis?

But of course this depends on your task.

There's a book about pronouns and how pronouns are important

for human language, um, so you could actually like for

example, you might want to include gender pronouns if you're

studying, if you're, you're working on a topic related to

gender representation and politics.

You might not want to remove the gendered pronouns which

appear in the list of stop words.

So if you just like don't think about it and

you remove the stop words, you're going to be removing

information that you Uh, actually want to study, so sometimes

people will start with a A stopboard list and they'll

take out certain stop words that they actually want to

keep.

You can do that, but it depends on your task.

Again, I'm repeating myself over and over and over again.

Depends on what you want to do, how you decide

to do this stuff.

Now, I'm going to remove, I just took this list

of stock words and I'm going to remove all of

the stop words from the Trump tweet that we're looking

at.

So the word 2 that appeared 4 times is now

gone because it's a stop word.

Um, so we're left with words that have more, you

know, more meaning than, than a lot of these other

words like 2 doesn't have a tonne of meaning, you

doesn't have a tonne of meaning.

Um, so although, you know, actually I'm thinking about it,

you like you versus us in political speech is actually

really important, like how much politicians say you versus us,

like conceptualising themselves as part of the group or not.

You can imagine a QTA project that that's important, but

we're ignoring it here.

We're getting rid of you, us, all those words.

This is like the, this is like the downside of

like pulling up all these QTA tools and just like

do it, you know, copying code that you see elsewhere.

It's like you've got to actually every step of the

way ask yourself like, do I want to be doing

this given what I'm trying to answer?

Um, OK, but here is the, the, the words that

are left after removing the stop words.

OK, so we still have substantively interesting words.

Um, although I don't know if Manny is very interesting,

but maybe it is.

OK.

So now we might want to even go further.

OK, what are we doing here?

Let me back up and say, Notice what we're doing

is we're culling our list of tokens, meaning we're removing

tokens.

Why are we doing this?

Well, I already kind of gave you an intuition.

Like you don't want to have too large of a

vocabulary for QTA.

Like you don't want to have a lot of like

meaningless words in your QTA analysis.

It's redundant, but in QTA.

You also don't want to have a lot of repetition,

so the capital 2 versus lowercase 2, you don't want

to treat those as separate words.

So there are, what we're doing is we're basically we're

Reducing the vocabulary by taking a set of steps to

kind of format and remove like meaningless words, but the

goal here is to get to a point where we

have a smaller vocabulary that hopefully is richer in its

in its, well, where each feature has richer meaning and

gives you better results in the final analysis.

And I'm going to come back to this a little

bit later, um, but we're trying to reduce the number

of columns in our document feature matrix by reducing the

number of words that we're considering as part of our

vocabulary.

So this is another way to reduce the number of

words in your vocabulary.

Some words have multiple forms with different spellings, so it

would technically be treated as different tokens, but they're technically

they're kind of the same word, right?

So for example, tenses, the word, the word the phrase

I see versus I saw, see and saw are the

same word, at least in this context, saw has another

meaning, but And see actually has another meaning as well,

but um, in this context, that's the same word.

So what you want, might do, there's just two different

tenses of the same verb.

So you might want to go to the root form

of the verb.

And just take that as your token, so that every

time C comes up and every time Saul comes up,

they're counted as the same word.

Pluralization, family and families.

Obviously, these are the same word at a substantive meaning

level, but they would be treated as different words.

Also families and families with the apostrophe, like, again, same

word, but treated differently based on spelling.

Now, you might want to, when you're confronted with a

situation like this, create what are called equivalence classes.

So the way to think about this is like you

say, here's a set of words.

This all these words are equivalent.

C and so, set of words, two words, C and

so.

Those are equivalent to each other.

Because they are the same word, they are just different

tenses of the same word.

So I've created an equivalence class, meaning anytime I see

the word C or I see this, the word saw

in my corpus, I'm going to treat them as the

same word.

So there are different approaches to doing this, um, uh,

but basically what you want to do is for every

token, you identify the root word and then you replace

whatever word you're looking at with that root word.

So for for example for C and saw you might,

you might just say, you know, the root word here

is C and I'm just going to replace every instance

of the word C, saw, seen, so on all these

different tenses of the word C.

I'm just going to replace them with C.

The root form of the word.

Um, and it reduces the vocabulary and it's, it's quite

useful to compare across documents.

You might want to, it might not be important that

one person is writing in past tense and one person

is writing in present tense, and they're both talking about

seeing or or saw, and you, you.

Want to compare across documents how much people are referring

to the word C, but they're using it for tenses

and so you may want to get rid of the

information about the tenses.

Now there are of course times where whether somebody is

talking in past tense or present tense or future tense

is important because of the QTA task you're doing, in

which case you wouldn't want to get rid of the

tenses.

But again, for a lot of things that we're doing

in this class in a lot of applications, that's not

the focus of study.

It's like whether somebody's speaking in past tense or future

tense.

And so therefore we want to merge together all the

words that are have the same root word but are

just different tenses.

OK.

Now, two ways to do it.

Practically speaking, you can limitize, that's called limitization.

That's an algorithmic process.

You have to like, you have to find some software

to do this basically.

Um, it's an algorithm that uses a set of rules

to convert every word into their lemma, the lemma being

the canonical form of the word.

So I've been talking about root words, but like you

can think of like.

Alemma is, this is like the linguistic term for the

root word in quotes, but it's the, it's like the

basic form of the word.

This is an algorithmic approach.

Stemming is a simpler approach.

What stemming does is it just, it literally just removes

the ends of words.

So it stems, it takes, you know, like chops off

the ends of words.

Um.

So the basic difference is stemming stemmers operate on single

words without knowledge of the context.

Leizers are more powerful.

They're algorithmic.

They, it depends on context.

They, they will limitize using word order.

So they're more powerful, but there's a computational trade-off.

Almost nobody uses limitizers in the applications that I am

aware of.

They mostly just do stemming.

It's much simpler and it turns out it, it works

for most situations.

Like you would be, you, it's computationally expensive to do

limitizing.

But let me show you an example.

The word production, producer, produce, produces, all of those words

are produced, all of those words would be replaced with

P R O D U C if you used a

stemmer.

It just chopped off.

Chopped off the ends of the words that created different

spellings.

Now, uh, tools for each of these are available in

the quantity package that we're going to be using for

text analysis that you're going to see in seminar.

Now, a lot of different ways to STEM and limitize.

You could, I could teach probably 3 lectures on this.

The porter stemmer is the most common one that's used,

so it's a stemmer that, again, remember, stemmers just chop

off the ends of words, but of course there's different

ways to chop off the ends of words.

Porter came up with one way of doing it, uh,

but it gets some stuff wrong.

So policy and police are considered to be the same

words because it chops off the last letter and then

they become the same word, but we all know policy

is not the same thing as police, OK.

Uh, general becomes Jenner, um, iteration becomes itter, so, so

you know.

These are just bad, bad examples.

Now there are other stemmers you can use.

There are corpus-based ones, statistical ones, mixed approaches, like there's

a whole bunch of things you can do with stemming

and limitmatizing.

Um, it's, this is like in the weeds of like

computer science NLP that we're not going to get into

in this class.

Um, most of the time people just use something like

the porter stemmer and in our class we're going to

use something like the porter stemmer.

It's going to be like a variation of the, of

the porter stemmer, um.

Uh, and we're just going to like accept that the

price we pay for simplicity is like sometimes getting the

wrong, you know, the wrong words are merged together and

considered the same word.

It's life.

It's life with any kind of data analysis, you know,

quantitative data analysis, like there are always errors in measurement

and you just, it took me a long time in

grad school to come to terms with this.

Like I just wanted to get it perfect, but, you

know, you got to move on.

If it works, it works.

OK.

But I do want to say, it's not always appropriate

to use stemming.

I again being extremely repetitive here depends on your task.

There's this paper by Schofield and Mimno that find that

stemmers produce no meaningful improvement in the likelihood and coherence

of topic models and can fact degrade topic stability.

So we have a whole unit in this class later

in the class about topic models, which is taking a

set of texts and inferring the topics those texts are

about using quantitative tools.

This paper shows that using stemming as a preprocessing step

prior to doing text analysis doesn't help.

And actually can, can make your, can make it more

difficult to do the topic modelling.

OK.

Take away, you just got to read the documents and

you got to validate.

You got to try stuff, see if it works, go

back to the board if it doesn't, I don't know

what to tell you.

Like, it's, this is like part, this is like the,

the, the depressing lesson of QTA is that you still

got to read a lot of stuff.

Um, you just don't have to read as much as

you, as you otherwise would have to.

OK, so we're going to use Quantator's default stemming function,

Token's Word stem, and what it does is it takes

the word, for example, Americans and turns it into American,

takes the word many with a Y and turns it

to many with an I.

Takes the word blessing and turns it into bless, uh,

and so on and so forth.

So, um, Quanta uses a stball stemmer, which is a

variation of the Porter stemmer.

Just keep that in the back of your mind in

case anybody asks.

Uh, and the stemmer did many transformations.

I already talked about them, Americans to American, many to

many, so on and so forth together together.

It's stemmers are kind of dumb, right?

They see, you know, they see an ER end up

together and they say, oh, that looks like the kind

of thing you would chop off the end of a

word, so it doesn't.

Oh, sees a Y at the end of a word.

That's the kind of thing you turn into an I.

It just does it.

OK.

All right, so so far we've just preprocessed one example

document.

We got to this point with this document, all right,

but if you have a corpus of many documents, you

just repeat that, that, those steps over and over and

over again.

You apply the same steps to all the documents.

And then this will yield a vocabulary for the entire

corpus.

So after you've preprocessed all your documents, you'll be left

with a vector for each document, a vector of tokens,

and then altogether the set of tokens that's created by

All of those documents is the vocabulary for your corpus.

Um, that, that's also going to be the features you

analyse.

So if we're doing stemming, we're going to be using

word stems as our feature of interest.

For example, if we don't do stemming, we're just using

words.

But of course we've done some pre-processings.

We've gotten rid of some things, right?

We've got rid of stop words, so it's not all

words that are features of the features we're studying.

It's like all words minus the stop words.

Oh, and also minus the URLs and minus symbols and

minus, you know, whatever.

But what you for each document, what you can do,

so each document is going to end up using some

of the types in the vocabulary.

Because of course the vocabulary was created from all the

words that were used in the documents and you can

count how many times each document used each of the

types that creates a row in your data frame.

And you put together all the rows from each individual

document and you get a document feature matrix and in

math notation we're going to use a bold W.

That's just like the mathy way of like indicating we're

talking about a document feature matrix, bold W, no subscripts.

It's a matrix that has n rows, so that's the

N documents.

So N is the standard notation I'm going to use

in this class for the number of documents in a

document feature matrix.

It's gonna have J columns.

That's going to be the features.

J is going to be the standard notation I'm going

to use for the size of the vocabulary, that is

the number of columns in the matrix, like how many

individual types there are in the vocabulary.

I'm using these.

So, so we're going to use the math notation in

this class.

It's fine.

I'm going to keep explaining to you what it is

we're doing, um, with the math notation, but I'm going

to try to use the notation that's used in the

book consistently.

Friedrich might diverge from this because stuff he's teaching is

actually not really covered in the book.

Um, and so he may go in a different direction

with notation, but at least for the 1st 6 weeks,

I'm going to try to use the notation as is

in the, the, the book that I'm kind of like

referring to a lot.

There's also a table in that book at the very

beginning with all of the notation.

It's really convenient.

So if you forget what capital bold W means, you

can go look and it says, oh, that's, that's the

notation we use for a document feature matrix.

And then each cell inside that counts the number of

times each in the column showed up in the document

in the row.

What's the math notation we're going to use for that?

It is non-bold italics W W I J subscripts.

That is that that symbol right there is telling you

in a particular cell, what is the number of times

feature J showed up in document I.

So it's just the generic math way we're going to

use of notating the individual accounts inside the cells of

the document feature matrix.

There's nothing interesting here.

I'm just telling you about the notation, OK?

No ideas yet.

OK, so here is all of Trump's tweets from January

2017, so I just, I took that original set collection

of tweets and I just said, let's just look at

January 2017.

There are 212 documents, meaning there were 212 tweets that

were posted in January 2017.

After I did all the preprocessing steps, you know, removing

stop words, all that kind of, all that jazz, we

ended up with 1,039 features, that is 1,0039 columns, and

you can see here that this is a printout of

what it looks like in Quanta.

It gives A little summary of what what's in the

DFM.

DFM is document feature matrix.

I'm going to keep saying DFM.

I might occasionally say DTM document term matrix.

That's like a, some people say that.

I learned it in that, in that context, but I'm

try to say document feature matrix because features don't have

to be terms.

OK.

You could, your features could be something other than words.

So I'm going to use a more general phrase, document

feature matrix DFM, but just don't worry if I say

DTM.

That's just me slipping up and reverting back to my

grad school days.

OK, so each row is a tweet.

So I've named the tweets based on the um Well,

I didn't do it.

Quanta did it.

Quanta named the tweets based on the time they were

posted.

So this is the first tweet that was posted at

at 5 a.m. and 10 seconds, uh, on January 1st.

These times are all in UK time.

And this is actually this first row is the tweet

we've been considering the whole time.

It had the word American and at one time it

had the hashtag # Happy New Year and at one

time it had the word Mani that was stem stemmed

to Manu and I one time and so on and

so forth.

Turns out the feature, the default in Quanta is to

arrange the columns based on the order in which the

To the types were introduced into the vocabulary.

So if it processes each tweet in order, chronological order,

which it did, starts the first tweet, extracts all the

tokens.

OK, those must be new columns and then and then

adds more columns based on new, new types that are

discovered as it gets through more of the tweets.

So conveniently for us, we've been looking at the first

tweet this whole time, and the first columns also correspond

to the tokens that were in that first tweet.

Um, you could rearrange these columns if you want, like

the most common word to the left, the least common

word to the right, but the way quantity does it

when it creates it, it is just like the order

in which the tokens first appeared or the order in

which the columns.

It doesn't matter.

The column order doesn't matter.

You can sort it however you want.

But notice.

American, the word American appears in this tweet one time.

The word bless occurs in this third tweet one time,

so on and so forth.

So this DFM has a number of columns of 1,039.

That's the number of words, types.

Stemmed words, whatever, it has a number of documents that

are tweets that's 212 and equals 212 in this particular

example.

So J and N are the general like in any

kind of DFM we're going to use capital J and

N to talk about the number of rows and and

number of columns, but in a particular DFM that we've

created, we know we know what the N and the

J are.

They're not variables anymore because we created the DFM and

we now known is equal to 212 and J is

equal to 1039, but sometimes I want to talk generally

about any kind of DFM you can create, and then

I'm going to use the math J and N.

OK, and each cell is a count.

So for example, W11, that is the count in row

1 column 1 is 1.

The count in W21, that is the count in row

2 column 1, is 0, and so on and so

forth.

I mean, I only showed you two of them here,

but just trying to get you used to the notation.

Now WordCloud, once you get a DFM, one thing you

can do is just look at it visually, which is

nice.

There's a visualisation that a lot of people like, see

in a lot of newspapers, so you can make a

Word cloud once you've created a DFM.

And what it does is it is a plot showing

you all the words that appear in the vocabulary, and

it makes words bigger if they occur more often in

the vocabulary.

So this is a word.

For the entire corpus, so it's counting how many times,

for example, the word great occurred in the entire corpus,

and it makes the word great bigger if it occurs

more, smaller if it occurs less.

Um, the, the word that occurs, the so I and

I like I, me and great appear the most common

commonly, um.

I should be a stop word.

I'm not sure why it's in there.

It could be because It could be because I took

lowercase before I took the stop words out and and

stop word list, maybe it's capital I.

I don't know.

But that shouldn't be in there, but it's uh Well,

whatever, we all know Trump does talk about himself a

lot.

Um, so it's fine, but Make America Great Again is

this like slogan from the Trump movement and so the

word great appears a lot in here because it appears

in the phrase Make America Great Again.

Um, his name appears a lot, uh, you know, make

America great.

I mean, you, it's, you can see this and you're

like, OK, I can see this is Trump suits, right?

This is like why, this is kind of why a

lot of people like quantitative text analysis, right?

You can do something, do all this like stuff that's

really technical, and then you do a word cloud and

you're like that looks like something I understand to be

real in the world.

Like I'm trying to learn about social, political, economic things

from text and you can like, you can start to

see it's working, right?

OK.

But Word clouds don't really, they don't really do a

lot besides let you just like see what's going on.

Also can point your attention to things that are potentially

problematic.

Like I looked at this and say, ah, why is

there a stop word in here?

So then I might go back and like look to

see in the steps.

I mean I can't do it here.

I'm, I'm lecturing right now, so I can't do it,

but I would go back and see like why didn't

this get removed when my understanding is that pronouns are

stop words.

So it's nice to visualise your data just to make

sure everything worked.

OK, cool.

Now, let's talk about some issues in feature selection.

So recall, I'm going to repeat myself again.

How you implement QTA depends on the task.

So this is going to apply to all steps in

your analysis, including constructing your DFM by way of your

preprocessing steps.

And there are some well known issues you have to

consider when thinking about defining and selecting features like what

columns you want.

Now we've already talked about them a little bit as

I walked you through the steps.

I kind of mentioned issues and repeatedly said, you know,

it depends on what you're trying to do.

But now I want to organise our thoughts a little

bit into thinking about like three very common sets of

issues that come up, um.

Uh, in the context of creating a DFM.

So one, in some languages, for example English, some phrases

with multiple words have singular meanings as if they were

single words.

I already mentioned this before.

New York is best considered one word, even though if

we tokenize into unigrams, that is, we tokenize into individual

words.

I keep saying unigrams, but I haven't defined it yet.

We'll get in there.

Um, then it's going to treat united and states as

separate words.

And so anytime Trump In the tweets says something like

we're going to be united in our fight against blah

blah blah.

That's going to be considered the same thing as when

he says United States Secretary of defence has issued a

proclamation blah blah blah, right?

But they're different like the use of United in the

first context is different than the use of unit inside

the phrase United States.

So that's another way of saying things like United States

should be one token, not two.

But tokenizing is dumb.

It's just like tokenize based on space and it's gonna,

it's gonna make them, it's gonna make the United States

into two tokens.

Now relationship between, so the second problem is that the

relationship between word frequencies and usefulness.

In analysis is actually not that obvious, right?

So, you know, you might have a sense that uh

really infrequent words are not going to be very useful

in analysis because they just don't occur that often, right?

It's like any kind of quantitative data analysis, if you

have a variable that has uh that that was a

very um So it's a 01 variable and the mean

is like very close to 0 or very close to

1.

It's not going to play a big role in the

analysis because it's, it's such a lopsided variable.

Same thing here, but also it's the, the converse is

also true, like highly frequent words are often not that

useful either.

Stop words are highly frequent words and we already, we

already kind of have an intuition that they're not that

useful, OK.

So there's a kind of like sweet spot.

Like you want words that are or used enough to

be useful in analysis, but like not so much that

they're, that they're just dumping noise into your, into your

analysis.

Um, and then final, the final issue is that the

bag of words assumption.

Dividing into these tokens, it creates really large vocabularies and

very sparse DFMs.

This is like a technical problem.

A sparse matrix is one that has a lot of

zeros in it, OK?

And so that happens in document DFM because A lot

of the types in the vocabulary are only used in

a small number of documents and so for the vast

majority of documents, there's 0, and then for, you know,

one or two documents, there might be, uh, uh, that

word might appear.

So very, you know, like you think of very infrequent

words, obviously.

Um, but this is like a general problem that creates

issues in analysis.

It creates computational issues.

So this is kind of narrow stuff, um, that, that

we're not going to be worried about here, but you

should just know like sparse matrices are harder to do

analysis with, um, for computational.

Reasons, reasons, but also it this bag of words assumption

creates a huge vocabulary of individual words and assumes it

kind of implicitly that each of those types in the

vocabulary has a distinct and unique meaning.

But we know there's a lot of words that are

kind of synonyms.

They're technically separate words, but they're synonyms, right?

And so maybe you want to actually consider them to

be.

The same word or or close to each other in

some sense, but the bag of words, I mean, it

just assumes every word is its distinct little snowflake, right?

It just has its own meaning and its own meaning

is special and, and it's unrelated to any other, uh,

the meaning of any other word and the converse is

right, right?

Like, uh, it also assumes that all the operations of

the of the instances of the same word have the

same meaning while context may dictate another meaning.

Yeah, yeah, yeah.

Right.

So, right, that's true.

And that's, that's, that probably be a 4th problem that

we're not going to talk about at all, right?

The problem being that like, even when you do have

the same words, sometimes they have multiple meanings.

And so, um, yeah, so homonyms, I guess is what's

that, what that's called, right?

Homonyms is like words have the same spelling but different

meanings.

Yeah.

Um.

Yeah, that's a 4th 1.

I should add that.

I add that next year, but it's, we're not really

gonna, there's nothing we can do about it, honestly, in

our context because we're using this dumb bag of words

assumption and just like similar words are similar.

Now what you can make some choices in your pre-processing

that kind of try to prevent this, right?

So certain words that you really care about that you

know are homonyms, um, but they're important for your QTA

task.

You might engage in some like long process of like

figuring out what they are and how to.

Code them separately as separate words, um, but that would

be like a specific thing you'd have to do for

your project.

OK, so first problem, not all tokens should be unigrams.

When we tokenized using whitespace, we created tokens that are

unigrams, individual words, uni meaning one, gramme meaning word.

OK, so now technically we eliminated a bunch of nonwords,

so the, well, whatever, but we, we created tokens that

are unigrams.

Now we could define tokens differently by choosing what's called

a co-location.

So we could do bigrams.

So bigrams would be like take every pair of words

and consider that a feature of the document.

Uh, you could do trigrams, so you say like every

triple of words as a, as a um as a

feature of a document, and more generally, you could do

n grammes, and being any number you want.

You could be 4 grammes, 5 grammes, so on and

so forth.

So for example, capital gains tax, which is a kind

of tax on capital gains, in fact, can be represented

as unigrams as we would do in our standard bag

of words approach.

So when you tokenize it, it would turn into capital

gains and tax, 3 different unigrams.

But you could also treat it as bigrams.

If you, if you made this into bigrams, you would

have two features.

You would have capital gains and then gains tax, because

every pair of words would be included as one of

the features.

Um, or you could do trigrams and you could say

every triple of words is its own feature, in which

case we would have when we do the tokenizing using

trigrams, we would have just one feature that would be

this three word thing, capital gains tax.

Why did I use capital gains tax as the example?

Well, when we, you know, for those of us interested

in economics, when we look at this, we see this

is a concept.

This is a concept where these three words should go

together.

So maybe there's a reason to consider it one token

instead of 3 different tokens.

OK, so why would you want to depart from doing

what we're usually doing, which is breaking it into unigrams?

Well, you might want to do your whole analysis using

some kind of n grammes instead of unigrams.

In this class, not going to be common.

We're not going to do that.

We're not going to tokenize documents into biograms or trigrams

or whatever, just not going to do it.

But it could be, it could be useful in situations

where the QTA task you're doing, um, depends on word

order.

So, for example, simple text completion, right?

Like if you wanted to, to create some kind of

app or something that uh does text completion, you know,

when you type emails now, like every email platform like

suggests that you write whatever, like.

Text completion tasks like that, you might want to do

your, when you're training a model to, to do that,

you might want to use Ngrams because the word matters.

You don't want the, you don't want the uh text

completion software to just like give random unigrams, right?

I mean, that wouldn't make sense.

OK, but this is gonna be less common in this

class.

You probably see more of this towards the end when

we talk about uh large language models.

The thing that's more common is that many co-locations like

biograms, grammes, whatever have independent meaning that you might want

to retain in your analysis.

OK, so the phrase United Kingdom makes more sense left

as a unigram, like, or sorry, left as a biogram

than left as two unigrams, because the United Kingdom is

one concept.

It just happens to be two words.

New York, same thing.

In that situation, in situation number 2, where you know

there are some things that you want to keep as

as you might want to keep some words together.

It's just manual.

You got, you got to figure out which ones you,

you want to keep and which ones you don't want

to keep.

Oh, it's annoying.

But there are tools to do this.

Um, so you can use statistical measures.

So again, I'm going to skip this slide because this

is all saying like, you could do some things in

R to figure out which pairs, triples, quadruples of words

appear together frequently and then go through the list and

figure out which ones you want to keep, OK?

So here's an example from Manning and Shia, and you,

what they've done here is they basically counted the number

of times the word, these two words appear together.

Of the appears together the most.

This is like the highest frequency to co-location, the co-location

of two words.

Um, do you want to keep those together?

Probably not.

I mean, you're probably getting rid of them if you

get rid of the stop words anyway, but like this

is not use, we don't care about this or this

or this or none of this stuff.

Go down here and then all of a sudden you

get to New York.

Like, well, OK, that one I do want to keep

together.

It, it appears a lot, you know, that combination of

words appears 11,428 times in this corpus.

So it's appearing a lot.

I should probably keep it together.

So that one I'm, I'm going to keep together.

But then none of these other ones, he said, I

don't want to keep that together as I don't want

to keep these things together.

But again, this is on a, you have to decide

what's important for you and if keeping names of cities

together is important for you, then you should keep them

together.

Now lots of ways to use statistical measures to find

these high frequency co-locations.

Here they just looked at co-locations of two words together,

but you might want to look at three words together

or 4 words together.

So we can use.

We can use a tool in Quanta, which you'll see

in the seminar to figure out the co-locations in um

the Trump tweets.

So what I'm gonna do is I'm going to find

the common co-locations of two words and I'm Let me

ask you, if I wanted to find the common co-locations

of four words, what's a four-word phrase that goes together

frequently in Trump tweets?

Yeah, exactly.

So if you did this for 4 letters going together,

you get one of the, one of the ones that's

going to appear this list is Make America great again.

But let's just do 2 bigrams, like 2.

So we have fake news, very common.

Uh, we might want to keep that together depending on

what we're trying to analyse.

We might want to, this might actually be an important

concept that we want to capture and we wouldn't be

capturing it if we broke those two words apart.

Tax cut, Make America, or we can make America uh

America great, great again.

So you'd notice the, the, the co-location is appearing here

too.

United States, North Korea, news media.

Illegal immigration, work hard.

I mean, except for this one.

I mean this one, so we've already removed all the

stop words, OK.

So I I did this on the DFM where we've

already removed all the stop words, so we're not seeing

the, we're not seeing like the kind of silly ones

like of the and stuff like that.

Um, but these are, you got to go through this

list and decide like which one of these things you

want to keep together.

I think probably the simplest thing is like, OK, country

names, yes, let's keep country names together because those, I

mean very clear there's no reason not to do that.

There might be a reason not to keep fake news

together, right?

Like what is the reason to keep it together?

I don't know.

Like, is there extra value added from considering it to

be one word instead of two words?

Not obvious to me.

Um, OK, so a subjective but reasonable judgement for most

UTA projects with this Trump tweet data is let's keep

the United States together.

OK, so what do you do practically?

So here's an example of a tweet that has the

phrase United States in it.

I just put instead of a space, I put an

underscore so that when I do the tokenization, it keeps

this as one.

One token.

Now, of course, if you remove punctuation before you tokenize,

then it's gonna remove that.

So, you know, be careful.

But that's a, this is like a little trick people

do to try to keep co-locations that they want to

keep manually together, just replace the space with something that's

not a space that won't get broken up in the

tokenizer.

Now, um, Problem #2.

The bag of words assumption creates really sparse DFMs, lots

of vocabulary.

OK, so this is computationally inefficient, rare words are generally

uninformative, so on and so forth.

So what we can do is we can philtre our

features based on the frequency in which they appear, right?

So we already saw some examples of frequencies based on

intuitions, right, intuitions in our head.

So we remove stop words because they occur way too

much and they don't have.

Any meaning.

So in some sense we're removing them based on their

frequency.

Like if, if stopboards didn't appear that much, we wouldn't

even bother removing them, maybe because they wouldn't matter, right?

But they occur all the time and so we don't

want them to like play a big role in our

analysis.

So we already did that, um.

We also consolidated words into root forms via stemming and

limitization.

This is another way of kind of like.

Filtering based on, on, on features.

You can also just choose.

I don't like you can go through your list of

features and be like, I'm going to drop these, I'm

going to keep these, so on and so forth, but

it depends on your task.

But there's a more structured way to philtre words in

or out of a DFM using frequencies.

Again, the core problem is that like when we create

a DFM, often there's too many columns, too many words.

Some that appear too much, some that appear too little,

and we want to have a way to like get

rid of words that we don't really want to have

in our analysis.

So we can do one way to do it is

by frequency.

So there's two measures.

There's document frequency of term J.

So this is the math for it.

If you, if you don't like math notation, that's totally

fine.

Let me just tell you what this is.

The number, so for a term J for, for, let's

say feature J.

How many documents does it appear in?

That's what this is.

So the word America, for example, is one of the

features of our Trump corpus.

How many documents, how many tweets did that appear in?

That would be the measure DF subscript J.

OK.

Subscript J because it's it depends on which feature we're

talking about.

Each feature has its own measure, because each word appears

in a different number of documents.

OK.

The other thing you can think about is total term

frequency of the term J.

That is how many times did feature J appear in

the corpus as a whole across all documents.

This is the math for it.

So for a feature J sum together all the documents

where that feature J occurred.

So you take a column, so we're taking a particular

feature, sum it.

That gives you TF for that particular feature.

How many times, and if you don't like math, uh,

TF is total term frequency of term J is how

many times did that feature appear in documents inside the

corpus.

So just count up the number of times.

This is how many documents did it appear in.

So, so this number is just like how many documents

appear in?

It doesn't matter if it appeared 10 times in this

document, 20 times in that document, 4 times in that

document.

It's just like how many documents was it in.

This is how many times did it appear.

So these are two different measures and oftentimes what people

will do is they will say, I only want to

keep features that appeared in at least 2 documents.

So where DF is.

At least 2, and or I only want to keep

terms that appeared at least 5 times in the corpus

as a whole.

So where TFJ is at least 5.

And there's functions for doing this in Quant that you'll

see in seminar.

I want to tell you something that is really unfortunate

and I tried to resolve myself through a lot of

reading and I could not.

This is like the most boring way to spend my

weekend.

But the, the term frequency and, and document frequency, it

turns out that phrase, those two phrases are used in

a lot of different ways, like mathematically, like I'm a

mathy person and so like I look at the formula

and I'm like, that's not the same thing as that.

But especially term frequency, that is used in two different

ways as far as I can tell.

Um, It's very unfortunate.

Um, you're about to see it used in another way

in a slide or two.

I don't know what to say about that except I

added the word total here to distinguish it from what

I'm going to call term frequency later on.

This is not that important.

Just mem like memorise it on this slide, what we're

talking about is like, do we want to get rid

of features that appear too few documents and that appear

too few times in the corpus as a whole.

There's also different ways to calculate this that do some

normalizations, logs and stuff like that.

That doesn't matter.

This is just normalizations.

It doesn't matter.

OK, don't worry about it, but if you, if you

are like a mathy person and you want to read

the Wikipedia page about this, you can like lists all

the different ways to calculate these things that do different

things.

Uh, it's boring.

I'm giving you the measures that we're going to use

that are intuitive, and we're going to use them to

create DFMs in R.

How many documents did a term show up in or

feature show up in?

How many times did a feature show up in a,

in a, in a, in a corpus?

Those two measures are going to be the things we're

going to use to get rid of features of our

document feature matrix.

OK.

Oh, and just quickly, this is a set.

So this is a set of all the documents.

I is the is the math symbol for documents where

the count in the cell is greater than 0.

So how many documents, so this is just going to

be a set of all the documents where the word

J appears.

That's how big is that set.

This is saying, uh, so all the WIJ's, um, for

the particular J, so in one column, sum all those

up.

Some all some across the rows.

For that column.

Again, don't worry about it too much.

As long as you can calculate it, it's fine.

You don't have to like memorise the math.

If you know the concept, you'd be able to calculate

it just by looking at a DFM.

OK, now let's philtre out any feature that doesn't appear

in at least 2 documents and that doesn't appear at

least twice in the DFM.

I happen to use the same number 2 and 2.

I could use different numbers.

They are different calculations.

DF and TF are different, but I'm just going to

do let's philtre out anything with DF less than 2

and TF less than 2.

Then this was the original matrix.

Bam, I filtered out.

Low frequency terms and you'll notice one thing that disappeared

was hashtag Happy New Year, which is the token that

was in our corpus.

You can imagine that only appears in, uh, uh, probably

just one, well, I would guess it appeared in two

tweets, um, but certainly not more because it disappeared and

so I, um.

Oh, that's weird.

I must have done this greater than or equal to.

I must have done greater because that this does see

notice Happy New Year does appear in two, it appears

twice and in two separate documents.

So it shouldn't be filtered based on this rule here,

but I must have used a greater than 2 then

instead of a greater than an equal to 20, math,

math is, you know, make weird things happen, because it

doesn't really matter that much in, in practise, like my

quantitative text analysis with The greater than or the not

greater than equal to is is not going to make

a difference one way or the other.

I promise you.

OK.

So raw frequency is really useful.

We just use raw frequency to like trim, trim our

DFM, meaning we removing columns.

Oh, and also, by the way, the total number of

columns, we went from.

over 1000 features to 103 features by by removing terms

that appeared in uh less than 2.

Uh, documents and less than twice.

So there were a lot of columns here where they

had a feature that appeared once in one document and

we just basically just got rid of all of them.

And that was about 900 of them were in that

category.

That's what we mean when we say DFMs are sparse

matrices.

There's a lot of columns that have lots of zeros

in them.

OK, so raw frequency is useful.

We got, we got our, we shrunk down our DFM

for example, like if sugar appears a lot in your

apricot, that's useful information, but frequent words like the, it,

they are not very informative about content, um.

So some terms carry more information about contents than others.

Uh, there's a word frequency paradox that arises where, uh,

things that don't come up, like features that don't come

up a lot in the corpus are not very informative.

Things that come up a lot in the corpus are

not very informative, and there's kind of this sweet spot

in the middle where, like, that's like where the most

informative words are.

Um, so I'm going to show you, uh, This is

the math behind this this plot, so you can look

at that slide if you want, if you want, but

there's this thing called zip zip zip's law that shows

a relationship between the rank order of a word that

is like the most common word, the most common word

in a in a.

DFM is going to be rank 1.

The next most common is going to be ranked number

2.

The next most common is ranked number 3.

And then for each of those words, the actual number

of times the word appeared.

So this is the number of times the words appeared.

This is the rank of the word in terms in

ordered in terms of fre frequency.

This is from a corpus of, uh, or this is

like uh words in Romeo and Juliet.

And you'll see the most, the highest rank or the

lowest ranked words, like the most common words appear a

lot and there's an exponential relationship here, right?

So it's like the, the, the lowest ranked words appear

a lot la la and then as you go up,

go down and the rank go up in the rankings,

it sort of levels off in terms of how often

the words appear.

The point of this is to look at the top

here and notice these words.

What do these words look like?

The very low rank high frequency words.

They look like stop words and that's going to be

the case most of the time.

The ones up here are going to be like the

least informative.

Now if you look at the Trump, if you look

at the Trump tweets, you can see the same pattern.

Now this is, I did this without removing the stop

words because if I remove the stop words then a

lot of these words would disappear.

But you'll notice here the to and uh uh in,

these are like the most frequent words and you see

this exponential relationship here between the rank of the word

and the frequency of the word.

Um, once we remove the stop words, notice that now

we're, we're in better territory where these most, more frequent

words are actually the words that are, that are more

substantively meaning, meaningful.

And as we remove words, you know, frequent words, we're,

we're gonna keep this, this relationship.

It's just going to get flatter and flatter and flatter

as we remove words up here.

OK.

Now, um, filtering strategy, filtering features based on stop words

or how frequently the words appear in a document is

a kind of waiting strategy, but it's like a, it's

like a, it's like a dumb waiting strategy where what

you do is you say I'm going to weight each

feature that I don't like stop words, high frequency words,

whatever with a zero, meaning that I'm just going to

zero it out of my matrix.

That is a kind of waiting, you're like waiting those

words down to zero.

So like really high frequency words like stop words, you,

you look in the call if it was in your

DFM, you would see just like it appears a lot

of times in every document, but you're going by, by

removing it from your DFM you're effectively pushing all those

numbers to zero.

So you're weighting them all to zero, OK, but you

want to do something more subtle in a lot of

situations because high frequency words are, are not that useful.

Low frequency words are not that useful.

You want to get that sweet spot, so you might

want to actually do a kind of weighting where you

weight the words inside your, the sorry, the accounts inside

your DFM.

Giving more weight to words that are in that sweet

spot.

Right, the ones that don't appear too much, but don't

appear too little.

You can do this.

Yeah, but doing this, well, maybe you should explain it,

but I was going to ask if this also gives

you a computational, a reducing the computational cost.

Like changing the words of the matrix but not eliminating

uh columns of the matrix, but you don't reduce the

dimension, but yeah, so when you're waiting you're not reducing

the, when you're doing what you're what we're about, I'm

about to tell you to do, you're not actually reducing

the number of columns, so you don't, you don't get

the computational benefit.

But you get, uh, but this is for substantive benefit,

right?

Like you want in your QT in your analysis, you

want words that are really meaningful to like carry more

weight, right?

This is like waiting when you do any kind of

quantitative analysis, regression or whatever context you're in.

When you do weighting, the whole point of that is

to like bring up the importance of observations you think

are important and bring down the importance of observations that

you don't think are important.

It's the same thing here, so you get a substantive

benefit, but you don't get the computational benefit.

Um, so what you can do is what's called TFIDF

term frequency inverse document frequency weighting.

Now it's people just say TFIDF.

So this way of using term frequency is different than

the previous way.

Don't really matter.

It's fine.

This is the formula for it.

You almost never have to use it, but if you

do, what you do is you look at your document

feature matrix and you go cell by cell.

And you calculate a TFIDF number for that cell and

you replace the original count there with the TFIDF count.

And the particular calculation you do is you take the

number that's already in the cell.

That's what this is in math, and you multiply it

times the log of this total number of documents in

the in the entire document firm, so number rows and

divided by this thing.

The number of documents.

Uh, that are weighted, that have a account greater than

0.

Ah, OK.

We would just do this calculation for everyone.

Now we're going to do this in seminar.

You're gonna see waiting for uh the Trump tweets, but,

um, let me show you the what happens when you

do this.

This is the original document feature matrix when I when

I apply TFIDF weighting.

These numbers are all weighted.

They're replaced.

So some of them are made bigger, some of them

are made smaller depending on how much information those words

convey and how much, when I, I'm like skipping over

a lot of technical detail, how much information those words

convey is based on this formula.

This is the weighting formula you use.

Now there are other approaches to weighting that are not

TFIDF.

This is the one everyone uses.

This is one we're going to use.

You could do Okapi BM 25.

You could do smart waiting.

So there's different ways.

It's the same concept, just different formula.

But we're going to use TFIDF and what TFIDF is

doing is it is, as it says here, it's putting

more weight to the word counts inside the DFM that

have high term high term frequency in a document and

low document frequency of the term in the whole corpus.

Why?

Because what we want is things that occur a lot.

But they don't occur a lot because they're just spread

across every document like the.

The appears a lot because it's spread across every document.

So what we want to do is we want to

find words that occur a lot, but they're concentrated in

documents.

That's what this does.

I mean, it's a kind of simple way of doing

it, but that's what this is doing.

So for example, the, the is going to appear a

lot.

So it's going to have a high W across a

lot of cells, but it's going to appear also in

a lot of documents.

It's going to appear in a lot of cells.

So this number is going to go down.

That number will be high, that number will be low,

and so it'll weight the down.

Yeah.

Does this mean that we might mess ourselves up, um,

having each document being a sentence?

Um, because we're less likely, like, let's say we, we

had about the term artificial intelligence, you would usually not

use that twice in one sentence you say like it

or something if you're referring to the same.

Mm.

So that's not really about TFIDF waiting.

That's more about like you have these two, yeah, you

have reference.

So here you would need, you would probably like if

you wanted to identify uses of the word it that

related to artificial intelligence, you would need to use like

word, like you would need to use techniques that use

word order.

It's kind of beyond the scope of what we're doing

here with a bag of words, but you would, you

would start by doing, you figure out a process for

identifying every example.

where the word it is referring to intelligent artificial intelligence.

So how you do that is kind of you got

to Google and figure it out.

But then what I would do is that I would

then replace every instance of it that I've identified as

referring to artificial intelligence just with artificial intelligence.

And I would treat that as a biogram, and then

you're going to get every, you'll be able to count

every time it's referred to either as artificial intelligence or

as it.

Yeah.

OK, cool.

Um, and also if you want to see the words

that have the, like, so these are the, so inverse

document frequency is like, is, is this part of the

It's, it's the waiting.

This is the waiting that's occurring, so it's like waiting

downwards that occur in a lot of documents, waiting upwards

that occur in few documents, and you'll see that high

IDF words, the ones that occur in, in very few

documents are Prosper, Behalf, POTUS, Team Trump, HTTPS and the

lowest IDF words are the ones that are um.

Was it the reverse inverse document document frequency, so it's

Highest would be lowest document frequency.

Yeah, so these appear in the fewest documents.

These appear in the most documents.

Anyway, if you want to, you don't have to look

at the words, you can just do the calculation and

it creates the weights.

Now, um, I want to end briefly with So we're

not going to get to describing tweets or describing texts,

which is fine because we're gonna, I'll just talk to

you about it in seminar because it's, it's actually kind

of, in my view, it's like there's nothing really that

conceptually deep to say.

It's just like, here is a way to measure how

complex the document is and here is a way to

measure how diverse the words in the document are.

So how like how rich the language is.

It's just like you just measure the thing and it's

fine.

Like, whatever.

I'll tell you about it in seminar.

There's nothing conceptually new there, but you'll do it in

the, you'll code it, so you'll see how to do

it.

But I do want to end by talking about word

embeddings just for a minute.

So the way that the bag of words assumption works

is that it treats every word as its own little

like distinct thing has its own meaning, but we all

know that lots of words have similar meanings to each

other or they're related to each other, and that could

be important.

But let's consider a situation where we have a vocabulary

of three words cat, dog, rat in that order.

The bag of words assumption treats every word, every distinct

type in the vocabulary as a one hot encoding.

Meaning that we can represent the word cat in vector

form like this.

So the word cat is represented by a vector that

has a 1 and just 11 and all other zeros,

a 1 corresponding to cat, a zero corresponding to dog,

a zero corresponding to rat.

So this is like turning the vector of words into

a numerical format.

OK, so the word dog is represented by this 0

for cat, 1 for dog, 0 for, for uh rat.

So these are called one hot encodings.

And if you know anything about linear algebra in the

background, you're thinking about like multidimensional spaces, you'll notice that

these are treated as orthogonal, like in kind of vector

algebra.

So they're treated as distinct concepts, but completely distinct.

Cat, rat and dog are completely distinct.

And they're called one hot encodings because they have 11

and zeros the rest of the way.

OK.

It's very similar to a dummy variable, how you would

code a dummy variable in a data frame.

So if that, if you're like, oh yeah, I know

a dummy variable, like this is basically, this is turning

these words, turning a list of words into a dummy

variable, OK?

Now, there's actually a conceptually deep thing going on here,

which is that like in, in the sort of math,

which is that all the words are treated as distinct

and completely separate from each other, independent orthogonal.

But maybe you want to instead say, I think words

are related to each other, and what I want to

do is I want to find a smaller dimensional embedding,

mean a smaller, uh like a smaller number of dimensions

than the total number of words that represent kind of

concepts.

And maybe I want to place each of the words

inside that lower dimensional space that is representing concepts.

And then words will have similarity to each other.

That's called word embeddings.

So for example, if you wanted to create a two-dimensional

embedding in this vocabulary of three words, you would, you

could then represent cat by this vector, dog by this

vector, and rat by this vector.

And now they're not orthogonal anymore.

There one can be more similar to the other.

In fact, you can look at this and see that

dog dog and cat are more similar than together than

either of the two are with, with rat in this

like fake, completely fake.

I didn't estimate this.

It was really fake.

I just put these numbers in here.

It's fake, it's fake.

OK.

I don't know if this is, you know, it's fake.

Um, you have to estimate these embeddings from a corpus,

but at the end of the course we're going to

get to that.

The key thing is bag of words just, it just

assumes every word has its own distinct meaning.

Word embeddings basically collapse each individual feature into a smaller

dimensional space and think of roughly as like concepts, and

then you can measure how similar the words are to

each other.

So you can find synonyms and you can treat words

similarly in your analysis in a way you can't do

when you use one hot encodings and bag of words.

But the last, the last few weeks of the class

is going to be about this.

And so you're going to see in detail how to

do this.

I just wanted to give you the con conceptual underpinnings.

All right, I'll see you in the seminar.

The seminar materials will be posted to the GitHub folder

prior to this first seminar.

So stay tuned.

They'll be there soon.

I.

Yeah, I got, I think there's another class in here,

but we can talk in the hall.

So it and not like.

Which courses are you taking to?

I'll do the thing where I got this one.

Just like is it 1 year, a year and 2

years, OK.

That's a lot of people.

OK.

OK.

Yeah, we have like in my in public policy and

exactly like like pages first weeks they gave us like

200 pages of machines.

OK.

Oh.

But.

Thanks.

Did you Which he's on my body.

It is just Hello?

Hello?

Hello?

Hello, hello.

What.

Yeah.

Lecture 3:

that predicted as basic decision, but I guess you're right.

So what I was just thinking is you want my

variables to be the predicted values.

But I'm not sure.

I'm so confused about this.

I'm like, I was sitting there for like a good

15-30 minutes, like, what do you mean yeah.

I mean because I, I don't really understand the same

thing to find the place and so you can you

can try to.

Yeah, that's what I mean.

I don't really care.

Yeah, you look refreshed.

I refreshed.

I went to bed like at 55 a.m. yeah, like

everyone left like uh between 1 and 2 and then

Carlos.

Yeah, like at 3 and then you know, and I

said OK we just this cola and we go to

bed but then that cola just lasted for 2 hours

4 hours it's like almost 5 and then I go

there and like it was like a good party, you

know it was a nice I mean it was just

a.

It was then I cannot regret because I woke up

on Saturday on Sunday on Saturday from 6 to 1,

so and I felt like shit, you know, because you

did the same.

It was nice 2 extra hours it was nice.

OK, well, I'm surprised you guys are saying I look

repressed because like I should.

I keep thinking that like 5 and then I had

to wake up early like at 10 and then I

just home now, then I got home at 1 a.m.

tonight, OK, you've just been on the schedule.

It's not even like a break, it's just like one

after the other.

How was the wedding?

Whose wedding was it?

I, no.

I.

he was there like a reunion.

So.

Yeah.

Yeah.

Yeah.

I don't know I don't know I guess I see

yeah.

I think that makes sense.

You can't leave them just after you guys you know

take care of yourself.

That's very considerate, yeah, we, we have those, yeah, like

people really like like like yeah.

I like an industry perspective as well like we like

the Polish market a lot.

Yeah, yeah, we have a lot of like Polish.

I lived in like a neighbourhood.

I had to go home.

Yeah, but yeah, generally I had a question, but uh.

this thing yeah.

OK.

OK, so.

So I don't know if you've done the clinic, but

the the the very binary categor OK and then like.

OK.

confused.

Yeah, I don't understand what he means because what I

did was I just OK, let's get started.

Um, two things.

One, my voice is disappearing, so you're going to have

to listen to me sound like this.

Sorry about that.

I'm trying to get it back, but Alas, no, no

luck yet, and I forgot my glasses.

So I may be struggling to see the slides at

times.

Uh, so it's a rough morning.

OK, so, um, this week, We're going to be exploiting

word means.

Uh, so mostly what we're going to cover in this

week is what are called automated, well, I shouldn't say

mostly.

The first part of this lecture is going to be

covering automated dictionary methods, which is a pretty, uh, common,

maybe one of the most famous methods used in quantitative

text analysis.

And then we're going to, uh, go on to something

called discriminating words, um.

Which is a a way of trying to figure out

based on some predefined categories, uh, how a set of

words in a vocabulary uh discriminates between those categories that

is teaches us about, um, about, uh, uh, which category

a word belongs to.

Um, we're also at some point, kind of in the

middle of this lecture, going to do a little kind

of math tour in probabilistic models of language.

Um, this is partly useful for some of the material

we're covering this week, but it's also going to be

useful going forward in the course.

So it's a sort of more general, uh, math, uh,

component.

Um, uh, yeah, so it should be fun.

OK.

So our, so I just, I talked about what we're

going to do here.

Um, this is the outline.

Oh, and let me just point out, there's a bunch

of slides at the end that I'm not going to

get to.

This is a very long lecture deck slide deck, you

probably noticed, but this stuff under extra material we're, we're

not going to get to in this lecture.

Maybe we'll get to it later in the course.

I haven't decided yet.

OK.

So let's start with automated dictionary methods.

So here's a pretty obvious point.

Words have meanings.

That's why we all speak to each other with words.

So word use in a document can give us a

sense about the overall meaning of that document.

So what do I mean by meaning?

So here are some examples we might care about the

sentiment conveyed by particular words or documents, so how positive

or negative a particular document is, that would be sentiment

analysis.

We might care about emotional content of texts, so do

the texts convey sadness, happiness, angriness, or anger, and so

on and so forth?

We might care about.

Uh, evaluating the extent to which a document conveys something

about cognitive processes, right?

So how much insight is a document giving, uh, how

much, uh, causal attribution, uh, is characterised in the document,

so on and so forth, and you might also be

interested in characterising hate speech.

So, uh, how much of the language in particular documents

is sexist, homophobic, xenophobic, etc.

OK, so these are different examples of how words have

meanings.

They can have meanings in sort of different directions, so

sentiment, emotionality, uh, hate speech, so on and so forth.

OK, so automated dictionary methods, which I have been abbreviating

as ADMs in the slides, they exploit word meanings to

learn about the meaning of documents, OK.

So there are two important steps to automated dictionary methods.

So first, you have to construct a list of words

and their associated meanings, and this is called a dictionary.

I'm going to talk in more detail about what these

dictionaries look like in a few minutes.

But step number 2 is you use this dictionary to

then quantify the overall meaning.

Of the documents based on the word counts, uh, based

on word counts in the documents and this mapping that

you created in the dictionary between words and concepts or

words and meanings.

OK.

So, important, before we proceed, I'm going to come back

to this over and over and over again.

If you construct a dictionary to to do automated dictionary

methods, the dictionary has to be appropriate for your task

and for your context, and it has to be validated.

I'm going to talk about that more sort of later

in the slides, what it means to validate, but one

of the things that goes on with dictionary methods is

that there are a bunch of pre-built dictionaries floating out

there in the world.

Some of them, one of them is embedded in the

Quant Quant package.

So it's very tempting to go grab one of these

things off the internet and then take a corpus of

texts and just like apply the dictionary to the corpus

of text to learn something about the text.

That's not a great idea in a lot of situations.

And in fact, the existence of these predefined dictionaries does

not excuse you from your responsibility as a quantitative text

analyst to do the hard work of sitting down and

figuring out for the particular task I have in front

of me for the thing I want to learn about.

Are the dictionaries I'm considering using appropriate for that thing?

OK.

Um, and I'm going to show you some examples later

on of, of situations where there somebody use dictionaries that

are not appropriate for the task.

OK.

I want to just also quickly point out that automated

dictionary methods, at least in my view, are kind of

another kind of descriptive statistics for texts.

So we covered two things last week, uh, lexical diversity

and richness, or lexical diversity and Is it richness?

Yeah, um, no, lexical diversity.

What's the second one?

Fisher Kincade is complexity.

OK, so, um, this is another kind of descriptive statistics

because you, you're getting a bunch of texts and you're

describing some features about them.

It's a little bit fancier than than those methods, um,

um, but you'll see it's pretty straightforward.

You're really just describing the kind of overall sentiment of

a text or the overall emotionality of a texts, so

on and so forth, OK.

So let's talk about the structure of a dictionary that

you would construct or you would find on the internet

to use for your purpose, OK?

So dictionary is a predefined list of features, so those

things like the columns of the document feature matrix.

So oftentimes it's words, right?

And in this predefined list of features, each feature is

associated with specific meanings.

OK, so a dictionary basically creates an equivalence class of

features.

We already saw equivalence classes once in the context of

stemming and limitization.

This is just another place where equivalence classes come up.

So basically what a dictionary will do is to take

a whole bunch of features and then create sets of

those features that.

different concepts.

So you might, so if you imagine all the features

in the vocabulary that is like all the words in

the vocabulary, and you might have one dictionary interest, you

might want to be building a dictionary interest that's designed

to measure sentiment.

So whether words are have positive or negative, um, sentiments

to them or valence.

So you would create equivalent if you built a dictionary

to do this, you would be creating equivalence classes by

saying these features over here, they're all positive words.

These features over here, they're all negative words.

So within the set of positive words and negative words

you've created an equivalence class.

They're all equivalent in terms of the sentiment.

These are all positive.

These are all negative, OK.

So oftentimes when you build a dictionary you have to

think about like do I want to do stemming and

limitization so like in my dictionary, do I want all

the features in there to be stemmed or end or

limitized or do I want them to not be?

It depends on your task again.

Just like when you're building a document feature matrix, you

have to think, should I limitize or stem based on

what I'm trying to do.

When you're building a dictionary, you should also think, do

I want the features in my dictionary to be stemmed

or limitized based on, of course, what I want to

do.

Right.

Sometimes word tenses and stuff like that is useful for

your particular context, sometimes it's not.

Now two important components of dictionaries just to get some

vocabulary straight.

There's keys.

These are the labels for the equivalence classes.

So in in a sentiment dictionary, so say you're trying

to figure out or you build a dictionary that that

classifies features as either positive or negative, the keys would

be positive and negative.

Those are the Those are the labels affixed to the

equivalence classes and then the values are the individual features

that are categorised inside that category, right?

So if again we have a sentiment dictionary that has

positive and negative words in it, the two keys would

be positive and negative.

And then the values would be, OK, which features are

classified as being positive, which features are classified as being

negative.

All of those features would be considered the values under

those keys.

OK?

Um, so here's an example from the quantit reference documents

of a dictionary.

This is just a dictionary that I've put here in

our list format, but what you can see is this

dictionary contains, uh, I can count correctly, 5 keys Christmas,

opposition, Talo, tax reg X, and country.

So those are the five broad categories that this dictionary

is is is doing categorization over.

And then you see the values for each key.

So for the key Christmas, there's a vector that contains

the words Christmas, Santa, and holiday.

So presumably whoever made this dictionary has decided that the

concept Christmas should be invoked any time the word Christmas,

the word Santa, or the word holiday comes up in

a text.

So these words would all be indicative of this category

Christmas.

And then you have opposition.

So this is a category that's designed to capture the

idea of opposing things.

So the word opposition, the word reject, the word not

in corpus, all are indicative of opposition.

So these, this is like usually what people think about

when they think about dictionaries.

They, you know, you have the concept as the key,

and then you have a list of words that fit

with that key.

But you can also, it's flexible, you can also define

a key and then have a reg X pattern.

So that's what's going on here for tax reg.

So this is an entry in this dictionary where the

key is called tax reg x.

So this is a tax related this is meant to

capture tax related concepts and instead of defining a list

of words.

About taxes, it just defines a reg X pattern, and

any, any feature that matches the reg X pattern is

considered part of this category tax reg x.

Now this is a tax glob.

This is just a different, I mentioned this briefly in

lecture one.

Globs are are sort of like a more basic form

of reg X, um.

It's not super important.

I included it here because this is uh what was

literally in the reference documents, but to think of if,

if you don't want to learn about what a glob

is, just think glob is reg X.

It's not true.

They're not exactly the same, but they're glob is just

like an old old version of reg X, OK?

And then here's the final.

Uh entry in this dictionary where we have the key

is country.

So this is, you know, I presume whoever created this

dictionary wanted this to be matched whenever a country name

was encountered.

So it includes the United States and Sweden.

Of course those are not all the countries, but this

is a tiny little simple example, right?

OK, so if I were building a real dictionary with

this stuff, I might want to include all the countries,

but I just want to have a small example that

I can fit on the slide.

All right, so this is a nice little example of

a dictionary because I think it indicates both types of

flexibilities, types of flexibility that you can build into dictionaries,

which is conceptual flexibility.

Like this is a dictionary that contains words related to

Christmas as well as words related to tax.

Taxation.

Weird.

Why would those two things go in the dictionary?

Depends on what you're studying, depends on what your task

is, right?

If your QTA.

That requires a dictionary that has a category for Christmas

and a category for taxation.

That's the dictionary you need to build, OK, so why

is this dictionary exist?

I don't know.

This, this is just an example dictionary, but it goes

to show that you can build a dictionary to capture

anything you want based on on the task that you

have in front of you.

But this dictionary, in addition to demonstrating conceptual flexibility, also

demonstrates look up flexibility.

What do I mean by that?

Well, these two keys here, they find words by exact

matching, right?

So like if this word Christmas appears in a text,

then that word is labelled with the Christmas key.

If the word Santa.

In the text that word is labelled with the Christmas

key, so on and so forth, but it also allows

you to do reg X pattern matching as well.

So the way that you do the lookup, meaning you

find you match the words in a text to the

keys in the dictionary, is flexible.

You can do exact matching through words or you can

do flexible matching through reg x patterns.

Cool.

Now you might, depending on the task you have ahead

of you, you might, if you needed this dictionary for

something, you might want to stem it, right?

So you might, you might want to create in here

lists of stemmed words that have reg wildcards on them.

So instead of opposition, you might want and you might

want to lowercase this, you might want OPPOs and then

and then star.

Right, or, or dot star, and that would be an

indication that any word that starts with oppose should be

considered part of opposition.

But this is, this here is not stemmed or limitized

or even made lower case, right?

So, so if you're looking, if you're using this list

to exact matching under this opposition key, it's gonna only

find the word opposition when it's capitalised at the front.

OK, again, it depends on your task.

Now, let me give you a real world example.

This is the Lexicoder Sentiment dictionary from Young and Siroka,

and this is a dictionary that is used for sentiment

analysis with political text, and it was originally validated with

human coded news content.

So why I bring this particular dictionary up is that

it's available in Quant.

As far as I know, it's the only dictionary available

in Quant data.

Um, and if you load the Quant data package, it's

just this object.

So you know, like R has a bunch of pre

like built in data sets.

Well, quantitative Package has a bunch of built in things

as well, has built-in corpuss and this built-in dictionary and

some other things.

Um, so what's nice is when you're learning dictionary methods

you can use the built-in dictionary to practise, but then

when you go to your your QTA task, you got

to find your own dictionary build or build your own

dictionary.

Yeah.

Do they have the same one that you can import

into Python?

I don't know.

Sorry, Quant, I mean, I don't think there's a Quantator

package for Python.

Yeah.

So I mean, this is just the Quant people put

this dictionary in the package.

Um, so here's, so here's what it looks.

So in Quant there's an object type called a dictionary,

and so when you print a preview of this particular

dictionary, this is what you get.

It tells you you're looking at a dictionary that has

4 key entries, meaning there's 4 categories.

This is a sentiment analysis dictionary, so it's positive, negative,

and then so negative, positive, and then they have neg

positive and negative, which are like negations of positive words

and negations of negative words.

OK, so this is like more complicated ways of thinking

about positive and negative words.

You'll notice here this is actually not even words, it's,

it's entire phrases.

So like a lie, like the word up and then

the word lie together is considered a negative token.

Um, so this, this dictionary actually is not just matching

individual words, it's matching phrases as well, and it includes,

uh, reggae pattern matching or glob pattern matching.

So this is saying match any word, abandon.

I think this is a glob, so it's a little

bit different than reggae, but it's like match any word

abandon, and if it has some kind of additional stuff

like abandoned mint, that also counts.

But abandoned it by itself counts.

So does abandonment, uh, abandoning, abandoned, like all those words

would count under this because it has this wildcard.

And, uh, OK, and you can find more about this,

find out more about this uh dictionary here, but we,

we, we may use this in exercises for learning about

how to apply dictionaries to text, OK.

So one thing to point out is that in the

context of automated dictionary methods, the word dictionary is a

little bit of a misnomer.

It's not quite a dictionary like you would look up

word meanings.

Uh, so an ADM is actually more similar to a

thesaurus.

Thesaurus is a book where you look up a particular

word and gives you a bunch of synonyms, and that's

kind of, kind of what's going on with an ADM

dictionary as well.

You look up a concept and it gives you a

bunch of words that are sort of related to that

concept.

Um, but ADM dictionaries aren't exactly thesaurus either because they're

not usually focused on defining synonyms, right?

They, they're used for bigger things, um, and they also

tend to be exclusive.

So a particular a particular feature of a document appears

as a value for only one key.

In most dictionaries, although thesaurus is, you can imagine like

if you have a thesaurus, a particular synonym can appear

as a synonym to multiple words.

So you can have multiple values for different you can

have the same value for different keys.

That's not usually how ADM dictionaries work, but you can

make an ADM dictionary that's non-exclusive.

So for example, here's a simple little dictionary.

Where there's a category marriage where all of these words

are matched to that category and another category interest where

all of these words are matched to that category.

And notice there's overlapping words.

So the word engage maps to two separate categories.

You can make a dictionary like that.

That doesn't tend to be how dictionaries are, are built,

um, mostly for practical reasons.

OK.

So some famous dictionaries, you know, I don't even want

to boom, boom, done.

OK, great.

You can read those slides if you want.

You can go to the links if you want to

look at them.

I don't have anything deep to say about them.

They're just dictionaries that other people have created.

They usually are creating them based on some example documents

that they had available to them, and so then they

build a dictionary that works for that particular those particular

documents.

And then the idea is they want you to use

their dictionary for your tasks, oftentimes they're trying to sell

you their dictionary, the This one is a pretty famous

one.

Luke, uh L I W C pronounced Luke, it you

have to pay for it because it's like text analysis

software that they're trying to get companies to buy this

software to like do like advertising related quantitative analysis.

OK, so like.

You can read up on these dictionaries on your own.

I don't have a tonne of interesting stuff to say

about them, but I do want to show you what

you would do with a dictionary and some examples of

what other social scientists have done with dictionaries.

OK, so suppose you have a dictionary in hand, two

common goals.

So one, you might want to quantify documents on a

scale like how positive or how negative they are, and

2, you might want to classify documents into discrete categories.

This is a positive document or this is a negative

document.

Yeah, if you were interested in one of these uh

dictionaries in the sense like, are they translated?

No, they're all in English.

Like if we are looking for another language, we should

look for a dictionary built on this language, right?

There are dictionaries that are multilingual.

I know that's true because there was a slide about

it that I removed from this slide deck.

I don't know what they are, but you can find

them and actually those are good those are good dictionaries

to use if you have a corpus that has multiple

languages in it or if you want your analysis to

sort of not be language specific.

Yeah.

OK, so, so 2, so once you have a dictionary

and you have a corpus of documents that you want

to use that dictionary on, you can calculate some statistics.

OK, let me tell you what the statistics you can

calculate are.

So let's talk about these statistics with a simple example.

Let's imagine we have a dictionary that's a sentiment dictionary

where the sentiment dictionary has a bunch of keys, so

that's uh.

In our, well, any dictionary has a bunch of keys,

but in our situation, a sentiment dictionary will have, let's

say 2 keys, positive and negative, because that's what a

sentiment dictionary is for is measuring positivity and negativity, two

keys, positive and negative, and then associated with each key

is a bunch of values.

Those are the words that are positive.

They're the words that are negative, OK.

So let's suppose that every word in the dictionary, remember

we're always using little J to be like a generic

token or a generic feature of a vocabulary.

So each one of those, the dictionary is going to

classify either as positive word, so, so.

The sentiment of word J is 1, positive, plus number,

or negative.

The sentiment of word J is -1 negative, or neither.

And basically this is just like any word that's not

in the dictionary.

It's just, it's just a, it's a 0.

It's neither positive nor negative, OK.

So then what you can do is you can take

your corpus and you have it and suppose you have

it already as a document feature matrix and you can

count.

All of the features that are positive and and Uh,

you can take all of the times positive words are

used and sum those those counts together.

OK, so that's what this formula is doing.

It's saying every time you encounter a positive word, make

put a 1 here and multiply that one times the

number of words that were counted, numbers of that word

that were counted.

And then every time a negative number was found, put

it in a -1 here and multiply that times the

number you were that you counted.

So for example, let's suppose you had a document that

has 10 positive words in it and 4 negative words

in it.

If you did this calculation here, you would get 10

minus 4.

You would get a total of 6, right?

So you get the 10 positive documents the 1010 words,

each of which counts as 1.

You get the 4 negative words, each of which counts

as a negative 1.

So you get 10 minus 4, that gives you 6.

So the overall score for the document would be 6.

Now that's just giving you like the net number of

positive words in a document, right?

Or the net number of times positive words were used

in the document.

You can normalise by document length if you want.

So for example, if you had counted 10 positive words

used in a document in 4 negative words, and so

this nets out to 6, and let's say there were

20 total documents.

So you could take 6 divided by 20, and what

is that?

30%?

I do my math correctly.

So that's a, so you could normalise by document length

by dividing by the total number of words in the

document.

That's what N sub I is, is the number of

words in document.

Remember our notation.

And so that's like this is like the percentage, how,

how positive or negative the document is in percentage terms.

Positive numbers being positive, negative numbers being negative.

And of course you can normalise things too if you

wanted to go between 0 and 1, whatever.

OK.

You can also classify using a decision rule, so you

can calculate the net number of positive words and then

if that number ends up being positive, you say that

is a positive document.

If that number ends up being negative, you say that's

a negative document because there is, because if, if this

number you calculate is positive, that means there were more

positive numbers than there were negative or positive words than

there were negative words.

If this number ends up being negative, that means that

in that document there were more negative words than there

were positive words.

So then you could, you could do this sort of

downcoding rule where you say, well, if the, if the

net, the net of the net of a document is

negative, it's just a negative document.

If the net of a document is positive, it's just

a positive document.

Yeah.

Um, that's not related to this, but thinking about creating

your own dictionaries because you think it's very specific to

your task.

How do you ensure that your dictionary is exhaustive and

the kind of different ways one can use language?

Ah, that's the hard work.

That's the hard work of building the dictionary.

So, um, The short answer is you just read a

lot of the documents and you can do some statistical

stuff, right?

So presumably if you're building a dictionary, you're going to

start with some kind of validation corpus.

So you're building a dictionary on some text you already

have.

Uh, that's preferable.

You don't want to just sort of Start a priority

and like I want to just build a dictionary, right?

Like you typically people, I mean, some of the older

dictionaries, that's kind of the philosophy behind them.

Let's just like build a dictionary of all the positive

words in English, but that's not typically what people do

now because QTA has gone down this validation.

A road where we think that we need to be

validating our quantitative text analysis methods for our particular task.

So typically you would start with example documents and then

you, you could, you could actually pull out word frequencies

and stuff to get a sense of like what words

are used, and then you would have to just look

at them and kind of categorise them and figure out

and then read the documents to see them in context

and stuff like that.

There's no, there's no easy answer beyond like you just

got to.

You just got to read the, read the documents.

Now there are some things you can do to point

you in the right direction that that's kind of what

the discriminating word stuff is getting at.

It will get to the, which we will get to

at the end of the lecture.

That's kind of like, if you wanted some kind of

statistical or or quantitative way to start building a dictionary,

you would go down that road, that road.

OK.

Now, let me give you some examples of people using

dictionary methods.

Yeah, sorry, um, if we can go back to the

last slide really quickly, um, I think I'm a little

confused on how to interpret a normalised.

Sentiment score, so like if I have an S of

I of -6 and I divide that by the document

length that gives me like a negative 30.

Yeah.

So the, the sign is telling you is, is telling

you the over so the sign is telling you the

net, right?

So is it overall positive or overall negative and then

the um The amount is like how, like how far

towards 100% you are.

Now, of course, this is giving you a scale of

-1 to 1, you can renormalize it if you want

like a simple like calculation, you can put it all

on on a 0 to 100.

So whereas, where higher numbers are more positive and lower

numbers are more negative, but you would, the way you

would do that is you, uh, So you take this

and then um.

You basically just.

Well, whatever.

I, I'm not gonna, I'm, I'm in lecture mode so

I'm not going to do this on the fly, but,

but you can, you can project this on a 01

space like easy math, like very simple transformation can do

that.

I just don't want to, I just don't want to.

You know, one of the things I've learned over the

years when I teach is like when you go into

lecture mode, you're like your brain is in a particular

place and it's hard to do very simple stuff.

So if you ever notice when professors like, they write

on the board and they spell things wrong all the

time, it's like stuff they don't spell wrong when they're

just like sitting in their office and writing, it's because

like your brain cannot.

So I'm like thinking of how to do the simple

math transformation here and I'm not going to do it

I get it wrong.

Yeah.

Um, I think on a similar note about normalising and

also about things that aren't in the dictionary, uh, presumably

there's some words that are either like neutral, um, in

like intentionally not in the dictionary neutral or just not

included.

You're normalising by document length, do you exclude those and

only look at the length as number of positive and

negative words or in the total?

No.

Uh, so, well, OK, I don't want to, I don't

want to be too stark, too stark about this.

Typically I don't think people do that, but you can

do whatever you want depending on your task, right?

So there, there's a distinction, OK, in a sentiment dictionary

specifically, let's talk about sentiment dictionaries because I think this

is where this is more like these questions are more

about sentiment, OK, positive versus negative.

In a sentiment dictionary there is positive or negative, of

course, but then there's like there's this like awkward third

thing which is like neutral, but there's two ways in

which something can be a zero.

It can either be like genuinely neutral in the sense

that you read the, you like looked at all of

these words and decided this is a neutral word, this

is a neutral word, this is a neutral word.

Or it could be like missing data, meaning like it

does like there's a word that doesn't appear in your

dictionary.

So it could be that nobody ever considered it and

so they didn't, nobody ever decided whether it was positive

or negative, or it could be missing because people did

consider it, but they knew it was neutral and so

they didn't include it in the list of positive and

negatives.

And what do you do about that?

I don't know.

This is like you like this is again this is,

I mean I feel like a broken record here.

This is where like you have to sit down and

think for my task, how do I want to think

about that problem of zeros being like true neutrals versus

like ah missing data like nobody thought about whether this

was a positive or negative word.

Does that make sense?

And, and, and that might like how you think about

the difference between those things might determine whether you think

you want to.

Change the denominator or not, right?

So if you want the denominator to be all the

words in the document regardless of whether they showed up

in the dictionary or not or if you want the

denominator to be only words that showed up in the

dictionary.

But like if it were me, like my rule, I

would, I would do all the documents because.

Weird things go on.

If you, if you make this just the documents appear

in the dictionary, then you got to think about the

selection into the dictionary and then it's like, I don't

even know how to think about that statistic.

OK.

Cool.

So let me show you some examples.

So these are some famous examples on terrorist speech.

Pennebaker and Chung from 2008 and Hancock at all in

202,010.

So they use automated dictionary methods to analyse al Qaeda

discourse in video tapes, interviews, and letters.

And so, So I'm just showing you from Hancocket all

one plot to indicate something that was done with automated

dictionary methods.

So in this Hancock it all paper, the key finding

was Al-Zawahiri, a famous al Qaeda terrorist, was feeling threatened,

indicating a rift with bin Laden, and what do you

mean feeling threatened like in the text that we're being

analysed, the lesson that the automated dictionary method.

Provided was that Al-Zawahiri was Al Zawahiri was feeling threatened

and this was coming out in the way that Al-Zarahiwi

is writing a text, and, and this is, you know,

according to the authors anyway indicative of some kind of

rift with bin Laden.

OK, so what I like about this, this first key

finding here is that he's a core example of like

what's going on with the underlying assumptions of quantity.

Of text analysis.

Remember we have a bunch of assumptions, one of which

is that texts are an observable implication of real world

things that we care about.

That's like I think that was assumption one quantitative text

analysis.

So these people collected a whole bunch of texts that

were written by terrorists and are learning about something bigger

than the text themselves, right, learning about a relationship between

two famous terrorists.

Under the assumption that these texts are actually indicative of

their relationship, right?

Now you might be asking yourself how did they come

to this conclusion?

Well, that's what the automated dictionary methods taught them and

in fact this conclusion come came.

In part from this observation, which is that the use

of first person pronouns I, me, my mind differed across

the two individuals.

So bin Laden's use was constant over time, but Zawahiri's

was increasing over time.

So this is interpreted as an indication that Zawahiri is

feeling more threatened over time, talking more about himself, and

the authors interpret this as a rift developing between these

two.

So here is the quantitative plot for this.

Right?

So this is, this line is Bin Laden's use of

1st, 1st person pronouns, and you see it's like, you

know, whatever, it's going up and down a little bit,

but it's like basically constant.

And then this line is Awahiri's use of first person

pronouns and, um, so this is over time, so you'll

see over time it spikes.

And so the, so and how this plot was generated

was by applying automated dictionary methods to quantify texts by

time and more specifically quantify the sort of proportion of

the text using first person pronouns.

OK, so another example is emotional contagion on Facebook.

So this is a study by Kramer at all from

2014.

It's a study of 689,000 Facebook users.

So what they did in this study was they experimentally

manipulated content shown on the news feeds of these subjects

to test what is known as the emotional contagion hypothesis,

the idea being that emotional content is contagious across people,

so particular emotions can make people like being exposed to

particular emotions.

Can make people then have those emotions going forward.

So what they did was they had two treatment groups

and a control group.

So the control group were people who did not, nothing

happened to them.

They just continued to use Facebook as usual.

Treatment group number one, the news feed that they see

on Facebook, the positive content was increased.

So this was the excitement and manipulation.

Facebook was increasing positive content for the people in this

treatment group.

In this other treatment group, the amount of negative content

was increased.

OK, so some people got their newsfeed was the same

as it was before.

Some people got an experimentally more positive news feed.

Some people got experimentally more negative news feed, and they

use automated dictionary methods to measure the sentiment of users'

Facebook posts after the treatment, right?

So after being either exposed to more positive content, more

negative content, or no intervention.

They then collected the posts of these of the subjects

in the study, and they use automated dictionary methods to

measure the sentiment of those posts.

So to to find out whether people exposed to more

positive content, made more positive posts, people exposed to more

negative content, made more negative posts, and so on.

I mean, the results are, are as the emotional contagion

hypothesis would expect people who are exposed to more positive

content.

Uh, the amount of positive language in their posts increased,

the amount of negative language in their posts decreased.

People were exposed to more negative content, the amount of

negative language in their subsequent posts increased and the amount

of positive language in their subsequent posts decreased.

But of course there were, so this is, there were

very large concerns about the ethics of this study, and

it turned out to be very controversial, um, uh.

Because you're exposing people to more negative content and that

presumably is harmful and you're not supposed to harm people,

uh, in studies, um, but you can read, I mean,

I'm also, I'm going to try to put all the

links to all these papers here because I was digging

through this, having to find them myself because previous years

they did not put any information about the citations.

OK, but here, I'm trying to put the links to

all of the papers down below so you can read

about it and then you can, you can Google it

to read about the controversy as well.

OK, but the use of automated dictionary methods in this

paper was in using a sentiment dictionary to measure the

subsequent the sentiment of Facebook users' posts after being exposed

to the one or none of the treatments.

OK.

Brady at all used dictionary, a dictionary of moral emotional

words to measure whether moral emotional language use predicts the

greatest number of retweets, which they call moral contagion.

OK, so let me just show you, so there's in

three issue areas, so tweets related to gun control, tweets

related to same-sex marriage, tweets related to climate change.

What they did is they looked to see whether um

There's a correlation between the number of moral emotional words

used in tweets and the number of retweets those tweets

get.

So this is the idea here is that if there's

more moral emotional language, that these are the kinds of

things that get retweeted more, and this will be evidence

of a moral contagion, meaning that moral emotional language is

contagious.

It gets spread more and so what they so what

they did is on the x axis here is they

count.

In, so they have a corpus of tweets and so

on the x axis is the number of moral, uh

emotional words in a tweet.

So as this is like, there's more moral emotional words

in a tweet and then on the Y axis, it's

the, it's the um how much they were retweeted.

Right, so in the area of gun control, as the

number of moral emotional words increased again, the moral emotional

words coming from a dictionary, so this is automated dictionary

methods, then the number of retweets increased and and they

compare this to just moral.

Language, right?

So there's a difference between moral emotional language and moral

language, apparently.

You can read the, the, you can read the, um,

the paper if you're, if you're interested.

But when it comes to the use of just moral

words, there's no correlation between the increase in number of

moral words and the number of retweets.

But if it's moral, emotional words, so words that have

both moral and emotional valence, then as the, the amount

of those words in a tweet increases, the number of

retweets increases.

And then that's the same in same-sex marriage and climate

change, same pattern.

OK, so, um, automated dictionary methods should always be validated.

OK.

Again, QTA is very like it's like a, in my

view, it should be more controversial than it is in

part because it's so easy to use tools without validation.

So I'm just like beating the drum over and over

and over again that when you use these tools you

want to validate.

Dictionaries are especially easy to use because there's all these

pre-made dictionaries.

So it's easy to just grab one and then start

doing some stuff, but you should Always be validating dictionary

methods against the task at hand.

OK, so I think about this in sort of two

buckets qualitative evaluations and quantitative evaluations.

So here's the qualitative evaluations, probably the more time consuming

ones, but in some sense the more important ones.

So when you're creating or choosing a dictionary, you have

to, you have to engage in careful identification of the

concepts that you're interested in.

The associated keys and categories and the textual features associated

with each key and category.

So this is a way of saying like you've got

to scrutinise all of the all of the potential keys

in your dictionary as well as the relationship between the

keys and the words.

OK.

Then you should read some documents to get a gut

check about whether the dictionary is capturing the, the meanings

correctly, like whether the dictionary seems to work well in

the context based on your own qualitative reading of some

of the documents.

And then you want to check the sensitivity of your

results when you exclude certain words.

OK, so these are sort of some qualitative evaluations you

can do.

Now there's some quantitative evaluations you can do.

The core one you can do is you can create

a human coded validation set.

And so what that means is you take some sample,

presumably a random sample of your documents, and you have

human coders actually apply concepts to the to to the

documents in the way in a similar way that the

dictionary would, but like using their own subjective judgement.

And then you treat that as kind of the gold

standard.

That's like the truth, right?

Whatever the humans said when they classified the documents was

like the truth, and then you compare how your dictionary

performs against how the humans performed in that sample.

And in order to do that you can measure some

statistics so you can ask how accurate the dictionary is

or the dictionary method.

So what percentage of the documents in your court in

your sort of validation set is correctly coded by your

ADM and what do I mean by correctly coded?

I mean coded in the same way that humans did.

We're assuming the humans are correct because all of everything

we're doing here is about trying to like.

Use computers to mimic what humans are doing.

OK.

You can also measure the precision of a particular category.

So what percentage of the documents coded in a category

by your ADM were indeed coded correctly.

So if you're, you know, what percentage of the documents

that were coded as positive documents were actually correctly coded

as positive documents, and then you might be interested in

the recall of a category.

So what percentage of the documents in a category.

Coded by human coders were coded correctly by the automated

dictionary method.

We're going to come back to these again because this

is going to come up in the context of classification

next week with machine learning, um, but you can.

You can get a quantitative sense of how well your

dictionary is performing by comparing the results of your dictionary,

your automated dictionary method, with results that would arise from

a human coded random sample of your texts, OK.

Now let me show you some examples of why this

is important, why it's important to validate and make sure

your dictionary works for your context.

So I'm going to talk you through a few weaknesses

of dictionary approaches, well known weaknesses.

So one is that dictionaries are very context specific.

This has been the theme of what I've been saying

from the beginning.

Make sure the dictionary you're using is appropriate for the

task and the context that you're studying.

So here's a very famous example.

This is a paper by Laughren and McDonald.

So they use this Harvard 4 tagneg file.

So what is this?

This is a diction, a sentiment dictionary, and then this

is negative words, OK.

So this is a, they're pulling out the negative key

from a particular sentiment dictionary that's the Harvard 4 dictionary.

That's inside General Enquirer if you're interested.

to find this.

Um, so they use that dictionary to classify the sentiment

for a corpus of 50,000 firm year 10K filings from

the 1994 to 2008 period.

What are 10K filings?

These are filings that companies do, uh, for the Securities

and Exchange Commission in the United States, which is the,

um, the regulator for the stock market and, and the

financial sector more generally.

So these, these filings are disclosure filings that are meant

to provide information to potential shareholders and shareholders of publicly

traded companies.

OK, so there's financial documents, basically the legal financial documents.

Now, What these scholars found in their research was that

almost 3/4 of the negative words in the particular dictionary

they used were not typically negative words in the context

of financial disclosure documents like 10K filings.

For example, tax cost.

Crude and cancer were not necessarily negative words in these

financial disclosure documents in these 10K filings, even though the

dictionary consider those considers those to be negative words.

So just to take an example, the word crude in

everyday speech has a negative connotation.

Somebody's being crude, they're being obnoxious or something.

In the context of these financial filings, this was almost

always used as uh uh adjective that goes before oil.

So oil companies were referring to crude oil.

In that context, it's not a negative word.

It's not a positive nerd.

It's not, it's just a neutral word.

It's a description of the industry that the company is

in.

It's not a negative adjective that's meant to evoke some

negative emotion.

Same thing with cancer.

A lot of pharmaceutical companies discuss cancer-related research.

Cancer in general usage is considered to be a negative

word.

You're a cancer on society, whatever.

People use that word in a negative way.

But in the context of financial filings from pharmaceutical companies,

talking about cancer-related research isn't negative or positive.

It's just like, uh, it's like a neutral description of

what the company is doing.

So then this negative sentiment dictionary, when applied to this

corpus is making these documents sound extremely negative relative to

their actual like negativity.

Um, so the problem here is, is polythemes, so that's

words that have multiple meanings, like for example crude, like

it can be crude, like you're being crude, like it's

a behaviour or it can also be a reference to

crude oil, right?

So it's a That word has multiple meanings.

But there's also another important problem that these authors identified,

which is that the dictionary, in addition to having lots

of negative words that weren't actually negative in the context

of this corpus, it did not include a lot of

words that were negative in the context of this corpus.

For example, felony.

When a company is revealing that one of its executives

was convicted of a felony, that's usually considered negative information

about that company.

Litigation also usually considered a financial risk for a company

as restated that word is sort of neutral in the

context, well, not considered negative in the context of the

dictionary, but I used to work at a corporate law

firm long ago and I would actually work on these

documents.

When you have a rest when you have the word

restated in these documents, what you're saying is that there

was there were accounting errors in the previous version and

there of the statements that were provided and they needed

to be restated.

So that's a bad thing.

That means that you have messed up somewhere, um, and

then unanticipated also not in the negative words dictionary, but

in the context of these documents is a negative word.

All right, so to continue on the theme of context

specificity of dictionaries, I want to show you something from

Gonzalez Byon and Poltallo from 2015, and what they did

here is they looked at different contexts, different corpuss essentially.

So corpus of BBC articles, corpus of tweets, corpus of

dig articles, corpus of YouTube transcripts, corpus of blogs.

OK, so these are different contexts.

And then what they did is they took a whole

bunch of dictionaries and used automated dictionary methods to code

the sentiment basically and for each of the particular corpuss

what they're showing you for each of the dictionaries, the

dictionaries are all in green, how accurate the dictionary was

against the human coded validation set.

And you'll notice that across the different corpuss, just looking

at the green for a moment, we'll get to the

red in a second, that the, the accuracy of the

different dictionaries varies quite a lot and it doesn't just

vary within the corpus across the dictionaries, but it varies

across the corpuses.

This is just showing you, this is just a way

of showing you that like these off the shelf dictionaries,

they perform better or worse across different contexts.

For example, LM here, which is Lab MT, performs pretty

poorly for BBC documents, better for Twitter documents, poorly for

dig documents, better for YouTube, and better for blogs.

Now the theme here is that this apparently is not

performing very well for news related texts because these are

both news.

This is a news aggregator and this is a news,

I mean, you know what the BBC is.

This is a news website.

Um, but it's performing better for social media.

I mean you can kind of squint here and see

some patterns, but the general point is that the accuracy

of these dictionaries is varying by context and for any

given corpus, like dictionaries are going to perform differently.

Now what's the red here?

OK, so the red is they also use machine learning

methods to do some classification.

Uh, and, uh, you can see a general pattern here

which is the machine learning methods did better in general

than the automated uh uh dictionary methods, but even then

there's some variation.

But I want to, I want to hone in on

that point even further, which is there's another paper by

Barbera et all from 2021.

They code a set of over 4000 articles, and what

they show in this paper is that machine learning classification

methods in general do better than automated dictionary methods because,

well, they do better than automated dictionary methods.

So you might be asking yourself.

Wait, what is Ryan talking about here?

He's telling, he's telling me that two papers show that

machine learning methods for classifying texts create more accurate coding

of a corpus of documents than automated dictionary methods do,

but classification of texts and automated dictionary methods are different.

Classification is for classifying.

Automated dictionary methods is for, say, measuring sentiment stuff.

They're actually kind of the same thing.

Dictionary, automated dictionary methods is like a sort of simpler

way of doing classification.

There's a reason why we're talking about dictionary methods this

week and we're talking about classification next week.

They're very related to each other.

So the easiest way you can see that they're related

to each other is that you can use automated dictionary

methods to classify documents as being either positive or negative.

So for example, if you, if you calculate a sentiment

score for a document and it's a positive number versus

a negative number, you can say it's a positive document

or a negative document.

That's a classification task.

You're using an automated dictionary method to do that.

You can also use machine learning to do it as

well.

And what they're showing is that machine learning outperforms the

automated dictionary methods, and this paper is also showing that

too, although that wasn't really the point of that's not

the point of this paper.

The point of this paper is just to show how

all over the place dictionary methods, different dictionaries are across

different contexts.

We're gonna talk more about machine learning and classification next

week and you'll see in more detail how that works.

OK.

Another problem with, uh, uh, a kind of shocking problem

actually.

I didn't, I had never thought about this, but, uh,

it goes to show how much I'm thinking about this

in general.

Um, it turns out automated dictionary methods are really sensitive

to frequent words.

OK.

So there's a famous example of a paper that looked

at, um, Text messages sent on 9/11, 2001, uh, and

the, this famous so this is, this is a plot,

OK.

So on the, on the x axis is the time

of day on 9/11.

And the y axis is the number of anger words

per text message.

So this thing on the y axis was a variable

that was created using automated dictionary methods.

So they were looking at text messages sent on 9/11

and using a sentiment dictionary to, to count the number

of anger words, not a sentiment dictionary, like a dictionary

that measures emotion.

They count the number of an anger words that were

in the text messages sent on that day, OK.

So the original paper on this is from Back at

all, and what they show is this dash line here.

So this dash line shows that as time is going

on on 9/11 on the day, the number of anger

words is increasing, and so what they argue in the

paper is that There was an increase in anger over

the course of the day on 9/11, and, and this

was kind of further reinforced by the way that they

set up their plot where they showed these are all

the events that happened.

So this is, this is when the planes originally crashed

into the World Trade centre and then there's, you know,

the building collapsed.

One of the buildings collapses.

This is a speech by the then President Bush.

This is a speech by the mayor of New York.

This is when it was announced bin Laden was responsible,

and so on and so forth.

So you see, so in the paper they're, they're showing

you using automated dictionary methods increase in anger as more

of this information came out on that day.

OK, so it turns out that uh Kuri in 2011

took a look at this data and found something really

weird, which is that as the day goes on, there

are problems with the cell network and so there's a

lot of automated text messages being generated that are um

along the lines of like unable to.

Send your texts or whatever it's just like automated text

messages and apparently these automated text messages use words that

are found in the anger corpus or in the anger

dictionary.

And so and this is just like this is a

fluke, right?

This is just like a this is a coincidence that

these automated text messages use language that codes as anger

according to that dictionary.

And if you remove the automated text messages, so the

ones that were not sent by humans, right, the ones

that were sent by computers, because we're not trying to

measure the anger of computers in QTA, right, like this,

we're trying to measure the anger of humans.

If you remove the automated text messages, the black line

is the amount of anger measured in text messages as

the day goes on.

It doesn't really increase.

So what the original paper is capturing is like an

increase in anger that is generated by a large number

of automated texts that are using words that are coded

in the anger dictionary.

Bad news.

Again, this, I, what I really like about this example

is it's such a, it's so easy to understand the

problem.

And it's another example of like you got to sit

down and you got to read your, you've got to

read your stuff.

You've got to take samples of documents and kind of

poke through it and just double, triple, quadruple check that.

Nothing weird is going on.

And, and let me just say like as somebody who's

been doing social science for a long time, it's like

most of the time you don't find anything interesting.

That's like the dirty little secret is like most of

the time you don't find anything interesting.

So if you find something interesting in your data, you

should really be like wondering if there's like a, if

it's a fluke, OK?

And so that, that should make you go even more

down the rabbit hole, like trying to figure out if

there's something wrong here.

So just keeping back in mind.

Most of the time we don't find anything interesting.

In fact, for those of you writing dissertations, you know,

like, reality is they are probably mostly going to be

uninteresting results, but that's what it, and that's just life,

you know, like most of the time things happen the

way we expect they do and you don't find anything

new.

Ah.

Humans are prone to patterns of behaviour that are persistent

over time.

OK, so now let's move on to a different way

of thinking about automated dictionary methods, discriminating words.

So, automated dictionary approaches presume the existence of a dictionary,

whether you find it on the internet that somebody else

made it or you make it yourself.

But what if you want to figure out which words,

which features of documents are most illustrative of some predefined

categories you have?

So what if you want to flip the script and

instead of like coming up with your dictionary based on

your intuitions about how things should go together, you want

to take a corpus of example text and figure out

which words are more associated with these people versus those

people or this category versus that category, so on and

so forth.

So why might you want to do this?

Well, you might want to do this for just basic.

I want to learn things about the world reasons, OK?

So for example, you might want to understand how rhetoric

differs between Democrats and Republicans.

That's what happens in this paper, or in any other

context, you know, like between Tories and labour politicians.

You might want to understand how rhetoric differs between first

wave and second wave feminist movements in the US.

That's what this paper is about.

So those are more kind of like in the mode

of like I just want to learn more about the

world and kind of understand rhetoric, but you could also

use these methods that I'm about to tell you about

to build new dictionaries based on example text that you

have available to you.

So you can figure out which words are most associated

with the cat, the predefined categories that you have in

front of you, and that can help you construct a

new dictionary.

OK.

So this is known as I, so this whole process

is known as identifying discriminating words, and I don't mean

discriminating in the way that maybe we use it in

daily speech.

I mean discriminating in a more technical sense, like how

does the word discriminate between categories?

I mean it in a kind of machine learning sense

or statistic sense, um.

So a highly discriminatory word is one that is highly

correlated with a category, meaning that if you see that

word, you know that there's a high chance that that

if you see a particular word in a document, you

know there's a high chance that that documents in a

particular category.

It discriminates well among the categories.

Uh, this is also called keywords.

Discriminating words are also called keywords in some places.

Why?

Well, because essentially what you're doing here is you're identifying

keys of a dictionary, right?

So you're using quantitative methods with an example set of

text to back out the keys of a dictionary.

So you can sometimes people call this identifying keywords instead

of identifying discriminating words.

And so I only mention that I don't like using

the word keywords for this because keywords have different meanings

in quantitative text analysis, but I bring it up because

quantator calls this measuring keyness.

So what we're going to call identifying discriminating words, they

call measuring keyness.

What does that mean?

It means like measuring how good of a key a

particular word is for a dictionary.

How much, how the keenness is like how good of

a key it would be if you put it in

a dictionary.

How discriminating it would be.

Yeah.

OK.

Great.

So let's think about the mental exercise here.

Suppose you have N documents, again, our notation for the

number of documents is capital N.

And each document I, little I is what we're generic

document is going to be denoted with little i as

we're always doing, and each one of those documents fits

into one of K known categories.

So capital K is like the number of categories we

have, and then we're going to use a little k

to talk about to be like a generic indication for

like a specific category.

So here's two examples partisanship of the author.

Those, that's one example of a categorization.

So either Democrat versus Republican.

So that would be a situation where we have two

categories.

So K is big K is equal to 21 of

the categories is Democratic, one of the categories is Republican,

and each document fits into one of the two categories

based on who the author of the document was.

Another categorization is time period of the document.

For example, is this document a text written by a

first wave feminist activist, or is this document written by

a second wave feminist activist?

So that's a categorization and each document fits into one

of the two categories.

So you have, so, so, so we're going to presume

we already have some categories we're interested in and we

know what the categories are, and each document fits into

exactly one of the categories.

Cool.

All right.

So the core question we ask when we're doing all

this discriminating word stuff is for each feature, think columns

of a DFM, like each word, although we're usually, you

know, sometimes we're not just using words like we're stemming,

so we're using like stems or sometimes we're using bigrams,

so we're using multiple words, right, but most of the

time we're using words, so we're just going to call

it discriminating words, but technically it's the features, right?

So for each feature that is each column of the

DFM.

Uh, so for each feature J in a corpus's vocabulary,

how much does the presence of that feature in a

particular document inform us about whether that document is in

one of the categories?

So words that are not discriminating at all, it means

that if you see that word in a document, it

doesn't give you much information about whether that document is

in Category 1, Category 2, Category 3, so on and

so forth.

If a word is highly discriminating, it means that if

you see it in a document, that's a pretty good

sign that document is in one of these categories.

OK, so the process for doing this is what we

do is we go feature by feature, so we look

at each individual word, but you know, and be more

general and say feature and then ask ourselves how discriminating

is that feature using some statistical analysis.

So I'm going to consider the example we're going to

work through is going to have 2 is going to

have 2 categories, OK, but you can extend this to

more categories in two categories, but the simplest way to

think about it is 2 categories, OK.

So the useful starting point in doing any of this

discriminating word stuff is a contingency table.

So this is a general concept from statistics.

I'm going to show you an example of it in

a second.

When I show you the example, you're going to be

like, OK, like I have a, I know, I know

what this is.

But if you remember this from statistics, it's a table

of counts for two categorical variables, OK.

But in our case, the contingency table is going to

be a consolidated document feature matrix.

So we're going to, when we do the discriminating words

process, so we're going to calculate whether a word is

discriminating or not or how discriminating it is, we're going

to start by constructing a contingency table for that word

for that feature and effectively what the contingency table is

is a consolidated DFM.

So what do you, what do I mean by consolidate?

Well, you consolidate together all so if you're looking for

the Whether a particular feature is a discriminating word, you

will consolidate the columns of the DFM, that particular feature,

and then all the other features consolidate them together as

not that feature.

And then you consolidate all the documents, all the rows

into the categories.

So if we have two categories, that's the example we'll

be considering, consolidate all the documents into Category 1 and

Category 2.

Now what do you mean by consolidate?

Some together are all the cells.

So collapse the columns, collapse the collapse the columns, collapse

the rows, so collapse the columns by which feature we're

considering, collapse the column or the rows, sorry, collapse the

columns by which feature we're considering, collapse the rows by

which categories we have, and then collapse meaning sum together

as you're doing the collapsing, sum together the values in

the cells.

Now the running example I'm going to use, there's this

corpus in Quanta.

I don't know if we've talked about it yet, but

it's a pre-built in corpus that has all of the

inaugural addresses of every US president since the original, like

the first president, George Washington.

So whenever a president comes into power in the US,

they usually give an address, um, and that address is

a speech.

And so there's this corpus that has the text of

all these speeches, OK.

Why are we using this?

I mean, look, Trump's in the news a lot, so

we're, you know, in some way I can pretend like

this is like a, oh yeah, I made this on

purpose, but actually this is a pre-built corpus in the

quantitative package, um, and so, you know, we're just, I'm

teaching you how to do this stuff using the pre-built

in corpus but you presumably want to do this on

your own corpus.

Um, so what we're gonna do here is we're gonna

take this corpus and we're gonna look at Trump's 2017

inaugural speech.

And we're going to ask ourselves how the word used

in Trump's 2017 inaugural address distinguishes Trump's rhetorical style relative

to all other post-war US presidents.

So another way of saying that is which words used

by Trump in his 2017 inaugural address are discriminating words

in the sense that if you see that word, you,

you can guess pretty likely that the document you're looking

at is Trump's speech.

If you, and if you, if you, and then you

can do the reverse as well, like you can find

discriminating words for the other presidents, right?

And if you see those words, you know you're not

looking at Trump's speech.

Yeah, um, I understand consolidating across the columns, sort of

at the frequencies of numbers if I'm not wrong, but

I don't understand what do we mean by consolidating over.

OK, so let's, so the thought exercise we're going to

do just to show you how these calculations work is

we're going to do, we're going to measure the keenness

or the discriminatingness of one particular feature, the word America,

but you presumably want to do this for all the

words, right, but we're just going to do it for

America.

And so The contingency table that we create, which is

a consolidated version of the DFM, looks like this.

So what we did was we took, so originally the

DFM, every row was a different speech and every column

was the word, you know, the features used in each

of the speeches.

So I took the America column, I kept it as

is, and then I took all the other columns and

I summed them all together and so it's like any

word that was not America.

So that was consolidated in the columns and then the

way I consolidated the rows is I, I added together

all the rows that were not Trump.

And all the rows that were Trump.

Now it turns out there's only one row that is

Trump because he only gives one speech in this data

set.

So we have the Trump speech here and then we

have all the other post-war speeches that have been consolidated

together, added up together.

So when you look at this, you get a 2

by 2 table that tells you as follows Trump used

the word America 19 times in his speech.

Any other word he used 694 times.

So the total number of words in Trump's speech were

19 + 694.

All other presidents that were not Trump, they use the

word America 167 times, and they use any other word,

17,490 times.

And then, so here are just important pieces of data

we can pull from this.

So the total number of tokens in this entire corpus,

capital W with no subscript, add up all these together

18,370 words total used in all the post-war speeches, OK.

Now what are the total instances of the word America

being used?

That's W subscript A for America.

That's 167 uses by non-Trump presidents, 19 uses by Trump,

186 total.

Total number of tokens spoken by Trump.

So that's W with a uh like a curly script

T as a subscript.

That's 19 + 694, 19 + 694, the total number

of words spoken by Trump, America and everything else.

Finally, total instances of the word America used by Trump

is W subscript TA, so Trump America, that's just 19,

that's that, that number.

I put this, you can see all that data in

this table, but I put this here partly as notation

so you can see these Ws because they're going to

come up again.

But I'll keep reminding you what they are, but you

can get all this data from this table.

Now contingency table gives us the basic word counts we

need to do our calculations.

Once we have it, what do we do next?

Two approaches for measuring, measuring discriminating words, statistical association, and

model-based approaches.

OK, so I'm going to kind of rush through the

statistical based approaches.

So the way the statistical association approaches work is as

follows.

You, you, um, set up an expectation based on an

independence assumption, OK?

So you assume that there actually is no relationship between

the use of the word America and whether it's a

Trump or not Trump speech.

So this is an assumption.

This is how the statistical approaches work.

You start with this assumption.

This is like hypothesis testing when you set up a

null hypothesis and you test against a null hypothesis.

We're doing that here too.

But our null hypothesis is we're saying let's assume there's

no correlation between the use of the word America and

whether or not the speech was by Trump or not.

So what we do, so we do some math here.

And so what we can do is we can, assuming

independence, we can calculate the probability of the use of

the word America.

And, and so for a particular document, the probability that

the word America is used and it's a Trump document,

that's what this is, if we assume those are two

independent events, this is the probability we can calculate.

You get these numbers from the previous from the contingency

table.

You should spend some time sort of working through this

process.

And then once you get the probability, you can calculate

an expected word count assuming independence.

So if we assume there's no correlation between the use

of the word America and and whether or not Trump

gave the speech.

How many times would we expect Trump to say the

word America?

You calculate it by doing this and then doing this,

and we would expect Trump to say the word America

7.2 times if There is no correlation between America and

whether Trump gave the speech.

Now we know in the real contingency table the real

number is 19, so you're already starting to get a

sense that like America must be correlated with Trump because

in our real data we saw 19 times Trump used

the word America.

If there was no correlation between Trump and America, we

would expect to see 7.

We're seeing more, so there must, Trump must use America

more than we would expect if they weren't really correlated.

So then you can calculate a hypothetical contingency table.

So this is where you fill in the values based

on what we would expect to see if there was

no correlation between whether Trump gave the speech and the

word America.

So this is a contingency table, but it's under this

assumption that there's actually no correlation.

So this is what we would expect if there was

no correlation.

The each, so each of the cells of the of

the real contingency table are like W, so this is

like W not Trump America, W not Trump, not America,

W, so these are all notated with W's.

We will notate these with E's, E for expected, like

what we would expect under independence.

OK.

So we have two contingency tables, the real one, the

one we actually saw in the in the data, and

then the one that's like the hypothetical one that would

have arisen if we had.

Had no correlation between whether the word America appears in

a document and whether it was a Trump speech or

not.

And what we can do is we can calculate a

few statistics.

One is the pointwise mutual information.

So this is PMI.

So for a category JK and a feature J, the

PMI is just calculated as this.

The word count from the real contingency table divided by

the word count from the hypothetical contingency table and then

take the log of that.

And that gives you PMI.

So for example, Um, oh, let me just point out

something that The book talks about something called mutual information

that's related to pointwise mutual information, but it's not the

same thing, and the differences are we can talk about

it in office hours if you're interested, but you can

also read the Wikipedia page if you want to know

the link between these two things, but they're not the

same thing.

OK.

Now suppose you want to, you want to calculate a

PMI.

So what is the PMI between a category K and

a feature J?

And you don't want to go through the thing of

calculating the stupid, the hypothetical contingency table.

You already have the the real contingency table, and then

you've got to go calculate this hypothetical one.

Well, you can use just the real one and plug

in all these numbers here because this number is created

from these numbers.

And so you can skip the process of creating a

hypothetical contingency table for PMI if you just want to

use the actual contingency table, you can plug in all

the numbers here.

OK.

Now, how would we calculate our Trump example?

So this was the real contingency table, this was the

hypothetical one with our expected values, 19, so we want,

I want to calculate PMI between Trump and America, OK.

So, so what I'm going to do is I'm going

to put in 19 divided by 7.2, so 19 divided

by 7.2.

So our real number of times Trump said America because

we're looking at the PMI between Trump and America, and

then the expected amount and the denominator to take the

log and get.

7.

I'm using, uh, E, um, the, like the mathematical E

is the base here.

It doesn't matter.

I think it's more standard to use two as the

base for this log, but I'm using E here because

that's what Quantator does.

This is boring math stuff.

It does not matter.

As long as you always use the same base, it's

fine.

OK, cool.

Now, when you get a PMI that's positive, that means

that that feature is more prevalent in that category.

You get a PMI that's negative, it means that feature

is less prevalent in that category relative to the other

categories.

So this positive PMI here is saying that if you

see the word feature, if you see the feature America

in a document, It's a safe bet to assume that's

more likely to be a Trump speech.

Now there's two other statistics you can calculate the likelihood

ratio statistic and the Pearson's chi squared statistic.

So if you remember from your hypothesis testing days and

stats classes, these are more you certainly have come across

chi squared statistics, especially, I mean if You know what

a contingency table is.

You've probably calculated a chi square statistic based on a

contingency table that's at one point in your life.

But these are two other measures of, of the, in

some sense, the correlation between a particular feature and a

category that um.

The technical details are not super important, but they're each

using all of the information the contingency table, whereas PMI

only uses information from one of the cells.

This is not super important.

They're both giving you the same information.

Higher numbers.

Well, and actually in both cases, positive, well, in this

case, higher numbers, higher association between the feature and the

and the category, and in this, this case is a

little bit more complicated.

It's the same thing, higher numbers is a higher correlation,

but for whatever reason in Quant data.

Uh, Quant adds negatives to this.

If you remember from statistics, chi square cannot be a

negative number.

If you look at this formula, it can, there's no

negatives in here.

But Quantator does, I could not figure out, I mean,

briefly mentions in the documentation that it does this, but

it adds negatives.

Don't know how, don't know why.

If I did, I would have incorporated it in this

formula and then called this something other than square because,

but whatever.

When you do, when you use Quant data to calculate

this, it's going to give you negative numbers.

Why?

Because it wants you to be able to compare it

with this and with the PMI who are on a

positive negative scale.

Whatever, it's not super important.

OK OK, cool.

So here you can use Quant data to do this,

and you can do, we, I did the calculations for

one particular feature, America, but you can do them for

all by plugging your DFM or your corpus into this

function in Quanta.

And then it'll give you a table that lists every

feature in the corpus, protect country America back, so, so

on and so forth, and then the relevant statistics.

So in this case what I did is I asked

one to calculate the likelihood ratio statistic or G2 or

D squad, and then the p value associated with that.

Um Again, if you're, if you remember statistics, G2d is

distributed according to a chi square distribution with one degree

of freedom in the case of a 2x2 contingency table,

and so the p values are just from The table

of chi squared p values for different degrees of freedom.

And, and obviously chi squared Pearson's chi squared statistic also

distributed according to a chi squared distribution, you can also

do the same thing, get the p values from the,

the calculated.

Statistic values.

OK, so you can plot this too.

So I did plot this.

So I did the top 10 and the bottom 10.

So I mean, obviously I didn't plot there was like

2000 words.

I didn't plot them all.

I just plotted the top 10 and the bottom 10.

OK, so these words are the ones that are most

associated with Trump, meaning that if you see these features,

you have a good chance of guessing that the document

you're looking at is a Trump document.

These are the ones that are least associated with Trump.

So if you see these words, you have a good

chance of, you'd be pretty confident that you're looking at

a not Trump document.

And then I have the uh dash line here is

the statistical significance threshold.

So, um, beyond the dash line it's statistically significant, but

inside the dash like between these two lines is not

statistically significant.

So down here, the 5 that are statistically significant indicators

of you're looking at not a Trump document peace, no

freedom, can, and the word us.

Now, these are the ones that are most indicative that

you're looking at a Trump speech, protect country America, back,

um, everyone job, American, so on and so forth.

There's um a little, oh, it doesn't appear here.

Oh yeah, it does appear here.

Obama appears here and then we think like, oh, you

know, let's talk about Obama a lot.

He's literally the last president, he was the first president

after Obama was president.

All those other presidents did not even know who Obama

was and so they did not say the word Obama

at all.

So there's like, this is a little bit of a

fluke.

I mean this is like one of the kinds of

things that you want to deal with.

You probably want to use a method to weight your

DFM to get rid of words like this, because the

fact that Trump said Obama is not particularly special because

no prior president.

Knew about Obama, so didn't speak about him.

OK, yeah.

All right, cool.

So now, let's take a little detour and talk about

proba probabilistic language models.

So Backing up, let's talk about probabilistic models in general.

I'm gonna kind of give you like a, I don't

know, a very rough.

Short crash course and stuff you hopefully remember from a

probability and stats class at one point.

So the basic idea of quantitative text analysis or of

using probability to model text is that documents are draws

from a probability distribution.

OK, so a document feature matrix is a sample that

has all the associated sampling process processes.

So the way that like social scientists tend to think

about the role of probability in in the quantitative work

we do is that like, The things we care about

are big social processes that are happening above us and

what we have access to are like data sets which

are specific observable implications of the bigger processes.

So typically social scientists will think of the big processes

as probability distributions.

Complicated ones where things are correlated, OK, but you know,

you can build probability distributions that have that are multi-dimensional

and correlations in them.

OK, fine.

But that's like, that's like the truth.

And then the data sets that we look at and

try to analyse are just Samples drawn, like randomly drawn

from the truth, the probability distribution.

They're not actually randomly drawn.

That's, that's like the, the dirty little secret here, but

this is like a useful fiction we use to try

to conceptualise what it is we're doing when we're doing

statistics, OK.

So our data sets are just samples from the larger

distribution.

The same principle applies with with text.

If you have a text that somebody wrote in the

world, if you want to think like a social scientist,

you think, well, there was some big data generating process

that generated the data you're looking at.

The data generating process we mathematically model as a probability

distribution, and that particular text is just a random draw

from that distribution.

Now of course that distribution up there in the sky,

it's got all the correlations embedded in it, like all

the things that we think in the world are correlated,

like, you know, somebody's partisanship is correlated with the kinds

of words they use, that's already embedded into that in

the truth up there in the cloud, you know, and

then the random draw we get should give us an

indication of that if we draw enough, if we draw

enough observations and our samples are large enough, we should

be able to kind of back out the correlations in

the cloud in our data.

That's like the idea of I just told you statistics

in like, you know, 5 minutes.

All right, so data generating process is the name we

give to the, the big, you know, the social forces

in the cloud that we're modelling as the probability distribution.

We assume this's just that that what's going on out

there is capturing what's really going on in the world.

Real life data sets are samples from the DGP.

Um, and then the DGP is what we care about

since it's like how things work, but we don't observe

the cloud.

We don't observe those real probability distributions up there in

the cloud.

That's actually what our job as social scientists is, is

like to learn about what's happening up there.

OK.

So we're going to use real life data to try

to learn about DGP.

Language is modelled the same way.

Language as we speak is just like there's some probability

distribution that describes how language is generated, and then when

I'm speaking here, I'm just like drawing, I'm taking random

draws from that distribution and like the words are coming

out of my mouth, OK.

So the simplest language model you can use, you can,

you can do all sorts of complicated stuff, but like

the simplest language model that we're going to like really

build on a lot in this class is to assume

that each document bold WI, so that's a row and

a DFM.

That's what the WI is.

I'm going to call it a document, but you know

it's just a vector of word counts, right?

OK.

So that's going to be a draw.

Each document is going to be a random draw from

a multinomial distribution.

So the DGP up there is a multinomial distribution.

Each individual document we see in the world is a

random draw from that multinomial distribution.

And then multinomial distribution has two parameters mu, this is

bold, in case it's not obvious.

This is a bold mu.

And then MI.

OK, so this is the magic thing.

This is telling us the like how the language is

generated.

OK, but I'll get back to this in a second.

What exactly is happening in the math.

This is the simpler thing.

This is just like how long the document is.

So if each document is a draw, this is the

link, so it's a draw from a distribution where this

is the length of the document encoded in the distribution

and then this is the, this is the um the

indication for like exactly how the document is going to

arise.

Mathematically, what this is is a vector and it just

said like every potential feature that could arise, so like

all the features of your vocabulary have an associated probability.

And so this, this mu here is just like for

each of the words, what's the probability that word can

arise in a document.

So then when we take a draw from the DGP.

The draw effectively takes this vector of probabilities and then

randomly samples words and then generates a generates a document.

OK, so in math notation, a document I is drawn

from a multinomial distribution with these two parameters, the length

of the document, and then the probability, so this is

a bold, this is a vector stating the probability that

each of the words would arise.

This is how we draw them out.

So here's like the, this is how, this is like

a probability diagram.

OK, so this is the main parameter that's generating the

language.

And then it, this is a diagram for a 3

document model.

So this is 3 draws from this distribution.

grey means things we see.

These are the documents we actually see this.

White means things we don't see.

This we don't know, we don't know this.

We're learning, we're trying to learn about this.

From the things that we actually see and this is

just a way of drawing.

OK, so this is easy to draw when you have

3 documents being drawn, but what if you have like

100?

Do I want to draw a diagram that has the

mu and then 100 sticks coming out of it?

No, it won't fit on the slide.

So instead, what I do is we do this plate

diagram where we say, here's the, here's the DGP as

represented by the mu, and then there are N documents

drawn, each one is indexed by I.

This is just a way of, of not having to,

you know.

Draw 100 sticks.

OK, cool.

So, Multinomial distribution makes total sense for the context of

bag of words.

So we're assuming bag of words the whole time, multinomial,

the way it works is, well, if we don't need

to get into the details here, but if you conceptualise

a document as being drawn from multinomial distribution, what's going

on is each you're drawing.

Uh, you're effectively drawing a bunch of words at random

according to the probabilities, right?

So some words will be more, more likely to be

drawn than others, but they're being drawn at random and

independently.

That's bag of words.

We can use multinomial because we assume bag of words.

If you think word structure matters, you can't use the

multinomial distribution to model language.

But we're just going to do it because it turns

out it's been, you know, the bag of words thing

has been validated so many times that we're going to

do it.

And then because we're going to assume bag of words,

we're going to use this multinomial distribution that's so easy

in terms of the math, so easy, so many things

you can do with the multinomial that like makes the

calculation simple.

Now let me show you an example, very simple example,

a small number, a small vocabulary just to like get

the idea straight.

Suppose we have a vocabulary with three features cat, dog,

and rat.

OK.

Now further, suppose that cat is twice as likely as

dog or rat, which are equally likely.

So what that means is that OK, this is an

assumption we're going to assume that the DGP, the mu

out there, looks like this, which is the probability that

cat gets drawn is 0.5.

The probability that dog gets drawn is 0.25, and the

probability that rat gets drawn is 0.25.

So anytime a document gets drawn, each particular word is

chosen according to these probabilities.

So if you look at this, it means that like

in any given document you should expect to see twice

as many instances of the word cat as you see

a dog or rat.

With a large enough sample, it should be about 1/2,

1/4, and 14th.

OK, now what I can, I can pull up my

fire up R and I can start doing some sampling

to like to show you what it looks like to

do the random draws from the probability distribution.

All right, so I do.

So here what I do is I, these are the

probabilities that I have in the previous slide.

This is our vocabulary.

So this probability corresponds to that word, that with that

word, that with that word, and I'm going to draw

a sample.

That's what the M is saying.

A document that has 2 words in it, a document

that has 3 words in it, a document that has

2 words in it, and a document that has 6

words in it.

So my sample is 4 total documents but varying word

lengths.

And then I'm going to set the seeds so that

I get the same answer every time.

That's how in our, if you want to do random

sampling, you're going to get different numbers every time, but

because I'm compiling these slides multiple times and I'm using

this information on further slides, I didn't want it to

change so that the information on the next slide isn't

match.

Information on this slide, so I sent the seed.

All right, so I did, I did the random sampling

here and this is what Docket 1 was rat cat.

Document 2 is cat cat dog.

Document 3 is rat cat.

Document 4 is dog rat rat rat cat dog.

So those are randomly drawn documents.

You can see here where bag of words matters, right?

Like this is just like a random sequence of words,

but that's how we're modelling the language in our context.

Well, so I can transform those 4 documents into a

DFM by counting the number of times the different words

appeared.

So the DFM has each document as a row, each

column as a feature, and so the first document, remember,

was rat cat.

So 1 instance of cat, 1 instance of rat.

The fourth document was dog rat rat rat, cat, dog.

So it's 1 instance of cat there, 2 instances of

dog, and then 3 instances of rat.

So from the random 4 documents I just created, I

can create a document feature matrix.

And I can represent each of these documents as just

the vector of word counts.

So document number one is 101.

Document 2 or 4 is 123.

And each one of those documents was drawn from the

multinomial distribution that I specified before.

So it's common to work with a multinomial distribution because

it's really easy to work with, partly because we're assuming

every word is drawn independently and that just like the

math gets so much easier.

If stuff gets correlated, if the words are being drawn

in a way that they correlated with each other, oh,

we then we're then, you know, life gets much more

difficult for us in mathematical sense.

But we're assuming bag of words, so it's fine.

We're going to use the multinomial distribution.

Turns out you can go to Wikipedia and find a

whole bunch of formulas that will help you calculate certain

things if you know you have a multinomial distribution.

One thing that's important is that you can calculate the

probability of getting a particular document.

Given the model that you have.

And this is the formula.

This formula looks kind of complicated, but it's actually really

easy to calculate, and I'll show you how to calculate

it for the simple example.

So you can see it's actually pretty easy to calculate.

So if this is the new that we have, let's

ask ourselves a question.

What is the probability of getting the document cat, rat,

dog, cat?

This is a wholly new document, OK, so I just,

what is the probability of getting that document if this

is the new that we have that is the model

of our language?

Well, if I write this in vector form, it's 211

because 2 instances of cats, one instance of a dog,

and one instance of rat.

So what is the probability of getting that document, given

those probabilities, plug in all the numbers?

211 plug them in here.

Take the exclamation point, which is a factorial, then do

the same thing here, plug in the numbers, then plug

in the probabilities.

Take exponents that are equal to the Vector, calculate it,

number.

Yeah, the order matters.

Uh, no.

No.

The order doesn't matter here.

The order doesn't matter here, the order doesn't matter here,

and then the, yeah, no, it doesn't matter.

Now you can do an R as well.

R has a function for calculating exactly that thing which

looks like this.

If you give it a vocabulary like this and probabilities

like that, you can ask it what is the probability

of getting this document.

So 2 instances of cat, one instance of dog, one

instance of rat, and then you, you feed in the

probabilities which we already specified.

It'll tell you the exact same probability I just calculated

by hand.

Using the D multignome function.

Cool, right?

Just do it quickly in R.

So if I ask you, let's assume we have a

language model with these specified probabilities.

You should be able and then and I say assume

that, and I give you a particular document and I

ask you what's the probability that we would get this

document drawn from that DGP that we just assumed.

In this case, the probability of getting cat, cat, dog,

rats is 18.75%, or probability terms is 0.1875.

That's a fairly high probability, but not like that.

Now In the real world, we do not see the

DGP.

This little hypothetical thought exercise that we went through, I

gave you the DGP.

I said, here, here's the news.

Now calculate the probability that we would get this particular

document.

But in real life we're doing the reverse.

We see documents, we don't know what the news are,

and we're trying to figure out what the mews are

from the documents.

So we're kind of going backwards.

That's what inference is.

And when people say we're doing inference, statistical inference, whatever,

that's like you're, you're drawing inferences from data about something

you don't know, OK?

And the thing we don't know is the muse.

What is the new conceptually, the mu is the model

of language.

The mew is telling us like how the language arose,

and it's actually giving us substantive information.

If word one is twice as likely to happen than

word two in a particular model, we know something about

the rhetoric of that this language model is generating.

Again, we're discarding word order, so OK, fine, we're not

getting word order, but we are getting word frequency and

our model of language is telling us something about the

rhetorical style of somebody.

That's what the, the new is like the important thing.

It helps us learn about, about like, about the world.

OK.

So if we have a corpus, a real word cor

corpus, we can estimate the new based on the data

in our corpus, and the multinomial distribution makes this so

easy.

That's why we're going to keep using it.

Given a language model, the probability of feature J arising.

We can estimate it.

That's what the hat means.

We're going to estimate it using our data.

We don't know the truth, but we're going to estimate

it using our our data.

It's just the number of times.

Uh There's something off here, hold on a second.

This should be summed differently.

Let me correct this.

I don't think this is quite right.

It's fine for the next example, but I think this

should be.

This should be summed.

I think the numerator needs to be summed over the

documents.

Yeah.

There's something off of this, but this doesn't matter.

Let me, let me, I'm going to correct that and

I'll send an announcement about it.

Let's get to the example to show you how this

works.

So when the US was established, OK, you might be

asking why would we ever want to estimate this new

thing.

What's the point of it?

Well, there's lots of things you can do once you

get the mu, as I said here, like once you

estimate the mu, you can do a bunch of cool

stuff.

So one cool thing you can do is what happened

in 1963.

There's these things called the Federalist Papers that are historical

documents from the US, and they were basically arguments in

favour of ratifying the US Constitution, and they were written

by three different people, but they were all published under

a pseudonym Publius, so they're all anonymous.

His history has gone forward and people figured out like

who wrote which ones.

The 3 people were Hamilton, Jay, and Madison.

And so of 73 of the 85 documents like we

know who wrote them because like there was, you know,

we figured it out, and historians figured it out like

through contemporaneous evidence.

But there was still the remainder, like the additional 12

or whatever that were unlabeled we weren't sure who wrote

them.

Historical records didn't like tell us.

So what they did, um, is they used the probability

model to try to figure out, sorry, figure out who

wrote the unlabeled Federalist Papers.

So the way it worked was as follows.

So we conceptualised they, not we, they conceptualised each different

author as using a different model of language.

That makes sense.

Each different author writes differently, uses different words, so each

one of them should be conceptualised as different models.

So Hamilton has a me, Jay has a U, and

Madison has a me.

So each document created by each one of these authors,

you can think of it as a random draw from

that author's model of language.

So Hamilton Each of the Federalists that Hamilton wrote were

draws from the distribution with Hamilton's model, Madison and Jay

the same.

OK.

So the basic process that these that we can go

through to try, OK, well, did I say what the

point is?

The point is to figure out the unlabeled ones who

wrote them.

OK.

All right, so the basic process for doing that is

step one, estimate the mu for each writer.

So estimate muh, muj, mm.

That's like, that's like a fancy math way of saying

like figure out their writing style, at least if we

define writing styles like how frequently you use words because

we're using bad words.

OK.

Step 2, see which model one of the 23 models

best explains the remaining unlabeled documents by calculating, well, what

would be the probability of getting those remain of those

remaining model, those remaining documents assuming each of the three

models, and whichever model generates the highest probability, well, that's

the one that that author is the more likely one

to have written the unlabeled documents.

Now if we assume the author's style is constant across

documents, so Hamilton's style is the same in the first

document he wrote and the second document he wrote, and

so on and so forth, then what we can effectively

do, given that we're using multinomial and everything's nice and

clean, is we can treat all of Hamilton's document as

just like one big document, so just like paste them

all together and so Hamilton is one row in the

DFM.

Where you're taking all of their documents and all the

word counts and all their documents and just adding them

all together to make one row, and you can do

that because of bag of words and multinomial distribution.

But of course you, you need to make the assumption

that all that Hamilton's model of language is the same

across all documents because if it's not, then you can't

do this, right?

Because then you're, you're gonna, well, it's a reasonable assumption,

but it's an assumption in the last.

OK.

So you can get a DFM like this.

So I'm not going to worry about all the words.

I'm only gonna worry about three of the words that

were used by man and a pun.

And Hamilton, this is Hamilton's document.

This is Jay's document.

This is Madison's document, and here are the unlabeled ones.

So what is the probability that Hamilton wrote the unlabeled

ones?

That's the first question.

OK, so first, we want to create a model of

Hamilton's language.

How do we do that?

Estimating his muh, we do it by calculating the probability

Hamilton used each of the three words, and it's a

very simple calculation.

What's the probability that Hamilton uses the word by?

Well, the number of times he used the word by

divided by the total number of times he used any

word.

859 divided by his total word count.

What's the probability he used the word man?

102 divided by the whole world, the whole word count.

A pun 374 divided by the whole word count.

And then if you calculate this out, you'll see there's

three probabilities and they'll add up to 1.

0.64 probability of using by, 0.08 probability of using man,

0.28 probability of using a pun.

This is Hamilton's model of language.

This is how Hamilton uses these, these three words in

this corpus.

Or the mixture in which he uses them.

Well then we can calculate the probability that Hamilton wrote

this set of documents.

OK, we're kind of implicitly assuming all of these documents

are written by the same person.

OK.

Let's not get into the possibility that these documents are

written by different multiple people.

All right, let's just assume that we're gonna, they're all

written by the same person.

We're just trying to figure out who it is.

OK.

So what we can do is we can calculate the

probability that this document would arise if Hamilton were the

author.

And the calculation is to use that formula from above.

So we take the counts for the document we're trying

to calculate the probability of, and then we take the

probabilities from the Hamilton model and we dump all those

into the formula and we calculate that there's a 0.001

probability that Hamilton wrote this document.

You do the same thing with Madison, calculate Madison's language

model, and then calculate what's the probability he wrote this.

0.077.

Just, I mean, I can dwell on what I did

because it's the same process.

You do the same thing for Jay.

0% chance that Jay wrote this document.

OK.

One issue that we had, OK, well, you can already

see here who do we think it's most likely to

be?

J is 0%, Madison is 8%, uh, Hamilton is 0.1%.

Madison is probably the most likely person.

Cause That document, the unlabeled document, has the highest probability

of arising under the Madison model of language.

Now, um, we had a problem here which is that

for Jy we, we calculated a 0% chance that Jay

wrote this document and it's because Jy uses the word

man 0 times and so you introduce a 0 into

our calculation and it zeros everything out.

But like that's weird.

Presumably Jay as a human uses the word man sometimes

in his life.

Just happened not to have used it in the particular

sample that was drawn that represents the Federalist Papers that

he wrote.

So it could be a sampling fluke that we got

a 0 for man and Jay.

Like clearly Jay knows the word man, right?

Do you agree with me?

Like it's not a weird word.

Like he probably has said the word man in his

life.

He just didn't write them in the Federalist Papers.

But what we did when we did those calculations is

we overfit.

So we, we generated a model of Jay's language use

that was like too closely fit to the sample that

we had.

The sample's pretty small and so we were overfitting.

Or if anything to come up again next week, by

the way.

So what we want to do, and this is a

general issue with sparse DFMs.

Remember, most DFMs have lots of zeros in them because

that's just how word use works.

And so you're going to get this problem a lot

of like when you build a model of language for

a particular person, you're going to get lots of zeros

because there's lots of zeros in the DFM.

So what you want to do to solve this problem

is what's called regularisation, which is you just introduce a

non-piece of data.

To try to get rid of the overfitting in our

case, just add a one to all of the cells.

It doesn't change the content of the models in the

sense that like it won't change the correlations between anything

in any real sense because you're doing it uniformly across

every single cell, add one, but what it does is

it wipes out the zeros.

So now we can, so this is called Laplace Laplace

smoothing when you just add like a 1 to everything

to get rid of the zeros.

This is a method for regularising that means that we

are going to prevent the situation where we had before

where we calculated a 0% probability that Jay wrote a

document simply because Jay didn't use a particular word in

the documents that he did write.

OK, so you can recalculate stuff, but It changes your

calculations slightly because remember I was calculating the probabilities for

each of the authors.

That was the model for the author, and that was

just the total times they were used the particular word

divided by the total number of words they used.

So the calculation changes a little bit when you do

Laplace smoothing.

It's the total number of times they use the word

plus the amount you're smoothing.

So just like in our case we're just adding a

word, alpha equals 1.

Divided by the total word count of that um that

individual plus a one for each of the, a one

smoother for each of the tokens in the in the

vocabulary.

Now, we can recalculate things and so for J, it

changes, we don't get a 0 anymore, but it's still

really, really small.

Then make.

OK, fine.

All right, so that leads me to the last thing

I'm going to spend 5 minutes on.

This is actually not OK, so, um.

I'm going to leave you to read this on your

own and to work through the calculations.

It's actually pretty straightforward, but let me tell you what's

going on in fighting words.

So back a while ago we were talking about ways

to measure whether a word is a discriminating word or

not and we were using these statistical measures where the

exercise we did was like let's assume no correlation between

categories and tokens and then let's ask ourselves how does

our real data match up to this hypothetical data where

there's no correlation and that gives us a way of

calculating how discriminating word is and also getting p values,

really nice, OK.

So that's one approach you can take.

Another approach you can take to measuring discriminating words is

what's called the fighting words approach, where you start with

a language model.

And then you estimate the language model just like we

did.

So for we were estimate, we estimated 3 language models,

one for each of the authors.

You do that and then you use the probabilities you

get from that, the muse, which are giving you an

indication of how the language is used by the members

of each category, to then calculate a statistic that is

quite similar to the the log likelihood ratio statistic we

had before.

It's a little bit different mathematically.

And then you get a measure of discriminatingness.

This is what you get for using that method, the

fighting words method, and it's very similar.

The most discriminating Trump words are protect country back everyone

job America.

But notice Obama has disappeared.

So this model-based approach to measuring discriminating words deals with

some problems we have with sparse data, which is nice,

um, but it generally looks similar, it looks similar to

what we got from before, but the, the process was

different.

So again, let me just repeat.

In the statistical association world, what we do is we

say, here's our real data, here's hypothetical data if there

was no correlation between our categories and our tokens.

How do those two things compare with each other that

generates test statistics or statistics?

In the fighting words approach, we say, OK, let's assume

that the language in our corpus is generated by some

model.

Let's estimate the model and we do a different model

for each category.

So let's estimate a model for Trump, the Trump inaugural

address, and let's estimate a model for all the other

presidents.

And so we have two separate models.

Those models capture something about the rhetorical style of Trump

versus the other, uh, the other presidents, and then we

use those models to calculate some log likelihood statistics that

we then normalise to deal with some problems, whatever, technical

details, but it then gives us.

A a Z score that tells us how discriminating uh,

words are as well.

You should Spend some time on the math here, but

I'm telling you the math is the same as we

just did with the Madison example.

You calculate the language model for Category 1 and Category

2, so for Trump and the non-Trump.

Then once you get the new hats, you plug them

into this formula.

And then do some math.

It's like get your calculator out and calculate, and then

you calculate this thing and plug it in again and

then you get the final Z score.

But this slide looks a little complicated.

It's not, it's just plugging stuff in.

The complicated part is thinking about how to get these

new hats, but you saw how to do that in

the context of Hamilton, Madison, and Jay, but you would

just do this in the context of Trump versus non-Trump.

And then in seminar we're going to do some dictionary,

some simple dictionary method stuff, and then we're going to

do this.

We're going to calculate this stuff.

Cool.

I'll see you next week.

My voice is coming back.

This is better than it was when I started.

It's nice.

Yeah Some stuff What I'm sorry.

And so yours are like overthinking uh was it was

like.

Deeply personal.

I eat everything.

And it has like one in overtaking and.

But Don't black OK, let's go.

There's another class assignment for next week for next week.

No.

Yeah Oh, not good.

I haven't even been able to like catch up with

QT.

It's OK.

I'm happy that I'm not.

Yeah, we don't have some.

Oh, OK.

OK, that's good.

But yeah, it is so much like it's so much

work with OK, good.

I I didn't know it was the.

Probably gives you more time to actually learn art, but

it's supposed to be the problems that they do on

the 13th of February we have 1.5.

Let's go Thursday.

Still and then yeah, it's just like I don't find

our modules like modules subject selection very good in terms

of like assignment deadlines basically like it's you have to

like choose countries and that's at least for us.

for that too, that that was like we haven't chosen

that, uh-huh and she has, but I'm like I know

we used to get likes.

the time.

I.

Hi good.

I O One So.

And then I.

Yeah.

No, no, I did maths I have in this.

like high school.

I feel like high school and then they go to

university it's like yeah they like yeah.

I feel like, oh my God, I um.

It's like the yellow bars, yeah, OK, yeah, yeah, yeah,

into like 7 worlds or something like that, yeah, and

like on the whole I disagree with that book, but

like there are some really interesting and then um one

thing that she said was like the different styles of

teaching math across the world and how in a lot

of like Southern Europe and stuff, um, people have a

lot more of a theoretical basis.

OK, whereas um.

You.

and like capitalist space right and like I was like

that's right and just see, I think that's true in

the success or in areas like where I grew up.

Lecture 4:

So I'm like contacted her just to be like, how

did you find I was like.

And then she was like, actually we have for this

call.

So Yeah.

That She just I mean what the.

Yeah, I actually, I watched them.

It is.

Oh really?

Oh my god.

What were you just like hanging up?

Yeah I was.

Oh, what is it cheaper?

Or do you like I take this or And Related

to my time I Yeah.

Oh really, I told you I was like a little.

Yeah, there's, it's literally more like the sensitive parts my

heart.

Yeah, I haven't know about Oh, I see.

Delay me to pass like when.

And it was, I was flying to Newton, which was

the train from the platform so I can get.

got cancelled and I want to play.

And I was like, oh my God.

to Yeah, it, I was like absolutely I went back

to my phone.

Yeah like it was like sitting on the orbit.

trying to book a bus that I just.

make a I And you know what it was, it

was the day before.

Oh I do I study?

I know.

Not a whole lot.

It was mostly alcohol related.

Where, where, so we did getness, which I thought was

kind of, yeah, it was really like it was a

lot of people it was a lot of like drug

class.

It's a lot of it's very.

I feel like it was nice so we did the

JMC tour, which I thought was really fun.

I actually like learned a lot and I was like,

yeah, yeah, like I was and I was like.

castles.

Have passed around the.

Yeah, I thought that one was.

I She literally.

Yeah, he's good.

Oh yeah.

I yeah, um.

I I OK.

It was great and I was just like walked around

a lot.

I think it was like shitty weather, yeah.

But Yeah, it's.

I In Sunny mountain, far out, far out, but like

it was yeah, it sounds like you like.

So it was there were some minor typos in the

slides that were posted yesterday.

I think that they were posted about 10 minutes ago.

It's not a big deal, just some of the numbers

were.

Incorrect, but um nothing major.

But if you want to get the most recent copy

drop on the website.

This This is my I'm at the point where like.

It's so much training to do that I'll just take

3 week and as cold as it is.

I don't have to find like something that.

you know I haven't got.

I.

No, no, yeah.

Yeah, OK, yeah, I redistribution.

I think it was actually he was a child, so

he needs to be.

Yeah, yeah, so we finished it.

I'm like, yeah.

It's that was a weird.

I.

Bob Dylan.

Speaking of But it's like, OK, let's go ahead and

get started, um.

Uh, I guess the department has already figured out all

the auditing, um, so you should have received them if

you were on the waitlist for auditing, you should have

received an email, an assignment to a seminar.

If you did not, that means that the department wasn't

able to use space in the seminar, but of course

you can still go into lectures.

OK, so, um, this week we're gonna talk about uh

classifying texts into known categories.

So first, let me just apologise for those of you

who are in MY 474 or 475 because you're gonna

hear a lot of stuff in this lecture that you

probably have heard in that context, um.

Two things to say about that.

I think I mentioned this in the past, but, uh,

you know, when, when we go to PhDs and we

become faculty at universities, we get very little training in

how to teach.

But the one thing they tell us over and over

and over again is repetition is the key to learning.

So for those of you who are in MY 474

and learning about machine learning this term.

Uh, it's not harmful to hear it again.

You're gonna hear it in a slightly different way and

also you're gonna hear a kind of very fast broad

overview.

But also there are a lot of people in this

class that are not in that course and have not

taken a course in machine learning.

So some of this stuff, uh, a lot of this

stuff is gonna be new to them, and so, um,

that's life.

OK.

So, uh, I'm gonna skip the outline and I'm gonna

jump right into some motivation.

So our goal today is to classify documents into known

categories, right?

You have to have a corpus with a bunch of

documents and you want to partition them into a set

of categories that you, that you predetermined in advance.

We already sort of saw this last week using automated

dictionary methods.

You can use.

Automated dictionary methods also to classify documents into known categories,

the categories created by the dictionary that you're considering.

So the advantage of using automated dictionary method methods to

do classification type stuff is that it's dictionaries are not

corporate specific, so you can have a dictionary that is,

for example, a sentiment dictionary that'll.

Allow you to classify text into positive or negative, those

two categories, um, and that dictionary is not specific to

the corpus, so you can apply that dictionary to multiple

corpuss, right?

And so, uh, the cost of extending dictionary methods from

one corpus to the other is trivial, but that's also

the disadvantage of dictionary methods.

Dictionary methods are not corpus specific, so the ability of

a dictionary to classify a particular corpus is going to

uh differ from corpus to corpus.

Dictionary is gonna work better in some contexts and other

contexts because it's not corpus specific, you got to do

a lot of kind of validation to try to figure

out whether the dictionary you're using is working for your

particular.

Uh, corpus.

This is known as domain shift, right?

As you shift domains, right, as your corpus, if you

use different corpus, if you have different corpuss that are

about different things, uh, dictionaries are gonna perform, uh, better

or worse.

So supervised learning is another way to classify text into

known categories that's different than automated dictionary methods.

The way you can think about supervised learning, at least

with, uh, in terms of its relationship with automated dictionary

methods, is the kind of generalisation of automated dictionary methods

where the association between the document features and the categories

of interest, the classes of interest, they come from the

data itself and not from a dictionary, right?

So that's, that's sort of the way you can think

about the relationship between a a supervised learning for classification

and Automated dictionary methods for classification.

And, and I'm just gonna point out that like by

construction, meaning like, by the way that supervised learning works,

it's always gonna be better than uh it's always gonna

be better classification than dictionary methods.

You know, I'm not going to go into, I mean,

I could, I could probably have devoted half this lecture

to explaining exactly why, uh, supervised learning methods for classifying

texts are always going to outperform automated dictionary methods, but

just take my word for it.

Um, and I showed you last week, actually previewed this

by showing you this chart, this, um, plot from uh

Barbera at all paper, um, and he, this paper is

a, it's a kind of survey paper and like uh.

Uh, text analysis methods.

Um, what you can see here is they, they did

this exercise where they coded a set of 4000, over

4000 articles, news articles, and they showed that machine learning,

supervised learning.

Perform better than automated dictionary methods and you can see

it on two different metrics.

So this is accuracy and precision, which I'm gonna talk

about in more detail a bit later, but these are

two ways of measuring the performance of a classifier, how

good a classifier is.

And here you see this is machine learning at the

top.

So higher accuracy, higher precision than three different dictionary methods.

Um, so, uh, some sense like if you want to

do classifications in known categories like this is the direction

you should go, not automated dictionary methods.

OK.

So I want to back up before we continue, in

case it wasn't apparent last week, I want to like

bring the big picture here is that um when we

want to classify.

Text into known categories, a predefined set of categories.

What we're engaged in is a measurement, um, a measurement

task, right?

So we're really trying to, uh, uh, measure something important

about a corpus that we're, we're studying.

It's not necessarily analysis per se, it's really just measurement,

OK?

So it's really important because a lot of times.

You need to measure variables that you then do analysis

on later, right?

So I'm not saying it's not important, but let's keep

in the back of our mind that like what we're

doing today is really about measurement, OK?

So the typical use case for this supervised learning or

machine learning, whatever we're gonna call it, um, classification and

doing categories if you start with a large collection of

documents that you want to partition into pre-defined categories, OK.

And then you have some of those documents in the

corpus are already labelled, meaning that they've already been allocated

into one of the predefined categories, but many other of

the documents are unlabeled, right?

So the whole reason you would want to do classification

is that you have a whole bunch of documents.

That are unlabeled and you want to measure the label

of those documents, but you physically as a human cannot

do it yourself.

If you have like 20,000 documents, you can't read them

all yourself.

So what you're going to do is you're going to

read a small number of them, label it by hand,

and then use fancy machine learning methods to then label

all the rest of them.

OK?

So that's like the typical use case for methods like

the ones we're going to talk about today.

Um, we call this supervised learning, and why do we

call this supervised learning?

The idea behind the phrase is that the method is

being the method of learning that's happening in the computer,

right, is being supervised by the existing, existing labels, right?

So you start with some smaller number of documents that

you have already labelled, either you labelled them yourself or

they, they already came pre-labeled.

Those labels are supervising the learning process that the computer's

engaged in.

And so that's what we call supervised learning and um.

Uh Yeah, the, the core thing that you're doing with

supervised learning is you're taking your already labelled documents using

machine learning to build a model of the relationship between

the features of the documents and the categories, and then

you take that model and you apply it to all

the unlabeled, um, the unlabeled documents and then that model

will give you predictions.

Now, of course, the predictions could be better or worse.

We're gonna talk a lot about that, but it will

give you predictions about what categories the unlabeled documents are

in.

OK.

So, um, quickly, I just want to say we're gonna

talk about supervised methods today.

Supervised methods are used to categorise documents into, uh, known

categories.

You could all, so what if you don't actually know

the categories?

You know you want to.

divide up a corpus of documents into categories.

Like you know that there must be patterns in the

data where certain documents are more related to each other

than other documents, but you actually don't know what the

categories are.

You don't have an ex ante like idea about what

they are.

Then you can use unsupervised methods and that's kind of

like clustering, right?

And so in the context of unsupervised learning methods, what's

going on is the machine is learning patterns in the

data, but without the structure of the, of the predefined

categories, right?

So it's not being supervised by the existence of these

pre-defined.

It's sort of just figuring out based on inherent patterns

in the data, which documents go together.

You tell it how many categories you want it to

create, and then it creates categories for you based on

the patterns in the data.

And then you got to go look at the documents

that are in the categories and try to figure out

what those categories are at a substantive level.

We're gonna talk more about that later in the next

few weeks actually, we're gonna talk about unsupervised methods, um.

But to be clear, unsupervised methods are really about discovery

more than they are about measurement.

Supervised methods that we're gonna talk about today about measurement.

You're trying to measure something in the corpus.

Unsupervised methods are really much more about discovery.

You're trying to discover patterns in the data that.

Can help you learn something more substantive.

OK.

Now, at a technical level, a useful way to, to

think, to remember the difference between supervised and unsupervised methods

is that supervised methods have outcome variables and unsupervised methods

don't.

That's the way I think about it, right?

So like, In the context of supervised methods, what we're

doing here is we're trying to classify documents into categories.

The outcome variable of interest is the categories.

And so we're, if you want to think about it

this way, we're, we're running a regression where we're taking

all the features of the, the variables and trying to

predict the category.

So that's supervised method.

It has an outcome variable that's that you're attempting to

predict.

Unsupervised methods don't have that, right?

You just have X variables and the machine is learning

about patterns in the data from the X variables.

OK.

Now, I'm gonna skip that.

uh, so that was about scaling versus classification, but we're

going to talk about scaling next week, so I'll just

defer that conversation until next week.

Um, but actually one thing I want to say about

it is today that the thing about the goal we

have in mind is we're trying, we have a corpus

of documents and we're trying to classify the documents into

pre-defined.

Categories, discrete categories, right?

So it's like category of two different categories we're trying

to classify documents into those.

You might also think about classifying, classifying documents on a

continuous scale at scaling, we're going to get to that,

um, but here we're thinking about trying to classify documents

into into discrete categories, OK.

Now, some issues we need to cover today.

So suppose we have a corpus containing documents that we

want to classify.

That's where we're starting.

How do we create or find a set of label

documents to do the learning, OK.

2, how do we do the classification?

So like, what is, what are the technical details of

the machine learning method?

And 3, how do we validate whether our classifier worked

well or not?

Those are the three issues we're gonna cover.

OK, we're not gonna cover them in this order, we're

gonna go, we're gonna put this first, this second, and

this, I think.

OK, I remember how I.

Every Yeah, no, that's, I'm right.

OK.

So we're gonna do this first, this 2nd, this 3rd.

OK, cool.

So, um, this is the basic workflow that you would

engage in to to do to use supervised methods to

do classification.

You gather your documents, you create a training set, meaning

you take some uh random sample from your collection of

documents and then you label them, create the labelled documents.

You pre-process all your documents, you know, do the usual

thing, tokenize, bag of words, all that stuff, transform it

into a document feature matrix, choose a classifier and specify

the parameters of that classifier.

We'll talk you'll see later on what I mean by

this, but for example, choose are you going to run

a regression or are you going to do some fancy,

some fancier machine learning algorithm, whatever.

Um, this is the way we talk about this is

that you're choosing the model that you're going to estimate.

I, I, I, I personally don't like the use of

the word model to describe classifiers or machine learning algorithms

because it's a little bit ambiguous, but That's what people

say.

So choosing the classifier is like we're choosing our model,

OK.

Then you train the model.

What that means is you're estimating the model and you

generate an estimated model, right?

So this is a theoretical model that you're specifying, then

you take it to your data and you estimate the

actual estimated model.

Using your data, then you take that estimated model and

you do prediction on the unlabeled documents to create, to

classify them.

Um, that's part 8, but before you get there, you

should check that your, your model, the model that you

train is actually a good model, right?

And there's ways to do that we'll get to in

a minute.

Now, how do you create a labelled set, uh, and

how do you evaluate its reliability?

I don't have a tonne to say about this, except

read the chapter in, in, in Grimmer, Roberts and Stewart,

um, because actually this is, this is like, you know,

the whole course, I, we get a whole course on

this.

There are two textbooks in the readings that you can

read about this.

Issue of how to create the the hand labelled, uh,

documents.

There's a lot of stuff to know.

So this is important stuff, but it's so important that

I'm actually not going to spend a tonne of time

on it because I, I can't.

You just got to, like, if this is something that

you're gonna do, you're just gonna have to sit down

and, and sort of read some of the details.

But I can say, I can say some broad things,

all right?

So first, we can ask, where do we get label

documents from?

Well, we have external sources of label labels, right?

So we could have, you know, a set of tweets

that were posted by political candidates and we might know

those political candidates a political party because we already know

them, right?

And so we, we can associate each tweet to a

party.

And then, but maybe say we have down the road,

we have a bunch of tweets and, and for whatever

reason, the author of the tweets has gotten lost.

And so we don't actually know who authored the tweets,

but we, we're not really, we don't care about who

authored it per se, but we want to know like,

was this tweet issued by a left-wing person or a

right-wing person or whatever.

Maybe we train a model on the set of tweets

where we did know the partisanship of the tweet author,

and then we apply that to another set of documents

we don't know the authorship of the tweets.

So this is an external source of label, like we,

we already, the label is just given to us by

the world.

You can also use expert labels, right?

So you can have, there are like the um canonical

data set from the comparative manifesto project is, it's a

project of, of party manifestos and there's a bunch of

expert coding of those manifestos, their ideological content and stuff

like that.

Um you can also train a bunch of undergraduate research

assistants to Just read a whole bunch of documents and

then classify them based on uh procedure that you give

them, right?

So you train them and how to identify different themes

or, or, or aspects of documents that you want to

classify and then you sort of let them do it,

right?

And then you can also, uh, uh, do what's known

as crowdsource labelling, where you use, there's these online platforms

like Mechanical Turk where you can like show documents to

random workers on the internet and then have them classify,

oh, this is a left-wing tweet or a right wing

tweet or this is, you know, this, this, uh, paragraph.

is, you know, evokes imagery around Law and Order or

doesn't, right?

So you can actually have a bunch of online workers

just look at a bunch of the documents and label

them for yourself.

Now obviously, if you're, you're gonna use online workers, like

you've got to give them really simple things like intuitive

things that randos can do.

If you have something more complicated, there's like really deep

concepts in the documents that you want to label, you

need to have a a complicated procedure with expert labelling

and a codebook and a whole thought out process.

Um, and in our class, when I'm showing you stuff

here, we're mostly gonna be working with this kind of

data, data where we already have labels, um, and we're

just gonna try to build classifiers to, to predict those

labels.

OK.

Um, so you can evaluate the quality.

So if you, you know, once you get a label

set together, you can evaluate the quality in many ways.

Here are some things that you should consider based on

this textbook by Nuendorf, um, uh, when you're evaluating the

quality of your, um, of your labelled set.

I don't want to get into this in too much

detail, but I want to focus on is when you

create a labelled set yourself.

It should be a random sample of the larger set

of documents that you're hoping to classify.

Why?

Well, because you want the smaller set of documents that

you actually label to be representative of the whole corpus

of documents.

If the smaller set of documents.

That you are labelling is not a random sample of

your larger set of documents.

It's not representative and so you're gonna label it and

then it's not gonna perform well in the class in

the prediction path because it's your training data on an

unrepresentative sub-sample of the data.

So, you know, First things first, when you sit down

to make your labelled, your labelled set, like, make sure

it's a random sample of the larger set of documents

that you eventually want to classify.

Otherwise you're just wasting your time.

OK, cool.

Now, um, Ah If you want to hand code a

label set.

Preferably, what you want to do is you want to

have multiple people can code each of the documents in

your labelled set.

Why?

Well, because, you know, people are imperfect and people disagree

and you get tired and you're, you're at, if you,

if you, if each document is only being seen by

one person, there's a lot higher chance that there's gonna

be just like a lot of error and noise in

the labelling.

So you also potentially might want to evaluate whether the

labelling task itself is impossible, right?

So if you, if you design some codebook to try

to label a set of documents into some categories that

you've decided, and then you have a bunch of people

start doing the hand labelling and you realise that the

people who are doing the hand labelling are disagreeing a

lot.

You might have to go back to the drawing board

and be like, OK, maybe I'm asking them to do

something that's not well thought through or conceptually kind of

vague or leaving too much room for interpretation.

And so, You want to have multiple people labelling the

same document just so that you can have a gut

check on how good the labelling is.

Also so that you can iterate and make your labelling

process better, maybe you make your codebook better, but also

just so you get a sense of how well your

labelers are, are, are working on the task.

Now that turns out, uh, there are statistical measures of

how much disagreement there are between uh hand coders.

This is called measuring in.

reliability or intracoder reliability.

It's kind of boring.

I don't want to dwell on it.

There are these different measures that you can measure, OK.

The most common ones are Cohen's Kappa and Cryffindor's alpha.

These are two measures of interrator reliability, where if you

get a 1, that means that the coders who are

coding documents are in perfect agreement.

Um, and as you go towards zero, there, there's more

and more disagreement and actually you can get negative numbers

where there's like.

There's like purposeful disagreements, right?

Like 0 is like they're uncorrelated, they're unrelated.

They're just kind of like guessing, and then you get

negative numbers and you're like, oh, they're like they're actively

disagreeing about stuff.

So, so there, there are these measures you can use

to evaluate the quality of your hand labelled set and

one of the things you would evaluate is how much

agreement there are between the coders who are coding the

the data.

OK, cool.

That's all I have to say about labelling, uh, handling

labelling a set.

OK, so now I want to talk about the performance

metrics of classifiers.

OK.

So our task here is to classify documents into a

known set of categories.

So we're gonna use classifiers to do that, statistical procedures

to do that.

But before we go down the path of talking about

the details of those statistical procedures, I want to like

think big picture, like, OK, let's, like at the end

at the end of getting classifications, how would we know

if our classifications were good or not?

OK.

So we're gonna talk about the performance metrics that are

typically used to evaluate that, um.

So, um, classifiers are built using label data.

So it's data where we already know the categories, we

talked about in the first part of this lecture.

We refer to those labels in the labelled set as

the truth or the gold standard.

Now, I, I put these in quotes because I just

spent a, you know, some minutes talking about how Creating

labelled sets involves a lot of consideration around the quality

of the labelled sets.

And so it's like a little bit awkward to then

just switch and say, OK, we're going to call it

the truth.

Like whatever labels we have in the labelled set is

the truth, even though I was kind of implicitly acknowledging

above that like, even when you're hand labelling stuff, people

disagree and like maybe the quality of your hand label

set is not that is not that high.

OK, but forget all that stuff for now.

Right now, let's just assume that our labelled set is

the truth, OK?

And, and, and maybe truth is too strong a word,

maybe we just want to call it the gold standard,

right?

This is the best we can do.

We have a hand labelled set, maybe it's not the

truth in some, you know, metaphysical sense, but it's like

it is the gold standard.

It is like the, it is like the best we

can do to create a baseline expectation about how the

documents should be capized, OK?

So the idea of of evaluating the performance of a

classifier is that we want to compare.

The truth, or the gold standard against what our classifier

is telling us the classifications are.

So we use the classifier to build the model inside

this labelled set, which we refer to as the training

set usually.

And and what's nice about the training set is it's

the set that includes all the label documents and so

the classifier is gonna give us a prediction of the

label for each document.

We also know what the label was and so we

can check to see, OK, so all the ones where

like where the the the truth is that the document

is in this category, how many times did the classifier

actually put those documents into that category and how many

times did it make a mistake, right?

So we can do that kind of evaluation.

So, to build ideas, I'm gonna walk through a simple

example and just show you how we would evaluate performance

for a hypothetical classifier that Uh, has 3 classes, meaning

that there's 3 different categories.

I'm going to use the word class over and over

and over again.

What I mean by that is category, right?

So if we have a set of predefined categories, each

of those categories could be referred to as a class.

Why I might, so class and categories, those are synonyms

in the context of this.

Class course, um, but I use the word class a

lot because we're talking about classification.

So it's like it's like the process of figuring out

which class the document is in, but we can also

say category if you want to.

So I'm just going to say classes most of the

time, but sometimes I'll say category.

OK, 3 classes A, B, and C.

There's a 100 labelled documents.

And those are the documents we're going to use the

trainer classifier.

So we know the class of those documents because they

were pre-labeled and um the classifier predicts for each of

those 100 which class the document should be in based

on its, you know, whatever math stuff that we'll talk

about later.

And so what we can get from that process is

what's called the confusion matrix, and the confusion matrix is

a, it's a, it's a 2 by 2 table where

each row represents a true class, and each column represents

a predicted class, and each cell is a count of

documents falling into uh a category of the true class

and a category of the predicted class.

OK.

So, for example, in each of these rows here, We

have all of the documents truly in Class A, truly

in Class B, truly in Class C, based on our

labels that we already know in advance.

So there are, and you can sum across the road,

there are 50 documents that are actually in Class A.

There are 39 documents that are actually in Class B,

and there are 31 documents that are actually in Class

C.

Now our classifier classifies documents into A, B, and C.

So these are the documents the classifier classified into into

Class A.

These are the documents that classify are classified into Class

B, and these are the documents of classifier classified in

Class C.

So, the classifier classified 49 documents into Class A, 41

documents into Class B, and 30 documents into Class C.

Now, So, of course, there's a typo here, another one,

the one I didn't catch.

This is 100 label documents.

We actually have 120.

Sorry about that.

So the previous line should say 120, not 100.

So oftentimes you'll see these confusion matrices like this, where

there's row sums and column sums and then the total

number of documents in the whole thing that are sort

of floating outside of the table, OK?

Those those row sums are actually important because this is

the total number of documents that are actually in Class

A.

This is the total number of documents that are classified

to be in Class A by the classifier.

Now you can see right away these are Different numbers.

So the classifier is not actually classifying all the documents

that are in Class A into Class A.

But what's neat about the confusion, confusion matrix, and I've

highlighted it here, is that along the diagonal, you have

all the documents that were accurately classified.

So these are the documents that are truly in Class

A and were predicted to be in Class A by

a classifier for Class B and for Class C.

This, for example, just taking this other example, is that

these are the documents that are truly in Class B,

but that the classifier class classified into Class C.

So if the classifier is perfect, it does a perfect

job, you should see all positive numbers greater than 0

along the diagonal and all zeros off the diagonal.

That would be a perfect classifier.

That doesn't exist in the real world, so you're never

going to see a confusion matrix matrix that looks like

that with real data.

OK, so starting with this, you can already see from

this matrix, you get a general sense of how well

the classifier is just by looking at it, but you

can use this matrix to calculate some statistics that that

are commonly used to evaluate the performance of a classifier.

So there's 3 particular to keep in mind.

One is accuracy.

This is a simple one.

It's just like, how many of the documents that uh

are in your confusion matrix will collect correctly classified, OK?

What percentage were correctly classified by a classifier?

There's two other statistics that are a little bit more

complicated, and each of these statistics is defined for each

of the classes separately, OK?

So for a class K, little K, we'll just use

a little K for a generic class.

We can define the precision for that class as follows.

What is the proportion of observations classified into Class A

by the classifier that are truly in that class?

Think about it this way, how precise is the classifier

for classifying Class K?

Meaning that of all the ones that classified as Class

A or Class K, which, how many of them were

actually in Class A, that they precisely classified.

Recall for a class K is as follows.

What proportion of the observations that are truly in class

K were classified correctly as being in Class K by

the classifier?

So this you can think of like how good a

recall does a classifier have in the sense that like

there are, there's a truth out there and how good

is a classifier at like recalling what the truth is.

Now, here are the formulas for it, but let me

skip to the, uh, oh, I'm just going to say

this.

There's also this thing called F1, which is a way

of taking, it's a kind of mean of precision and

recall.

So for a particular class K, you can calculate the

F1 score, which is a kind of like, you know.

Yeah, like getting the best of both worlds of precision

and recall, um.

Whatever.

We're mostly going to talk about precision and recall.

I just wanted to put it up here because you're

going to see this, but F1 is nothing special.

It's just like it's a way of like averaging together

precision and recall for, OK.

So let's go back to confusion matrix.

So the accuracy of this classifier is pretty easy to

calculate.

You just take all of the numbers in the diagonal.

So these are all the correct classifications and divide by

the total number of documents in your confusion matrix.

Total number of documents that were classified.

So, so, uh, the accuracy of this, um, classifier is

0.625.

If you want to interpret this in in kind of

probability-ish terms, you can think about it as the classifier

has like a 62.5% chance of getting, uh, getting the

correct classification, OK.

You can also calculate the precision for each of the

classes, so let's do it for class A.

So the precision for class A takes the total number

of of documents correctly classified as A, that's gonna be

in this cell here and you divide it by the

total number of documents that were classified uh into A.

So that would be um.

This row here, so 35 + 6 + 8, 35

+ 6 + 8.

So these are all the documents that classified classified into

A.

This is the number of them that were accurately classified

into class A.

OK.

So you can also calculate recall, which is the total

number that we correctly classified in de A.

So we're calculating recall for Class A, divided by the

total number that are truly in Class A.

So the total number that are truly in Class A

would be 35 plus 10 + 5.

That goes into.

So you can calculate.

Precision and recall for each class and if you, if

you notice the pattern, precision.

Take the true number divided by the column, recall, take

the true number divided by the row.

The sum of the room, the sum of the problem.

OK.

You can also do the same thing.

You can calculate precision and recall for Class B and

Class C, exact same process.

Precision and recall are defined for a specific class.

Now, Which performance metric do I use?

We got accuracy, precision, recall, and then we got this

F1 thing.

Let's ignore that.

F1, fine.

What do I want to, what do I care about

when I'm actually trying to evaluate my classifier?

You might think, well, accuracy seems the simplest, like, what

is the percentage of documents that were correctly classified?

Well, it turns out that it's more complicated than that.

Deciding how to evaluate your classifier sort of depends on

what you're trying to do.

Your task, this is like the theme of this class,

like the task you're trying to do, like plays a

role in you figuring out which.

Performance metrics you care about.

OK, so there's this, uh, it's very common example.

It's they talk about it in the book, um, but

this you can read about this online too if you

want to, if you Google this, you'll see this is

the classic example of, of thinking about the difference between

the various performance metrics, spam philtres, OK, so, you know,

spam sucks, everybody hates getting junk mail on their email,

um.

So you want, say you wanted to design, you're like

working for an email company and you want to design

a classifier that will classify each message as either being

spam or not spam, and then the classifier, you know,

that classification will then determine whether the email message goes

into the junk mailbox or not, OK?

So, um, uh, if you want to make sure that

the messages sent into the junk folder are actually spam.

Meaning that if you want to make sure that the

classifier, when it classifies spam, or not spam, is uh

uh doing a really good job of making sure that

it's only classifying spam, you care about precision.

If you want to make sure that all spam ends

up in the junk folder, it's recalled, right?

So there's two different things you might care about, right?

You might care about, let's make sure that anything that

spam ends up in the junk mail folder because spam

sucks.

You also might care about, I don't want there to

be stuff in the junk folder that's not spam because

then the person's going to miss that important email.

Probably most email providers care about precision more than recall

in the context of a spam philtre because it's more,

I mean, again, this is up to the, this is

up to the email provider to decide which thing they

care about, right?

OK.

It depends on profit incentives of the email provider, whatever,

but probably if you're an email provider, you're going to

get more angry.

Customer service calls from people who have tonnes of not

spam going into their junk folder because it's gonna piss

them off because they're gonna realise they're they're like missing

a lot of important emails.

They're probably going to be more annoyed with that than

they are if, you know, some spam ends up into

their inbox.

So, if you want to optimise on them getting no

spam into the inbox, you want to care a lot

about recall.

If you want to optimise on things that are not

spam, never going to the junk mail folder, you want

to optimise on the precision.

And different performance metrics give you different information about the

quality of your classifier.

And one thing to keep in mind is accuracy, the

simplest of this, like what is the proportion of the

documents that were correctly classified is often misleading.

I'll show you two examples.

So here is a confusion matrix for a hypothetical spam

philtre.

OK.

So this is a classifier that classifies uh emails into

not spam or spam, not spam.

There's a true, a brown truth.

Some emails are not spam and some emails are spam.

So here's a, here's a, some cooked up numbers where

you can see that the accuracy is 92%.

OK, so 92% of the time this classifier is doing

a good job classifying as either spam or not spam.

But the recall for spam is 50% and the precision

for spam is 25%.

So to interpret this in more substantive language, this confusion

matrix is show that is generated from this particular classifier,

showing us that while 92% of messages are correctly classified

as either spam or not spam.

50% of 50% of spam messages are getting through the

philtre, and 75% of messages in the junk mail folder

aren't spam.

That's bad, but the accuracy statistic is really good.

What's going on here is there's an imbalance, like there's

a lot more not spam than spam and so when

you get this imbalance between classes, there's often a divergence

between accuracy and precision and recall.

That happens pretty commonly.

But you would in this context, if you're running an

email provider, like, you know, you're a data scientist for

an email provider and your boss wants to know how

well things are going on the spam detection front, if

you tell your boss about accuracy, you're gonna give your

boss a misleading impression about the thing that your boss

probably cares about, which is, uh, precision, and really what's

going on here is 75% of the messages in the

junk mail holder are not junk mail, and that's probably

really bad for an email provider.

OK, here's another example.

Um, this is about illegal content.

So a lot of times social media companies will build

classifiers, like automated classifiers to classify whether or not content

is illegal or not, so child pornography or violent threats

or terrorism or stuff like that, right?

So every email or every social media provider does want

to philtre out messages that or posts that um are.

That involve illegal content.

And so here's a cooked up example where you have

a classifier that that for each social media post, for

example, is uh is classifying that the content is either

legal or illegal, and we have a ground truth as

well.

The content is either legal or illegal, let's set aside

like philosophical questions about what's truly legal and what's not

legal.

OK, but let's just assume there is a ground truth.

We can if with these numbers we can calculate accuracy

of the whole classifier, we also calculate the recall for

illegal information and the precision for illegal information as follows.

So there's 86% accuracy here.

So this, this classifier is doing a pretty good job

at classifying stuff as either illegal or illegal in the

kind of in the overall sense, but the recall precision

scores are, are not great.

So let me just put these in um.

In substantive terms, so with these numbers, our classifier uh

has the following performance.

86% are accurately classified, 86% of social media posts are

accurately classified as either illegal or legal, but 66% of

illegal content is getting through the philtre and showing up

in the public feed.

And 50% of the stuff that's being blocked by the

illegal content philtre is actually not illegal content.

That's not great.

You got an 86% accuracy rate, but 2/3 of illegal

content is getting through the philtre, and half of the

stuff that's being blocked as illegal is actually not.

And if you are online at all on social media,

you hear people complaining all the time about having their

accounts suspended for like really mundane things, and it's because

they have automated processes that are deciding whether or not

an account violates terms of service and then, you know,

a human has to go ahead and fix it, but

they're clearly not always, the philtre is not always working

correctly.

And you can quantify that using accurately, accuracy recall and

precision.

Now, presumably when the company who builds this philtre, builds

the philtre, they have some labelled set, right?

They have some human labelled set that they're building their

classifier on, and then they can calculate the the performance

metrics based on that.

OK.

I was OK, this is like a little bit annoying

thing.

There's some like vocabulary that I want to talk about

briefly because if you've ever heard about precision or recall,

you've probably heard it used slightly differently than the way

I'm using it here.

I'm using it more precisely, but let me tell you

about something that's uh important for you to keep in

mind as you're Googling things or talking to chat GPT

or whatever.

So binary classifications are really like common in the world,

like spam versus not spam, illegal content versus legal content,

you know, Republican speech versus Democratic speech.

So these are common classification casts.

So there's two categories and you're trying to choose the

classifier is trying to choose which category a particular document

falls into.

So we have some special but confusing terminology for just

such scenarios where we refer to the classes as positives

and negatives.

So here's the confusion matrix for a binary classification task,

and we're gonna refer to one of the categories is

positive and one of the categories is negative.

So let's say spam and not spam.

So positive meaning we're detecting spam, negative meaning we're not

detecting spam.

So this is the truth and this is the prediction.

And then in each cell, I have the phrase people

use for the count in that cell.

So the count in the cell where the true class

is positive and the predicted class is positive, those are

called true positives.

The count over here is true negatives, and then the

count here is false negatives and the count here as

false positives.

Why are these called false positives?

Because in reality, the truth is that they're negative, but

they're being classified as positive, so they're false positives.

Why are these false negatives?

Well, in reality, they're positive, but they're being classified as

negative, so they're false negatives.

We call this type 1 error and we call this

type 2 error.

Oh my God, this is annoying.

I hate this.

I'm just showing you this because this is, this is,

this is stuff people say.

And I think this is extremely confusing, and I have

been doing academic research forever and I have refused to

learn the difference between a type 1 and type 2

error, and I encourage you to refuse to learn the

difference as well because it doesn't matter, OK?

But in the world of classification, people tend to Binary

classifications, people tend to relabel their categories as positive and

negative and then they use all this terminology around false

positives and false negatives and blah blah blah and type

1 errors and type 2 errors.

The reason I find this confusing is like what's positive

and what's negative.

Like somebody decided to make spam positive and not spam

negative.

They didn't tell you that, and then all of a

sudden they show you a confusion matrix that says positive

and negative, and it's like, well, what's positive and what's

negative?

Which of the two categories do you consider to be

positive and which do you consider to be negative?

Somebody gotta tell you that, OK.

So I prefer being more precise and saying like, look,

I'm going to be clear about what categories I'm talking

about.

When I'm talking about the, the, the recall and the

precision of a particular category, I'm gonna be clear about

which category I'm talking about the precision and recall for.

When people do this trick of taking their binary classification

and turning it into positive and negative, they just talk

about precision and recall without any reference to class because

they assume they're they're implicitly assuming we're all talking about

the precision and recall of the positive class.

But you could also calculate the precision and the recall

of the negative class if you wanted to.

But in the terminology around binary classification, if you Google

stuff, you talk to chat GPT, they'll just be like,

oh, there's this concept of precision that you can calculate,

and they don't make mention of the fact that precision

is specific to a class, specific to a category, because

they are implicitly assuming you're thinking in binary terms, where

there's positive and negatives, and all you care about is

the the the positive class.

This, is this makes sense?

OK, I, I didn't want to spend a tonne of

time on this, but then I was like, you know,

if you Google it, you're just gonna, this is, you're

gonna see this.

This is gonna be like incredibly confusing thing about terminology

that um whatever.

I should talk about.

Any questions, concerns?

Again, don't worry about type 1, type 2 error.

It doesn't matter, who cares.

Now, uh, which is another way of saying I'm never

going to ask you that on an exam because I

think too and I would not ask you to memorise

something that I, uh, that I myself cannot keep apart,

you know, 15 years into that.

OK, so now I want to, we've talked about how

we would evaluate the performance of a classifier, um, using

a confusion matrix and these 3/4 uh common, um, performance

metrics.

But I want to now talk in more detail about

the classifiers themselves.

So it's kind of weird, we talked about evaluating classifiers

before we actually talked about the classifiers.

Um, so, Now we're gonna ask how do we do

classification, not how do we evaluate it, but how do

we do it?

OK.

So there are a lot of algorithms slash models slash

classifiers.

These are all like synonyms ish.

I mean, they have slightly different meanings and slightly different

connotations, but like for our purposes, let's just like think

of these as synonyms, algorithms, models, classifiers, and they're designed

for the task of classification.

So the, what, what happens with these classifiers like what

they're doing is they're taking a labelled set, we call

it a training set because it's the set of labelled

documents that the classifier is using to train as models,

to figure out its model, OK.

Um, and what it does is it for every document,

it will generate a probability that that document is in

class K, Class K, uh, 1, class 2, class 3,

class 4, right?

So it'll generate a separate probability.

For each class.

So each document gets a set of probabilities and that

set of probabilities will will tell you like what's the

probability this document belongs in class 1 versus class 2

versus class 3 versus 4.

On the same page?

OK.

What we actually eventually want to do is classify the

documents into the categories.

We don't just want to get the probabilities that they're

in the categories.

We want to know like, OK, which category does it

go into?

Those are two separate steps.

Step 1, the classifier gives you the probabilities.

And then step 2, you gotta make a choice about

how you use those probabilities to execute the classification.

So I'm gonna talk about them differently and um I'm

gonna talk about them through an example.

OK.

So suppose you have N documents, capital N, using the

same notation I've been using the whole time, each of

which you want to classify into one of capital K

classes.

So capital K is the total number of classes.

So if you had a binary classification task, K K

would be 2 because there's two classes.

Um, each document I's class is gonna be denoted by

little Y with an I subscript.

So this is the, the actual class that document I

belongs to.

That's just the variable I'm gonna use for that.

Documents with known that is already labelled classes are used

to train the classifier, training meaning like get it to

escalated model.

And then we're, what I'm gonna do, this is, I'm

just telling you this for for notation reasons.

Think about a class.

That you would have, so binary class, class with 3

classification task with 3 classes, whatever, we're going to turn

into w variable.

OK, so like, so if it's like Democrat, Republican, Democrat,

Republican, so every document is classified as either being Written

by a Democrat or written by a Republican.

I'm going to dummy it, so I'm gonna, I'm gonna

create a new 101 variable that's like, was this a

democrat or not.

So for a binary classification task, dummying out a class

variable would just entail turning it into a 01 variable.

But if you have multiple more than two classes, if

you had 3 classes, so it's like, you know, Class

A, Class B, Class C.

Dummying that, creating dummy vari variables around that would entail

creating two new variables.

So a variable for A, which is like 01, was

this in class A or not, a variable for B,

01 was this in class B or not.

And then the third, I mean, you could create a

third one C, but you never really use it, right?

Because it's implied that if A is 0 and B

is 0, it's implied that Z must be 1, right?

Because everything has to go in a class.

But what I want to do is I want to

think about classes in like dummy, dummy variable terms, 01,

you know, is it in this class or not?

So here's the, here's the um the notation I'm use

for that, i.

So bold ie with a little I is for a

document I.

It's, it's a 01 vector that specifies which class the

documents in.

There's only it's, it's a one hot encoding.

This is like the fancy word for like it's gonna

be all zeros, but 11, OK.

So each document is going to be in one category

only and whichever category it's in, there's going to be

a corresponding one in the vector of zeros and ones

and then all the other categories can be set to

zero.

So, um, if I'm trying to predict for a particular

document I, this pi subscript I subscript K, what I'm

doing is I'm trying to predict whether document I falls

into class K, and i subscript I subscript K is

going to be probability.

I mean, eventually when we estimate this, it's going to

be a probability.

The probability that document I is in class K.

All right, it's just notation.

So.

What do we want?

What, what's our goal?

All right.

So we have a document D subscript I.

We always using little eyes, little eyes to indicate specific

documents.

So we have a document D.

For each class K, what we want to know is

this.

What is the probability?

That document is in that class.

This is just the math way of writing, probability of

being in class K and conditional on being document I.

So if you have document I, what is the probability

that that document is in class K?

That's what this quantity is.

And then if we have, for example, 3 classes for

every document, we're gonna have 3 of these guys, 3

of these probabilities.

What's the probability of being class A?

What's the probability in being class 2 Class B?

What's the probability of being in class C?

So for each document, there's going to be a 3

probabilities, and they're gonna sum to 1, right?

Because probabilities all sum to 1.

Now, We can't observe this.

Well, I mean, we can observe this.

In our labelled set, we know what the, we know

what the labels are, right?

So we do know, we actually do know the probability

of being in a particular class.

Of a particular document being in a particular class because

we are looking at our label data and it already

tells us what class the documents in.

But remember, our goal is to eventually look at unlabeled

data and the unlabeled data, we're not gonna know what

the class is, so we're not gonna be able to

observe the probability that a particular document uh uh is

in a particular class.

We're not gonna be able to observe this in the

unlabeled data, but in the label data, we can observe

this or we can observe it through some asterisks, OK?

So, What we're gonna do is we're gonna use Bayes'

theorem to rewrite this.

So this, the probability of being in class K conditional

on being document I is equal to the probability of

being class K times the probability of getting the document

conditional on being in this class and divided by the

probability of getting that document.

OK, so this is just based here as you can

find it on Wikipedia, it's just, it's like a formula,

right?

You should remember it though because it's important.

All right, so again, this is the thing we wanna

know.

For every document, we wanna know what's the possibility of

being in a class, given that you have that document.

This is the thing we're ultimately wanting to estimate because

once we estimate this thing, we could take this thing

and then we could use it on on label data.

We can observe this, but, so we're gonna transform it

using data rule into this.

We can observe these things, we can observe, we have

label data, so we, we can observe uh conditional on

the label like what is the possibility of getting a

particular document.

We do observe the total number of the uh documents

that are in that class and we do observe, you

know, the the basic statistics about how long the documents

are.

OK.

This denominator here is a scaling factor, so we're gonna

drop it.

OK, it's a scaling factor.

It doesn't matter for ultimate calculations.

So I'm going to drop it and I'm going to

write this probability, the thing we're interested in, this is

what we're interested in probability, give it a, what is

the probability that it's in class K.

That's what classification is, figuring this out, OK?

This is the magic thing we want to know.

I just I keep going back to it because I

want every time you see this, I want you to

think, oh, the whole point of classification is figuring this

out.

And all this, all the rest of this math is

like us doing tricks so that we can figure this

out.

OK.

So I'm, I'm telling you what the tricks are now.

So the first trick is you apply base rule, and

then you can rearrange this, this into this.

The second trick is you drop the denominator because it's

a scaling factor.

It doesn't, it won't end up mattering for our ultimate

calculations.

You don't need to talk about why.

Don't worry about it.

It doesn't matter with the calculations.

If you want to talk about it more later, we

can in office hours.

So I'm gonna drop this and then I, I'm just

left with what's in the numerator.

Now, technically, once I drop this thing, cause it's just

a scaling factor, this is no longer equal to that.

Because like literally the equal means that has to be

equal to that.

So if I take that away without doing anything this

side, these two things are no longer equal anymore, but

this is proportional to that, and this means proportional to.

Now, again, if you've never seen this before, don't worry

about it.

It doesn't mean anything.

Just think of it as an equal sign.

For what we're doing, think of it as an equal

sign.

But I'm trying to be precise.

It's technically not equal.

It's proportional to, but fine, but it doesn't matter.

It's not gonna matter for our calculations later on, so

you can just pretend it's an equal sign you look

at.

Now, Next trick, we're just doing tricks.

We're trying to figure this out.

We just keep doing tricks until we figure it out.

OK, next trick.

Oh, let me tell you what, OK, so this, I,

I neglected to talk about the things that were uh

races here.

So this thing that we're ultimately interested in, so we

want to be given a document and be able to

predict what it's class is.

That's what this is.

That's equal to equal to.

The class prevalence times the class-specific language model.

So this is, how prevalent is the class in your

data?

So how many documents are in that class, basically.

That's easy.

You can calculate that.

You can just look at your data set and like,

oh, how many documents are in this class?

Cool.

This is a little more complicated.

This is the class-specific language model.

So given a class K, given a class K, what

is the probability of getting that particular document?

No, that, that's a bigger deal, right?

We talked about language models last week.

Language models are ways of thinking about the data generating

process for real life data set, or real life uh

documents.

And we, and we're using this multinomial distribution assumption where

every document is a draw from, from multinomial distribution.

This kind of, you know, This is the model that

we talked about last week.

Here, I'm not giving you any specific model.

I'm just saying I want that probability, but this probability

is the mathematical representation of the language model.

So we gotta actually, we can't just work with this

probability.

It's too vague, it's too general.

We need to actually say, OK, well, what do we

think the model is?

What do we think the relationship between a class and

a document is, and I'm just gonna use the same

one I used last week, which is the multinomial distribution.

So we're gonna model each document as a draw for

multinomial distribution, but the key thing here is we're just

gonna assume that each class has a different multinomial distribution.

So if you're in Class A, documents are drawn from

Class A's multinomial distribution.

If you're in Class B, documents are drawn from Class

B's multinomial distribution and so on and so forth.

That of the slide is is.

Talking about.

You saw this last week, you're seeing it again.

Our model of language for the time being is we're

gonna assume that conditional on knowing what the class of

a bit of a document is.

The documents tokens, the words in the document, the word

counts in the documents because we we're working with document

feature matrix matrices which represent documents as word counts are

drawn from a multinomial distribution with two parameters, MI being

the length of the documents and new subk being.

A vector that specifies the probability that each of the

particular types in the vocabulary would arise under this distribution.

OK.

So, there is a full vocabulary available and new sub

K is telling you what probability to place on each

of the words in the vocabulary.

And new sub-K can be different depending on whether you're

in Class A or Class B or Class C.

So the word, the words used in Class A could

be different, or they could, they could be used with

different probabilities than in Class B than in Class C.

That's how we're gonna model language in our context.

Essentially, once you know the class that, once you know

the class that the documents in, The language model says

there's some specified probabilities with which words arise under that

class.

And so, you know, the data generating process creates documents

that reflect the probabilities for that class.

So you might imagine like if you're interested in classifying

democratic versus Republican speech in the US context or more

generally left wing versus right-wing speech, you might imagine that

the, the model of speech for Democrats places a higher

probability on the word care arising or on billionaire arising,

right?

Because Democrats talk about that stuff more and the class.

For Republicans, the, the, the language model for Republicans probably

places a higher uh probability on the word law, order.

Make these days because Make America Great Again is something

that Republicans say all the time, right?

But you can imagine the language model for that class

generates different probabilities of words arising than the, than the

first class.

So that's how we're going to think about language.

In the context of classification.

So, all right, cool.

Once we do that, once we decide that the model

of language we're gonna use is that once you know

a class, Then it's just a multinomial distribution.

Documents are just draws from the multinomial distribution where the

The news are new probabilities are determined by class.

You can go to Wikipedia and you can figure out,

well, OK, with the multinomial distribution, what is the probability

of getting the particular uh document WI given that it's

in this class?

Well, this is the formula.

OK, or whatever.

Again, this is a scaling factor and we can drop

it, so I'm gonna drop it again.

So even though this is equal to that, this isn't

gonna matter for the calculations, so I'm gonna make life

simpler and just drop it.

And so now this thing is proportional to this thing.

Because I just brought this game.

But if you don't, if you, if this is throwing

me off, just think of this as an equal sign.

This is a trick we're using here because this won't

matter for the calculations later on, so we're just going

to ignore it.

Why make do more calculations when we don't need to.

And so, but we're just going to be mathematically precise.

And so I, I don't want to lose track of

the fact that I dropped this.

And so the way I don't lose track of that

fact is I keep the, keep this here to let

everyone know, like, oh, something was dropped, the scaling factor

was dropped earlier on, just warning like this, these two

things aren't really equal.

OK.

So what that means is that when you, if you

were to be able to calculate this thing here, you're

not literally gonna get the probability.

This probability, but if you were to take this thing

you calculated and then add that back in, then you

would get this probability.

But for our purposes, it's not gonna matter, so we're

not gonna worry about it.

OK.

Now, back up, what were we interested in?

We want to know, given a document DI, what is

the probability that document is in class K?

This is the magic thing we want.

This is what classfiers are trying to figure out.

We already saw that that is proportional to Class prevalence

times the class-specific language model, OK?

And then we said, this is gonna be our class-specific

language model, OK?

It's this multinomial thing.

So we're gonna take this out and substitute in the

classific the class-specific language model we're gonna assume, which is

the multinomial.

So we're substituting what we are, we just uh got

um for this.

Now, what is the, what is this?

This is the probability that a particular document is in

Class A.

Well, that's just But what's the proportion of documents that's

in class, OK, whatever.

I'm just gonna call that delta K.

The book calls it alpha K.

I'm sorry about that, but I'm gonna call it Delta

Delta K.

The book is inconsistent.

I'm not the inconsistent one, tell me.

So what's going on in the book.

OK.

In the book, earlier chapters, they use alpha for Laplace

smoothers, OK, so they're using it for regularisation.

And then all of a sudden you get to the

chapter on classification and now they're using alphas for the,

the class prevalence.

stupid.

So I am going to try to use more consistent

notation and I'm not gonna, I'm not gonna, I'm gonna

call this delta instead of alpha, all right.

So yeah, the book is using inconsistent notation between chapter

6 and chapter whatever this shows up in.

And I don't like that.

So, OK.

Now, cool, cool, so we got.

Doing all these tricks.

This is the thing we want to know about.

Again, just keep repeating myself.

If we have a document DI, what's the probability that's

in class K?

That's classification.

That's all it is, right?

This is the thing we want to know about.

We did a whole bunch of tricks, dropping scaling factors,

assuming a multinomial distribution, so on and so on and

so forth to get to this.

This is the thing we want to ask.

We did all our tricks to get down to something

we can deal with.

So this approach of estimating this thing to teach us

about the probability a particular document is in a particular

class, this is called the naive Bas approach.

Let's not talk too much about why it's called this,

but the naive part is important.

Well, we use Bay rules, that's why it's called Bay.

All right, fine, but that's easy.

The naive part is that we're assuming.

Here In our language model that tokens in each document.

are drawn independently once you conditional class.

words.

The reason why we're calling this naive phase is we're

using Bayes rule and we're using the bag of words

assumptions.

So we're, we're assuming word order doesn't matter and, um,

and that there's each token is an independent draw from

the multinomial distribution, but like we all speak languages and

we know that's not true, right?

Like whether I use particular words after I use the

particular word, uh.

I, well, I don't know where I'm I'm going to

finish that sentence, but we all know that when we're

speaking, we're not just speaking random words, right?

We're not just like every word I speak is not

just a random draw from a distribution.

But we're going to naively assume that that's the case

because it makes the math really easy and it turns

out, as I mentioned before, bag of works, it's just

been validated and it works, and so it's fine.

We're just going to do it.

OK, so naive phase, we're using naive phase because of

bag of words.

Now, Some caveats, the assumptions of the naive base model,

that this is the naive base model, that the thing

we're interested in classify the probability of being in a

class K, uh, given that you have a particular document

I.

This is equal to or proportional to this, this model

is wrong.

We already know what's wrong because this multinomial thing is

assuming conditional independence of words, bad words that we know

is not true, like it's not accurate in real life.

So the assumptions of nice nba are obviously wrong, but

so were bag of words.

We already talked about this ad nauseam, right?

This is, these are all like the models we're using

to model language are all wrong in some sense.

But the reason why we use wrong models is that

they're simpler to use mathematically, and if they work, they

work.

And that's what I said about bag of words and

that's what I'm gonna say about nighthase too, right?

If it works, if it turns out we use it

as a classifier and it does a good job classifying,

who cares if the the model's assumptions are wrong.

The proof is in the pudding, we'll see how well

the classifier performs.

There isn't, but there are.

When you use models to do things and the models

are so obviously wrong, you do have to like ask

yourself, OK, well, where, where could things break?

So GRS talks about one way in which naive bays,

these uh these these bad assumptions can break your classification

process.

Turns out the naba ignores because it ignores correlations between

words and documents, it can, it can yield incorrect estimates

of the total proportion of documents that are in a

class.

So that's your goal.

If you're not actually just trying to classify every document

into class, into a class, instead you're trying to do

something more like macro level, like you just want to

know what proportion of the documents fall in Class A

versus Cla B versus Class C.

You're not going to get great estimates from make base,

and that it can be traced back to the, the,

the assumptions.

But it still works for classifying individual documents.

That's what you want to do, then cool, you can,

you know, you can use it.

You can also naive phase is a naive phase even

if you don't use a multinomial distribution, that's the specific

distribution we're gonna use because we need a formula to

calculate stuff, but you can use different distributions that also

assume conditional independence.

What those are, we won't talk about it.

Who cares?

We're gonna use multinomial distribution here.

But just gonna say like naive Bay is a broader

category than the specific naive Bay model that we're gonna

estimate here.

This is a specific version of it, but there are

other.

As long as the language model involves an assumption of

conditional independence, then you're good.

It's a nice base.

OK, cool.

OK, so remember, this is what we're interested in estimating.

This is the, this is classification.

What's the probability document is in class K, uh, given

that you have the, the document that's gonna be equal

to this thing.

All of this stuff can be estimated.

That could be estimated, that could be estimated, and well,

we know the word counts.

This is the word counts.

From our document feature matrix.

That's what this is.

So this is the word count in row I, uh,

column.

So in that particular cell, this is the word count.

These two things we can estimate.

With data How do we do it?

Here's some formulas, but let me tell you what you

do.

To estimate the delta thing, you just calculate what percentage

of the documents in your sample are in that particular

class, because remember, we're doing training on a set of

documents already labelled, so we already know the labels, OK?

So you just say like what percentage of the documents

are in a particular class, that gives you an estimate

of delta K.

Hats mean estimates.

No hat.

This is just a theoretical model.

Hat means, OK, this is we're going to actually do

the calculation and estimate it with our DS.

Now you can also estimate the news, you have to

estimate a lot of them.

So there's a, remember in a language model that uh

the form that we're looking at, each new is telling

you the probability, each new is associated with a particular

type token vocabulary, and that's telling you the probability that

that word would arise, OK.

Now, we want to allow for the possibility that those

probabilities differ depending on which class the document is in.

So each class has its own language model.

What does that mean?

So For every class K and every word in the

vocabulary J, there is a particular new hat.

So we could just go one by one by one

and calculate them all.

So if you have a vocabulary of like 1000 words,

you have a document feature matrix that has 1000 columns.

You're gonna have 1000 of these.

These little guys Uh, times the number of classes you

have, because there's gonna be 1000 of them for each

class.

And each of those 1000 views is going to give

you a probability of that particular word arising in that

particular class.

How do you calculate it?

Well, within a class, I like, well, you can look

at the math that's that's your thing, but let me

tell you intuitively, how do you calculate it?

Well, you look at all the documents inside the class

K.

And then you count the number of, so you.

Creating a new for every token.

So for that particular token, you count the number of

times that token arises in that class.

Divided by the total number of tokens used.

In that class as a whole, like all the tokens

that are J plus all the other ones too.

So it's like, think about it like if, if, if

m is supposed to be a probability, we're estimating the

new with our data by just like, I don't know

like, what is the purport, like how often did this

word get used in this class?

That's the estimate of the probability.

Now, for reasons we talked about last time, you probably

want to add, you want to regularise, add a little

plot smoother.

This is like mathematical details, right?

But in document each matrix, you get a lot of

zeros.

zeros messing up, mess things up.

OK.

So, a lot of documents, never use certain words in

the vocabulary, but when you include those zeros, you get

lots of news that are estimated to be zero.

And then you get weird things happening in your Calculations.

OK, so we're going to always get rid of that

problem by basically, so we're adding a Laplace smoother, so

we're adding alpha, this is why I didn't use alpha,

OK.

So we're adding a little bit to the numerator and

then J times a little bit to the denominator.

Practically speaking, what does this look like?

If you were like working with a data set in

R, add a 1 to every cell and you're not

gonna be your matrix.

That's it.

I mean, that's assuming half is equal to 1, then

that's usually what people choose.

So you're, you're, you're ensuring that every single document every

single document has every word occurring at least once.

It's obviously not true, but that little bit of difference

isn't gonna make a huge difference in terms of the

actual calculations because you're uniformly doing it across all the

documents, so it's not gonna change any of the informational

content of the, the, the data frame you're looking at.

It's just gonna get rid of the corner solutions, the

zeros that create mathematical problems.

OK.

So now, great.

Again, this is the this is the thing we wanna

know.

What's the probability that a given document is in a

class K?

Now, here, it's an estimate, we're estimating it.

Because we're using estimates now from our data set, and

the other change I made is I, I, we're not

representing documents as these document feature matrices, so the document

DI is now just represented as a word count vector

because that's what we do to to represent documents mathematically

is we just turn them into to a DFM and

then each document is just like it's like a row

that has like word count.

So, um, OK, so I can now estimate for any

document, I can calculate all this stuff.

I do it by hand, you can't, you should be

able to do it by hand.

I mean, it's, it's If you have a document feature

matrix that has 1000 columns, you can do it by

hand, but you have to do, let's say you have

1000 columns in 3 classes, you have to do 3000

calculations.

OK, so obviously you can't really do that by hand,

like you want to use a computer to do that.

But like if you had a small document feature matrix

that had 3 columns.

So 3 words.

You can calculate, you can calculate all this stuff.

You do buy it.

And seminar exercises you're gonna do it by hand.

OK.

So I'm gonna skip over this because actually in the

seminar, you're gonna do some, you're gonna kind of do

this by hand and exercises by hand, meaning like with

R but whatever, you'll see the logic.

You, you won't just use some function, right?

You'll just, you'll calculate by hand.

But I want to show you that if, if I

wanted to, so if I had this um this DFM

here where I have The documents are inaugural speeches from

200 on, so 2001, 2005, 2009, 2013, 2017, 2021.

So these are the six inaugural speeches that have happened

since uh 2000.

This is a corpus that's built in.

And I create a DFM that's a small DFM that

just has three tokens in it, America, uh, us, and

nation.

OK, so very simple problem.

I can try to clap you could build a naive

base classifier that classifies whether a speech was written by

Trump or was spoken by Trump or not, OK?

So using the language of these speeches, I could build

this naive-based model and then do a classification task.

And the way I could do it is I could

just calculate all the quantities and you can follow through

the slides to, to see how to do it.

But what I would end up with This is my

mom.

This is my mom.

This is the, this is the calculations I get.

So, these are the two classes.

This is the class prevalence.

So, you know, um, 16% of the 17% of the

documents were Trump documents and and 82% were this 1/6,

it's 5/6, because 1 out of the 6 was the

Trump speech and 5 out of the 6 were not

a Trump speech.

And then for each of the tokens, America, us, and

um nation, I can calculate the new, so the probability

that that word would arise in a not Trump speech

and in the Trump speech and they did the calculations

on the previous slides, but here you'll notice.

That in a not Trump speech, we have these three

probabilities.

So this is the language model for speeches that were

not Trump speeches.

There's about like roughly equal weight on each of these

words, right?

I mean, with a little bit more weight on their

word us than on these two words, but a Trump

speech is more distinctive.

You'll see here that the, the model that we estimated.

Says that a Trump speech uh has a 54% chance

of drawing the word America.

Makes sense.

I mean, his whole campaign is built around Make America

Great Again, which is like a very.

If he just repeats his own slogan over and over

again, it's gonna, the word America is gonna appear more

often, right?

I mean, uh, and then us, like us collective, the

collective pronoun appears a lot less for Trump than it

does for the others.

And then there's not a huge difference between the way

that the others and Trump used the word nation.

But these, these are, these are just the estimated numbers

based on the data I can feature matrix that I

calculate by hand.

So then once I have this, these numbers, I can

for every particular document, so this is, this is the

2001 Bush speech.

I can go back up to my, my formula for

what's the probability of being in a class given a

particular document, and I can plug in all these muses

and the word counts, and I can calculate the probability

that this document is a Trump speech and the probability

that this document is not Trump speech.

That number is bigger than that number.

I mean these are very, very small decimals, it's written

in scientific notation, but this is a bigger number than

that.

So this classifier has said, once I did the calculation,

that the probability that George W.

Bush's 2001 speech was a Trump speech is lower than

the probability that George W.

Bush's 2001 speech was a not Trump speech.

Is that good?

Yes, that's good because it's, it's not a Trump speech,

it's a George W.

Bush speech.

So the class actually did classify that correctly.

Now it turns out I did the calculations for the

other ones and I think they're There might be one

that it misclassifies, I think, uh, the second Bush speech

it actually classifies the Trump speech.

If I remember correctly, um, but that's the thing, classifiers

are not perfect, right?

They don't.

All right, and there's another example you can look at

your on your own, on your own time.

OK, so, Naive phase is like one particular kind of

classifier you can use.

There are many others you can use.

Another one you could use is called regularised regression.

So, um, this is just regression.

Which you're all familiar with, you're supposed to be it's

correct for the class, but regression, what it does is

it, uh, minimises the residual sum of squares.

So if we remember this, right, so it, you know,

it's creating a model that generates predictions where the, the

gap between those predictions and the truth squared is minimised.

OK, that's, that's what regression does.

It's an algorithm that does that.

It's You know, I don't have to say a lot

more than that.

We all know regression, OK.

So the question is, can I run a regression?

Can I use regression as a classifier for my, for

my classification task?

So I have a whole bunch of features, so words

in my document feature matrix, and I have a bunch

of observations.

So can I just, can I like run a regression

and then I put all the words as the X

variables in my regression to predict the class, to predict

the category?

People use regression for prediction all the time, why not?

I mean, the new zeitgeist is like, why would you

use a regression when you do all the machine learning

stuff, but like, hey, what, like you can use a

regression, OK?

Now, the problem with trying to run a regression.

With a document feature matrix.

The first problem is that document feature matrices usually have

more columns than new rows.

You remember from your like intro regression class, that's illegal.

You can't do that.

You can't run a regression on a data frame that

has more columns than rows.

It's not full rank.

I mean, by definition, it's not full rank.

You can also have lack of full rank even when

you have fewer columns and rows, but you definitely cannot

have full rank if you have more columns than you

have rows.

Right, so just keep in mind, it's illegal to run

a regression on a data frame with more columns or

more x variables than uh data points that you have.

OK.

And most DFMs have that feature to them.

Can you actually run a regression on a DFM?

No.

Even if you had, like, suppose you had more documents

than you have features, right?

You like trimmed your document feature matrix in some way,

you got rid of a bunch of features, whatever.

OLS has pretty low bias and high variance, and so

it tends to overfit.

This is a problem.

We'll get to, we'll get to problem.

So what we do instead, and here's the math on

here, but let me just tell you intuitively what's going

on.

So what we do instead, if we want to run

a regression to try to do classification on documented feature

matrix, we got to deal with the fact that there's

probably too many columns, too many columns for us to

be able to estimate regression.

Let's get rid of a bunch of columns.

How do we do it?

Well, there's two ways to do it.

You run two different types of what are called regularised

regressions.

So one's called a rid regression, one's called a lasso

regression.

There are just two ways of penalising experience.

And so the easiest one to think about is a

lasso regression, because what it does, a lasso regression basically

just drops a bunch of variables from your regression.

And it does it through this complicated process that where

it estimates the best ones to drop.

OK, so like, let's not get into the details of

what exactly is happening.

But the conceptual reason why you might use a lasso

regression, if you're committed to using a regression to do

classification on a DFM is the lasso regression is going

to drop a bunch of the, the, the columns and

create a matrix that is full rank and that you

can estimate with.

Does it guarantee that?

Uh, I don't know.

And I can say one way or the other.

I, I I don't want to say.

I don't want to say because I'm going to say

something wrong.

I have an intuition, but I'm being recorded, so I

don't want to say something wrong.

OK.

Uh, so, what technically goes on in the math is

like this is the old regression formula for regular like

linear regression, and then it adds a penalty.

So regularised regression adds a penalty and then the two

different kinds of regularised regressions, ridge regression, lasso regression, add

different uh penalties with different functional forms.

This is a quadratic penalty.

This is a like half value, and the, the size

of the penalty is lambda, OK.

So what happens is, is usually when you estimate these,

these kinds of regressions, a two-step process, you estimate the

lambda and then you apply the lambda, you like put

it in as a, as a, as, as the number

and then you estimate the regression.

OK, so, um, and then there's this other thing called

elastic net regression that takes the, it, it basically, it's

a penalty that's like a weighted average of ridge and,

uh, la.

But the technical details of this are not so important.

What this penalty is doing is it's, it's, it's, it's

literally penalising.

Um, It's penalising coefficients, regression coefficients that are too large.

And so Lasso ends up just knocking a bunch of

variables out of the res altogether.

Ridge doesn't do that, but it does push a lot

of, a lot of coefficients like close to zero.

It almost drops them out of the repression.

OK, um.

So you've probably run a regression before and you've estimated

it and then you've gotten a bunch of estimates.

And those estimates usually typically refer to as beta coefficients,

OK, but we're now using a regression to estimate classes

with a DFM.

So what, instead of using betas as the coefficients, I'm

gonna use muses because that's what we're like each think

about in the regression equation, like each X variable is

a different word.

And then we're estimating is the The amount that that

word contributes to the prediction of the class.

OK, so instead of using beta, I'm gonna use muse.

So if you've ever run a regression, the thing you

probably cared about was all the betas, like all the

coefficient estimates.

We don't care about that here.

We're not, I don't, I'm gonna tell you, I'm going

to be a zealot about this, probably more of a

zeal than your other people who taught you machine learning.

You do not want to interpret these.

Don't interpret them.

They don't mean anything.

We're trying to classify documents in the classes.

I don't care what news I got.

What I care about is Once I get my estimated

views, I can build an equation, a regression equation where

I for every document can predict.

The class.

I care about the predictions.

I don't care about the coefficients.

This is not what usually you do when you run

Russian, usually care about the betas, don't care about them

here.

We care about the white hats, just trying to predict

stuff.

But wouldn't you care about the variables, for example, if

you use lasso, the ones that are left, wouldn't be

like words or features that are specifically important for prediction?

I mean what you said is correct.

Why does it matter?

Why do we need to know what the words are?

Well, I mean, so the reason why I'm a zealot

about this is because once you start looking at it,

it starts, you start to like get tempted to think,

oh, this has a causal meaning.

It doesn't have a causal meaning, you know, like, it's

just like these are the words that have to be

correlated with something.

Now, I'm being, I'm a little loose about this, even

I'm you know, I'm showing you this up here and

and breathing causal meaning into these estimates.

So like I'm breaking my own rule here.

But the, but we don't, there's no causal meaning to

this, right?

It's just like we're doing this in service as a

good prediction.

If we want a causal meaning out of this, we

got to use a causal identification track.

You can take M15 or something, whatever to hear about

that.

OK.

Now, I know I'm a zeal about this.

Some people are like, oh yeah, look at the features

and the variable importance.

I, sure, you can do that, but in my view,

it, it, it's like opens up the Pandora's box and,

and people tend to start attributing substantive meaning to it

and then I'm like, you're going down a causal path,

and I'm like, don't do that, you want to go

down a causal path.

OK, so we can get this regression equation.

We can predict what the, we can predict why hats,

OK.

This isn't exactly a probability because if you weren't really

paying too close attention, you probably didn't notice that I

just like running a regular linear regression here, not a

logistic regression.

OK, fine, whatever, it doesn't matter.

Doesn't matter, OK, really doesn't matter.

But you could, you could run a logistic regression to

keep the predictions constrained between 0 and 1.

But you know, like, what we're at, we're not like,

we're not really that interested in the probability per se.

We're interested in like, is this number higher than that

number and it doesn't matter the probability.

Now there are some functional form reasons why you might

want to use the logistic regression and in fact in

the centre exercises you will use a logistic regression.

So, you know, that's fine.

You wanna do that.

Now, just like naive phase, using regression gives you an

estimate of the probability that a particular document is in

a particular class.

It does not give you the specific categorization.

So I'm going to skip forward to talk about that.

The last two slides were like, what advantages disadvantage of

using regression.

It it's, you can do that.

So they re regression, two different ways of doing classification.

Now, once you do the classification, you, you're like left

with a bunch of probabilities.

So for any given document, the classifier is gonna give

you the probability that documents in Class A, the probabilities

in Class B, and the probabilities in Class C.

So what do you do from there?

Well, it might be intuitive to think, well, I need

to, I need to actually classify these documents into a

category.

So if I'm looking at document number one, I'm just

gonna say that document is going to be classified in

the class that gives them where the probability that I

got from my classifier is the highest.

That's like what people mostly do, right?

That's not like, that's, you can do that, that's fine,

and a lot of times you will do that, but

it doesn't always make sense to do that.

OK?

So let me just talk you, I have this on

the slide here, but let me talk you through an

example.

Like, let's suppose that you are trying to classify whether

uh social media posts contain illegal pornographic material.

OK, so, not legal, but like illegal, like think about

child pornography or something like that.

You're working for a big social media company, this is

like a thing you need to do.

So you're gonna build a classifier where you're gonna try

to classify whether or not any given Social media post

has illegal pornographic content in it, OK?

Probably, hopefully, if hopefully if your social media site is

being run relatively well, you're not gonna have a lot

of that stuff, right?

So you're gonna be training a classifier in a context

where the prevalence of the class you're trying to classify

is, is very low, right?

So like imagine that like in reality, like 1% of

stuff contained, you know, and that would still probably be

pretty high, but 1% of posts contain some kind of

illegal pornographic content.

Well, if when you train a classifier.

You presumably do not want to, um, so you could

train a classifier, typically what people do is they'll have

like a dummy variable as the outcome variable, right?

So it's like, is it illegal or not?

And then you say, run a regression and then for

everything, you're, you know, every, uh, every, um, Social media

posts, you're gonna get a probability that this is illegal

content or not, and it's tempting thing to do in

that context, say, well, OK.

Anything that has a greater than not chance of being

illegal content and let's Nixon.

So probability greater than 0.5.

Is that gonna work in this context?

No, because the mean predicted probability is gonna be about

1%.

So like all your data is gonna have very low

predictive probabilities.

Like every single observation is gonna have a very low

predicting probability because this stuff is rare.

So if you use this 0.5 cutoff, you're gonna basically

say nothing is illegal content and all the illegal content

is gonna go.

So this is kind of an obvious example, but it's

very tempting for people when they're taking the probabilities and

then making classifications to just like use simple rules like,

well, if there's a higher than 50% chance that it's

in Class A versus class not A, I'm gonna to

classify it as Class A.

You need to think about what your data looks like

and what you're trying to do.

And in particular, you need to worry that like if

all your predictions are below some magic threshold or above

some magic threshold, you're not gonna get any useful classification.

Cool.

All right.

So, um, what do people really, like, in reality, what

do people do when they're confronted with this problem?

There are things that they do.

I don't even want to spend a lot of time

on this, but Typically, if you have a binary classification

task.

You run your classifier, every document you get a predicted

probability.

And typically what people do is they, they, they.

They go across all the possible thresholds that they could

use to determine the actual classifications, and they measure the

performance of the classifications under each of those thresholds, and

the one that does the best is the one they

choose.

And so they decide to draw the line here and

anything with a predict probability above that is classified as

a positive, anything below that is a negative.

That's It's a little bit complicated, but.

Questions?

OK, but let's step back though, and keep in mind

that there's a distinction between the classification process.

The class is always giving you a probability, so for,

for each document, you're gonna get a set of probabilities

and the probabilities are what's the probability that I belongs

in each of the classes.

Then you got to decide what to do with those

probabilities if your goal is to actually put documents into

categories.

You gotta figure out where you want to draw the

line, like probably has to be greater than whatever to

be in this class versus that class.

And that's a little bit.

Potentially you could look at all the values of the

threshold and just choose the one that works for you

for your application, right?

That, that so binary classification that's what people do.

I mean that's what people do, that's what software does

and people use what the software does.

So yeah, so you could, you start at 0, you

go up to 1.

And you say, OK, if I draw the, so like,

if I, if I do the classification based on the

threshold of 0.01, what is my precision recall?

If I do it on 0.02, what is the precision

recall?

And then they do it, you know, a whole bunch,

and then they find the one that has the best

performance metrics, and that's the threshold they use to, to

classify.

Uh Yeah, that's what software does.

I mean, and that's typically people just take the software

fortunate.

OK.

Now, here's a boring terminology issue again, if you Google

stuff, you're gonna, you're gonna see that uh there's two

other words people use sensitivity and specificity.

So sensitivity is um Uh, let's see here, uh.

I don't even know.

Sensitivity is, is recall for the positive category, and I

think specificity is recall for the negative.

Yeah, it is boring, doesn't matter.

You'd really is boring, who cares what people call this.

I mean, it's important because when you're googling stuff and

asking LGBT it's going to use this terminology, then you're

gonna be like, what what's that because it's like why

I'm just telling you stuff.

One of the things I find frustrating about working in

computational social science is that like there's this whole world

of people out there and like CS data science, industry

land uses a completely different language than like social scientists

do, and there's people in stats use different languages and

social sciences, and it's, it's hard to keep track of

like how people refer to the exact same idea.

That's like the, that's that's like, OK.

Oh, I, oh God, 15 minutes for the, probably the

most important part of this lecture.

All right, so you want to use a classifier to

classify documents, right?

That's the whole point of everything we've done so far.

So we've talked about how you would measure the performance

of a classifier.

We've talked about two possible classifiers you would use, and

you, you can, you know, you can actually do the

calculation yourself to do the classification by hand if you

want to take small that data set, um.

So then the, the question is like when you're deciding

what classifier to use, do I use naive bath?

Do I use regularised regression, whatever.

Like, what considerations do you think about when you're making

that decision?

So that's under the category of like building a robust

classifier.

So what classation should you use?

The, the easy, simple answer is obviously use the classifier

that performs the best, OK?

So if any base performs the best, use that if

regularised regression performs the best, use that.

And remember, we can quantify best by looking at accuracy,

recall, and precision.

That's, that's, uh, that's a little bit too snarky because

there's more to it than just like, oh, pick the

best one.

So, first of all, you have to, when you're doing

a classification task, you are choosing at some level, you

are choosing what classifier you want to use.

That involves two things, really.

I mean, they're all kind of in the same category

as choosing a classifier, but like you're choosing the algorithm,

right?

Do you use naive phase, use regularised regression, whatever, use

a combination of them, of the stacking for those of

you machine learning.

But then you're also like choosing hyper parameters, which is

like, OK, like if I'm gonna run a regression, like

what are all the variables I'm gonna include in that

regression?

So these are hyper parameters in the sense they're not

like the estimates that are generated from the estimation process.

So they're not the sort of the narrow parameters you're

estimating, but they're like, they're still like points of discretion

where you have to make a decision.

So for example, in 90 phase, it's like, well, am

I gonna use multinomial or am I gonna use a

different distribution for progression?

It's like which variables am I gonna include so.

So both of these things are amount to choosing a

classifier, OK?

And people refer to this process as like tuning the

classifier, and it's a little bit imprecise, this language tuning

usually in the you know, it's like sort of.

Changing little things here and there, but like I think

it's all under the broad category like choosing which classifier

to use is like all under the category of like

you're tuning your classifier, OK, like the, the ML lingo.

Now, it's tempting to try a lot of options and

it just like, pick the best one.

But let's be careful.

OK.

So, classifiers are optimised to maximise performance statistics within the

training sample.

That's what they do.

That's the, the math behind it.

They're like on the label documents, you're training your model

on, they're just trying to do the best it can

possibly do.

So, they're optimised for in-sample performance, only for the label

training set.

But in general, the whole point of doing classification is

you want to create the model of the training set,

which then you take to other data, like unlabeled data

and apply the model.

So you do not want to train a model on

this data that is like very bespoke and very specific

to the training, the, the training set.

So overfitting is the word we use for a situation

when a classifier builds a model that is too closely

tailored to the sample that it's trained on.

It's overfit to that sample.

So what it means, overfitting means it's not like, it's

maybe performs well in the training sample, but it's not

going to perform well in other unseen.

Now, contrived example.

Here's the Federalist Papers.

Hamilton J.

Madison, by man a pun.

You can train a classifier that says a document is

is going to be considered to be written by Hamilton

if and only if it contains 859 uses of the

word by.

It's going to be considered to be written by J

if and only if it contains 82 uses of the

word pi, and it's going to be considered written by

Madison if and only if it contains 474 uses of

the word by.

You can get a classifier that tells you that gives

you this classification rule, right?

I mean, whatever, it's a weird classifier, but you know

it's a weird classifier because you look at that, you're

like that's a little too specific.

Like, it's gonna perfectly classify these three documents, perfect.

So in sample, in the sample you classify you go,

0, 100% accuracy, amazing, like everything's, it's so great.

But do you think that's gonna work well when you

try to classify unlabeled documents?

No, obviously not, you know, like.

Roughly speaking, overfitting is like taking sampling variation too seriously.

Like you are training on a sample, and like every

sample has its quirks, you know, and like overfitting is

the process of like, like reading, if you want to

like think of machines as people, like they're reading too

much into the sampling the bill.

That's over, overfitting essentially.

So, overfitting occurs when the model you want to estimate

is too complex.

Machines are very good at finding patterns, even spurious ones,

ones that don't mean anything.

So giving your machine a lot of flexibility to find

patterns by like allowing it to fit a very complicated

model is like opening yourself up to overfitting.

Um, But the problem is that it's, machines are always

just finding patterns in the training set, right?

And so you you like you need to like discipline

your machine, right, some level, you need it to do

well out of sample as well.

And so you can't give it a lot of flexibility

or discretion.

Because it's gonna just find spurious patterns in the trend.

If you, I mean, look, it's 2025.

Machines are very good at finding patterns, and they will

find patterns, but they may be meaningless patterns, at least

meaningless if your goal is to predict in an unseen

data set.

Now, um, you can also get overfitting with reports in

the data like lots of zeros, which have little.

OK.

So, um, we make models.

OK, so, so back up, like I said, overfitting can

happen when you make your models too complex.

Well, why do we make models more complex?

Because we're worried about bias.

When you add variables to a regression, you're making the

model more complex, and you're doing it because you're trying

to deal with potential sources of bias.

So we make models more complex to deal with bias,

but the more complex we make models, the more prone

we will be to overfitting.

So, um, at, so here's a nice little plot that

I, I quite like.

As you make a model more complex, um, what, uh,

what you see in terms of prediction error in the,

in the training sample, so the sample you're using to

make your model is that as you make the model

more complex, the errors it makes, the classification errors it

makes decline, always.

It'll just always decline, because again, you're giving your, you're

giving your machine more and more flexibility to find patterns

in the data.

But it's just finding patterns in the training data, OK?

And so if you were to look at a test

sample, this is a sample of documents that has not

been seen by the, the classifier.

What you'll notice is that as you make the model

more complex, it gets, your classifier gets better in the

the test sample, but then it starts getting worse.

This is overfitting.

So The test sample, you think of the test sample

as like data that has not been used to train

the classifier.

That's like the unlabeled data, for example.

And as you make your model more complex, there's this

like U-shaped relationship between um uh bias and, and variants,

right?

So making a model more complex, you're, you're reducing bias,

making your model less complex, you're increasing bias, but making

your model more complex, you're increasing variance, making it less

complex, you're losing variants.

And so there's this U-shaped relationship between how well a

classifier will do, um, uh.

Out of sample between model complexity and and error.

And this year, this gap between how well your data

does in your training set and how well you do

your data would do in a new data, this gap

is what we're worried about when we're worried about overfitting.

Underfitting is a problem too.

But you, it's a problem in a philosophical sense, but

like, because when you're underfitting, your test sample will do

about as well as your training sample, we just don't

worry about it so much.

With overfitting, like we don't see it in our training

sample.

We see underfitting in our in our training example, we

don't see underfitting in our test, sorry, we don't see

underfitting in our training field.

OK, so what do you do?

So what you do, just to cut to the chase,

whenever you're trying to figure out what classifier to use,

you choose a whole bunch of candidate classifiers, you run

them on the training set.

And then you evaluate the performance on a validation set.

So what is that?

Well, before you start the training process, you take your

label data and you split it up.

70% for your training, 30% for your validation.

So you train your model on the 70%.

And then you validate its performance on the 30%.

So in that validation set, you have the labels there

too.

So you're going to use the classifier to predict in

the validation set.

And so you're going to be able to measure precision

and recall an accuracy in the validation set.

But the validation set was not touched when it was

when the classifier was estimating the model.

So you'll be able to detect overfitting in the validation

set.

But I want to tell you that this isn't magic.

You can't.

Use a validation set.

Over and over and over and over again, you get

one chance.

So whenever you are trying to pick the best classifier,

the process you want to use is Have a whole

bunch of candidate classifiers, train them all, evaluate their performance

in the validation set in in a set that was

not used to train the model, pick the best one,

and then whatever the best one is, run it on

the whole data set and use that prediction model for

your you know, you're labelling unlabeled data later on.

But don't train, validate, train, validate, train, validate in sequence

where you're changing the classifier in response to what you're

seeing in the validation set because now you're training the

machine to optimise on the validation set too.

So you've lost the whole, the whole point of the

validation set was it's not seen, but if you're tuning

your model in response to what you're seeing in the

validation set, you're now pushing the model to learn from

the validation set.

Now, um, one thing that people do, OK, so I,

I'm gonna conclude with this, this is important, and then

I Yeah, you can read the rest of the stuff,

because I, I just want to say that, that, I

want to say that the take away here, right?

So validation is the way that you pick a classifier,

right?

You take your label documents, split to a training set

and a and a validation set, randomly, train on the

training set.

A bunch of different candidate models, test their performance on

the validation set, pick whichever one's the best, and then

run that on the whole model and then you've got

your predictions.

Cool.

Um, You can, so this is wasteful, OK?

This is a very wasteful way to do validation.

Why?

Well, you took all your label documents and you split

them into 70 and 30, and you held out this

30% here that you didn't use to train your model.

But machines are really, like, they're much better at prediction

when you give them more data.

But you're wasting this 30%.

The only thing you're using this validation set for is

to like, see how well your classifier did.

So like, what?

That's annoying, right?

I could have used Use that data to train my

model, of course, I can't because I need it for

the validation set.

Oh, so what can I do instead?

Well, I can use the logic of validation, but I

can do cross validation.

So what I do instead of, so like think about

cross validation in the context where the 70% and the

30%.

Well, I can just train a model in the 70%,

test it in the 30%, train the model in the

30%, tested in the 70%.

So I I do it one way and then I

cross over and do it the other way.

Why don't I just take that logic to its its

logical extreme?

why don't I split the data into 10 randomly into

10 groups, and then each of those groups I consider

to be a validation set and then the other 90%

I use as a training set.

So, so.

Split number 1, 90% is the training validated on that,

that first split.

OK, then split number 2, the 90% is the training

and then validate on that split number 2.

And then just do that.

So then you've trained using every observation in your training

in your labelled set, and you validate it using every

observation in your uh labelled set.

And you didn't like waste data by like holding aside

this valuation set that you never used to do any

training.

And the more splits you make, the better it is

because it means that the training you're doing is on

data that is almost the same as the full labelled

set, right?

If you have like 100 splits, you're using 99% of

your data to train every time you do, but of

course it's computation, you have to do 100 different.

OK, so in seminar this week, starting today, and uh

for those of you today, what we're gonna do is

we're gonna do some dictionary methods, some uh discriminating words,

and some classification.

And so what you're gonna see, especially in the classification

part is you're gonna See the functions and processes we

would use to classify a set of documents, um, using,

uh, sort of out of the box our stuff, but

then there's also some exercises in there where you manually

calculate some of these classifiers kind of by hand.

The idea is you get some intuition here and you

also get some practise doing it for, I don't know,

like an exam.

OK, I'll see you in a minute.

Yeah Yeah Yeah.

That This Yeah.

That has come back That's a lot so yeah.

I.

So.

At least they're over it.

Yeah.

It was it was like.

Right.

You should, by the way I did right.

Lecture 5:

My sister and I both had Anchorman themed posters and

mine said You stay classy Eagle Scout, and I think

I put more effort into that, yeah, yeah, yeah, no,

that's OK.

That's what she wants.

But like in that like if you do want to

marry somebody, but then they do something like that like

uh oh like actually I like get up and try

again later.

be my like if someone proposed to me the sporting

event, I would be like, we haven't met.

Uh, actually one of my favourite stories of friend of

a friend used to work with like the AV department

at Fenway Park, so the Boston Red Sox, uh, and

he, um.

The seats where you propose are like specific seats that

they know to have the camera there at a certain

time, um, and apparently fucking yeah, you have to make

a donation to the like Red Stock scholarship foundation to

do it, um, but apparently this one girl was like,

like this one proposal.

So the girl was like, what is she?

it's like, are you kidding?

Are you?

And apparently they had been friends for years and she

had finally agreed to a date.

Oh, and he got through, he, he right away was

like 0 to 1000 and it went like so viral

that the next couple who was supposed to get engaged

there, like the seat numbers had like gone sufficiently viral

that they were like, we can't.

We can't put you in those seats again and so

we're gonna have you come onto the pitch, but like,

are you sure she's gonna say yes because we, there's

no turning the cameras away, there's no like anything else.

I was like oh yeah, I think if I was

put in that position I would just say yes.

Yeah, I would probably say yes like I think in.

Most circumstances of the public proposal, I probably would do

that, but in that specific circumstance, I think I'd be

so shocked.

If you're proposing to someone in public, but like what

if you don't know that they want to do.

Especially the ones where it's like, yeah, oh gosh.

Here's the thing, if someone did that kind of proposal

to me, it would confirm to me that they don't

actually know me well enough to remarry because they should

know that I would literally rather die like awkwardly like

stand smiling oh God oh God.

But the point is that like the other girl was

so happy, you know, yeah, but there was like a

conversation that I had around like.

like to join showbiz basically so he proposed to her.

I've seen this yeah, yeah, yeah, knows her.

I saw that one that was a huge, huge, and

then he like made this whole, I think most people

see I was like, should I really get married to

her?

I'm not so sure she's going to make up her

mind he did like this whole thing and like like

I mean he did like this whole like.

No, I'm like.

of like I can't I can't even.

I I don't know we don't, no, no, you're gonna

get the stopping, stopping them from, yeah, they're like, will

you be in a flash mob so I can close?

I'd be like, no, but my God, I'm like.

So yeah, I don't know.

I don't have guys like that.

If my friends, I I I I would hope my

friends, if my partner asked them, you know, would be

like, no, don't marry, that's protect me.

I'm gonna it's terrible.

That's hilarious.

It's I would go as far as that's a trip

to my thing that happened was a promu at my

high school.

You seem like the girl said no the guy kept

playing guitar walking down the hallway.

That's friendship.

I don't ruin my yeah yeah.

I have seen some terrible like they were at a

concert that I went on was like she knew they

were going to a concert like two hours before she

was like.

My boy has never once iron.

there was a lot of like OK let's go ahead

and get started.

OK, so this week we're gonna be talking about scaling,

um, and we're gonna be talking about two types of

scaling today, um.

So I'm just going to get started.

We have 3 main parts of this lecture.

So one, I want to Discuss what scaling is.

And then I want to talk about two approaches to

scaling one, which is supervised and one which is unsupervised.

The basic distinction between these two things is that supervised

scaling methods, uh, start with a set of texts like

a training set that we know the positions of, uh,

and unsupervised scaling does not.

OK, so let's just jump into.

Defining what scaling is.

So scaling is a set of quantitative tools for measuring

latent traits, um, and those latent traits are usually measured

on some kind of continuous scale.

That's why it's called scaling.

So what do I mean by a latent trait?

Well, one classic example of a latent trait is political

ideology.

It's latent in the sense that it can't be observed,

but it's a trait of people or texts or whatever.

And so in political science, where I'm from, uh, political

ideology is like a, is a very standard latent trait

of interest, and there's a huge cottage industry devoted to

trying to measure political ideology of politicians, judges, techs, all

sorts of things, OK, but if you wanted to.

To, uh, let me give you a little bit of

notation and a and a diagram to kind of make

this a little bit clearer.

So let's suppose that we're talking about political ideology and

I'm going to use the symbol pi, the Greek pi

to indicate a particular political ideology.

Why that symbol?

Well, P for position on a scale, i for position.

Um, and, uh, I like it, so I'm going to

use it and we're gonna think of political ideology as

being on like a left right scale, right?

So typically when we talk about politicians, we talk about

them as being from the left wing or the right

wing.

So mathematically we can think about that as like a

line, the real number line.

That's what this thing is here.

And so we have, I've illustrated here a line and

notated that this line is meant to represent positions on

a political ideology scale.

And so to the left it's more left wing, to

the right it's more right wing, but to put this

in math terms, lower numbers, more left wing, higher numbers,

more right wing, OK.

So, again, in political science, a lot of research placing

individual politicians on ideological scales.

So you might imagine two politicians, a Labour MP and

a conservative MP, and you might illustrate where they are

on this ideological spectrum.

The labour MP obviously.

more to the left than the conservative MP, um, but

in the context of what we're doing in this class,

there's also a tonne of research, uh, placing documents on,

um, ideological scales, politically political political ideology skills.

And so for example here I've illustrated maybe you know

where the labour manifesto from a particular election would be

found relative to the Conservative Party manifesto.

So this is placing the documents in ideological space and

this little parenthetical here is remember one of the assumptions

of QTA of all of QTA.

Is that texts are observable implications of things we care

about.

So why would you ever want to place texts on

an ideological scale?

Well, you have an intuition that the texts are representing

real things in the world.

So in the context of thinking about the manifestos of

British political parties, you think that those.

those are actually a reflection of the policies that the

parties are going to pursue or the ideologies of the

politicians themselves.

And so we might be interested in measuring the ideological

position of texts because we think those texts are observable

implications of larger social and political phenomena we care about,

OK?

And that I feel like you should just Acknowledge this

elephant in the room because it's a little bit artificial

to talk about the ideological position of texts in some

kind of abstract way, but we do it because we

think that the texts are representing something real, OK.

Now I'm talking about political ideology here.

Most of this lecture I'm gonna continually talk about political

ideology as the scale of interest.

Why?

Well, because a lot of these techniques were developed.

By political science scientists within political science to try to

measure political ideology.

This is like an obsession of political scientists like trying

to measure the political ideology of things.

So the latent trait of interest oftentimes is political ideology,

but there are lots of other latent traits, things that

we can conceptualise as being on a scale that are

latent, meaning that we can't directly observe them, that we

might care about.

So, um, for example, You might care about soft news

versus hard news, right?

So you might want to create a scale of like

the extent to which something is soft versus hard news,

um, uh, so on, I mean, there's just a tonne

of things you can think of, right?

Like any kind of concept that you could think can

be arranged on a scale, you can do scaling for.

OK, now backing up from a methodological perspective, scaling methods

are quite, I mean, Should just say like as we're

walking through it, you're just gonna see echoes of stuff

we've already talked about and that's because all of these

methods in some sense are like loosely related to each

other, um, and, and so you'll see, especially for supervised

scaling, there's gonna be a lot of, uh, echoes of

what we talked about last week with classification, supervised learning.

All right, so let's jump into supervised learning or supervised

scaling with word scores.

OK, so Word scores is a method by Lever, Benoit,

and Gary from 2003.

So LBG is the acronym that I'll use throughout the

slides.

So the basic idea behind the Word scores method for

scaling texts is you start with a set of texts

that have known positions on one or more left right

scale.

Again, I'm talking, I'm just going to be talking in

terms of political ideology a lot, but you can envision

any kind of latent trait scale you care about.

It doesn't have to be political ideology, but I'm going

to use a lot of vocabulary and language that is

like about political ideology because that's mostly how these methods

have been used.

But again, you can use it for any kind of

latent trait that you can think of as being on

a scale, OK.

But so you're gonna start with a set of texts

with no positions on one or more left right scale.

Mostly today we're gonna talk about situations and context in

which we're scaling text on a single scale, a single

dimension.

But if you read a lot of the original papers

and some of the uh resulting research on, on scaling

methods, you'll see that, uh, scholars will often scale techs,

politicians, so on and so forth on multiple dimensions, so

like on an economic dimension and on the social dimension

and so on and so forth.

But we're going to keep things simple here today and

envision there's one just kind of left right scale that

captures all of the, the things that we care about.

All right.

So you start with a set of texts with known

positions, then you use those texts to measure so-called word

scores.

That's where the name comes from, for each of the

words that's used in the, uh, the set of texts

where you know the positions.

So this is the supervision part of the supervised scaling,

right?

You have a set of documents.

You know the the positions on the scale.

You use those to create a model and what is

the model?

Well, you're going to for all the words used in

those texts, you're going to assign a score to each

of those words.

The score, as you can kind of think about it,

is like how ideological that particular word is, OK?

And then once you have that, you're going to then

take all the word scores, go to a set of

texts that aren't labelled where you don't know the positions

and then uh basically count the number of times the,

the words that you've scaled up here appear in these

uh texts and multiply them, multiply those counts by the.

scores that you came up here, OK?

So you're, you're, um, supervising.

A model in a sense, you're creating word scores and

then you're using those word scores to then place unlabeled

documents into the ideological space.

OK.

Uh, very similar procedure to classification, training and then applying

to a new set of untouched documents.

That's why it's called supervised scaling.

OK.

So just to diagram this out a little bit, this

is what the word score procedure looks like using, um,

using an example of UK manifestos from 19, 1992 to

2001, that should say 1992 to 2001.

So I'm going to just show you with this diagram

the process.

So first you obtain a bunch of reference texts.

So lean back up and say the language, maybe it's,

it's actually here.

I already.

I have it there, but I should have said this

first.

The language used in the original paper is a little

bit different than the language I'm going to use today.

The original paper refers to the set of reference texts.

I'm going to call those the labelled texts because I

want to try to make this seem like classification which

we talked about last week.

So I'm going to call the reference text label text.

In the original paper they call it reference text.

I'm going to call the set of texts that we're

going to scale, the ones that have not been labelled

yet, the unlabeled texts in the spirit of classification.

They call them virgin texts, and I love that phrase,

so I'm not going to use it, but just so

you know, reference text here is the labelled text.

So you start with the label text.

So in this particular example, we have three labelled texts,

the labour manifesto from '92, the liberal Democrat manifesto from

1992, and the Conservative manifesto from 1992.

We happen to know the positions of each of these,

where do these positions come from, expert coding.

The black boxes that we're not going to talk about

these, these, these are just known from prior research, OK,

but you start with the text where you know the

positions of the text.

So labor's 5.35, liberals is 8.21, conservatives is 17.21.

The next step is you take these documents and then

you extract all, you create a document feature matrix and

then you calculate word scores for each of the features

inside the document feature matrix from these labelled documents.

So these are all the words that appear in those

labelled documents and the associated word scores that go with

them.

I'm going to show you the exact procedure to do

this.

In in a few slides um and then once you

have word scores for each of the words, you then

score the unlabeled texts.

So these are labour, liberal Democrat, and conservative manifestos from

1997 where we don't actually know the ideological position.

But what we do is we basically for each of

these we take the weighted mean of the words.

Scores.

So we created the word scores from the label text,

and we're going to take those word scores over here

and we're going to calculate the weighted mean of the

word use in these unlabeled texts and that will give

us an overall score for each of the documents.

So the labour 1997 manifesto is 9.17 on this scale,

liberal Democrats is 5 and conservative is 17.8.

I mean.

The sort of interesting thing from from a substantive social

science perspective of of this example is notice the Labour

manifesto in 1992 is at a position of 5.35% and

by 1997 is 9.17%. As you might know what happened

in 1997.

This is the Blair.

This is like the first Tony Blair election and famously

Tony Blair, pushed the Labour Party to the centre.

And so you can see that in the manifesto, um,

from 1992 to 1997 moves to the right from 5

to 9.

OK.

Um, and then an optional fourth step that uh that

we will talk about is that you take the scores.

Although these are exactly the same.

That's bad illustration.

But you take the scores that you get from step

3, and then you, um, You rescale them to be

on the original metric.

OK, so what, what's going on here?

Well, it turns out when you do this process, take

the label documents, create the word scores, and then for

each of the unlabeled texts you create a score for

that document.

The when you get these scores, they're going to be

bunched towards the middle.

Mathematical reasons and so they're not going to be on

the same scale as the original scale you used to

create the word scores.

So you've got to do a mathematical, a simple mathematical

transformation to put them back on the same scale.

Now this unfortunate thing about this diagram is that these

numbers appear to be the same.

From step 3 to step 4.

So whoever created this diagram had already rescaled these before

they created the diagram.

Um, but you know, if I were to recreate this

diagram, I mean this is from the original paper, but

if I were to recreate this, I would have in

step 3, I would have made these the original unscaled,

um, scores and then the scaled ones.

But alas.

OK, So now I want to walk through the process

you go through to calculate word scores, you know, there's

one of two directions you can go.

You can fire up R load quant data, and then

type in text models underscore word scores, put in your

DFM.

And get some, get some results or you can do

it the manual way, which I'm going to show you

how to do here.

It's actually pretty simple.

But the way to do it manually is actually is

very, very straightforward.

It's nice that you have this quantitative function to do

it quickly, but it's actually mathematically speaking, not complicated.

So let's go through the steps you would go through

to do this by hand.

So you have to start with the prerequisites, which is

a set of texts divided into the label set and

the unlabeled set or in the word in the language

used by LBG reference text and Virgin texts.

Each text I in the set of labelled documents must

have a predefined score.

So I'm going to notate that as i i.

Why?

Because I'm using pi throughout these slides.

I'm hopefully using pi consistently to mean like the position

on the scale of interest that we're scaling.

So we start with the labelled set, each of which

has a position already which I'm going to use pi

i to be the mathematical notation for that.

Um, and it's going to be for each of these

label documents it's going to be a single number locating

that document on that scale of interest, and then the

scale, we'll talk a little bit more about this later

on, but the scale itself is kind of arbitrary, can

be, you know, any kind of scale like 1 to

20, -1 to 1, whatever, it doesn't really matter.

I'm gonna talk more about the meaning of the numbers

later on, but Like, look, scale, when we're talking about

doing scaling of latent traits, like latent traits are not

observed.

This isn't real, right?

Like there's no, there's no like number for where you

all are on the ideological left-right spectrum, right?

Like you can't be like, I declare my, my political

ideology is 7.

43, right?

Like this is, we're translating abstract ideas into math and

so the scale, like do we go from -1 to

11 to 20, whatever is like it's a little bit

of a fiction that we are creating and that's fine.

It means that we can use any scale we want.

We just constantly have to think about rescaling stuff.

So if we start from 0 to 0 to 1

and then we want to make our scale go from

-20 to 20, we just like blow everything up to

a larger scale.

It doesn't really matter.

The number itself doesn't mean anything in any deep level.

OK.

So, um, We don't know the scores of the documents

in the unlabeled set.

That's the whole point of what we're doing, right?

If we, if we knew the positions, the ideological positions

of all of the documents that we have access to,

that we wouldn't do any of this because we would

already know the ideological positions of all the texts.

OK, um.

Alright, so the writing example I'm gonna use to show

you how this works is the one from that diagram,

which is it's a corpus of UK political party manifestos

for elections that are it's available in Quantata.

And so I'm going to look at a small corpus

of 6 documents, 3 manifestos from 1992, the conservative, the

Labour, and the Liberal Democrat, and then 3 from 1997.

So sort of matching the diagram you've just, you've just

seen.

But the corpus already exists in Quante, so you can

play with it if you want.

So here's, here is what the corpus looks like and

like a brief little preview of each of the documents.

All right, cool, um, so we're going to use the

1992 documents where we know the positions to train.

Uh, the, or to create the word scores to do

the training, and then we're going to apply the word

scores to the 1997 documents and we're going to assume

the following scores for the label documents.

So these are the labels.

Conservative manifesto 1717.21 from 1992.

The labour from 1992 is 5.35%, and the Liberal Democrat

from 1992 is 8.21.

Where do those numbers come from?

Again, just to reiterate, this is expert coding.

OK, so these numbers come from somewhere.

You've got to make the, you have to justify where

these numbers come from, but you know these are like

fake, right?

So, um, you know.

Uh, expert coding in quotes, like I don't know where

these come from, but When you do scaling and you

have your label set, you have to make a case

for why you're using the numbers you, you're using.

Um, and I want to talk a little bit more

about this later on, but let's just take as given

for now that those are the those are the ideological

positions of the three labelled documents that we, we have

in our small little corpus.

OK.

All right, so next you do some preprocessing.

So you take the label texts only and you convert

them into a document feature matrix.

You don't need to do like stemming or removing stop

words or anything like that.

You can if you want to.

You can read more about this in Lowe and Benoit

2013 where they talk about this issue.

Um, they seem to suggest you don't need to do

any of this stuff and you shouldn't do any of

this stuff.

You should just use the document feature matrix as is.

The one exception I think I would give is that

you should make everything lower case, right?

You don't want to be treating the exact same word

differently simply because of capitalization.

So that's like the one major prepro major, that's the

one preprocessing step that I would definitely do.

But they, they say you don't have to remove stop

words or, or stem, and you know, that guy is

one of the ones that came up with this method.

So I assume he spent a lot of time thinking

about this, um, and validating it in, in various, various

ways.

But key thing is be consistent, right?

If you decide to stem and remove stop words from

your Training set from your labelled set because you think

this is going to help.

Generate better scores.

You have to have a reason to do it.

Then make sure you do it in your, your unlabeled

set as well.

Just be consistent and explain why you're doing it.

OK, we're going to use the usual notation we've developed

so far.

So bold capital W is going to be an N

by J document feature matrix.

This is only for the labelled set.

We're only talking about the labelled set.

Now, so let's just, yeah, so this W is for

the labelled set.

W with the subscript I is a specific document.

It's a row in the document feature matrix that's so

it's a represents a document and it's a word count

for each of the possible tokens that could be in

the document.

And then capital M, not bold subscript I is the

total number of words used in document I.

So that's if you took this row and you summed

it up.

So this row is the document I, so it's the

word counts and document I sum all those together.

That's the total number of words used in that document.

Same notation we've used before, going to use it again.

Again, this is just for the label documents only.

OK, cool.

So I have a document feature matrix from the label

documents, the 392 manifestos, and you can see here that,

I mean, obviously the word conservative comes up a lot

in the Conservative Party manifesto.

When I created this document feature matrix, I didn't.

reorganise.

I didn't reorder the columns in any way, so when

it creates it, it creates it based on the order

in which the words come up.

And the first document in this corpus is the Conservative

Party manifesto and the title has the phrase Conservative Party

in it.

And so you, you get Conservative Party as the first

two words in your, in your Uh, document feature matrix,

but then there's a bunch of other words, and of

course the, so the label of the labelled documents, there's

only 3, and then when you create the document feature

matrix, there are 7,103 features, meaning there were 7,103 uh

unique words.

I removed the stock words here.

I only did that because it looks nicer for pedagogical

reasons, so you don't have like the and of and

stuff in here, but um, you don't have to do

that if you are actually uh uh um.

Doing this yourself for a real reason.

In that case, you would have a lot more features

because you wouldn't have removed the stop words.

OK.

Now, um, so next step is you take that DFM

and you normalise it.

So what you do is you take the DFM and

then you create a new Matrix, we'll call it F,

and that is created by taking each of the word

counts in the original DFM and dividing it by.

The length of each document.

OK, so think about you have your DFM, the original

document feature matrix.

Each row is a document.

So in our case we have 3 documents 123.

You create this F matrix by taking each of these

numbers and dividing that by the total sum of the

row.

So it's basically what Proportion of this document is this

word.

What proportion of this document is this word?

What proportion of this document is this word and so

on and so forth.

So that's going to be the F matrix you create.

So this is the formula for doing it and then

you get something that looks like this.

So this is, this is the F matrix, OK.

So here, um, 0.113% of the conservative manifesto uses the

word conservative.

0.072% of the Conservative Party manifesto uses the word party,

so on and so on and so forth.

So we're normalising the document feature matrix by um dividing

by the length of each of the documents.

We're getting, we're sort of putting them all on the

same.

Document length scale, in some sense.

OK.

Then next step, make another matrix.

So you make a matrix of the relative document probability.

So this is a matrix we'll call P.

So you take the F matrix, and take each of

the cells of the F matrix and divide it by

um the For divided by the column sums.

OK.

So if we have this F matrix, now we're making

the P matrix, The P matrix is going to take

each of these numbers and then divide it by the

sum of the column.

So to create the F matrix from the DFM we're

taking each of the numbers in the DFM dividing by

the row sum, the sum of the row, and then

to create the P matrix from the F matrix, we're

taking each of the cells of the F matrix and

divide it by the 7th column.

OK, cool.

And then you end up with the P matrix like

that looks like this.

And so, What do these numbers represent?

Well, if you'll note, if you look across the columns,

you'll notice that each of the columns sum to one.

OK, so what is this?

Well, conditional on seeing this word, what is the probability

you're looking at the Conservative Party manifesto?

What is the probability you're looking at the labour Party

manifesto, and what is the probability you're looking at the

Liberal Democratic Party manifesto?

Well, so when it comes to the word conservative, if

you see the word conservative in a document, well, from

the labelled set, you like, you know, 50% chance of

it being from the Conservative Party manifesto.

And then you have about a 1/3 probability of it

being from the Labour Party manifesto.

Well, why would that be?

Well, because the Labour Party manifesto is referring to the

Conservative government's poor, you know, well, in, in their words,

right, the poor performance of the conservative government.

They're gonna use the word conservative a lot, so that's

why you're going to see the word conservative in the

Labour Party manifesto a lot.

Um, but each of these columns is telling you for

that particular word the probability that you're looking at this

document, that document, or that document conditional on seeing that

word.

Cool.

And roughly speaking you can think about it as like,

you know, of the number of times the word conservative

is used, like, uh, uh, how much like what proportion

of it is in the conservative manifesto and labour manifesto

for democratic manifesto, OK.

Finally, taking the P matrix, this last thing that we've

created, you can compute a J length vector.

So J length, what's J?

J is the total number of tokens in your corpus

or in your document feature matrix.

It's the number of columns.

We've always been using capital J to be like the

number of columns in your document feature matrix.

That's like the number of types in the vocabulary, right,

to use different terminology.

So we can create a vectors that has J elements

in it.

So for each of the words in the corpus, the

train, you know, the labelled corpus, we can compute a

score and then we can take all the scores and

put them into a vectors.

Each specific word's score is calculated using this function.

Where you multiply the position on the scale times the

um the probability weight from this matrix and then sum

up over the document.

Oh sorry.

Uh, you sum up across the, across the road, so

you, you sum across the columns.

OK, so, um, to put it more intuitive words, SJ,

each word's word score is the average of each document

I scores pi weighted by each word's probability PIJ.

All right, so if you do this in the in

the corpus, you get something like this.

So when I do the calculations myself, I got a

word score for the word conservative of 11.6.

I got a word score for the word party of

10.5, and so on and so on and so forth.

Now remember the Conservative Party manifestos.

The score was 17.something.

The word conservative, which appears the most in the Conservative

Party manifesto, has a relatively high word score, but it's

not at 17.

Why?

Well, because the word conservative is not just used in

the Conservative Party manifesto.

So mathematically, the word score for the word conservative is

going to be somewhere between the extremes of the scale

because the word conservative isn't uniquely used by the Conservative

Party.

Uh, manifesto.

Now if this word only appeared in the Conservative Party

manifesto, then again this math is pretty simple.

If this only appeared in the Conservative Party manifesto, then

this score should be roughly equal to the Conservative Party

manifesto score overall.

But because it's used in the other ones, it's like

a weighted average.

Among the scores for the 3, for the 3 labelled

texts, weighted by the proportions in which the word is

used across those three texts.

So if the if the word conservative is used about

half the time, like conditional on seeing the word conservative,

if you, if there's like 50% probability that coming from

the the Conservative Party manifesto, that's 0.5 times the score

for the Conservative Party manifesto if it's in the labour

Party manifesto.

So about a third of the time it's plus 1/3

times the labour Party manifesto score and then the same

thing for the Liberal Democratic score.

So this is like a weighted average of the 3

document positions weighted by the word usage of the word

conservative, so roughly speaking, OK.

Now I'm going to plot them, and I want to

illustrate this a little bit what I'm talking about here

that might seem a little bit abstract.

So I plotted them from the most to the sort

of the highest to the lowest word score.

So these are the top 10 word scores, top meaning

like the highest numbers, and these are the bottom 10

word word scores, meaning that they have the lowest scores.

Um, so what you can see and I've, I've Plotted

also vertical lines corresponding to the document score for each

of the three manifestos.

So the Conservative Party manifesto is here, the Liberal Democratic

Party manifesto is here, and the labour Party manifesto is

here.

And so these words down here.

You can sort of imagine that these must be used

mostly in the labour Party manifesto because each of these

words have word scores that are very close to the

document level estimate the document level position for the labour

Party.

So that kind of gives you an indication that these

words are like conditional on seeing them.

There's like a really, really, really high probability that they,

it's in the labour Party manifesto.

Same for these words up here.

These are probably exclusively, not probably, these are mostly used

in the Conservative Party manifesto.

That's why they have word scores that are close to

the document level estimate the document level position for the

Conservative Party.

And then everything in between is kind of like averaging,

you know, there, these are in the middle, these are

words that are used sort of uh uh more equally

across the, the three documents.

OK.

But one thing you'll always notice.

you're never going to get a word score that's lower

than the lowest position of the reference documents or the

label documents.

You're never going to get a word score that is

higher than the highest position among the labelled documents.

That's because to create each of the word scores you're

creating a weighted mean of the three.

Positions weighted by the usage of the word across the

three documents.

So think about how averages work.

Like you have two numbers, you average them together, you're

going to get something in between them.

It's the same thing here.

So your word scores are always going to be each

of the word scores for the individual words are always

going to be constrained to be between the extreme.

Document positions that you started with.

Cool.

Yeah.

OK.

All right.

So now what do you do?

We have the word score, so each word in the

original labelled set corpus is now an associated word score.

You can kind of think of those as like what

is the ideological position of that particular word.

Well then you have a whole bunch of unlabeled texts

where you don't know the positions, but you do know

the word used because you have the corpus and you

can create a document feature matrix for those unlabeled texts.

So what you do is for the unlabeled text, you

create the F matrix the same way you did for

the label, but I'm going to put a little u

here because this is the F matrix for the unlabeled

set and then you can read about the details on

your own time.

You take that F matrix and you calculate a position

for each of the documents using the word scores that

you have just calculated and multiplying them times the the

normalised word counts in in as created by the F

matrix.

OK, so when you do this, When I did the

calculation, this is what I got, um, and again, you're

going to see, you're going to see code with this

as well.

So you're gonna, you know, I could sit here for

days and talk to you about this equation, but like,

you know, if you're anything like me, you're probably looking

at this being like I just need to see it

done.

I need to do it myself.

So in the we're going to do our code in

seminar where you're going to work through these calculations, but

when you do them, You end up with three document

level scores for the unlabeled texts.

So for the Conservative Party in 1997 manifesto of the

Labour Party in 1997 manifesto and the Liberal Democratic manifesto

in 1997, and what do you notice about these three

scores?

They're very close to each other.

It's kind of weird, because if you remember the original

positions of the labelled set manifestos were like 5.

59, 17, something like that ish.

OK.

Here we've got 10.88, 10.39, 10.35.

And these are like, if you look, if you just

like sort of naively look at these, you're like, oh

well, you know, from 1992 to 1997, they all converged

to the exact same point basically.

Well, this is the problem that I was speaking about

many slides ago, whereas when you, when you use this

method to create a document level score for the unlabeled

text.

They're not giving you numbers that are on the same

scale, and this has to do with the fact that

you're doing a lot of averaging and averaging just as

a general rule, averaging pulls everything to the centre.

That's like what literally what an average is.

And so you're sort of building, averaging on top of

averaging and so you're going to get these scores, they

are really tightly clustered together, but you can rescale them

back to the original scale that you started with with

the labelled text.

Um, so.

So, uh, the why, OK, so more in more detail,

why are you getting this bunching?

Well, there's a lot of non-discriminating words in the corpus

of label text.

What do I mean by non-discriminating?

Well, there's a lot of words that like they don't

really signal much ideological content.

They're used pretty commonly among the three manifestos, and this

is especially true if you don't remove stop words.

You're gonna get a lot of words that are like.

A third, you know, the conditional on saying that word,

it's like a 3rd probability that was in a conservative

manifesto, a third probability was in the labour manifesto, and

a third probability it was in the liberal democratic manifesto

just because it's a word that's like commonly used in

English.

And so there's gonna be a lot of those words.

That are going to have like scores that are kind

of in the middle of the 3 and they're going

to appear in the unlabeled text as well and you're

going to be using the, you're going to be assuming

they're ideological words and using the word scores from those

like nondiscriminating words in the calculation for the unlabeled text

and so you're going to, you're going to get kind

of compression towards the middle because you're including a lot

of nondiscriminating words.

Remember, discriminating words are Something we talked about a couple

of weeks ago, but the idea of a discriminating word

is that it's a word that gives you information about

where it comes from.

So lots of these words in the label text are

not discriminating words, and but they're being used to calculate

the scores for the unlabeled texts.

So you have to rescale them, fine, whatever.

Two different ways you can do it.

The original Leverre Benoit and Gary Paper proposes one way

to do it.

Martin and Van Berg in 2007 have a paper where

they propose another way of doing it.

Turns out in the quantity package when you, when you

uh calculate word scores, you can specify which rescaling you

want to do.

And so I In the quantitative, so this is the

original numbers that I just showed you with no rescaling,

and then if I use the LBG rescaling, these are

the numbers I get if I use the Martin Vanberg

rescaling, these are the numbers I get.

But let's focus on on this rescaling for a second.

So notice this is the Original raw scores that we

got, once they rescale them back onto the original scale

we started with, this looks a little bit more normal,

right?

This looks more like what we should expect to see

that the 1997 manifestos are spread out more than what

would be suggested by this number here.

OK.

Uh, and importantly, we still see the same trend of,

of labour, uh, moving to the right in this rescaling,

although we don't see it as clearly in this other

rescaling.

These numbers are all fake, so you know, we're just

rescaling stuff and like.

You know, you, you got to have to squint and,

and make some substantive meaning out of it.

OK.

Now, there's a couple of other things you can do

with this technique.

So one, you're probably going to want to provide confidence

intervals, right?

So you in your unlabeled text, you're basically estimating what

the ideological positions of those texts are.

Well, as you should do in any kind of uh

context where you are estimating things, you should.

Provide confidence intervals.

So like how certain are you of your estimates.

So you can uh do that in one of two

ways.

You can use an analytical approach.

That's what they do in LBG in the original paper.

What do I mean by that?

We'll use some formulas to calculate a standard error, uh,

based on the assumed uh distributional properties of the estimator.

Um, that's like, you know, when you run a regression

and you get your like, your, uh, standard errors and

your T scores and all that stuff, that's all analytical

in the sense it's derived from some formulas.

You can also bootstrap, which is a way of trying

to estimate confidence intervals or standard errors by resampling.

And so this is, um, this does not involve taking

some formulas and calculating things.

This involves like, uh, doing a sampling, computationally intensive sampling

technique.

Uh, to, to get, um, standard errors.

This is not really like specific to this class, but

like this is an approach you would use to calculate

standard errors if you're pretty confident that the data you're

looking at is distributed nicely, like normally or whatever.

Of course, you just have to know that it's like,

you know, an assumption in some sense.

This is what you would do often if you have

like weird data that you think is distributed in a

weird way or an unknown way you use bootstrapping.

OK, whatever.

But the quantit package nicely provides you with confidence intervals,

so that's good.

So you can give a sense of how confident you

are in your estimates and um and the range of

plausible estimates given some assumptions about the uncertainty.

Uh, and then also you might want to smooth your

DFM to deal with the zeros.

Remember we talked about Laplace smoothing a while ago, and

that's basically a technique where you get rid of a

bunch of zeros in your DFM by just simply adding

one to every element in your DFM.

This deals with some like weird mathematical stuff that arises

when you have tonnes of zeros.

and so you may want to do that and, and

one of the things you'll do in seminars you'll play

random smoothing, you'll see the difference in the estimates you

get if you smooth and if you don't smooth.

Cool.

Now, let's talk about the the elephant in the room

here.

All right.

So we started with this, in this, I, I basically

just showed you what you would do, assuming you have

a set of label documents and you know the positions

of those label documents, OK.

But like, where do these documents come from and where

do those numbers come from?

Like the positions of the, the label documents, like how

do I know what those positions are?

OK, so first let's talk about like how do you

choose the label documents.

So like how do you choose the set of documents

that you want to use to train your word scores

model, that is to like create the word scores.

So there's a few rules of thumb you should consider.

So first of all, the set of documents you you're

using with the known positions to create the word scores

should Clearly represent, uh, what they're calling an a priority

known dimension, I mean they meaning the LGB uh guys.

um, and the reason for this is that you don't

want to create Word scores and then scale unlabeled text

based on a set of documents you started with that

don't clearly represent the dimension of interest that you're studying.

So, you know, like, if you're trying to uh Measure

the ideological, political, ideological content of um financial reports filed

by companies and stock markets and you think I'm gonna

select these three companies' financial reports and I'm gonna do

word scores and I'm gonna put them on an ideological

spectrum.

Somebody's gonna look at you and say, wait, are you

telling me that the the these financial documents that you're

using to create word scores represent an ideological dimension like

that they capture ideological content.

You can make the argument.

You can say I read them really closely.

Here's all the reasons why I actually do think that

they represent an ideological dimension, but like people are gonna

look at you funny, right?

So the, the labelled documents, they need to actually, you

have to make a plausible argument that they like represent

the scale that you're trying to, to estimate.

OK.

The other thing is the specific documents you you choose

as your to be in your labelled set should be

like extreme versions.

Um, so why?

Well, you want to extract out from them the most

information about the scale of interest and so you in

some sense you want to anchor the scale with the

extreme documents and, and so when you're choosing your label

texts, like choose um.

Uh, uh, texts that are as discriminating as possible, meaning

that the word use is like clearly ideological if if

ideology is your dimension of interest.

Um, the label texts have to have a lot of

words because when you create the word scores from the

label text, you then use those wordscores for your unlabeled

texts.

But if your unlabeled texts are a lot longer and

have a lot more words than your label texts, then

there's going to be a lot of words in your

unlabeled text where there's no words.

And so those words are wasted in the label text.

They're not being used to create the, the document position

of the unlabeled text.

So you want the label text to have as many

words as possible so that you can create as many

word scores as possible that you can then apply to

the, the other, the unlabeled texts.

And then also the unlabeled, I mean it's maybe a

little bit obvious, but like the labelled and the unlabeled

documents have to come from the same lexical universe.

You wouldn't want to create, uh, word scores, so like

ideological scores for each word based on conservative, you know,

on, uh, UK, um, uh, UK, uh, political party manifestos

and then use those words scores to, uh, scale a

bunch of Speeches given by United States members of Congress

because they're not those documents that the UK party manifestos

and the speeches of US members of Congress are not

from the same lexical universe.

Sure, they speak English in both of those countries, but

they, the way the ideological language gets produced in those

two contexts is very different.

And so measuring the word scores in the UK context

is not gonna, is not gonna transport over to the

US context.

Now in this example that we're using here, it's very

clean because the labelled and the unlabeled texts are all

party manifestos from the UK and they're only a few

years apart, right?

So you can plausibly claim that they're from the same

lexical universe.

Um, but again, this is like, you know, this is

kind of like the art of QTA and scaling where

you're persuading your reader that you have chosen labelled texts

that, um, that, uh, uh, are going to be good

for the purpose of doing scaling.

Yeah.

In terms of the like lexical universe, what about different

types of texts in the same context?

So, for example, manifestos versus speeches versus campaign ads versus

social media posts.

Yeah, so I would, my view, OK, so the, the

generic answer that I'm always wanting to give for questions

like this is like, depends on your task.

And if you can persuade the reader that this, that

these things are all from the same lexical unive universe,

then great.

A lot of research is actually persuasion, right?

Like it turns out that like you have to persuade

the reader that what you're doing is valid in some

sense.

So in your, in that context, what I would say

is like, if you're gonna, if you're gonna If you're

going to train word scores using manifestos, for example, and

then you're gonna use them to scale, let's say, social

media posts by the politicians, I would say that like,

first of all, you have to emphasise that you're scaling

text by the politicians, right, because the, the manifestos are

a reflection of like, like professional politicians speak and you

wouldn't want to be then using that to scale social

media posts by like random people in the world.

OK, so that's one thing you would do as an

active persuasion.

Another thing you might say is like, well, look, I'm

using the manifestos from this year, you know, this particular

year.

And then I'm going to use the word scores from

that those manifestos to scale social media posts in that

same year, right?

So you, so that is going to persuade the reader

that like, OK, so those two things aren't that far

apart, right?

But if you were like, I'm going to use 1992

manifestos to create word scores, then I'm going to then

apply to social media posts in 2025, you're somebody's gonna

be like, whoa, whoa, whoa, like people have like the

way people have talked about politics and uh.

The way people talk about politics has changed a lot

since 1992 and so again it's part of the act

of persuasion, I think, um, but that's like the, the,

that's like the qualitative arts of, of like setting up

the research in such a way that the reader is

like, OK, yeah, this, this makes sense as a, as

a, as a choice in terms of research design that

help?

OK.

All right, so, uh, maybe more importantly than choosing the

label text is like, OK, well, what about the labels

themselves?

So, you know, I It's a little bit weird for

me to start by saying how do you choose which

text to be your label text?

Well, the, the, like all this stuff is, is important,

but like sort of the dumb answers like you choose

like the label texts are the ones where you have

labels for them.

That's like usually like the, the way people think about

this.

I guess I want to emphasise this stuff here because

like if you start with a bunch of documents that

are already labelled but they don't meet these criteria then

they're not going to work for your, for your, your

scaling, OK?

So, so this stuff is, is very important, but presumably

you start like the way you decide what texts or

your label texts is like you have labels for them.

What are those labels?

Well, The short answer here is like this stuff is

all made up and so when you are starting with

ideological positions, I mean I'm being a little snarky, it's

not completely made up right like I'm, I'm trying to

be dramatic for effect, but like when you're starting with

some label texts and you want them to be in

some ideological space or some other kind of latent space.

The decision you make about where they're located in that

space is based on you reading the literature and like

finding, for example, expert coding of the ideological position of

these documents that can guide your decision making.

And so, um, LBG, they, they have actually done a

lot of this stuff where they've done a lot of

hand labelling of documents, um, that they then use for

things like Word score and that's like part of the

process is like coming up with the, the, the positions

of the of the labelled or reference text that is

is like it's some sense the hard part right?

because you have to.

Like that's really the art of like persuading the, the

reader that like the original positions you started with are

like valid and and mean something.

Now the nice, OK, so, so if you're thinking about

like, OK, where can I get label texts or what

kind of labels do I, do I want to have,

and by labels here I'm, I'm using classification language but

I'm really talking about the positions on the scale.

You might choose a scale that already exists, right?

So, in our case, we scaled party manifestos, the three

positions for the 1992 documents are based on expert coding.

That means something if you're in that research world, right?

And so it's a good starting point to use because

when you do the scaling on the unlabeled text, it's

on a scale that's interpretable to the experts who work

on this stuff because they're familiar with the expert coding

and so they kind of have a sense of like

what the numbers mean, um, but.

You can always rescale stuff like that's the thing about

scaling.

Like you put it on a different scale if you

want just squishing it together, uh, uh, spreading it out.

So if for example you have two label documents, so

left, right, so you know you're thinking about a context

like the United States where there's two main parties, the

Democratic Party and a Republican Party, and you, you want

to do scaling in that context, well, you only have

2, you only have two positions, right?

Like, I mean, assuming you're not doing overtime stuff or

whatever.

And so, uh.

So why not just say one of them is -1

and one of them is 1.

Does it matter?

No, it's just like, you know, you only have 2

and so it's like, pick 2 numbers and then.

When you do the words, the word score estimation process,

you're gonna get like, you know, stuff on a scale

between -1 and 1, and that's fine if you, if

that, you know, works for you.

Cool.

And then if you need to rescale it to be

on a different scale that somebody else cares about, you

know, some like some American.

Uh, political scientist has created some authoritative scale of political

positions in the United States and you can take your,

your negative 1 to 1 scale and just like, you

know, expand it out or shrink it down and, and

whatever.

But if you got two, it's easy because it's just

you pick.

One number and another number and then everything is between

that.

If you got 3, like this Labour, conservative, Liberal Democrat,

it's more complicated because you have to anchor the two

ends.

That's the easy part, but then like, OK, so labour

conservative, but like where does the Liberal Democratic Party go

here?

Like it actually matters whether they're closer to the labour

Party or closer to the Conservative Party.

So like once you get to 3, it's like, uh,

then you, you have to think about like how far

away the middle one is from the two extremes and

because that means something, right?

It means something if the liberate it like substantively if

we, if the.

Labels for the Liberal Democratic Party are closer to the

Conservative Party or closer to liberal to the Labour Party

that has like a substantive meaning in terms of the

political ideology of that party, right?

So then things become more complicated.

OK.

Oh, so I don't want to dwell on this too

much.

Actually, I think I'm going to skip this because this

is, you should read this.

It's important, but You can link word scores to a

naive Bay model, like the naive Bay classification model that

we saw last week.

And it's actually like pretty straightforward.

So the P matrix thing we got.

That P matrix, each of the columns was saying, all

right, conditional on seeing that word, what is the probability

we're looking at this manifesto?

What is the probability we're looking at this manifesto?

What is the probability we're looking at that manifesto?

Well, that is, what is the probability of seeing document

I given that you're see you're looking at word J?

This is like the naive Bayes formulation that we saw

last week, except last week we weren't, it was, what

is the probability of a document given.

I think it was given something, oh, I, I think

it, what was it exactly?

It's a little bit different because this is at the

word level and naive phase was at the document level,

but it's the same idea which is that uh documents

are can be quantified in terms of the probability of

seeing them conditional on seeing a particular word, OK?

And so.

Starting with this naive Bay concept, you can actually uh

reconceptualize the math behind the P matrix and the word

scores, um, uh, in this very simple way, so you

can calculate the word score, um.

Uh, for each word each word J using this.

So I don't want to dwell on this too much

except to say that like it turns out that uh

I think this was a real, I, I, I'm not

sure about the intellectual history of this, but I think

that there, um, long after Word scores was invented, there

was a, there was a, a realisation by, uh, some

people that the naive Bays, uh, classification model is actually

like kind of the same thing as this, um, expressed

slightly differently and in different mathematical terms, uh, but they're,

uh, these two things are actually quite linked to each

other, uh, and.

In a fundamental way that you can read about in

these two slides.

It's kind of neat.

Everything is the same, you know, it's nothing new.

OK, so now let's switch over to unsupervised scaling, and

this is uh using two methods.

I'll talk in detail about Wordfish and maybe briefly about

Word shoal.

Um, so, uh, Let's shift our mindset a little bit.

So unsupervised scaling, using docket feature matrices, they don't involve

scaling texts with reference to some known positions of, uh,

like documents in a quote unquote training set.

Here you're just getting, you're just like taking a DFM.

Of all your documents and you're, you're just like letting

the data speak for itself and so you're placing all

the documents on some kind of scale based on patterns

in the in the data, like not based on some

predefined preexisting class classification scheme, but it's like based on

the patterns in the data.

So essentially unsupervised scaling.

I just measuring the similarity across documents and so the

documents that are most similar to each other will be

scaled closer to each other in the scale.

And that's like that's the essence of unsupervised scaling methods

and we're going to talk more about similarity and distance

metrics in a more pure sense next week, um, but

in the back of your mind you should be thinking

like unsupervised scaling methods are ways of um placing documents

on a scale based on how similar those documents are

to one another, OK.

So the fundamental problem for unsupervised scaling methods is like

what's the scale, right?

So if you're doing unsupervised scaling, it means you're using

patterns in the data to place documents on a scale

based on how similar they are.

But then the resulting scale you get is like what

is it, you know, like what is this?

It's like a, it's a mathematical contraption, but like does

it have a substantive meaning?

Supervised scaling methods like word scores, it has a substantive

meaning.

Now again, like, you know, we can quibble about where

the positions of the labelled texts come from.

You know, expert coding, whatever, but at least they're rooted

in, they're anchored in some kind of like, like agreed

upon uh scale that others have developed.

Here you're just, you're like letting the data speak for

itself and you're placing things on a scale, but it's

not relative to anything that has like necessarily substantive meaning.

Now it turns out a lot of times when people

do unsupervised.

Methods when they look at the resulting uh scale it

like looks like something that's substantively meaningful and so these

methods are really popular anyway because it's like, well, the

magic of like it turns out that if you try

to scale political texts, they're gonna reveal to you something

like a political dimension.

Why?

Well, because like as your intuition would suggest, like political

documents can be represented on political I mean to the

extent we agree that political ideology is a real thing

like.

It turns out that there just will be patterns in

the data that make it so that the scale looks

interpretable as a um as an ideological scale.

But we should still keep in the back of our

mind, like from a kind of narrow methodological slash mathematical

perspective, like unsupervised methods aren't, they're not.

Like, to the extent that they give you something interpretable,

it's like magic, you know, it's not really magic, but

it's like it's, you're sort of lucky in that sense

because they're like mathematically, like methodologically they're just measuring how

similar documents are, and that doesn't have any inherent substantive

meaning as political ideology or any kind of latent trait

you care about, but it turns out in reality, like

in practise, usually, you know.

If you, if you're smart about the things you're scaling,

then you're gonna get something that's interpretable.

But key point, unlike supervised methods, the validation of the

scale has to happen after the estimation of it.

So you estimate the scale, you get the scale, and

then you got to like sit down, look at it,

read it, look at the documents, kind of make sense

of what the scale is telling you.

And if you're lucky, it's gonna be really obvious.

If you're less lucky, you're gonna have to Engage in

some persuasion as uh as uh academics like to do.

OK.

Um, So this is, these bullet points are kind of

channelling what I'm just saying here, except there's something else

going on in these bullet points that I want to

mention.

Which is that typically, as is the case in a

lot of scaling contexts, you would scale across multiple dimensions

because there's, uh, you know, agreement between documents.

If you're measuring like how much similarity there are across

documents like there could be similarity that is multidimen you

know, like the similarity metric can be multidimensional to put

it in a more sort of narrow methodological uh jargon.

And so a lot of times these unsupervised methods, they

will they will scale on multiple dimensions so like in

the political ideology context like scaling legislators in the United

States, there's like famously uh two major dimensions that have

been scaled in the past like the sort of standard

um uh economic left right dimension and then some kind

of social dimension, um.

I, I, I don't want to get into that too

much here because it's sort of not important.

Here we're going to focus on scaling on a single

dimension for the purposes of like keeping things simple and

like pedagogical ease, but you can do scaling across multiple

dimensions.

So you can, you can scale texts on like how

far left or right are they on the economic dimension,

like economic left versus economic right, how far, uh, well,

left or right are they on the social dimension like

social liberalism versus social conservativeism, so on and so forth.

But OK, uh, OK, so our goal again is unsupervised

scaling of ideological positions.

It could be any latent trait, but we're going to

talk about ideology here.

There's two main approaches to doing this with unsupervised methods.

One is parametric methods, and, and with parametric methods you

model feature occurrence, meaning word occurrence, feature in the sense

of a document feature matrix.

You model feature occurrence according to some distribution you're assuming.

OK, so we're going to use the poisson distribution here

in the context of Wordfish, but like remember we've talked

about models of language before where you conceptualise language generation

as uh as uh draws from some uh probability distribution,

right?

We've used the multinomial in the past.

We're going to use the post on here but parametric

methods are, are, are going to start with this like

model of language, uh, and then, um, and then, uh,

do estimation from there.

Um, and so the When you do, when you take

this, uh, approach, word effects and positional effects are treated

as unobserved parameters that need to be estimated, like in

the classical, you know, statistical estimation sense.

Now, what do I mean by word effects and positional

effects?

Well, there's gonna, as you'll see in the, in the

word fish model, there's two kinds.

Parameters.

There's parameters that relate to the words themselves and the

word use and parameters that relate to the ideological position

of the speaker or the writer or whatever.

And those two types of parameters are going to need

to be estimated.

I mean there's 4 parameters total, but 2 of them

are word effects and 2 of them are positional effects,

um.

And that's what happens in Wordfish and Wordsholl.

There's also non-parametric methods for doing unsupervised scaling.

Um, this is, uh, uh, typically based on singular value

decomposition of a matrix, but we're not going to do

that here.

It's too much.

In previous years, scaling was two different lectures and so

there was like a whole lot on this, but alas,

we only have one lecture this year, so we're gonna,

we're going to skip that.

OK, so let's talk about Wordfish.

So, um, Slain androk introduced this idea of Wordfish, and

it is built on a model of language but not

the same one that you've seen already.

The model of language that we've mostly talked about is

the multinomial model of language.

Here we're going to use a model of language where

we assume that the frequency with which politician I uses

the word J is drawn from Croissant distribution.

And so that's going to be here let's, let's think

about um.

I'm being a little bit loose here, so politician I

is also going to be document I.

Let's just like have each of the documents be from

a different politician, OK?

So I'm going to interchangeably talk about document I versus

politician I, but like in the back of your mind

just assume, OK, so we're we're conceptualising a corpus where

each of the documents.

It is written by a different politician.

And so there's a 1 to 1 relationship between documents

and politicians.

Are we on the same page here?

So the I subscript here is going to be for

politicians and, and documents.

Now we're scaling the documents, OK, so that's like the

whole point of of scaling texts is that we're scaling

the actual documents, but we want to think about them

as observ the text as observable implications of real things

we care about.

So that's why I'm gonna like interchangeably talk about politicians

and documents because like we don't care about the documents

per se, we care about what they represent about the

politicians.

OK.

And then words or like you know, columns of the

DFM is still going to be J.

So The word count used by politician I slash indo

I um for word specifically for word J is going

to be distributed according to a poon distribution with Parameter

lambda, which is actually equal to this, right?

So EXP is E, you know, raised to this exponent,

but this is like the, I don't know, computer science

way to write that.

Um, and so the four main parameters of interest that

we can estimate are these four parameters alpha.

that?

Anybody can correct me if I'm wrong.

Yeah, it's si, right?

I don't use this word very often, so I don't

know.m and then pi or P I guess is what

it would be in Greek, but I'm going to say

pi.

Uh, so these are the four parameters in the model

that we're going to estimate.

And uh alpha I is gonna be the loquaciousness of

politician I.

So this is gonna be a document fixed effect, talking

in sort of regression terms.

OK, so like loquaciousness is like how, how much they

talk, um.

J is going to be the frequency of word J.

So this is gonna be a word fixed effect.

So this is going to account for words that just

like the variation in how much a particular word is.

And you might hope that this absorbs the effect of

like stop words are very commonly used words that don't

discriminate well.

And then uhm J is going to be a discrimination

parameter for word J.

Now we know what discriminating words are.

Discriminating words are words that contain a lot of information

about an underlying categorization, right?

So this is like the fighting words approach we talked

about a few weeks ago, weeks ago.

So one of the parameters of this model we're going

to estimate is actually a discrimination parameter, um.

And then finally, ie I is going to be the

politician's ideological position.

This is the thing that we're interested in.

The whole point of doing scaling is to place documents

or politicians on a particular scale.

So we're going to estimate this model, and these are

the parameters that we're going to estimate.

And that's the one that we ultimately care about because

this is the one that corresponds to the position of

the document slash politician on the scale.

So that's the one we care about, but we're gonna

have to estimate all this stuff together in one model.

Think of these other things as control variables.

Roughly speaking, OK, if if that helps.

OK.

Now, Um, remember the idea behind language models.

So language models encode ideas about the data generating process

for texts.

I won't go into the details again, but you know,

The, you know, the basic, the basic thing of a

language model is you assume that there is some probability

distribution up there that is the, that is the true

description about how language is generated and then individual texts

you see in the world are just samples drawn from

that distribution, right?

And we saw the multinomial language model in the past.

We also talked about how language models encode information about

the way we think language works and with like the

multinomial distribution and then again here with the post distribution,

like we're encoding bag of words.

I mean we're using bag of words in our pre-processing,

meaning we're not, we don't care about the order in

which words appear, we just care about the word counts.

That's like an assumption we're making about how language is

generated that we know is not true, but like we're,

it turns out that it, it, it, I owe you

an email.

It turns out that um it turns out that it

works in a lot of contexts, so it's fine.

Um, models are always simplifications.

Anytime you want to use a model for anything, it's

a simplification and like if, if I could go on

a tangent about how that's like good actually and it's

fine.

Um, but again, like the basic idea is a language

model is a is a statement we're making about how

we think language gets generated, and then we go from

there, OK?

And so why are we not using the multinomial model

again?

I mean, we're we're now using a different language model

which is that word counts are generated by aposon distribution.

That has this parameter lambda which is itself a function

of 4 parameters.

So this is like a different language model than we

had before.

Like why don't we just use the multinomial model before?

The short answer is that's not what the original authors

of this approach used.

So you know, when you're coming up with, and these

are all abstractions, right?

And so like when you're coming up with a new

method for something like you have to conceptualise how you

want to model it in terms of probability terms and

they use a Pason model.

OK, fine, but also like a little bit more substantively,

we want to estimate ideology, right?

We we're trying to scale text on this ideological scale,

so we're trying to estimate the position of text on

this ideological scale and the multinomial model just doesn't have

enough parameters.

It's like not, it's not like it's like underspecified in

the sense that like it doesn't allow us to specifically

pinpoint that particular uh parameter of interest.

Um, also, the Poisson distribution, uh, or the estimation process

around the Posson distribution is easier to deal with, uh,

computationally, um, in this, for this particular context now.

I should back up and say like why are we

using the Poisson distribution specifically?

Well, Poisson distributions are a common way to model count

data and like remember DFM, what it is like is

every row is a document, every column is a particular

word, and every cell is a word count.

So at some level, a DFM is a data set

of counts, and Poisson distributions are very commonly used to

model count data.

So this is like not a weird choice for a

distribution to use.

I mean, if you, if you've taken like advanced regression

classes, you've, you've, I'm sure, uh, run aus on regression

because you had count data.

There's a lot of, I don't know if this was

your experience, but my experience learning intro stats method.

There's a lot of like.

It's very like algorithmic.

It's like you have this kind of data, you do

this kind of regression.

You have this kind of data, you do that kind

of regression.

I don't know if that was like your experience.

Oh, so boring, but whatever.

You probably heard like if you have count data use

to put on regression.

OK.

So and and DFMs are just count data, so it's

a natural choice to use.

Now how do we estimate this model?

OK, so we're going to use conditional, well, not we,

but the, the classical way to estimate this.

We're not going to do this by hand or anything,

so don't worry, is to use conditional maximum maximum likelihood

estimation.

So here's the, here's the basic idea.

Um, let me go back to the Let me go

back to the model here.

OK.

So if we knew.

I, OK, so how do I say this in the

slide?

OK.

So if we know the word parameters, that's this one

and this one, they are indexed by J.

That's how we know the word, the word parameters.

So this is the frequency of word J like the

word fixed effects, and this is the discrimination parameter for

that word.

So how discriminating is that word?

So suppose we knew that.

Then, and so, so we just know what these numbers

are.

We then have a model where we don't know this

and this, and we can estimate a coon regression.

These are both specified at the, at the document level,

OK?

Now if we knew these, So if we knew the

the document fixed effect and we knew the position of

the document on the scale, we can then estimate the

word parameters.

Now we can't estimate these all together because they're at

different levels.

This is for words and this is for documents, OK.

So what you can do though is you can fix

these at some value and estimate these, then fix these

at some value and then estimate these.

And you and you can just iterate that.

You can do it over and over and over again.

So you just start with some values.

So start with like, you know, a good educated guess

about what these numbers would be.

Then you estimate these and then when you get that,

you plug those estimated numbers in here and then you

estimate these and then you plug those estimated numbers in

here and you estimate these and you keep going iterating

over and over and over again.

Until the estimates that come out of every time you're

estimating your person regression converge, meaning that they, that between

each iteration they don't change much.

This is what it means to converge.

So you're sort of getting to a stable.

A numeric estimate for each of the parameters.

OK.

So that's the basic process behind estimation, estimation of this

model, and it's just we're going to do it in

the, we're going to use the function in quantity, OK.

And the function is text model word fish.

There is another way to do it that uses a

MCMC approach.

This is like fancier stuff, but it's, it's more computationally.

Efficient than this.

These are like details that are beyond the scope of

what we're we're dealing with in this class.

OK.

So after you estimate the model, any questions about the

basic process, fix the word parameters at some value, estimate

the document parameters.

Then take those estimates, fix the document parameters, estimate the

word parameters back and forth and back and forth and

back and forth until every time you estimate your number

your estimates aren't changing very much.

The, I mean, obviously devils in the details not changing

very much.

What does that mean?

Well, you know, that's the details I'm not gonna talk

about, but it's, it's intuitive, I think what I mean,

just like your estimates are staying pretty stable.

Now, there will be sometimes when you try, this process

doesn't always work because when you start, you have to

like guess, right?

You have to have a nice starting point, um.

So you like say I'm gonna, I'm gonna assume the

two word parameters are this and this and then I'm

going to estimate the document parameters and then I'm gonna

do the process.

Sometimes the process does not converge.

And then that's a problem and so you have to,

you, it's.

You have to deal with it by starting with a

different original guess or sort of like altering your data

a little bit to ensure convergence, so like dropping really

infrequently used words or something, and but that doesn't happen

very often, but it occasionally does happen, you don't get

convergence, um, and, and then you just like do a

little fetzing with your data until, until it does converge.

OK.

Now after you estimate, you should end up with a

model and it's going to be, it's going to look

like this.

You're going to get an estimated value for alpha, an

estimated value for Pi, an estimated value for mu, and

an estimated value for pi.

The hats mean these are the estimates you've got as

the, you know, at the end process of your um

of your estimation process.

Again, like we don't really care about these variables here.

I mean, you, yeah, I don't know, you, you could

care about them depending on the thing you, your task

at hand, right?

But like if we're trying to scale, this is the

thing we really care about is like the position of

the document on the ideological scale, which is represented by

this estimate here.

And so, um, and we're interpret those as our latent

trait, but, but.

Like I said, unsupervised methods, they will like place documents

on a scale based on similarity between documents, but like

what is it in a substantive sense?

Well, that's to be determined.

So this both the scale and the direction of this

of this pi variable or undetermined, so we got to

identify the model to make it interpretable.

What does this mean?

Well, It means that you have to place this, you

have to place this, these pie estimates on some kind

of scale.

I hope I didn't have any.

OK, no typos great on some sort of scale that

that makes sense.

So there's two directions you can go.

You can do simple normalisation.

So what you would do with that is you would

um Take all your estimated pi.

So these are the ideological positions estimated for each of

your documents, and then you rescale them to have a

mean of zero.

So you shift, shift them to have a mean of

zero and then to have a variance of 1 so

you either compress them or, or um.

Whatever the opposite of like anti-compress them to have to

ensure that the the variance is one.

So that's one thing to do.

This is like simple normalisation because it doesn't do any

um It doesn't do anything about the direction of the

scale, OK, but typically what you want to do is

what's called anchoring where you set a left right direction,

so you think lower numbers mean left wing, higher numbers

mean right wing, uh, by fixing two values of the

pi.

So like think of like the, you can, you know,

whichever documents you think are the most extreme, you set

those values and then you normalise all the remaining pies

to be uh.

Um, on that particular scale.

OK.

So, um, so that's cool.

So then you can get a, you can get estimates

for a bunch of documents on some kind of scale.

That if you're lucky and you look at the, you

look at the scale and you look at the documents,

you think that seems to represent an ideological dimension.

If you're not lucky, it's gonna, well, if your intention

was to measure political ideology and then you get some

kind of ordering of the documents where it's like all

the political parties' documents are mixed together, then it's like,

oh, you did not find, probably did not find political

ideology.

You found something else that's distinguishing whether the documents are

similar or not could be just like differences in Differences

in like rhetorical approaches or something that seem to be

unrelated to party.

So like, you know, hopefully you're lucky and you find

the you you find the dimension you want to find,

but you, you'll find something because some documents will be

more similar than than others.

But what's neat about this um parametric scaling approach is

that, well, I guess it's not really neat, it's just

like some things you can do.

You can do standard statistical inferences, so you can get

standard errors.

That's great, um.

Also, the distributional assumptions you're making about how the data

came to be are explicit.

You're using Pason distribution, and so that's like transparency in

a sense, right?

So people can uh investigate weird things that result from

your estimation process because they know what distribution it came

from, right?

So it's like it, it provides your reading audience with

the ability to engage with your underlying methodology in a

way that um less transparent methods don't do.

Um, because you're using an explicit model of language here,

you can actually make the model more complicated if you

want.

So you can do what you, uh, you can do,

for example, hierarchical reparameterization.

What does this mean?

Well, like, you can add covariants, right?

So you can build a more complicated model to control

for other things if you think that doing so is

going to get you better ideological estimates.

I mean that that model, that model we have is

like is a simple model.

You can add more stuff to it if you think

it's important if you think it's important.

In fact, we're going to see an example where more

stuff is added because of a particular problem that arises

with Wordfish, but I'll get there in a second.

The other thing is it's a generative model.

And so I don't know, people like this about these

kinds of models, but what do I mean by this?

So anytime you're using like a language model like this

and doing estimation from a language model, you get an

estimated model that you can then use to like generate

fake text, right?

So I can take, when I, when I estimate a

model and I get these parameters for my corpus, I

can then go like generate fake texts so I can

generate a, a, a.

A conservative text and a liberal text.

Now of course this model, it's weird.

The text is going to be weird because it's gonna

be, there's not gonna be any ordering to the words

because we don't, we don't care about that.

But when you look at the generated text, they'll be

like, oh, it kind of like looks conservative, look, even

though it's, it's gibberish in some sense because it's not

the order word orders is broken, you'll be like, oh

it does kind of seem conservative, that does seem kind

of uh labour like or whatever, and that's kind of

neat, um.

So yeah, so, yeah.

Would it have like the appropriate parts of speech to

make it comprehensible, or would it just be like kind

of like random, like a random like nouns and verbs

or whatever?

It will just be, I mean, quite literally, it will

be like a random uh sequence of, of words, but

It will be, I mean, it's not, it's not completely

random.

I mean it's random in the sense that you're drawing

from a distribution, but you're drawing from this distribution with

these parameters.

So it's gonna, the word usage.

So if you want to draw a hypothetical document that's

like a conservative document, you would um use the conservative

position and, and, you know, set these other values, and

you can generate a document and you'll look at it

and be like, all these words do look like conservative

party type words, but it's not gonna be texts like.

It's not gonna be, it's just gonna be a bunch

of random words, but you will look at the words

and be like, oh yeah, they seem kind of conservative,

but it's not gonna have like documents, it's not gonna

have like a grammar or syntax or anything because we,

we got rid of all that when we did the

pre-processing to create the document feature matrix.

So it's not the word order and all that stuff

wasn't used to, to train the model.

Now later on, the last few weeks of this class,

3 weeks.

You're gonna talk about models of language where word order

matters like the models underlying uh chat GPT and uh

other large language models that are generating text that is

human readable.

In which case, then, you know, you, the generative model

there is going to produce something that looks like real

text.

The generative model here, the model is too simple.

It doesn't have word order, so you're gonna, the text

is going to be like substantively meaningful in the sense

that the words will like strike you as conservative or,

or liberal words, but they're not gonna be, it's not

gonna be like real text you could use as a

speech or anything because it's going to be all out

of work.

And if you remove stop words too, there's not gonna

be a stop words.

And so it's gonna be, it's gonna be no the,

no uh, and so it's gonna be, yeah.

OK, cool.

But yeah, actually this is, as a, uh, I'm, I'm

sort of like I initially when I said this is

a gene, generative model, I said, oh, people really like

this for some reason.

I was kind of like pooh poohing it, but actually,

like generative models are underlying chat GPT like the whole

point of stuff like.

LGBT is to generate text like on its own.

And so, um, this is a really simple example where

the model is very simple and so the, the language

you get from it is, or the, the hypothetical speech

you get from it is not gonna be like very

nice to read, but like, um, you can model, you

can model text using this model and create like new

speech from it.

All right.

So, um, Yes, OK.

Uh, some reasons why this model is wrong.

OK, so like all language models, we talked about conditional

independence of words is incorrect.

We have the same problem with the multinomial model.

This is a bag of words, right?

We are getting rid of word.

Order when we do all this stuff and that's like

we do it all the time and it's fine and

we're just gonna keep calm and carry on because uh

it seems to work pretty well for a lot of

the things we want to do.

So I, I had to mention this again but I

feel, I felt a little weird mentioning it because it's

like we have talked about this over and over and

over again, but again, the same problem as always we're

not considering word order, so, uh, so there can be

downsides depending on the task that you're doing of not

considering word order.

Um, the more acute thing, the thing that's like special

to this method is that it produces heteroscholastic errors, um,

and so there can be either overdispersion or under dispersion.

From what I understand, this is pretty rare.

This is the thing you care about the most is

overdispersion.

So this, this occurs when informative words tend to cluster

together.

So that's when you get the in overdispersion.

Um, so over dispersion would be that the residuals are

the, the variance of the residuals is like higher than

what it's supposed to be based on the poissant, the

assumption of the poisson, uh, uh, model.

Um, so here's an example of overdispersion in the German

manifesto data from the original paper that developed Word Wordfish.

I don't think this is in the paper, but this

is, this is somebody who got the data and played

with it themselves.

But the key point to notice here is here on

the y axis we have the residuals and then on

the X axis we have word frequency and if if

there was homo homosodatic if homostasticity was satisfied in this

context, we would not see a relationship between word frequency

and the residuals, so the residuals being the difference between

the actual word count and the predicted word count from

the model.

But of course we see a negative relationship.

So as the word count goes up, the um the

res the residuals are declining, but also in addition to

that you can see the um the the variance of

the residuals is decreasing as, as the word count goes

up.

So and this is all this is all a kind

of technical sidebar in.

It does not look like Wordfish produces homosodastic uh errors,

which is what it's supposed to do.

I mean that's what the, the assumptions of the of

like any regression model presume homo schodesticity and, and we're

not getting, we're not getting that.

OK, so what can you do about it?

Well, it turns out that instead of using a poisson

model, you can use a negative binomial model, which is

a generalisation of the Poisson model um and basically here

what you do is, um.

It's almost the same as the Fuson model.

Like if you squint, you see it's like almost the

same.

But what you're allowing for is this R parameter, and

this is going to be a variance inflation parameter that

you're going to allow to vary across documents.

So what do I mean by variance and inflation?

So you're, you're pushing up the variance or, or pushing

it down.

And so when you estimate this, it's going to sort

of force all the residuals to, um, uh, to, uh,

to be the same.

Uh, so the difference in the, sorry, the This thing

all to be the same.

Absolute value of residuals all to be the same.

OK.

So this is like a, this is a technical fix

to deal with the fact that you get uh a

heteroscholastic errors from estimating the word fish model.

Uh, but there's also a claim that this hasn't substantive

interpretation that this RI parameter that you're adding to the

model is like ideological ambiguity of a document, um, so

it's like tacking on an interpretation.

After this technical fix.

OK, great.

So let me talk about an example from, uh, the

original text on here.

So this is using the word fish model to estimate

positions of German, uh, political parties.

Um, and so here, what they're doing is they're estimating

Wordfish over years and going from 1990 to 2005.

And, um, what they do in Wordfish is they.

Estimate ideology in subsets of um the data corresponding to

different dimensions, right?

So this is like a standard left right dimension and

this is a dimension on economic policy.

This is a dimension on societal policy, this is a

dimension of foreign policy.

I'm mentioning this because it's important for what's gonna happen

next.

So they, the word fish model, um, one of the,

the, the issues here is that um.

It turns out that word use Uh, ideological word use

varies across different domains economic policy, foreign policy, social policy,

and if you estimate uh wordish model to extract out

ideological positions, treating all documents as though they're from the

same, you know, lexical universe in some sense, then you're

gonna, you're gonna get kind of like something that's less

interpretable and also kind of pushing everything to the centre.

And so what they do to deal with this is

they say well we're just going to subset and we're

gonna estimate our positions within speeches that have to do

with economic policy, policy within speeches that have to do

with foreign policy, within speeches that have to do with

social societal policy.

And then I'll skip this to get to why this

is important.

So it turns out that Uh, this issue of word

fish needing to be estimated in subsets of data that

correspond to different areas of political speech, um, is a

problem because you may not know ex ante what dimensions

of of political speech you need a subset to to

get valid ideological positions.

So the, the key problem in Wordfish is that the

word discrimination parameters assumed to be constant across different debates,

meaning across different contexts, right?

So economic policy, social policy, whatever, um, which is unrealistic,

so you can think about the word debt.

It might, you know, mean something different in the context

of foreign policy as opposed to economic policy, uh, and

even social policy, um.

So effectively what can happen is if you try to

estimate a word fish model.

While, while just taking all of the, the universe of

political text is like being part of 11 thing and

then estimate a model on it, you might not capture

left right variation, you might just capture your, your, because

at some level unsupervised, unsupervised methods are capturing similarity in

documents.

You might be capturing topic variation and not actual left

right variation, OK?

So you don't want your word, I mean this is,

this goes back to a point that I keep raising

with unsupervised methods is like you're going to get.

Estimates on some kind of scale and then you got

to figure out what the scale is.

And if you, um, if you, the more documents, the

more heterogeneous documents you're including in your training of the

Wordfish model, the less clear the scale you're going to

get from it is going to be, OK?

And so, um.

One of the problems that people have realised in using

Wordfish is exactly this problem of you include a bunch

of political texts that are on a wide range of

topics and it turns out Wordfish just tells you what

the topics are and not the, not the left right

position on that topic.

OK, so Word Shoal is a model proposed by Lauderdale

and Herzog that um Basically builds a hierarchical model that

allows each of the debates we say like each like

area of politics to um have uh debate specific ideal

points, right?

So we're creating more flexible model that's allowing for both

ideal points that are debates specific or like area specific

economic policy, social policy, so on and so forth, but

then also overall uh positions.

And the way they do it is they build a

postson model, but it looks like the postson model from

above.

That's what this is, except now here we're, we're allowing

the word counts are being drawn for a specific document,

a specific word in a specific debate.

So K is going to be like the subset of

the data that corresponds to, for example, economic policy or,

or, you know, uh, social policy or whatever.

And so.

We're going to have ideal points that are specific to

a politician I or to a document I within a

specific debate K and then that is going to be

modelled in a hierarchical sense as being drawn from a

normal distribution that is a function of the politician's overall

ideological position plus some stuff that's specific to the debate

at hand.

So it's a way of trying to incorporate into the

model the idea that Ideological positions can vary across debates.

So it's going to be directly in the model and

then when you estimate the model, you're gonna estimate, you're

gonna estimate ideal points that are specific to the debate

as well as ideal points that are uh overall for

uh the particular politician.

Um, so this in some sense is like a cleaner,

a cleaner way of doing what the original Wordfish people

did, which is they just like qualitatively decided to split

up their, their data and they looked at these different

areas to try to solve this problem, but they did

it in this kind of like, this like more qualitative

way.

This is, uh, OK, well, let's build this idea into

the model and like let the, let and like let's

estimate this.

Let's not just like hope we get it right, that

we split up economic policy, social policy, whatever, and like

let's let the model itself figure out.

How these, um, uh, how these things should be weighted.

OK.

Um, so there's, yeah, whatever.

Uh, let me show you what happens when you do

this.

So one of the nice things that happens when you

estimate this more complicated model and you allow for more

debate specific, uh, when you allow for debate specific ideal

points is that you can start to look at individual

debates as being more or less polarising, meaning more or

less ideological, right?

So in the word fish.

Approach.

You don't have the ability to treat debates as uh

they're all like if you dump them all into the

same model, you're, they're all treated the same, right?

But here, you get this kappa subscript K parameter that

tells you how, um, uh, polarising debates are.

I think this is like a discrimination parameter on the

debate itself.

So that's another way of saying like, this is giving

you a metric for like How ideological this particular debate

was.

So these are like the 5 most polarising.

These are like the five least polarising in their analysis.

And so for example, uh, this is a uh a

debate about social welfare and pensions.

This is a contested area of policy in most, uh,

sort of, um.

Most countries, um, especially European countries, so you can imagine

it makes sense that this is an area with a

lot of political polarisation.

It is a high, this is a debate where the,

the ideological dimension is very strong.

Because right wing parties and left wing parties deeply disagree

about how to structure social welfare and pensions.

However, this debate down here about cancer services reports not

very polarising because it turns out that parties on the

left and parties on the right don't really disagree that

like cancer is bad and and that, you know, cancer

research and cancer treatment should be funded, right?

So this is like not very polarising.

Because it's not a topic where the left right, um,

ideological dimension is activated.

Um, you know, and then private members business mortgage arrears,

and these are like these are boring like non-ideological stuff

where this stuff is like ideological, right?

And then confidence in the governments like that is obviously

ideological, right?

Like the opposition is gonna, is gonna have less confidence

in the government or gonna vote against, uh, uh, uh,

vote against the government than members of the government are.

So you can imagine that being ideologically polarising as well,

yeah.

But so then I would be like if I were

doing this analysis, I would be picking all of the

case myself and having that as a feature in my

data, right?

Like each document is part of the debate and I'm

specifying what each document's gonna be like the speech of

the politician within a, yeah, and the debate is like,

yeah, you're categorising the speeches into larger categories depending on

what the like, yeah, when.

Speech but you each like each row is going to

be an utterance, right?

So like person one says this and the person two

says that person one says, you know, so on that

will have like like, yeah, which actual on what date

in which particular session did it occur?

I mean, you can figure out how you want to

lump it together.

You can lump it together by parliament or you wouldn't

want to do that.

It wouldn't be, it wouldn't be informative enough, but here

it's by a specific debate.

OK, yeah.

OK, um, and then one of the nice things from

this is you can get a whole, if you read

the original paper, you'll see there's like a whole bunch

of stuff.

I mean, look, this model has a bunch of quantities

to estimate that you can use to create interesting things.

So for example, I just showed you one here, which

is like how polarising debates are.

So that's like 11 of the parameters of the model

that you can estimate is like this thing that has

a substantive meaning that's kind of interesting.

You look as well.

This isn't notice nothing about this is like actually measuring

any ideal points.

You know the whole point of this is doing scaling,

sorry, when I said measuring ideal points, I mean measuring

positions on the scale.

Um, the whole point of the whole lecture is that

we're doing scaling, but I'm not even showing you any

scales here.

I'm just showing you this other random parameter from the

model, which is like how polarising the particular debate is.

And then within each debate I can look at the

ideological.

Uh, positions, um, but the nice thing about a, a,

a big model like this is like you get a

bunch of parameters here that have that will end up

having substantive meaning, uh, and so, so let me just,

I keep skipping this slide.

This is a validation exercise.

So they're, they're correlating the results from their word shoal

model with the results from.

Uh, other approaches, so I don't know why it's even

in here.

It's, I don't want to talk about it because it's

not that important.

Um, but I want to show you some of the

stuff you can get out of this model.

OK, so this, so you can get out the, the

polarisation of each debate, but then also you can get

out what they call word loading.

So they, this is defined as being association of a

word with a with the general scale across debates.

So, um, the general scale being like the, the sort

of overall because remember you can estimate debate specific scales

and then you can estimate uh overall scale, the overall

scale being the general scale, um, and so here we

have um.

Uh, the word loading over time from the US context.

So this is debates in, in, in the US Congress,

and then you can see from 95, so this is

the 9596 Congress up until the 2013 to 2014 Congress,

and you can see for specific words how the word

loading changes over time.

So Effectively you can calculate this thing.

This is a thing you can calculate from the parameters

of the model.

Let's not talk about the technical details, but what you're

looking at here is how word use is being, is

polarising over time or becoming more polarised or less polarised

polarised over time.

So for example, the word tax, you'll see here that

this is the Dotted red line, I guess.

So over time starting in 1995, this is kind of

around the middle, meaning it's not particularly loaded in a

in a uh ideological sense.

But then you get to to this Congress.

This is the 111th Congress, and then this becomes a

very loaded word in this sense becauses very Right wing,

right, so this indicates a um a very right wing

orientation and if you're old enough to remember what is

going on in politics in the US at this time,

this is when, uh, the Obama administration was pushing this

uh massive health care uh law called the Affordable Care

Act, um, and one of the pieces of this act

was, um, a tax, uh, for people who don't, um,

for, for people who choose not to buy health insurance.

By the way, in the United States, uh, as a

private healthcare system for the most part and so people

choose whether to have health, I choose whether to have

health insurance, and one of the debates around this law

was like whether there should be a tax for people

who choose not to buy health insurance for like technical

boring reasons we don't need to talk about.

So this becomes a highly polarised word.

This is also the rise of a of a movement

in the United States called the Tea Party movement.

It happened around the same time, which was like a

a a very um strong kind of anti-taxation right wing

movement.

On the other end here, the word pre-exist becomes very

loaded on the left, so this is left, this is

right.

And um one of the key provisions of this new

law, this healthcare law, was that it was going to

become illegal for insurance companies to not provide insurance on

the basis of pre-existing conditions.

Um, which used to be legal before, uh, 2009 in

the United States, uh, so the idea being that if

you had some pre-existing condition, you had to switch insurance

providers, they could say no.

Um, so this became a really big talking point for

members of, uh, the Democratic Party who are, uh, uh,

proponents of this legislation.

And so it this word, which is kind of like

a weird word, like when would the word pre like

would you think intuitively preexist would be an ideological word?

Well, not usually, it usually is not, right?

But then all of a sudden in, in the context

of this big debate over this massive piece of legislation,

it becomes an ideologically loaded word.

And so this, um, word loading, you can calculate using

the parameters of the model and you in this is

actually quite nice validation because you can, I mean, I'm

telling you the story about what's happening in this Congress

because I, I know it and it's kind of nice

that you see this in the data because it gives

you some reassurance that the that the statistical contraption that

you're estimating here is actually giving you something substantively meaningful.

So this is what we would call validation, you know,

I'm looking at it and saying, well, this, this does

appear to, to make sense, um.

And of course they're only when they're showing you this

plot, they're only showing you a few different words, right?

You could do this.

I mean, there's obviously thousands of words, right, uh, in

any given document feature matrix.

And so you could do this with other words, and

I would imagine that around this time, this is like

September 11th you will probably see different kinds of word

loadings related to national security stuff, um, then, uh, uh,

but this is, you know, this is an illustration of

these handful of words that are showing us something about

what's going on during the first Obama or first two

years of the Obama administration.

OK, cool.

That's what I have to say.

I will see you next week.

And next week we're gonna do, uh, we're gonna do

clustering and text similarity metrics.

And then, um, after that you get topic models and

that'll be the end of like the standard text analysis

portion of this course.

Then you're going to get like the new stuff, you

know, like what's going on with large language models and

uh.

Besides, you know, taking all of our jobs, said that.

Getty I No, yeah.

Uh, I mean, I literally like maybe like I I

I I love it when I see someone referencing.

everyone.

It doesn't work and he gives you.

I.

But surely just picking up noise at that unless there's.

Solving equation.

I'm not sure what the math is just like how

it works.

I'm just wondering, like with the preexisting there is some

like you know that there is there is an estimated

model.

In the abstract and you're trying to find, right?

So, um, when you start by guessing to, I mean,

you start by guessing the first two parameters, estimating and

then iterating, uh, from there, you are when you guess,

those aren't right, right?

And so you're going to estimate a model for the

other two parameters that is not going to be correct

for those two problems because you, the parameters you assumed

are not correct.

So then, um, but you, but the estimation process is

extracting information, right?

So it is gonna be like pushing you in the

right direction, um, but it's still not gonna be, it's

not gonna like be at the final place it should

be because you, you have primers model that are not

that are not correct based on the like what is

in the data.

And so then, um, so then when you so so

you get those estimates, you put them in and then

you uh you uh estimate these, these first two.

Well, those, those.

The estimates you got weren't fully correct.

Yeah, they hadn't been optimised yet so they're also so

then you're estimating these two based on incorrect parameters and

then, but the idea is that over time there are

genuine patterns in the data that are pushing your estimates

slowly towards um towards the correct estimate.

I think of this aspects you just choose two random

coefficients in your mobile and then you.

estimate the other models, the coefficients, and now you use

those coefficient and you do this or you would get

if you yeah yeah so you kind of optimise the

I'm gonna do the same with the same unit of

analysis typically in like but you could do it in

principle.

It would be the same idea of like random.

And I just like I have very specific memories of

my mother like on the Delaware Memorial Bridge like trying

to get over the same line with incorrect ate and

then you could do the iterative process and actually it's

just if I can come up with an exercise for

seminar.

It's a good like, like regression is a thing that

most people have in tuition for so it would be

good to show it in that context, but you don't

need to do it for regression because of, um, because

they're all in the same unit of analysis and also

like simple linear regression is is computationally extremely easy so

you wouldn't need to do this kind of iterative process,

so when we look at.

ideological it said that the most left-wing word in the

manifesto and I understand why you would want that to

go but I mean unless I'm missing something.

Uh, no, I don't know if it's noise and data,

but it's like definitely a, it's a an issue with

this particular approach because you are assuming any of the

words in the documents, especially you don't really do any

preprocessing to get rid of words, you're assuming they all

have.

And so if it happens to be the case, for

example, I don't know, for whatever reason, the labour Party

the introduction to theories.

Oh Don't go on it.

I think they took.

solo solo they edit uh reproduction and and you play.

Good.

Well, it make it.

Are you sure?

you one Yeah you.

That were involved also.

What.

I forget He Um You watch Well even so.

You so she didn't she.

like Yeah Yeah.

she That And Don't send it.

And She For you

Lecture 6:

Yeah.

00, I think you mentioned that you were gonna.

I was really.

You must be Oh.

you put your and my.

You have some siblings out there.

Yeah, so my partners.

That's such a nice place Yeah, oh my gosh, it

was sunny, it was funny.

I love Sand, like it's been one.

Or the things so how old is your niece?

She's 2.

That's a good age.

It's the best age.

It's very nice.

He's like old enough to.

Talk and kind of like joke around.

She has like a personality.

It's like it's cooking, she's cooking up there.

Oh really, I bet they're close to the beach too.

I mean everyone else, but oh yeah, it's like 10

minutes and you can just like.

Yeah, yeah, so nice.

Uh, you said San Diego, San, San Diego.

That that's good.

It's good.

I was in Paris with the ins.

Oh my gosh, yes, how was that?

It was lovely, you know, like, yeah, yeah, yeah, yeah.

Don't.

Now there's a couple of times I was like, hey,

like I can give you to.

Buck up for me a little bit more, and he

was like heard like.

Uh, it's because they're pain was so nice, but I

was like.

It.

Really, but it was, it was also like positive.

I think it was funny like going with people.

I don't know how much they travel, but it's like

in states like it's like in Versailles.

Yeah.

There was a lot of like I like this isn't

really my style.

I'm more like New England coastal shape or style and

I was like.

Vail Yes, I did.

I went to William and Mary.

Oh, you did.

You're familiar with the brick better than.

No, don't tell me it truthfully worse than like it

to be like if I'm designing my home, how am

I gonna.

To be like in Versailles and people like not my

vibe because truly Williamsburg is the single most I've ever.

Yeah, and I for some reason.

It's, it's like, hey, do you guys wanna go to

the one bar?

What about the one restaurant like it sounds ugly.

My friend is a professor there.

One year and she was like, I think I.

And I might die.

It's devastated.

My um postdoc, who was my research advisor, he was

always like.

I hate it here.

What can I be?

No, he's this little African man.

He was like, this is terrible.

This is worse than one thing I've ever like I

wanna go back to Ethiopia.

It's He's like, I want to go back and live.

No running water.

whatever this is, now that.

I, I've seen the campus.

I, I mean it was pretty, but I don't know,

like living.

It's colonial Rural Virginia.

Oh I go, it's like an hour from Richmond and

an hour from.

Civilization.

Yeah, we have, we have a target and tortures.

That's nice.

Yeah, sounds like.

No it's true.

I describe it's like a suburb of nothing.

No, but it's like a suburb, but there's, there's no

city to go to.

So, it's, it's a tough place, but I do you

know where your partner's family is from, or did they

just, they're from like Greenwich.

So they don't even live in Williamsburg and then Sunday.

I don't know if it was like a joke.

Like I, I was like, Pardon me, maybe this is

a joke, you know, that that kept coming up.

And my, my partner loves calling and I'm like, you

know you did, um, um, that's a weird well it's

like where we met and stuff, but.

There's no microphone, so.

Because There's, isn't there a phone there?

Oh yeah.

Oh yeah, nice landline?

OK.

Oh, was it good?

It's fantastic.

Here's the thing.

It's like music and a little bit of plot, you

know.

There is no plot.

I, there is a plot.

She was like, I want more plot.

Hi, um, he's awesome.

Do you watch the Oscars?

It's um uh Clements 3.0.

I was like, I'm good.

I was like I'm the Oscar for that.

The cupboard, um, there's a lot, it's.

I thought it was just yeah.

It's also that's not it's a door, the door.

Um Thank you.

My parents told me they tried to watch it.

Does anybody see the microphone anywhere underneath the tape and

floor, chair?

No, OK.

OK, well.

OK OK, um, we'll go without a microphone for now.

Uh, so, uh, welcome to week 7.

I hope you had a relaxing, reading week, um.

So this is the last week that I will be

lecturing, and then next week uh you'll start getting freere

and and so the course is gonna kind of transition

away from what you might call classical text analysis, like

the stuff that we've been covering it's like, you know,

it's in the textbook, it's been widely used across a

range of research applications, uh, and you're gonna, you're gonna,

you're gonna get a little bit of that next week.

You get topic models, which are also widely used in

the literature and covered in the textbook, um.

But then, uh, Friedrich is going to transition and uh

get to the, uh, by the end of the course,

you get to the point where you'll have a kind

of uh a little bit of an overview of large

language models.

Um, we also talk about word embeddings or sort of.

I, I wouldn't say that this is like new areas

of research but uh newly um used in political science

and then And social sciences.

OK, cool.

So this week, uh, is actually pretty short and simple

week.

So this week is about uh text similarity and clustering.

So we're going to cover a few different things.

So first, I want to talk about something very simple,

string similarity, um, and I'll explain why we're going to

talk about that in a second, but that's just one

slide.

And then I want to introduce the vector space model

of language.

So so far when we've talked about models of language,

they've all been probabilistic, um, but now I want to

talk about a model of language that relies on results

from linear algebra, um, and this is uh.

Metro space mode language and that's useful because uh we're

gonna be talking about distance metrics and similarity metrics which

which um rely on ideas from linear algebra and vector

spaces and then we'll end by talking about the clustering

methods, OK.

The string similarity.

So, um, you might want to know how similar two

strings are, uh, to one another, and a way to

measure that is something called edit distance.

And so the edit distance is basically the number of

operations that's needed to transform one string into another string.

So this is really useful in context where you want

to measure text reuse, um, so for example when application

of this is, uh, detecting plagiarism, so I mean obviously

you're in an academic setting, so you know about plagiarism,

but plagiarism detection software has been used for all sorts

of research uh applications as well, trying to see how

often, for example, political actors reuse like soft language that

they might get from activist groups or from each other.

So there are uh oh and then also the uh

another example from the area research that I'm in where

plagiarism detection is used to try to see how often

courts in their opinions incorporate language like verbatim from the

legal briefs that they get from the parties who are

uh uh filing legal briefs in cases, um, so.

So, I'm gonna talk about edit distance in the context

of trying to compare two strings to see how many

transformations you would need to turn one string into another

string.

You can also think of edit distance in a in

a sort of broader way than thinking about a collection

of vectors, right?

So documents we've been conceiving as a conceiving of as

collections of vectors.

And so you can also think of edit distance as

the number of transformations to a vector that would turn

that vector into another vector.

But I just want to talk about it in the

simplest setting here, which is just two strings, how many

transformations you would have to do to turn one string

into another string.

So the most common edit distance metrics is called the

Lebinstein distance.

Um, so I'll just show you an example of Lewinstein

distance.

So, so the Lebenstein distance between two strings, kitten and

sitting is 3.

Why?

Well, because there's 3 transformations you have to make to

get from the word kitten to the word sitting.

So the first transformation is to turn the K into

an S, so you get from kitten to to sitting.

Then the next transformation is to turnitin into sitin with

an I instead of an E, and then the next

transformation is to turn sit with no G into sitting

with a G.

So there's 3 transformations you have to do, of course,

you don't have to do them in that.

Or that you don't, you could do, you could do

the, the EI transit.

I mean this isn't, you don't have to use this

particular order, but there are 3 things you need to

do to get from kittens to sitting.

OK.

Now, you can also calculate Lewinstein distance.

Uh, between two strings in R and base are actually

with this function a disc, um, it's quite nice.

So if you, if you in R use a disc

and then you put kitten and sitting, it's gonna return

3 because the 19 distance between those two strings is

3.

you can also use this function for Measuring the distance

between vectors.

Now why might you want to do that?

Well, suppose you have full texts, so you're not thinking

about like just individual words or short strings to get

full text that you've tokenized into a collection of tokens.

You might want to know um how many of those

tokens you would need to modify to get from one

vector to another.

That would be more.

That would be closer to what you would be doing

to do like plagiarism detection type stuff where you look

at a large text and you're seeing how many of

the words would I need to change.

Now, it's a little more complicated than that because we

can use lenstein distance for vectors, but for plagiarism detection

type tasks more specifically, you have to think a lot

about word order and um.

You want to detect sort of subtle changes that um

still would be consistent with plagiarism overall but that are

um that are uh a little more subtle to detect

than just like the simple kind of algorithmic transformation.

OK, that's all I, that's all I'm gonna say about

edit distance.

It's pretty simple and um.

With, uh, with that function in R you can calculate

the edit distance of, of strings and I encourage you

to do that.

It's kind of, kind of fun, um.

Now, I want to, we're gonna talk about comparing documents

and so in order to do that, I need to

introduce this vector space model of language.

So we've already seen one way to model language using

probabilistic models.

So for example, we've conceptualised in a mathematical sense language

as being um uh as consisting of, of draws from

either a multinomial or uh plus on distribution depending on

the particular um model that we're looking at.

But the, the bigger picture is that we've conceptualised language

as being draws from distributions, the random draws so there's

a stochastic probabilistic element to it.

OK.

Another common way to model language language uses ideas from

linear algebra.

So the basic idea is that a document is mathematically

represented as a vector in a J dimensional uh vector

space.

So, um, Normally, if you have a document feature matrix

W capital bold W, which is the standard notation we've

been using all of the term, then in the vector

space model of language, you can think of each document

I as being a vector itself and in our bag

of words world.

an email, uh, in our bag of words world, um,

these vectors are gonna be vectors of word counts.

OK, so this conceptualization of documents as being vectors in

a vector space is not new to us.

We've been using it already all term, right?

We've already talked about what about document term matrix is.

We've already repeatedly talked about the idea that a document

can be thought of as a row in a document

feature matrix, which is just a vector of word counts.

So we're already thinking of documents as vectors in a

vector space.

But we haven't used the ideas from linear algebra to

exploit the fact that we're treating documents as vectors and

vector spaces.

We've, uh, mostly kind of ignored that for the moment,

but we're gonna pay attention to it now, um, for

some reasons, uh, that are on the slide.

So we're now interested in thinking about how similar documents

are to one another.

And so we're going to use the vector space model

to do that because vectors are geometric representations.

If you think about a two dimensional space, it's the

Cartesian plane, right?

So you have an X-axis and the y axis, and

you can represent lines and points on this plane, and

those lines and points have distances from one another.

Um, and so you sort of have this intuition that

when you think about vectors and vector spaces that there's

a kind of geometric representation which lends itself to measuring

things like distance and similarity, OK?

So in, in the vector space model of language where

we're using linear algebraic tools, the closeness of documents, how

similar they are, for example, which we have, we know

substantively what that means, has a mathematical meaning.

Just like measure the distance between points.

Or measure the angle between vectors.

OK, so these are two different ways of comparing documents

and in fact those are the two different ways that

we're going to compare documents.

We're going to compare documents.

I think about how far apart they are, but the

distance between them, as well as how directionally similar they

are.

And if you think about documents as being vectors pointing

in different directions, OK.

So here in this lecture, we're gonna be focused on

comparisons across documents.

The point of this lecture is to to ask how

similar are documents to one another.

But these ideas can be used also, this sort of

vector representation model of language can be used for characterising

features within documents as well.

So you can, um, uh.

Uh, well, actually, in coming weeks we're going to talk

about this in more detail, but word embeddings are a

way, way of using the vector space model of language

to try and characterise features of documents.

So rather than focusing on documents as being Um, uh,

vectors of word counts where each word is its own

little thing.

You can conceptualise documents as being made up of a

bunch of words which are not words per se, but

they're sort of vector representations of combinations of words, put

it in a very rough in rough sense, but Friedrich

is going to cover this in more detail.

But so in this class, key thing to keep in

mind is we're, we're still just treating documents as collections

of words and we're going to try to look at

the similarity between the documents.

So we're using the vector space model stuff for uh

inter-document comparisons, but you might use this model also to

come up with the features inside of a specific document

as well.

Cool.

Uh, so I want to just jump into an example

to show you how you might use this model in

a kind of real world, well, hypothetical real world, real

world setting.

So let's think of a simple example with 3 documents,

favourite example with the 2 tokens available, cat and dog.

Um, so document number 1 is cat dog dog.

document number 2 is dog, cat, cat, cat, cat, cat.

Document number 3 is dog, dog, cat, dog, cat, dog.

OK, this is obviously just fake text, right?

Of course, because I want to have an example that

only has 2 tokens in it just to make things

simple, because I want to have a two dimensional space

because I can draw it on a slide.

OK.

So these are the three hypothetical documents we have.

If you turn each of these documents into a vector

of word counts and then construct the document feature matrix,

you'll get something that looks like this.

So document, so if you want to have the first

column B cat and the second column B dog, document

one is the vector 12, because there's one instance of

cat and 2 instances of dog.

Document number 2 is the vector 51, because there's 5

instances of cat and 1 instance of dog.

And then document number 3 is the vector 24 because

there's 2 instances of cat and then 4 instances of

dog.

Cool, OK, so this is just creating a document feature

matrix from text.

So I can write out these vectors.

I can represent each of these documents as vectors.

So, B capital W.

It's bold because it's indicating it's a vector.

Um, it's W because we've been using W all term

to Best notation for document feature matrix and then I'm

indexing it, so I'm putting the subscript here based on

which document we're talking about.

So W1 is document 1, W2 is document 2, W3

is document 3, and I'm writing out each of these

documents in vector format, word count vector format.

OK, so you've seen this stuff before.

I'm also colouring these differently because I'm gonna have a

plot in the next slide colouring the vectors differently and

so I'm gonna try to keep the colouring consistent, but

that's just, that's just, you know, lecture presentation stuff.

You don't actually have the colour here.

Mathematical notation in real life.

OK, so here is how you might Uh, visually, geometrically

represent these three words which are just vectors in a

vector space.

And the way we do it, because, so this is

a, this is gonna be a two-dimensional vector space.

Why?

Because there's only two words in a document feature matrix.

Anytime we're talking about vector spaces, the number of dimensions

is going to be equal to the number of columns

in the matrix.

OK.

So the matrix that we're considering here has two columns

in it, so the.

The space we're considering is going to have two dimensions.

This is quite nice because I can draw on a,

on a flat slide.

If we had 3 tokens in our vocabulary, so we

have a 3 dimensional document feature matrix, we'd have to

have another, um, another, uh, axis coming out of the

screen here because we'd have to have a 3 dimensional

space.

But I know, I want to keep things simple for

now and so we're just going to represent 3 documents

in this 2 dimensional space.

And so, for example, document one is the vector 12,

so 12 is document one.

So I've illustrated document one here as an arrow that

goes to that point, ends at that point.

The documents, um, document 2 was the vector 51.

So this document 2, so 123451, and so the, the

document can be represented as an arrow going from the

origin 002.

The vector, which is 51, and then the same thing

for um document three which is represented in the document

feature matrix is the vector 24 121234.

So uh document 3 is that vector.

Cool, great.

Now, the core question here, so this is just, I'm

just showing you geometrically what this looks like, and obviously

you can already start to see two things.

One, you can visually notice that some of these documents

are longer than others.

I should say have greater magnitude than others because longer

I don't mean it like in the total number of

words, although these two things are related.

I mean it in the like lengths of the vector,

OK?

You can also see that the vectors point in different

directions.

These two vectors are pointing in that direction, and this

vectors pointing in that direction.

OK.

So that those two things you can visually see are

gonna be the core ideas behind distance matrix, uh, distance,

distance metrics, which is the absolute separation between points and

similarity, which is the directional closeness of vectors.

So how, how close the direction are they?

And then finally this little 3rd thing out here which

is going to be the kind of climax of this

lecture is clustering methods.

clustering methods are methods where you identify groups of similar

documents using in our case, distance metrics.

But you can imagine other kinds of clustering methods.

You can imagine an infinite number of different ways of

clustering documents where you, you categorise documents into clusters, into

groups based on how close they are, but how close

they are is up to you to decide which metric

you're going to use to determine that, OK?

But clustering is using these ideas of close of document

closeness and then creating uh from the data of uh

clusters of similar documents.

OK, cool.

So let's talk about distance, distance metrics first.

I, I should point out that like, this is kind

of the point of this lecture is the clustering stuff,

because this is like in real, in the real world

applications of QTA this is like what people are doing.

And people are doing this too in in sort of

simpler senses, but this is like a plumbing for this,

right, that this is like the underlying foundational ideas that

are useful for thinking about uh clustering in a more

concrete way.

When you say clustering, identifying groups that like similar documents,

but you mean similar in terms of distance or similar

in terms of similarity.

We're gonna do distance, yeah, it's a little OK, so

let me back up and say um.

When I was writing these slides, I was struggling to

figure out exactly how to use, how to like talk

about this stuff because.

Uh, the words are, like we're talking about tech similarity

in this lecture, and I'm using that phrase in a

very colloquial sense, a very general sort of abstract sense.

I'm not using it in a specific sense of text

similarity metrics.

So when we're talking in more narrow jargony like technical

terms, I want to focus on distance metrics and similarity

metrics as two concrete mathematical ideas.

And in fact, I probably should have put distance metric

here and similarity metric to indicate that we're when we're

talking about distance metrics and similarity metrics, we're talking about

specific mathematical ideas.

But then when I'm just sort of talking generally about

the similarity between documents, I'm, I'm talking about how close

they are and then there we have to make a

decision.

Do we Do we define closeness as being about the

absolute distance between points or do we find closeness as

being about the directional closeness, and that's a choice you

have to make.

When you're trying to assess how similar in the generic

sense documents are in clustering at some level is creating

groups based on a determination you make about what counts

as similar documents and we're gonna talk about K means

clustering and hierarchical clustering, both of which use uh uh

distance metrics to to determine uh what closeness.

OK, so let's talk about distance metrics.

OK, so I'm gonna give you uh examples of distances

between vectors in this uh hypothetical example.

So they're illustrating great here.

The colours don't really show up very well here, but

the distance, the, uh, distance between W document 3 and

document 2.

You might imagine by looking at the, the visualisation here

is just like that length, like that's the, I mean

that intuitively that's what I would think is like the

distance between those two vectors is like the distance from

here to here.

OK.

So that's one example of how you might think about

the distance between these two documents.

Another example so and then you also might think about

the distance between this document and this document as this,

as this uh length as well, OK?

Now, I'm highlighting examples here because this is just like

one way to think about distance.

It's like the most intuitive way, I think you look

at it, you're like, yeah, the distance between that point

and that point is just that, OK?

Um, but this is called Euclidean distance.

Um, it's basically, you know, like in a two-dimensional space,

Euclidean distance is just like the line that the straight

line that connects the two points is the Euclidean distance.

Um, and I illustrated in that slide the Euclidean distance

between docks 1 and 3.

documents 2 and 3.

Now, in a two-dimensional space, Euclidean, I mean this to

be a little bit more specific in a two dimensional

space, the Euclidean distance is based on the Pythagorean theorem

which you might remember from like high school math, right?

You think about the right triangles and how you calculate

the potenies and stuff.

That's, that's where Euclidean, I mean, that's where the formula

for Euclidean distance comes from, or, or I should say

the Pythagoan theorem is just the formula for Euclidean distance

expressed in a slightly different way in a two dimensional

space.

But more generally, if you're beyond a two-dimensional space, if

you have two documents, say document A and document B,

so it's just two general documents and have J features,

so that's like J tokens in the document feature matrix

or J features, columns.

Then you can calculate Euclidean distance with this formula.

So for every word, so every feature J, you subtract

document A's word count.

Oh, you have it, great.

Thank you.

Yeah Cool.

Thank you.

All right, so for every feature J, you calculate the

difference in the word count for that feature J for

document A and document B.

So if you imagine A and B is two rows

in document feature matrix.

What you do is you go column by column and

you take the difference between A and B, difference between

A and B, difference between A and B, each of

those word counts.

You square each of those differences and then you sum

them all up.

OK, so that's how you're going to get the Euclidean

distance.

Now, this is just notation.

Um, Distance metrics are often indicated are often notated this

way with a little D here, and I put a

subscript E because we're talking about the Euclidean distance specifically.

That's one way of measuring distance, but little D is

often the the like notation people use for distance metrics,

and then inside the parenthesis you put the like the

two objects you're measuring the distance between, OK, so document

A and document B.

For Euclidean distance specifically, there's special notation.

This is the special notation for indicating Euclidean distance.

It's the double absolute value bars, and then inside of

it you're measuring the distance between document A and B,

so you just subtract the two from each other and

surround it with the double absolute value bar.

So when you see this, you should say, oh, this

is Euclidean distance between that and that.

OK, and this is how you actually calculate it if

you want to get, you got your calculator out and

you start to calculate things, you choose.

OK, so if we have document 2 and document 3,

document 2 being vector 51 and document 3 being vector

24, what we do is we subtract that from that,

square it, subtract that from that.

No, wait, subtract that from that, square it, and then

take the square root of that whole thing, and that

gives you approximately 4.24.

Now you'll probably like if again, if you want to

think about the Pythagorean theorem.

You You'll, you'll, you can look at this and see,

oh, this is like calculating the length of the hypotenuse.

Is that what it's called the hypotenuse is the, is

the, yeah, OK.

I always forget it's like, what are the other two

called in the right triangle.

I just call them legs.

I don't know what they're actually called hypotenuse.

OK, so you have a right triangle like this, that

thing is the hypotenuse.

Oh, OK, anyway.

Um, now, I've kind of been emphasising that the Euclidean

distance is just one way to measure distance between two

points or two vectors.

There are other ways to do it.

A very common other way to do it is what's

called the Manhattan or taxicab distance.

And that is a distance metric that requires that you

can only travel in straight lines and, and at right

angles, sorry, you can only travel at right angles.

So, um, the reason it's called Manhattan or taxicab distance

is because Manhattan is arranged as a grid, right?

And so the way you get from point A to

point B in Manhattan is through a series of these

kind of right angle.

Uh, turns, uh, because there aren't, I mean, London is

not like this, right?

Like London has crazy roads going in all different directions

and so this, you know, this doesn't make sense in

this context, but if you think of a grid system

getting from point A to point B, you got to

like travel along the grid.

So that's, that's why this is called the Manhattan distance,

um, or the taxi cab distance because think of yourself

in a taxi cab in Manhattan trying to get from

point A to point B.

You're going to have to travel on a bunch of

right lines, OK.

So the Manhattan distance between vector 2 and vector 3

is 123, wait, no, 123456.

That's double counting or half counting.

Um, so it's 6, and you can illustrate it here

as the line that travels along right angles that goes

between the two vectors.

Now, I could also illustrate it this way and it

gives you the exact same number, doesn't matter.

If you want a formula for how to do that,

this is how you do it.

So the distance metric, the taxicab distance metrics, T for

taxicab between vectors A and vectors B is calculated like

this.

So you're summing over all the columns, and then for

each column you're just taking the absolute value of the

difference between the word counts and you sum those up.

And then if you do it for the two vectors

I illustrated over here, use this formula.

What you do is you take 5 minus 2, absolute

value it, 1 minus 4, absolute value it, and it

turns out to be 6, which is exactly what you

can see if you looked at it visually.

It's 6.

Now, there are other distance metrics.

There's the Minkowski distance of which Euclidean distance is a

special case.

Uh, there's also Mahola nobus distance, um.

I don't, we're not going to go into this.

This is like a little bit boring and beyond the

scope of this class, right?

This is a little, little small and technical for the

purposes of the, the points to, to learn about how

to do text analysis.

Um, but the, the stepping back, like if you wanted

to measure the distance between two vectors, so two documents

in a vector space.

You can choose which metric you want to use.

You can use the Euclidean distance, you can use um

uh taxicab distance, you can use Maikowski distance, you can

use Maha nobus distance.

These will be all highly correlated with one another, right?

The further away in a generic sense the vectors are,

the, the larger the distance you calculate is going to

be, um, but they're slightly different and they have different

properties and they have different use cases.

Um, again, beyond the, the, it's like too small for

this class to really talk about this, but if you

are in a situation where um you do want to

measure the distance between two documents using distance metrics, you

should sit down and think to yourself, well, like what

exactly is it that I want to learn by measuring

the distance and that will determine how you, um.

How you, how you pick which distance metric, and there's

a lot more reading you can do about this.

Like there's technical reasons why you might use one over

the other.

Uh, again, we're not going to talk about here.

OK, cool.

Now, let's move on to similarity metrics.

There's another way of measuring how similar in the generic

sense two documents are.

OK, so similarity metrics are basically just how directionally close

to documents.

It's not obvious how you would answer that question, it

turns out.

So there's an example from the book that I quite

like, which is, if you see, if you have two

documents that are a positive review and a negative review

of the same film.

Are those two directionally closer or further apart than two

positive reviews of two different films?

You might think positive review and negative review are going

to be directionally far apart because one's sort of positive

in tone and one's negative in tones.

You may have an intuition that they're going to be

going in different directions.

But Is the, is the difference in the directional closeness

of a positive negative view of the same film different

or like would we expect it to be bigger or

smaller than the directional difference between two positive reviews of

of different films that might use different languages, it might

be different genres?

Like, I don't know, right?

This is like an ambiguous, there's no, there's nothing you

could, there's no like right answer to this.

But we can maybe back out for properties that we

would think would be desirable properties for measures of how

directionally close documents are to each other.

We might want the maximum similarity to occur when you're

comparing a document to itself, right?

So you might want to anchor the sort of similarity

metrics at uh most similar being like compare the document

to itself, OK.

You might want to have the minimum similarity be when

you have two documents that have no words in common

at all.

Like they don't use, like there's no overlap in the

set of words they use.

These would be called orthogonal documents.

This is sort of more technical language, but you can

think about orthogonal documents as being at like right, you

know, they're like right angles from each other, OK?

They're not using any of the same words.

You might want the similarity metric to increase as more

of the same words are used between the two documents

you're calculating the similarity metric between.

OK, that seems obvious, right?

And then you also might want the similarity metric to

be symmetric in the sense that if you want to

know the similarity between document A and B, it should

be equal to the similarity of document B to document

A.

It shouldn't matter which order you're considering the the two

documents.

OK.

So one straightforward similarity metric that satisfies these properties, and

we won't go into detail about why because this is

very like this is very basic stuff from linear algebra

is the inner product.

OK, so or sometimes called the dot product.

So the inner product is calculated as this.

So if you have two documents, document A and B,

that have J features.

The inner product between those two vectors illustrated with a

dot is going to be calculated with this formula.

So you, you go, so in the document feature matrix,

you go column by column and you multiply the two

numbers for A and B, the two word counts.

Then so you do that for column 1, then you

do it for column 2, and you do it for

column 3, and so on and so forth, and then

you sum together all of those products.

That's what this formula is telling you to do.

It's pretty easy to calculate.

You don't, if you had a document feature matrix, what

you would do is you just multiply each of the

cells across the two rows and then you sum all

those together.

You could do that in our probably pretty simply.

Um, so for document 2 and 3 from our hypothetical

example, what we would do is we multiply these two

things together, then we multiply these two things together and

then we sum that.

And so the inner product of W2 and W3 or

document 2 and 3 is 14.

Simple calculation.

OK.

The simple calculation, but there's a major downside with trying

to use the inner product to measure how directionally close

to documents are.

And that is that.

The inner product is very sensitive to vector magnitude, so

to how long the vector is in the vector space,

right?

So how, how long it is.

So, oh, and so this number that you calculate here

is going to be so the similarity between two documents

is going to be sensitive to how, how the magnitude

of the vectors representing those two documents, but our job

with similarity metrics is to simply measure the directional closeness

like.

Whether they are close in direction or far in direction,

and we don't want that to be tainted with information

about the distance between the, the, the absolute distance between

the vectors as well.

The inner product is getting both.

You are getting directional closeness and magnet and absolute distance.

So what we want to do is, and I will

show you an example to illustrate this.

OK.

So this is document 2, this is document one, this

is document 3.

Now document 2 is just as directionally close to document

1 as it is to document 3 because document 1

and document 3, they are directionally pointing in the same

direction.

Document 2 is pointing in a different direction.

This is the angle between these two.

That angle is the same regardless of whether you're comparing

document 1 and document 2 or document 3 and document

2.

So we want, we would ideally want it to be

the case that when we calculate the similarity metric between

this one, between these two, and the similarity metric between

these two, should get the same number because it's the

same angle between the two.

But we don't if we use the inner product.

If we use the inner product, when we calculate the

similarity between document 1 and 2, we get 7 and

we calculate the similarity between document 2 and document 3,

we get 14.

So, uh, this looks less similar to document 2 than

this does, but that's weird because they should look identical

because they're both, both document one and document 3 are

pointing in the same direction, so they should be just

as directionally far from document 2 as each other.

OK.

So to deal with this, as you might imagine, we

can just normalise the inner product, normalise it by the,

the magnitude of the vector, how long the vector is,

OK.

So, um, Well, so just to back up a little

bit, how do we do this?

Well, what we want to do is we want to,

we basically want to, when we are calculating the similarity

between this vector and this vector, for example, we, we

do not, we want to like take out any information

about the fact that this vector is this long and

this vector is that long.

So we, we first need to calculate the length of

each vector and that's basically the Euclidean distance between the

vector and itself, OK?

So, um, so.

The the formula you use for it is basically the

Euclidean distance.

From The vector in itself and so this is what

it looks like.

This is illustrate it.

So this is the length or the magnitude of a

vector.

This is often, so if you have a vector X,

this is the notation we use, we use the same

notation as Euclidean distance because it's the same thing as

calculating the Euclidean distance of the vector with itself.

Um, this is how you calculate it.

OK, cool.

So then once you do that for each of the

vectors under consideration, you calculate the magnitude of that vector.

Then you take each of the vectors.

That you're working with and normalise them.

So if we're thinking about document A, so this is

the vector for document A, let's normalise it by dividing

it by The um magnitude of that vector A.

That is the Euclidean distance of the vector with its

uh with itself or against itself from itself.

And so this is how you would calculate that um

in document feature matrix.

OK.

So then, the cosine similarity between two documents, WA and

WB is just the inner product of those two documents

normalised by the vector lengths of the two vectors.

So cosine similarity from document A to B is just

document A to B.

You might think we're doing the inner product, but no,

we're actually first normalising each of these vectors by the

vector lengths or the vector magnitude.

I, let me back up, just say one thing.

It's on the slide here that I just skipped over.

I'm calling the vector length and the vector magnitude.

I'm using those words interchangeably here.

I hope you understand what I'm talking about the the

length of the vector.

I'm talking about that length.

It's not the same thing as the length of the

document.

Like the length of the document, like how many tokens

are in the document is not the same thing as

how, how long this vector is.

So it's unfortunate that the same word is being used

in two different ways.

So the textbook prefers to refer to this as the

the vector magnitude so that you don't, you don't think

oh this is the same thing as the document links.

Things are obviously highly correlated.

The longer the document is, the higher magnitude the vector

is, but they're, they're actually not the same number, OK.

So I just want to be clear when I'm talking

about the length of a vector.

I'm not talking about the word length of the document.

I'm talking about like this geometric length.

OK, cool.

Boring notation stuff.

So why is it called cosine similarity?

Well, it turns out that thing is equal to the

cosine of the angle between the vectors.

That's very convenient for us.

Um, so once you see that, once you see that

the cosine similarity between two vectors is just cosine of

the angle between the two vectors, it's very clear that

cosine similarity is only measuring.

How directionally close the two vectors are, because the only

thing.

That's in the formula for cosine similarity is the angle

between the two vectors.

So you now are no longer this, this statistic, this

metric is no longer.

Containing any information about the, the vector lengths.

That's cool.

That's what we wanted to get to.

We wanted to get to a metric of similarity that

only considered how directionally close they were independent of the

vector magnitudes.

Um, cool.

So this metric ranges from 0 to 1.

Um, I mean, technically, we all know cosines can go

negative, but in our context, you're not going to see

that because these are all going to be positive always,

right?

Your word counts are never negative.

Hopefully, if you get a document feature metric that has

negative word counts in it, something has gone really wrong.

Um, so the metrics can range from 0 to 1.

It has this intuitive interpretation that a cosine similarity of

zero is the least similar.

There's like literally no similarity.

Think about it that way.

Um, and cosine similarity of 1 is going to be

the most similar.

So, so again, think about zero being context where the

two vectors at 90 degree angles.

I think about a two dimensional space, 90 degree angles

from one another.

Um, and then of course documents that are most similar

are can be literally pointing in the same direction.

They can be like on top of each other.

So, what would you imagine the cosine similarity between document

one and document 3 are?

Yes.

It's one, cause they're pointing in the same direction.

OK, cool.

Now, so let me also, let me quickly build it,

I should have put this on the plot, but let

me build an intuition for why if you have two

documents at 90 degree angles, they're orthogonal, and since they

have like literally no uh no similarity.

So if you had a document that was on, that

was just like on top of the X axis, and

then another document that was on top of the y

axis, there would be at a 90 degree angle from

each other, right?

Cause you know, axis are 90 degrees from each other.

So for example, let's suppose we had a document cat

cat, so it would be that vector, and we had

another document that was dog dog dog, so it would

be that vector.

Clearly, there's no similarity between the two things because they

literally do not have any overlapping words.

And that's why they would be on the axis on

the axis, because this one has no dog in it

and that one has no cat in it, and so

they have zero similarity.

Cool.

Now, um, so for our, for we can calculate this

cosine similarity between document 2 and document 3 using the

formula from above, we get 0.614.

Um, now, so that's the cosine similarity.

So what this number is giving you is a number

of how directionally similar these two documents are on a

scale from 0 to 1.

Cool.

Great.

So.

Has a kind of intuitive uh interpretation.

I don't think it's quite, it's, you don't want to

think about it as quite linear because you remember cosine

is a curved function, but you know, you can roughly

think about, uh, as you go up in this, you're

getting, I mean, obviously as you go up in this,

you're getting more similar, um, and, and, uh, but it's

not quite linear, right?

So remember the cosine functions kind of curved.

So it's, you can't think of like going from 0

to 1, it's like every little notch up is an

exact is an exact same increase in similarity because the

curve, the function is kind of curved.

OK, cool, but we might also want to just know

what the angle between the two documents is.

I don't know why you might want to know that.

Well, you might want to know that because it has

a more, I don't know, for, for like real life

humans that don't think can.

In cosine terms or algebra terms that, you know, they,

they often do like to think about angles and, and

they do have an intuition about angles.

And so the intuition obviously comes from the fact that

when people plot vectors, they can see the angle and

like I can see this angle here and I know

it's less than 90 degrees, but I, I don't know

exactly what it is.

Well, you can, you can back it out if you

want, because you know that this cosine similarity between these

two vectors is just equal to cosine of theta.

So if you want to figure out what the theta

is, that is the angle, you just take arc cosine,

so you, the inverse of cosine of this number, and

it's going to give you 0.9098 radiances because remember, um,

Oh gosh, this is like pulling back from high school

trigonometry here, but like when you're, when you're calculating um

trigonometric functions, you don't work in degrees, you work in

radiances, but you can, I mean it's easy, it's like

Fahrenheit and Celsius, right?

It's easy to translate that into degrees and it's 52

degrees approximately.

So that, that angle between those two, those vectors is

about 52 degrees.

OK.

Now, there are other similarity metrics that we could talk

about, but we're not going to.

In the same spirit of why we didn't talk about

their, the these other distance metrics, it's like a little

bit too small and technical for the purposes of this

course, like the big picture, we want to still keep

in mind.

And also I want to get to clustering, which is

actually like the, the, um, the, the climax of this

lecture.

Moreover, cosine similarity does appear, at least in my reading

of like the the QTA stuff that I've come across

in my own research, it seems to be the most

common thing that's used in at least in my little

corner of academic research to measure the directional closeness of

documents, but there are other, other things you can do.

Um, if you are doing a research project where a

distance metric or similarity metric is a, is like a

core part of the thing you're trying to learn about

the world, like it's a core piece in the puzzle,

the, the thing, the largest thing you're trying to learn

about in the world.

You might want to do some digging on like the

various metrics you can use and figure out which one

would be the best one to use for your context.

But it's, you know, that, that's, that's going to be

context dependent, uh, in a lot of ways.

And it turns out that you'll see, I mean in

seminar, you're going to be calculating some of these things.

And so to calculate the Other distance metrics or similarity

metrics is like, is you use the same process as

you would for calculating cosine similarity and Euclidean distance.

You just put different arguments in the function.

So it's not like, it's not rocket science to, to

measure different, uh, metrics.

I just don't want to go through the kind of

theory and math behind it.

For, for lack of time, probably interest.

OK, so let's talk about clustering.

So, cluster is a group of documents that are very

similar to each other, but very different from the documents

that are outside of the cluster, OK?

So clustering as a method is an is a sort

of unsupervised classification.

We already talked about classification before, so we're trying to,

you know, we have a set of documents we don't

know what the category of the documents are, but we

use classification methods to try to figure out what the

category of those documents are.

But when we were talking about classification before, we had

already had a set of categories in our mind that

we were trying to.

To allocate our documents into, it's just that we didn't

know, we didn't actually know which category the documents were

um uh we're supposed to be in.

Clustering is classification again, but we don't actually know what

the categories are either.

So it's unsupervised in the sense that we're, we're aiming

to create groups, categories of documents, but we're doing it

in a kind of unstructured way where we're like not

pre-setting what the categories are.

We're just sort of allowing the algorithm to learn from

the data, like what the categories are.

Now of course it's a little more structured than that

as you'll see in a minute, um, but what this

means is that the clusters that are formed via clustering

don't relate to pre-existing classes or pre-existing categories that we

might intuitively think about partisanship of the speaker or whether

the speaker is in the government or in the opposition,

so on and so forth, nor do they, nor do

they capture latent traits per se, like we would capture

when we're, um, estimating, uh, uh, when we're doing scaling.

But they estimate membership of each document in a predetermined

number of groups, so you don't know what the groups

are, but you do say I want to create X

number of groups.

I put yikes here because like that you got to

choose that.

So this isn't magic when you're trying to cluster documents

into.

Into um groups of uh groups of similar documents.

Like there's like you gotta, there's stuff you got to

specify.

One thing you gotta specify is like how many groups

do I want to create, and that's like, oh, it

turns out to be a kind of annoying thing to

try to figure out.

Um, moreover, after you do the clustering and you get

clusters of documents into different groups, You got to give

them labels, so you got to give them interpretations like

what does cluster one mean?

What does cluster 2 mean?

I mean, you have to do that because what's the

point in doing quantitative text analysis to try to learn

about a set of documents if you just cluster them

into groups of similar documents and you don't tell your

audience or the reader like what those clusters mean, right?

Like it's the point we're trying to learn stuff about

the world.

So you got to like give them labels, right?

You gotta say, well, cluster one is about this and

cluster 2 is about that.

Whoa, you gotta do that after you've got the clusters

and you gotta just like interpret the clusters.

How do you do that?

Well, you read them, you, you kind of, you guess,

you, you know, you, you do a little armchair philosophising.

Um, and I'm going to talk more about that at

the end of this lecture because this to me, like

I told you at the beginning of this class that

you're, you're learning text analysis from someone who's a little

bit sceptical.

This is like, uh, this is where my biggest scepticism

about text analysis comes in.

It comes in clustering, where you have to label the

clusters.

It also comes in in topic models, which you'll have

next week, where you got to label the topics.

This is where like a lot of sketchy stuff happens

and and I'm gonna talk about that at the end

of the, the end of this lecture because it's so

important.

OK, now.

These methods are typically used when you want to, you

have a set of documents and you want to classify

them into clusters, but you, you, you don't, you, you

don't know the true groups that they should be classified

into and you never will know it, right?

Like that's, that's the, that's when you would use clustering.

If you could know the true class labels, you might

not use clustering, right?

You might use other methods, right?

So for example, suppose you had a set of documents

and you know, and these are like tweets issued by

politicians posted by politicians, and like the data got messed

up along the way and so you have to text

the tweets but you don't actually know who tweeted them.

Well then you're not in, you're like, you're not really

in clustering world because there are, like you're trying to

figure out who wrote each tweet.

So there is a truth, and you know there is

a, a predefined set of categories.

It's the people who issues or who post the tweets.

You just don't know them.

Closing method is not great for that context.

Like you wanna actually just go figure out who wrote

the tweets, right?

So you, you got to do other things.

Like part of that is like shoe leather work of

like going back and figuring out just like Trying to

find the the data that got lost, and part of

that might be to use other tweets to try to

um build a supervised classifier to figure out who the

unlabeled tweets are written by, right?

But you wouldn't want to use clustering for that because

there are true labels there, it's just you don't know

them.

OK.

Clustering methods not specific to quantitative text analysis.

I'm going to keep talking about them as you have

a set of documents and you're trying to cluster the

documents into clusters.

But of course, as those of you in the machine

learning class know, this is like, you know, these methods

can be used to cluster all sorts of kinds of

data, right?

Um.

And also, even within quantitative text analysis, you don't have

to just use clustering methods to cluster documents into groups

of similar documents.

You can also use clustering methods to cluster words, right?

So if you want to um Want to figure out

like words that are most similar to each other, for

example, like we've done this kind of in other contexts

in the course, you can use clustering for that.

But when this lecture, I'm again just to repeat myself,

just focused on thinking about clustering a set of documents.

So each document is the unit of observation and is

going to go into a cluster.

Yeah.

If you wanted to cluster accept documents, but you wanted

to first aggregate embeddings, do you have any advice on

how to aggregate embeddings?

Do you think you should retrain the model to create

new embeddings from like this kind of generated like different

bits of text?

Let's say you wanted every line that Micusio said and

to see how similar Micusio is, plus Micusho is a

Shakespearean character to another Shakespeare character, so you want all

of his lines together.

OK, so let me just say that, uh, I've never

done this or thought about this, so I'm just giving

you my gut feeling right now.

My gut feeling is you got to do what you

suggested, which is you comb you basically consolidate all of

Mercutio's documents into an aggregate document.

This is like what we did for the Federalist Papers.

What we didn't, when you're trying to predict who wrote

the Federo's papers.

We took the papers by Jay Hamilton and Madison and

we just collapsed them all into like one mega document.

Um, and so it's basically take all the documents by

Mercutio and sum together the, um, sum together the document

counts and then you'll create, basically you're pasting together all

the documents written by Mercutio into some one long document.

That's what I, I'm, I'm like 90% sure that's what

you should do.

Because I've seen online averaging.

Averaging kind of, yeah, that seems a bit.

Well, OK, so you, uh.

I could see reasons to do that, probably related to

sample size considerations and.

But that, yeah, I, I, yeah, I I couldn't.

OK, thank you.

Yeah, if it were me, and I, there were, there

were enough, um, Shakespearean characters, let's say, that you, if

when you consolidate them into sort of mega documents for

each character, there's still a lot of them left.

I would do that and then uh estimate, like re-estimate

the word embeddings before going on to clustering.

That's what I would do if it were me.

Ask Friedrich that question too, because he, he, he may

have a more uh more than just a gut feeling

about what to do.

Also, you'll see this in the seminar, but this is

pretty common, we do this thing where we'll like You

know, we're interested in clustering, not necessarily individual documents, but

like groups of documents, right?

So like we wanna cluster uh Shakespearean characters, for example,

so each character has, you know, different lines that they're

saying which could be represented as different, um.

Different rows and document feature matrix, but we want to

consolidate them together.

There's a function in in Quantity to do this, and

it's DFM_ group, and that'll that'll do is to create

a new DFM that consolidates by whatever variable you tell

it to consolidate by.

So like the Shakespearean character variable, whatever.

OK, cool.

So very common method for clustering is K means clustering.

I'm actually a little embarrassed to even be teaching this

because I'm guessing like many of you have seen this

many times in the past, but maybe not, in which

case I will feel less embarrassed.

But the basic idea of K means clustering is that

each document in the set of documents that you're clustering

is going to be assigned into one of K clusters.

OK, so you choose the K, like how many clusters

you want.

And then assignment of documents into those clusters occurs in

a way that minimises the within cluster differences and maximises

the between cluster differences.

OK, that's a very like, yeah, easy to say that,

but like how do you actually do it?

That's, you know, the devil's in the details.

And you, when you're doing K means clustering, you'll use

a random starting position, so you can do this in

different ways, but basically you'll randomly allocate in some, you

know, again there are different ways to do this, but

you'll speaking more generally, you will like randomly allocate the

documents into clusters.

And then you start an iterative process where you continually

move around documents into different clusters until you satisfy some

kind of stopping condition so until the clusters are stable

in some sense.

And um.

This process is going to treat feature values as coordinates

in a multi-dimensional space.

There is another way of saying this is Every document

is going to be a vector of word counts.

And the vector of word counts is the is the

mathematical object the algorithm uses to figure out how close

or far away individual documents are.

OK.

Now there are some unfortunate conventions around notation here that

I just have to illustrate the for for reasons I

don't quite understand, but it seems to be most of

the time when you see people talk about K means

clustering, they, they use a little K here seems to

be what most people do, although I, I can't obviously

speak universally because notation is.

One of those things that everyone kind of tries to

be consistent, but not really.

Um, so the, the, the title K means clustering is

referring to the fact that you're going to create K

clusters by doing some fancy stuff around the means of

those clusters that I'll talk about in a second.

That little K here should actually be a big K.

Because in our world when we're thinking about using our

notation consistency, we're going to have big KB, the number

of clusters, and we have a little KB, a generic

index for a specific cluster.

But for whatever reason, most people when they're writing out

the phrase K means clustering, put the little K here,

which really this doesn't make sense.

They should put this as a big K.

So I'm going to try to because I went through

these slides from last year and I like really like

redid the notation here because I want to try to

be consistent.

When I'm talking about the number of clusters, I want

to use capital K.

It's the thing I'm choosing.

Capital K is the number of clusters, and when I'm

talking about a specific cluster, a generic specific cluster, I'm

going to use little K as the indication of that

specific cluster.

But in the title K means clustering, we're going to

use a little K even though we really should be

using a big K.

Cool.

Great.

Now, Advantages of using K means clustering to cluster documents

into clusters of similar documents.

Very simple to do, very simple to understand, it's very

flexible, and it's really efficient, meaning that it doesn't take

a long time.

Uh, the algorithm doesn't take a long time to run.

Disadvantages, two main disadvantages.

One, there's no fixed rules for determining the number of

clusters you want to create.

I mean this is like all clustering so I don't

even know, I don't know why this would even be

considered a disadvantage of Kamian's clustering.

I mean there are clustering methods that like kind of

pretend like there's some kind of principled way of choosing

the number of clusters, but ultimately it all comes down

to like you gotta just make a choice about how

many clusters you want.

There's some things that you can do that, you know,

make the decision a little bit more of an educated

guess, um, or an educated choice, but there are no

fixed rules about this, OK?

The other thing is, um, The K means clustering algorithm.

Starts by randomly allocating in some way, there's multiple ways

to do it, but again, randomly allocating documents into clusters

and then starting the iterative process of making the clusters

better.

That means that like you're sometimes going to get like

rerunning the same clustering algorithm with the same specifying the

same number of clusters, you're going to get slightly different

clusters.

Now hopefully they'll only be slightly different or if you

rerun the same code over and over again, hopefully they'll

only be slightly different because there are genuine patterns in

the data that the clustering algorithm finds and so you're

only getting like a little bit of like things being

like slightly different.

But because there's a random starting point, it means that

you can run this algorithm and get slightly different results

every time.

So you have to set your seeds if you want

to reproduce and get the same clusters every single time.

OK.

So, let me talk about the algorithm details.

OK, yes.

So, first thing you do, you have a set of

documents.

You choose starting values.

Well, actually, the first thing you do is you choose

how many clusters you want to make.

So you choose K.

OK.

But then, then the next thing you do is you

choose the starting values.

There's different ways to do this.

This is the, this is the point where you're going

to randomly allocate documents into clusters.

So one thing you can do.

Is um.

How did I do this here?

Yeah, so The way that the Kames clustering algorithm is

going to work, we'll see down here, is it's going

to calculate a centroid for each of the, the clusters,

and then that's going to be the basis for the

determination about whether to continue to a different step, whether

the clusters are good or not.

OK, so I'm skipping forward a little bit because it's

important to say the two ways you can randomly start

yourcas clustering algorithm is you can say.

Or the two main ways I guess there's probably other

ways you can do it is you can randomly create

some centroids, so like you randomly create centres of different

clusters, and then you can calculate.

For each of the documents, which cluster it would go

into, and then you start the iterative process of making

it better.

The other thing you can do is you can just

like take all the documents and you can randomly cluster

them into K clusters.

So you randomly partition the documents into K clusters maybe

of equal size, calculate the centres of those, and then

start the inter process of making it better.

It doesn't really matter.

I mean, it does, I'm, I'm sure it matters.

I'm sure there's a technical literature on this, but for

our purposes of learning, you know, like learning from text

using clustering, this is like a minor issue.

Yeah.

Can, can we pick the centre rather than doing it

randomly?

Let's say we were clustering countries and we really cared

who was clustering around the US and China.

Can we pick US and China as centre of a

cluster or is that not acceptable?

No, I think you could do that.

Not in.

Well, um, you're, so you're kind of hinting that you

want to do supervised classification, right?

Because you have predefined categories that you're interested in.

And so you probably would, you would want to go

back to like classification methods, I would think.

Because this, yeah, so here we don't have any, there's

this is unsupervised in the sense that we don't have

any guiding categories we care about.

And also because it, the clusters do depend on how

you start, like you wouldn't want to start in a

deterministic way like that.

OK.

So, uh, so you do that first and then you,

um, Assign each of the documents in your, uh, in

your corpus to the cluster where um the centroid of

that cluster is closest to that document.

OK.

So that would be like if you started by creating

a bunch of random centroids, you then would Take each

of the documents and put it in the cluster based

on whichever centroid is closest to that document.

So each document remember is is represented as a vector

of word counts, and we can measure the distance between

that and then this sort of centre of the cluster,

which I'm calling a centroid.

The centre of a cluster is just going to be

like the average of the, I'm gonna show you how

to calculate in a second, but it's going to be

like if you had a cluster of documents, which is

a bunch of vectors, you could calculate the average, so

like the centre of that cluster, and we'll call it

a centroid, um.

And so the, so if you started with random centroids,

you would assign each of the documents to whichever centroid

is the closest to that document's vector.

Um, typically, Euclidean distance is the distance metric used to

determine how close a particular document vector is to the

centroid of a cluster.

So it's just like the sort of straight line distance,

right?

Um, but you can use other distance metrics.

The key thing is you're using a distance metric to

calculate how close an individual document is to a Syroid.

So then, um, So then what you will do is

you'll have a bunch of new clusters because you've randomly

created some centroids, you've assigned each document to clusters based

on those centroids.

You can recomute the the cluster centroids for the meaning

of the clusters, and it's going to be different.

And then you can reallocate cluster reallocate documents to different

clusters.

If it turned out that when you recalculated the centroid,

a particular document and now becomes makes more sense to

go into a different cluster, you would move it over

to that other cluster, right?

So you just keep doing this, right?

You like move documents into clusters, calculate the centroids, and

then Uh, you move them again based on how it

goes, the centroids are going to change and so you

move them again to, uh, to the, to the centroids

that are closest to them and you keep doing this

until the, um, until a stopping condition is satisfied.

So basically, um, for example, like you, you keep doing

this process until no items get reclassified every time you

recalculate the centroids.

And, and that, if that happens, you're gonna be done,

right?

Because like every time you're not moving documents, the centroids

are gonna be the same and so you're not moving

documents again and the centroids are gonna be the same,

OK.

Now The like the sort of, uh, I don't know

if this is interesting actually.

I thought it was kind of interesting, so I have

a slide on it, but so the interesting thing here

is actually calculating the centroids of each cluster.

So, um, let's assume we have a document feature matrix

W with N documents and J features, so J columns,

and we want to cluster into K clusters.

Let's use this symbol pi ik as an indicator of

whether or not document I is in cluster K.

So 0 means no.

So document I is not in cluster K if this

pi ik is equal to 0, and it is in

cluster K if that's equal to 1.

Then you can, you know, conditional on moving documents into

clusters.

So at each step in the iteration you can you

can recalculate the centroid of cluster K as follows.

So it's, it's basically you're going to get a vector

of J total word counts across the documents in the

cluster.

So you take all the documents in the cluster, so

all the rows from that cluster, and you, you.

Yes, you take all the documents, sorry, minute to translate

the math into my brain.

Um, that should not be a big end.

That's what's confusing me.

This should be a little N.

No, that should be an I, not an N.

Sorry, there should be an I.

Change this big N into an I.

This is what's throwing me off.

OK, so, um, so basically the way this works, so

if you have a, if you've already created some clusters,

so you're one step in the iteration and you want

to calculate the centroid of that cluster, what you do

is you take all of the documents that are in

that cluster, so each of those rows, you sum together

the word counts in each of those rows, and you're

going to get like one new vector consolidated across the

cluster, kind of like in the spirit of what we're

talking about with the Shakespearean characters.

And then you divide that vector by the total number

of documents that are in the cluster.

And so then you, this is going to give you

the average like sort of the, the mean of the

cluster.

Now it's, it's a J dimensional, I mean, it's a

J-dimensional vector because remember we're in a J dimensional space

and so the mean is somewhere, you think about 3D

space, right?

The mean is somewhere in the 3D space.

So it's gonna, it's gonna have multiple, it's going to

have multiple dimensions to it.

But this is basically gonna be a a a J

link vector that specifies the the point where the centre

of the cluster is.

And then from that point, You can calculate how far

away each document is from that point, using Euclidean distance.

Which is a simple calculation, although it's you know, you

got J.

Words and so you get, it's like a it's a

long calculation, but it's mathematically simple.

OK.

So here's how it might work visually.

So suppose you started, so this is like a 2,

think of this as like a two dimensional document feature

matrix, right?

So one word and then another word, and these are

a bunch of documents that are that are in this

document feature matrix.

So what you might do, so here's your starting point.

This is like a bunch of documents.

It's kind of like my 3 vector thing, except they're

not drawing the arrows, they're just showing you the points,

the end points.

So you might start with 3 red, so this is

like 3 centroids that are randomly created, the red, green,

blue, green, and red ones.

And then what happens is you decide for all of

the documents in the, in the data set, like which.

Centroid that document is closer to.

And so here we're drawing a line that is, that

is determining whether the documents are closest to this, this

or this.

And so these are all the ones that are closest

to that.

These are all the ones that are closest to that,

these are all the ones closest to that.

OK, so then, Once you do that, you have these

3 clusters.

So each of these three clusters has a new Centroid.

It's not the, it's not the original randomly started centroid.

It's like going to be somewhere in the, in the

middle ish of these, as you can imagine.

And so the new centroids are here.

So then once you do that, you'll notice that like

actually.

Now we might not want to, now that these are

the three centroids, some of these documents need to be

moved around because they're not in the right cluster anymore.

And so that's what's going on here.

You got a new division.

So like, for example, um, This centre moves up to

here.

So some of these documents now belong in this cluster,

and you can, you can see the new illustration here.

So this is the new division based on these new

centroids and unfortunately it's an ugly plot.

So the old cluster separation is still there and grey

in the background, so to make this even more confusing

to look at.

But this bold black is like the new cluster creations

based on the new centroids.

Well, now that you've created new clusters, maybe the centroids

would move.

So maybe this centroid needs to go a little bit

this direction, which it does.

Maybe this centroid needs to go a little bit that

direction, which it does, and maybe this centroid, yeah, it's

pretty close.

So but here you see it's going up.

So we're calculating new centroids based on our new.

Clusters.

Well then, because we calculated new centroids, it's possible that

some of these documents are no longer in the right

cluster.

And so we could draw a new we draw a

new division based on these new centroids and then maybe

these clusters, some of these documents are no longer in

the right cluster anymore and so you, you know, you

just keep repeating this process until you don't end up

moving documents anymore, um.

Or you could even get to a point where you

move, if you're only moving one document, you stop.

OK.

Now choosing the appropriate number of clusters, the major point

of analyst discretion with K means clustering really with any

kind of clustering is that you have to choose how

many clusters you want.

So this is very often based on prior information about

the number of categories that you want to create.

So for example, here's an example.

It's not a text analysis example, but a more general

example.

Like, let's suppose that you have a set of tutorial

groups and a course.

Like you're, you're basically, you're, you're dividing up a class

into groups of students to do group work.

Uh, that sucks, I know.

Um, but you know that you need like 5 groups

in the class, right?

So you, then you can you can use K means

clustering to create the clusters knowing that you need to

have 5 groups in the class, right?

So you might have like some Some set number of

categories that you know you need, you just don't know

what the categories are, but you know you need 5.

Now, a rough guideline if you're not in that world

is to set the number of clusters to be equal

to this, where the number of documents divided by 2

take the square root of that.

Usually this number is actually too big, um.

So you're going to get overfitting, meaning that, I mean,

just imagine the the like the most extreme example where

obviously not, you wouldn't do it with this formula, but

like just to illustrate the idea of overfitting, like let's

suppose you wanted to create N clusters, so same number

of clusters as you have documents.

Well then every document is going to be in its

own cluster and then like what are you learning?

OK, so this is overfitting, right?

You're like, you're really like.

Your clustering algorithm is giving you a result that is

too sensitive to the specific sample of documents you have,

um, and so you don't want to do that.

So it turns out that this rough guideline that people

use often creates too many, often indicates that you make

too many clusters and then you get overfitting problems.

OK.

So the other thing people do is that they do

these things called elbow plots where they will basically refit

K means clustering algorithm with different numbers of clusters and

then see, um.

You know, the point at which there are diminishing gains

from adding additional clusters because the more clusters you add,

the more the better it's going to be in some

sense, right, because you're going to have, you know, the

documents inside the clusters are going to be more are

gonna be closer to each other because the cluster is

smaller.

And so you can imagine like more clusters is going

to be sort of better in some sense, but there's

a point where like the gains of adding additional clusters

are not going to matter much.

And so this is what elbow plots look like.

So here it's like the number of clusters that you're

using, um, and so you, there's various measures of like

how homogeneous clusters are, um.

And also measures of how heterogeneous clusters are.

You want homogeneous clusters.

You don't want heterogeneous clusters.

These are like two sides of the same coin, basically.

So you might say, let's start with one cluster or

2 clusters, 3 clusters, 4 clusters, 5 clusters, and to

see how homogeneous the documents are inside that using various

metrics that you can use that we're not going to

talk about.

Um, and then you'll see that at this number of

clusters, you have an elbow point, meaning that like after

this number of clusters, you're really not getting a lot

of additional homogeneity inside your clusters.

And then this is just like the other way to

look at it is like starting with two clusters, you

still have a lot of heterogeneity within the clusters.

You add another cluster, you reduce the heterogeneity, you add

another cluster, you reduce the heterogeneity.

But then here there's a, I mean, I don't know,

you could debate is this an elbow point or is

this an elbow point, but like one of these two

points, you're more clusters is not going to give you

additional like benefit.

Now, um, K means cluster.

OK, so I want to move to a different kind

of clustering now.

Um, so K means clustering requires you to pre-specify the

number of clusters, which is capital K.

There's this thing called hierarchical clustering, which is an alternative

approach that does not require we commit to a particular

choice of the number of categories.

That's false, I think, personally, and I'll explain why I

think that's false in a few minutes.

But let me give you the most optimistic take here,

which is that hierarchical clustering solves the problem of K

means clustering because you don't have to choose the K.

You have to choose the K, but we'll get there

in a second.

Now there are a lot of methods for hierarchical clustering,

and we're going to focus on bottom up or agglomerative

clustering.

This is the most common type of hierarchical clustering, and

it results usually in a dendrogram, I would say that

wrong, a dendrogram, which is like a tree diagram showing

you um splits in the data and, and, and if

you, it's built from the bottom up and then um.

If you, as you go up the dendrogram, you will

see clusters.

OK.

So I'm gonna show you, this is a little abstract,

but I'll show you that, I'll show you some pictures

in a in a moment.

Now, this is, this I think this is like a

little, this is like, I don't think this is true

because if you're trying to actually create clusters at some

point, you're gonna have to make a choice about how

many clusters you want.

In the context of a dendrogram, this tree diagram that

that effectively amounts to deciding where you want to cut

the tree, and I'll show you that um when we

get to this point.

But like, if your goal is to get clusters, you

still got to decide how many clusters you want.

Now, with dendrogram, with hierarchical clustering, where you have this

dendrogram, you can do this post estimation.

So I guess that's a benefit, right?

You can choose the number of clusters after you've done

the estimation.

Um, But it's a little bit too strong to say

that we, we don't have to choose the number of

clusters that we want when we do hierarchical clustering.

I mean, you're still doing clustering.

You still need to have clusters.

So it's weird to say like we don't have to,

we don't have to commit to a particular choice of

OK.

Now, let me show you the idea behind hierarchical clustering.

So here you have 5 documents in a 2 dimensional

space.

So it's an iterative process where you identify the closest

documents to each other, um, and then once then you

group the two documents that are closest together, then that

then you treat that as a document.

It's like a metado or mega document, and then you

calculate the next two closest documents and so on and

so forth.

So for example if we were looking at this, if

we're starting the process, this bottom up process of hierarchical

hierarchical clustering, if you look at this, you'll see that

the two closest documents just visually using Euclidean distance kind

of guesstimating would be A and C, right?

So those go into a cluster.

All right, so once you do that, then you have

to decide, well, what are the next two closest things?

Is it this and this, or is it this and

this?

Because now we're treating this as like one, you know,

like one mega document.

OK, so we want to know the distance between this

and well, the mean of this versus that.

Well, that that those two things are the closest now.

So now we have three things whose distances we're trying

to compare.

What's the next close, uh, the next pair that's closest

together, given that we've now grouped these two and these

two?

Well, it's this and this.

OK.

And so then finally, the last step is everything, because

these are the only two things left and so now

we have a We have a um So I, I'm

a game theorist in a different life of mine, and

this is like reminding me of information sets.

If anybody is like familiar with that, that's like, yeah.

OK, so, um, here's the approach in words.

Start with each point in its own cluster, so start

with the, the, the data like all where each point

is considered its own thing.

Identify the two closest clusters, merge them.

Repeat this process over and over and over again until

you get to the point where the last step is

you've you've concluded everything into one cluster.

So that we did that on the previous slide and

this is what it looked like kind of visually, but

it yields a dendrogram.

So here the dendrogram is a tree diagram showing you

how the clustering occurs or could occur.

Right.

So first, we put A and C together.

So we have A and C together, and then next

we put D&E together.

And then next we put B and A B together

with this cluster.

And then finally, we put all the, the, the two

clusters that were formed together.

Now, the algorithm is a little more complicated than this.

I don't really want to talk about this in too

much detail, but Uh, uh, I guess, is there one

technical point I want to make?

Yeah, there's one technical point I want to make here,

um.

That when you are determining, for example, if you are

at this step and you want to know is, is,

you know, which of the three things here, which is

the which are the two closest, you are going to

have to make a decision about how to Figure out

the location of this cluster.

So you could do centroid, right?

It could be like the, the mean of these two

things, like sort of the the centre of these two

things.

You could also, there's other things you do, right?

But like, you're going to have to, for this cluster

you've created in a previous step, now it has to

have one measure of like where it is located and

the centroid is the easiest thing to do here.

Um.

So that's typically uh a thing people do, but there

are other things you could do as well.

It's sort of a technical point, but Not important for

understanding the conceptual.

Stuff.

Now.

And so you can read, if you want to sort

of read a little bit more detail about the exact

process you can read here.

Now, after you finish the process of doing the hierarchical

clustering, you can create a dendrogram.

This is a tree-like structure and you can cut it

wherever you want.

So it's like how many clusters do you want to

end up with.

Now, one thing I got to say here that is

like, I guess kind of important is that there's not

a unique ordering to the, to the um to the

leads, and these are called leads.

There's not a unique ordering here.

Um, so, like you can, you can see here, I

could have flipped this, right?

I could have the, the A and the C here

and I can, you know, whatever, like, obviously, it doesn't

have to be this exact order down here.

The clustering is always going to be the same, right?

In terms of like when you go up the tree,

you're always going to get the same clusters.

It's just like exactly how they're ordered along this x-axis,

the, the individual do, it's not unique.

This turns out to be important not for any like.

Real technical reason, but just because human beings look at

dendrograms and they make interpretations based on the order in

which they see things appear in the dendrogram.

And that's a little unfortunate.

That's like our cognitive biases as humans, but like it

does mean that you have to think about like exactly

how your dendrogram is ordered because you do want to,

like you gotta care about the reader, right?

And like what they're going to take from the thing

that you're showing them.

OK, so, um, here's an example of a dendrogram from

the uh Presidential State of the Union addresses corpus that's

available in Quanta.

So this actually you're gonna, you're gonna make this in

seminar, but the In the United States every year the

president gives a State of the Union address, which is

a an address before Congress that um something, something, I

don't know, whatever people think it's important, but it's, it's

a speech uh that presidents give and so here what

we have is a corpus that has a bunch of

speeches, so one for every president year combination, um, technically.

When a president first enters office, the first speech they

give is not technically a State of the Union speech.

This is like boring details, but um, I, I still

think that we consider, yeah, yeah.

So they're still, they're still included in here.

They're not called State of the Union speeches because of

legal reasons or something.

But so this, I think this goes, well, let's see,

what's the myths, 1989.

86, 85.

OK, so yeah, so this is all the speeches from

President Reagan to Obama 2014.

Yeah.

So this goes from 1980.

No, it's not even all of Reagan's.

So this goes from like second term Reagan to, to

second-ish term Obama.

OK, so it covers 85 through 2015 or something like

that.

But each of these leaves is a specific speech given

by a president in a year, specific state of the

Union speech.

And so the hierarchical process, you know, if you see

like the lowest tier on this tree is like here,

right?

So these two things were grouped together first.

Obama's 2013 and 2014 speech.

I mean, looking at that, you should, you should think,

OK, that's like, you know, that's nice to see that

the same president one year apart, that those two speeches

are being clustered together first.

That's like proof that This isn't wild what is happening

here, right?

And then the other two ones that are at the

lowest level, so that means they were, they were clustered

to combined together first.

Clinton's 98 and 2000 speech.

Also good, nice to see that because this is literally

the same person, so you would have and pretty, you

know, like two years apart, you would hope that the

speeches would be.

Um, and then you can also see one level up.

Clinton's 1999 speech, which is between these two speeches, was

then merged with these two speeches.

So this is like, I mean, you're looking at this

thinking, OK, I like what I see here in the

sense that the algorithm without any guidance, I mean this

is a this is a computer algorithm, it doesn't know

who these presidents are, doesn't even know the names, right?

Like you're not including the names of the presidents in

the, in the uh document feature matrix.

those are just the document labels, right?

And so it's nice that the dendrogram is dendrogram is

finding patterns that make sense at an intuitive level, OK.

So, let's suppose you do wanna make a certain number

of clusters.

So you've got this nice dendrogram here and it's like

you can look at it and you can feel good

about it, but you do, you do want to make

clusters of speeches.

Well, you've got to cut the cluster at some point.

So one thing you could do is you could cut

it at 0.04, so cut it here.

And if you cut it here, you're going to have

a certain number of clusters.

It's hard to see it on the dendrogram, but you,

you.

You can, if you see, actually, let's let's look at

a smaller one because it's a lot easier.

Um, let's say you want to cut this dendrogram at

2, so you want to cut it at 2.

Well, then you have this cluster, this cluster, and this

cluster.

See 3 clusters.

If you want to cut it at um at 0,

you got 5 clusters.

You got that cluster, that cluster, that cluster, that cluster,

and that cluster.

If you want to cut it at 3, you got

2 clusters.

You got this cluster and you got this cluster.

This is like, you know.

Yeah, information sets again.

Uh, OK, so in seminar, you're going to see there's

a function you can use to cut the cut the

dendogram and get an extract from a certain number of,

of clusters.

Now, oftentimes when people use hierarchical clustering.

And they teach it, they do it, you know, whatever.

They, they like stop at the dendrogram.

They're like, oh, look, we have this dendrogram.

I don't find this to be particularly useful.

Like, I, I mean, I know people like visualisations, but

when I look at that, I don't learn, I don't

feel like I'm learning a lot because it's too much.

So My recommendation is like treat it like real clustering,

cut the dendrogram, create clusters, and then do the work

of trying to label the clusters the the normal, you

know, the normal thing.

Cause like, I don't know what you learned from this

besides the, you know, the, the algorithm does appear to,

to notice similarities between presidents speeches.

But that's like, OK, I'm glad that that happened, but

I'm trying to learn about the substantive world, you know.

What I want to know is like presidents that are

not the same, are they ending up in the same

cluster?

OK, cool.

So, advantages of hierarchical clustering, it's deterministic, meaning that every

time you run hierarchical clustering on The same or the

simple one that the simple form of hierarchical clustering we're

doing here that you will do in seminar as well,

you're going to get the same, you're gonna get the

same clusters, OK?

So it's always going to give you this, it's always

going to give you a deterministic process.

That's good.

It means that you're not going to have to set

the seed or worry that like different times you run

the clustering algorithm, you're going to get different clusters.

You don't need to decide on the number of clusters

in advance, although you can specify a stopping condition.

You can say like stop building the stop clustering when

there's X clusters.

That's another way of saying you can cut the tree,

right?

Like, or you can just cut the tree while you're

doing the clustering or you can cut the tree X

post.

Um, and it allows for hierarchical relations to be examined

through the dendrograms.

OK.

Disadvantages, more complex to compute.

And the decision about where to create the branches in

what order could be somewhat arbitrary, um.

Determined by the method of figuring out what the, what

you mean by the distance between a cluster and another

and another cluster or another document.

So the distance, the distance metrics you choose this side

here matters.

But conditional on choosing distance metric, you're gonna get the

same clusters every time.

That's nice.

And then of course another disadvantage I guess is that

when you're plotting a dendrogram, the ordering is not unique,

but that's like a more, it's more about like readability.

It's not really a, it's not, it's not like a

real problem.

It's a, it's a, I mean it's a real problem,

but it's not like a.

Yeah, it's a presentational problem.

OK.

Now, I want to conclude by talking about the elephant

in the room, and I what I think is the

elephant in the room, um, is, which is that you

want to do all this stuff to, to learn about

substantive things that you care about in the world.

The whole point of quantitative text analysis is to learn

about observable implications in the real world from text.

We don't care about the text per se.

I mean some, you know, digital humanities context, you might,

but like for social sciences, we want to know what

does this teach us about the world, OK?

So the point of clustering is to discover new ways

of classifying a corpus of documents and the discovery of

the new ways of, of, of classifying a corpus can

actually maybe even teach you about substantive things, right?

If you notice that certain documents are being clustered together.

Um, and so for example, you look at presidential speeches

and you see like, you know, Ronald Reagan, and Barack

Obama are like their speeches are clustered together a lot.

You're like, whoa, something is going on about the at

least the rhetoric of Obama versus Reagan, Obama being a

Democrat, Reagan being a Republican, both of which, both of

whom were seen as fairly liberal and conservative, so pretty

extreme from one another.

So you might learn something new about the world by

doing this.

But what this but like backing up, if you're trying

to use clustering to learn about the world.

Discover new things about the world.

You, you got to think about what exactly you're doing.

So one clusters are going to be corpus dependent, so

different corpuss are going to reveal different clusters.

So you might notice that Obama and Reagan are clustered

together in terms of their State of the Union speeches,

but that might not hold when you're looking at their

inaugural addresses or their tweets.

Reagan didn't tweet, um, or you know whatever, you know,

OK, Trump and Obama's tweets, let's say.

Um, so it's going to be corp, you know, the

clusters you get are going to be corpus dependent and

it's not obvious that you can, uh, learn much about

other unseen data using clustering methods, OK.

So the other thing, this is like in my view,

the more important thing potentially is that the way you

label the cluster clusters ends up being crucial and potentially

misleading.

So in my view, the most controversial part of clustering

and topic models, which you'll see next week, is the

issue of labelling.

So what happens is the analyst takes a statistically created

contraption that we spend a lot of time talking about

what's the centroid, how do you calculate it, you know,

what's the process by which you do the clustering.

And then attempts to make meaningful what these clusters are

using concise labels.

OK.

So this happens really obviously in topic models where you'll

see next week people take a set of documents and

then they estimate the topics that are covered in those

documents, but what are the topics?

They end up being just like a list of commonly

used words.

I mean, at least in the most common topic model

varieties, and then the analyst just looks at those words

and says, I think this is about abortion.

I think this is about the economy.

OK, fine.

That, that's, that's like that's what people do.

You got to do it, right?

Like it's necessary.

The consumer reader needs an interpretation.

What's the point of doing complicated statistical stuff to get

clusters or topics if you don't tell the reader what

it means, right?

Like you got to tell the reader what it means.

But it can also be sketchy because people disagree about

how to interpret text.

That's life, you know that.

I mean this is like, you knew that before you

started this class, people will read the same text and

disagree about what it means and so.

A person who is labelling texts that are in a

labelling a cluster of texts or labelling a topic covered

in text may have one interpretation, but another person may

have a different interpretation.

That's just life.

You got to deal with it, but we can try

to be more principled, so let me give you some

rule of thumbs.

I think these are nice guiding principles for thinking about

how to label clusters once you get the clusters.

Um, this also might guide the way you think about

the number of clusters you're estimating even, but this is

also these, these principles should be thought about in the

context of topic modelling as well.

When you're thinking about how to label your topics, like

you should do some of this stuff as well.

So first of all, choose the number of clusters carefully,

thoughtfully, and transparently, as that's going to, I'm losing my

voice, as that's going to affect how you interpret the

resulting clusters.

You'll notice that if you cluster with more or fewer

clusters, you're going to get different clusters a lot of

the time.

And so like the interpretation of those clusters is going

to depend on the number of clusters you chose ex

ante, and so you want to have some.

Careful, thoughtful, and transparent way of choosing number of clusters,

like elbow plot.

I mean, it's, you know, these, there's no hard and

fast rules of this, but if you show me an

elbow plot that you use to figure out how many

clusters to cluster, I'm going to be much more convinced

that you did, you like made the choice of how

many clusters in a principled way rather than just kind

of guessing.

You'll notice that in the section, we're not going to

do any principles that we're just gonna cluster based on

the number of clusters that, that I tell you to

do.

But that's bad form, that's just me, you know, trying

to get you to learn how to do the clustering,

but in, in real research, you should care a lot

about the number of clusters.

The other thing you should do is you should carefully

read a random selection of documents in each cluster that

you estimate.

Uh, the recommendation in the book is 10 to 30.

So you randomly selected, and then you should take notes

on those documents and synthesise those notes into a label.

And more importantly, you should provide your notes and your,

your thought process, um, to readers of your research or

readers of your, of your clustering methods.

You can also have multiple people provide labels and compare

them.

So, uh, you can have research assistants, colleagues, whatever, um,

try to label the clusters that you've created and have

a bunch of different people do it.

And if there's a lot of agreement between them, then

you know you, you're hitting on something nice.

You can also take this to even higher level and

you can run some experiments where you have real people

in the world, recruited online or whatever, like read your

uh evaluate the labels for your clusters and tell you

if they agree with you or not.

That's, that's also a nice thing you can do, particularly

if you want to know how the wider public might

think about the labels that you're, you're creating.

You also might want to provide a list of the

most distinctive words in each cluster that, um, uh, gives

you a sense of what the cluster is about.

That's not always possible, but some clustering methods don't, don't

work in that way.

But with K means clustering, you can provide a list

of the most, um, the, the most distinctive words of

each cluster.

Um, sorry.

Only 2 more lines, so I think I'll get to

it.

I'll get through it.

You can check if your clustering labels slash interpretations would

hold up if you use different numbers of clusters.

If, if, if like adding another cluster is going to

wildly change your cluster labels based on your own process

you're using to deliver your clusters, that's a bad sign,

right?

So you want, you want your labels, that's kind of

connected to this, this point up here.

You want your labels to be pretty stable.

At least for small changes in the number of clusters

that you're making.

Obviously you make massive changes, they're going to be very

different.

So, you want to know that they're robust in the

sense that if you add one more cluster or take

one cluster away, they're not going to dramatically change the

labels.

Um.

And then, uh, finally, importantly, always provide the details of

your process and your replication materials.

So if you're doing a research study, this is common

in, in academic context now.

Uh, you're supposed to provide replication materials which are code.

An appendix that explains your process and actually you should

spend a lot of your replication materials talking about how

you do labelling because that's like, you know, that's where

the, that's where the real hard, hard people are going

to go after you, right?

So that they're gonna look for that kind of stuff

and that's, it's important to provide to people um.

Uh, you know, this is what I, when I read

topic model papers, when I read clustering papers, like my,

I always am like, well, how did you make these

labels and are you, are you sort of like pushing

me to understand your results in the way you want

me to understand them or is it a reflection of

something genuine in the data?

Because the labels you choose are, you know, that's what

people will queue off of, right?

Like, reality is the reader is going to read the

labels, they're not going to look at all the details

of all the stuff you did to create those labels

and um a persnickety person might want to, want some,

some evidence that what you, you're telling them is right.

OK, cool.

So, we're done, uh, with this lecture, yeah.

So, As I said, you're gonna do topic models next

week, which is, yeah, the last topic in the kind

of standard QTA tool tool kit, and then you're going

to go to, um, then, and then you're gonna go

on to talk about word embeddings and um language model,

like large language models.

Uh, I'm not exactly sure 100% what Friedrich is going

to cover because I, I haven't seen it myself, um,

but, uh, he's actively in this, in this research space

and so I think you're going to find it to

be, um, quite nice and, uh, hopefully you walk away

from this course with a clear understanding of like what's

going on when you're, when you're looking at chat GPT,

um, administrative stuff, uh, you will get a problem set

very soon.

Uh, it's a formative problem set, um, and It's going

to basically, it's gonna mostly be coding type stuff, right,

because one of the goals of this course is for

you to learn how to implement some of these tools

in our using quantitative package and even sometimes doing things

manually.

So it'll be, it will be that you're also going

to get a uh some example questions that will help

you prepare for the exam.

Because there is no, this is the first year of

this exam.

Uh, this predates me coming here.

I just, you know, I just, I just work here.

I don't know this, I was told there's an exam

in this class, uh, and there hasn't been an exam

in the past, so there aren't any exams in the

library for you to refer to, but we are going

to provide you with some sample questions just so you

get a sense of what the exam format is going

to be like.

But I can tell you it's going to be a

mixture of Multiple choice, true false, short answer, and coding

questions.

That's like the four broad categories.

So you can imagine the multiple choice that true false

are going to be conceptual type stuff, right?

Like bigger pictures in most, most of the time.

The short answers are going to be mostly calculations.

So you're going to be calculating stuff.

And then the coding questions are going to be like

you're gonna either be given code and asked to fill

in stuff or find bugs or given output and asked

to interpret something.

We're going to give you more guidance than that.

So you're going to get the example questions and you're

gonna also get some kind of document at the exam

that you can use as reference.

It's closed book, the exam, but we're going to give

you something.

We haven't made it yet, but it's gonna be something

that is going to help you do the exam.

My, I am not, my philosophy about exams is that

like, you know, I You got to be responsible for

all the material in the course, of course, like that's,

you know, expected, but I don't want to.

This is not like a rote memorization class.

I hope it's, I hope it doesn't come off that

way, right?

So.

We're going to give you some stuff on this like

reference sheet that's gonna uh uh free you from having

to um uh walk into the exam, having memorise everything

and the idea is that we will give you this

reference sheet in advance that you'll know what it will

be on the reference sheet so that you're not gonna

have to.

Focus your time doing things that I think are a

waste of time, like memorising formulas and stuff like that.

Now, there might be some formulas you might need to

memorise because they're simple or something, right?

Like, I hope you know how to calculate an average

without me giving you the formula for an average, um,

but we do want to give you guidance for how

to be successful on this exam.

Um, and how to treat it like what we think

of it is, which is like a pretty straightforward, uh,

exercise where you sit for two hours and demonstrate that

you learned stuff in the class, and if you did,

great, and if you didn't, well, I don't know, it's

not so great, um, uh, and, uh, we don't want

it to be, I, I don't really want it to

be like tricky, you know, it's not like that, it's

not the point of this.

The point of this is just like, show us that

you, you learn stuff in this class and that will

be great.

Cool.

And, uh, I still have office hours, obviously through the

rest of the term, so feel free to come by,

if you want to talk about the stuff in the

1st 6 weeks of the, of the class, um, and

then you're gonna have to redir next week, but I'll

see you in seminar, uh, this week, uh, and then

rerich will pick up everything next starting next week.

OK.

like how am I gonna study for this.

This is muted the whole time.

Did you see.

Yeah I was I had a.

So I, I.

So.

I actually.

Yeah, but it's also um because it's at least like

borderline because I did shit in the.

So.

I just didn't have time.

I didn't references which I'm really excited about.

Yeah, she's like we're literally the same yeah.

I'm sorry.

It was good, I did not read.

What about you?

It's really nice.

I'm like maybe maybe there's no more.

I read on where I could have read on reading,

but that's OK.

That's my true.

I truly don't understand how people back so fast.

Yeah, I mean, there's people who don't back.

Bye.

Uh 8 to 8 because I mean I know horrific.

Tomorrow is That's Mo That's You know, and the love.

OK I feel the temperature in here is.

This is Like I come back from the workers' group.

Pretty specific.

So yeah, there's companies that are like, what sorry what's

it was just upsetting that so no it was crum

um.

Yeah.

I I do little no.

all over my And you know like she's done.

OK I'm really sorry Let me see.

Um They can.

Of course I did.

work.

How I had that event I would.

I ended up interviewing like an early careers person there,

which is a part of it, and then she asked

me to go to their office, so I went to

the office and I applied for that scheme.

I did an interview on like Friday and Monday she

called me and said, Do you want it?

It's like work experience me up I actually I I

I vacation next year I need to have like some

form of I need to not be I need to

not seem stupid, which I feel very stupid, but.

Yeah, go back to me which is phenomenal.

I do think they take ages.

Um I, I, I apologise in advance the person I

about to become oh over the next.

12 months.

Football.

I'm just gonna, I thought you were gonna be like

I'm moving out.

big Right, I know what that's fine, so.

OK.

And would you, would you go?

Try.

tell me I'm literally I'm 10 minutes away.

I'm not safe.

And the I just show up where you are, which

would be very weird, which I would never do.

Some to I.

He's such a lover.

So yeah for like becoming an organised.

Oh no one.

Yeah.

Yeah.

And No.

Yeah, that's true.

It shows how dedicated I am because instead of staying

in this, I get all my way up.

I.

Oh, I never mentioned.

that.

It's been a long no one sat down, so I

was just sat and I sat down.

S.

I always wanted to fight.

20 year old.

But the 2 old I say he was like 15.

I was like, I'm 17, you look way older than

me.

I quite down, down, and so that was.

I just.

Uh Just a you know a sea of winded.

I OK Yeah No, I was not.

And you I want that.

I just.

Stress like the this is the wrong way to send

it back.

Oh.

I Is that I think you should take her.

Yeah She.

But Oh Yes.

So that's why um Oh, the the the.

Just wait, that's all S Yes.

There was a st.

And Well.

Oh.

Well she's only 2 so she's.

You know the lecture they got you know.

Lecture 7:

But it was like, yeah, it was very weird sounds

terrible.

The same math classroom where my parents met.

God, really?

that's, that's really nice.

That's lovely.

I'm not detracting from the trauma of learning math in

a basement, I will say, but like my parents went

to my parent teacher conference and we're like.

That's Yeah, it's passing by so fast.

It's already we can see it, yeah, but not that

much.

Good, I don't think my brain can pull with more

information.

It's like it's pretty like.

It is definitely fast.

Actually, I will say the bus actually makes a lot

of.

Yeah, it's very grounded and like things that I like

reality.

It doesn't feel like you're talking to explain.

He's like, here's this, and this is why we use

it.

And here's an example of it.

Although he's done teaching so.

No, it's a, he's, he's great.

I thought he was.

I think he like, I thought he was very like

I've been joking about a couple of his all stars.

What his teaching style He's very.

I No.

It's like a Yeah, I'm a simple gal.

I'm gonna game.

I didn't want like a big.

What is your, what's my, that's a great question.

OK.

I think we had planned to do it, um, summer.

But um so like, yeah, but then we kind of

like had a decision I like do we go to

grad school or do we get married and ultimately the

money that we would.

Yeah, you both came here, so I think we're just

gonna be local.

A lot less stressful.

Are you afraid I A maximum amount.

Because I was like I am not here.

And I'm gonna lose something.

social Well, he's in Ohio.

Yeah, I know you met him, right?

Yeah.

He's in MY 405.

What is that?

Um, it's policy that I.

The programming he's been telling me about it.

I'm like, wow, that would be useful for me.

It's a really good and it also talks a lot

about like.

It's a study, it's like an overview of how to

do.

That would have been nice.

Like someone should talk about that.

I see you know what I even.

Yeah.

Yeah, I was like I'm either gonna take more.

I see.

I don't want to one class that was like.

No way, yeah, I'm just gonna be chatting.

Not sick.

No way.

It's a crazy, you know.

I'm not Yeah, yeah, yeah, you just plus ones get

me on.

Yeah, I'm about to go It.

I know you don't like this, so I think we

should be good.

Last two years has been like this I mean this

year they pushed it back to last week February to

like the second one.

I truly there's like it sucks like you don't want

to have to spend but it's.

Yeah.

And there's this guy on YouTube who like takes you

through the person who is like a class.

Yeah, that's good like he says that he adopted, yeah,

yeah, yeah, that's what she was saying that she missed

the first.

And she was originally a.

I.

That's fucking.

Yeah, only for like a solid week and a half,

nothing, nothing bubbly sky, you know.

How do you feel about I I that.

So I've been hogging it for a week.

We're just gonna like my I didn't understand.

like it's like.

And.

But I think I mean it become a disaster, but

I think it's like a really good way to just

like there's like some words and some charms and like.

I like I like and then I.

Is it still right in this or is it?

So give me like I don't.

It's, it's, yeah.

I think machine learning is not to like really put

in, but I don't think.

I didn't even play.

Yeah, yeah, yeah, how about you?

But it was good.

I went to uh the varsity over the weekend but

like I was good as well because of the group

chat, only like 12 people said they would come, we

had like 30.

It was just a baby.

I know be mad.

We had a random Indian bloke up it's been like.

it was our first time going to that club.

It's either he'll bring a different one or yeah yeah

he's 3 years older than me.

What?

Yeah, older than like I think he's trying to.

I mean if I did the math correctly graduated uni

when I graduated high school.

Oh, it's a two way.

No, no two ways like.

We don't know yet, the other semifinals today.

No, no, I, it's the regional Sao Paulo championship.

Yeah, he's smart and you can, you know what's actually

kind of smart people who are in Brazil at the

beginning of every season there's like state tournaments.

So there's a real tournament Sao Paulo.

Yeah, it made a lot of sense 30 years ago

when Pele was playing the regional tournaments, especially the ones

who just always the best teams, it was as important

as.

It was a huge deal.

50 years ago it was hard to travel from Sao

Paulo to Bao, you know, it's not like today.

Nowadays it makes less sense, but it's still around, and

every year there's talk about changing or change the format

so there's fewer games, cause for the big clubs likes,

like it matters for the small clubs, you know, like

these are all like small clubs that are that are

from the countryside and so so for these clubs it's

a huge deal, but.

For because it's all garbage like you go through all

of this stuff and then you're like, and like the

best crumble and which is like the next level you

go to it's like actually, so yeah, I mean every

talk there's every year there's talk about their games.

No, no, no, it's, um, oh, yeah, they used to

be to some, yeah, one or two people, yeah, the,

uh, taxi man.

that's crazy.

But was it in Paraguay in Paraguay in Brazil and

masters in yeah.

Great George Street.

I guess.

I just Let's do it Is this is literally my

yeah yeah.

Who's who's Oxford star?

Like Great.

So, I've uploaded the materials, um, see them on the,

on the website.

Mhm Mhm.

I want Really.

Yeah, so I think I'm gonna need to.

I have to get.

Hm.

Ah there we go.

Great.

OK, does everyone have that version here?

Um, well, I shouldn't say 2024, just to make sure,

um, if anything went wrong with the data, um, upload,

just let me know.

Um.

Great.

So, I'm going to do the last 4 weeks here.

Um, I think we have a lot of really exciting

materials to cover.

Um, let me Do you the overview.

Um, so we're starting with probabilistic topic models today, uh,

which are a nice, uh, fundamental, uh, for kind of

also the rest we're going to cover.

And then before we are going to continue where we

need to then do a bit of a methods review

and we'll start with this next week.

Um, and review some basics of neural networks.

There will be some overlap with other courses you might

be taking, but there is no way around this because

every recent advance in this field has been neural network

based.

So, um, uh, that will, that will require this review.

And and based on these fundamentals, we also do a

bit of linear algebra, so we really don't need more

prerequisites than than for the previous part we'll recap everything

we need.

We then be able to to discuss static word embeddings

and then with two large language models that have dynamic

embeddings and so on.

Um, do you have any questions about kind of the

whole time?

But we're really going to cover, yeah, I mean, there's

only one lecture about it and so on, of course,

but we're really trying to go to uh to kind

of the state of the art in this field.

Um, and also I'll show you a couple of ways

which you in which you could use these to, uh,

to work in, um, you know, with, with social scientific

data and your your capitals and dissertations and if you're

interested.

Good.

So that's the outline for today.

Um, It's it's hard.

So, topic models.

You can think of them I like to think of

them in one way, not to necessarily give me the

the true topics.

There is no ground truth for this pile of data,

but more as some kind of automated reading, um, that

structures, uh, um, gives structures the data and gives me

a broad overview of themes that it contains.

Think of it, um, you have a library of books

and you want to see what are certain um uh

themes that are common in the library.

So we don't require any labelled data.

Do you know what label data is usually what people

call labelled data.

That's a bit of a machine I named after.

Mhm.

We have um like some combination of X's and we

know the why.

That's right, exactly.

So uh you have um data stored usually in an

X, like kind of the statistics kind of to think

about it, and then you have a Y that you

want to predict, um.

Think of it you have texts, for example, and you

want to create the sentiment, things like this.

Um, here we only have the text, so we really

only have the X, the data contained in, you know,

take, say a document feature matrix, uh, to use the

terminology of this course.

Um, and then there's this fundamental work around 20 years

ago, um, by, by Bla co-authors on the latent allocation

trying to build some probabilistic topic model that extras topics

based only on these texts.

Um, it's a mixture model and with this I mean

that, um, every document is assumed to be able to

contain or usually contains multiple topics, right?

So it's not that, um, each document is constrained only.

One topic and so on, and I'll show you this

in a moment, that should be very clear.

And then a recent alternatives are, for example, clustering of

transformer based events, um, and that is a bit similar

to what you did, I believe with Ryan last uh

last lecture, um, but we'll need to revise the neural

networks more for this to make more sense.

um, but I'll, I'll have an outlook at the end

of this, of this particular week and then we revisit

it in future weeks.

So that's first of all an illustration from this overview

paper from 2012 from Li um about these models and

here you see what, what is meant by mixture models.

A document is, is, uh, is modelled to contain multiple

topics, um, and it, it's made out of these.

This is, for example, you could call genetics, biology, neurology,

and then computer science, and it's a combination of words

from all these topics, and it contains these topics in

different proportions.

OK, so that's how you imagine a document was generated.

So what do you think?

When is this a realistic?

When is it, think about the length of documents.

When is this a good, when is this uh maybe

not so good assumption?

This underlying model.

When the topic is, I mean, when the text is

long, maybe not would be a good idea because the

text would cover a lot because the text would cover

a lot of topics.

That's right.

So perfect.

So when you have a long document, that's a, that's

a good model because it can um depict a lot

of topics that are in the document.

If the document is too long, then at some point

it won't be able to track.

anymore, so we usually need to chunk them.

But imagine you have something like a social media post.

That's tougher because it might be, say, a short tweet.

It might be on one topic.

It might not really be a good text for that

extra moral assumption.

So what you often see is that people combine tweets

into larger documents and then run these models or something

like this, or they run different models on shorter documents

where we assume that each document is about one topic.

These are kind of already the conceptual details, but I

think they are, uh, what is part of what's most

relevant for us working with these data.

Good.

OK, so now let's, uh, let's look at this uh

canonical example of the late and reallocation.

So before we do this, I want to also revise

a couple of concepts, right, that that that it's as

clear as possible how this underlying model works.

So these are different normal normal distributions I assume these

you've seen with uh with different means um and different

standard deviations.

Um, and the notation with this tilda here, um, needs

to sample from that distribution, OK.

So I draw an X from that normal distribution, that

would be the blue 10, sorry, the red one is

the standard normal.

So I draw from that distribution.

So most likely I draw a 0, quite unlikely I

draw a 3 and so on.

OK.

So I'll sample from that distribution.

That's what that notation means.

So now when I draw one sample from the distribution,

that could, for example, be minus 1.124.

Right.

So, so that's what I mean by sampling from the

distribution and just with a kind of super simple example

with the normal.

So that's normal, you can also generalise.

Imagine you have a normal distribution in two dimensions, so

X1 here, X2 here, and now you have the normal

density in this room.

And now I can sample and what I'm sampling is

a point in that space, somewhere I might sample this

point here.

OK, so that's something I drew and something in the

middle of the space because we're looking at normal distributions

is more likely because most probability masses in the middle

of the room.

Um, but in the end, I sample what you could

think of as a vector with two elements.

OK.

So that's a multivariate.

Most likely I sample 00.

So here I might have sampled, that's quite unlikely that

I went here, a minus 0.1.

But these are just, you know, numbers are made up

to make it a bit more concrete.

So I sample from distributions.

I might as well sample from multivariate distributions, and they

give me lists of numbers depending on how many, how

many dimensions, the underlying support has.

Well, and you generalise this with the parameters here, the

mu is now also two numbers here it's 00.

These are the means of each dimension, and these are

the variances and covariances.

Good.

So next we need to, like we're almost there, I

think one or two more slides you need to review

it, OK?

Not yet a probability distribution, that's just more, you know,

think of a mathematical concept.

So what this is, you take a 3D space and

you draw a triangle into that space such that you

intersect with each of these axes at one.

OK, um, so that means that any point on this

triangular plane, if you think of it just like now

2D, so you think of this plane, every point on

this, um, has these three coordinates that add up to

1.

Yeah, and that's it.

So this is basically the set of all numbers that

add up to one.

OK.

So, you know, a point on that simplex might be

005.8 and 0.15.

And you can, we can visualise this with these three

dimensions and the triangle, but you could also imagine this

in higher or like, you know, you can formalise this

and then extend this to higher dimensions if you just

trust the math, but it's tougher to form to visualise,

OK.

But again, in higher dimensions, imagine we had a simplex

over 10 elements or we would have we would force

these 10 elements to always add up to 1.

That's the property that we care about.

Any questions about about this year?

You'll see in a moment where we do this because

otherwise the generative model that's under the LDA is impossible

to understand, and then we would just be very hand

wavy and I guess that's not the point of reports

like this.

So the last thing that we now we have to

combine this year with the probability distribution.

So think of multivariate probability distribution.

You might already see where I'm going.

I'm just saying now we put a probability distribution over

this space.

And that's the the distribution.

And that's where the name from this topic model comes

from.

OK, so you take the triangle with a neat property

that every point on that triangle has these three coordinates

that add up to one.

That's all we care about.

And and then on that triangle, we have a probability

density function.

For that here, it's more likely to draw something closer.

That's X, Z, and Y, closer to Y, so that's

more likely to draw something closer to these values of

X and Z, different values of Y and so on.

And these are what are called contour plots.

Um, you might see these sometimes in papers or so

on.

That's the same thing as this just from above.

And these are the, where most density is.

So that is the simplex where most density is exactly

in the middle.

So I would, I would most likely sample X1, X2,

X3, that is exactly in the middle.

Um, this would be more similar to this one here.

That's just the 3D depiction, 2D depiction.

Does that make sense?

So these are called contour plots, OK.

So now I might draw from the deerishly distribution.

Just like I drew from the multinominal, so from the,

from the multivariate normal with these two axes.

Now I have 3 values I get and the only

added tweak is that these always add up to 1.

So whenever I draw from that distribution, is this convenient

vector where if I sum up all the elements, they

sum up to 1.

And you might already see where this is going, right?

So these you could think of topic shares, right?

So, and the topic shares like every word should be

assigned to a topic.

So the document then has these like consists 70% of

one topic, 2% of the other one.

That's why I want this property that things are up

to one, similarly to one science topics.

OK.

Good.

Um, and then the last thing, so that's key distribution

number one, we did all the recap before to arrive

at that distribution, redistribution.

We sampled from a simplex basically and put a probability

that somewhere on the simplex.

These are parameters that define where, you know, the mode,

the highest point of that distribution is.

Um, you can think of these, these are scale parameters

similarly to how you would parameterize the normal with a

or a standard deviation, right?

For the variously you parameterize it with these 3 values

if it's 3 dimensional.

Um, OK, lastly, this also extends to more dimensions.

Imagine I have a duly over 5 dimensional space, um,

not 3, but 5.

Now I sample from that drily.

What will be the property of the 5 numbers I

get out of this?

So I have a duly over 5 dimensional space, sample

from that drily.

What is the probability like say the main property of

the resulting numbers that I get that we care about.

Mhm.

Yeah, yeah, they sum up to one, exactly, that's it,

OK.

So the cool thing because they sum up to one

is that technically every draw we get from the drily

is in itself a distribution.

I'm multiominal.

Right?

So we draw from Dili, and every draw from the

Dili is in itself a distribution from which we can

draw again.

And that's the trick.

And here, you know, imagine we got that from the

Jewish, and actually this is also in itself a distribution,

namely a multinominal distribution, and we could possibly draw from

the multinomial, what would be most likely the value to

because that has the highest probability, right?

Um, multitinomal distribution is just, um, uh, a distribution over

a discrete set of choices.

Again, lost or probability, this should sum up to one.

That's why we need the dri, right?

That's why we need this to be over simplex before

because otherwise this thing wouldn't sum up to one.

So these are the two distributions.

That's kind of a bit like, yeah, first term statistics

stuff, the multi uh nominal, but this is a bit

less known that they originally defined over this syntax to

get this set of numbers that adds up to one.

Good.

Uh, any questions about the stats review?

OK, so now LDA, um, allocation we want to define

the generative model.

Um, you have seen a lot of generative AI I'm

sure over the recent years, um, we'll discuss what you

would most commonly refer to these days as general AI

and language in in our lecture on large models, of

course, but also actually here to At the topic shares,

we needed to define a generative model of how we

assume the data to be created.

Just here, the model helps us to pack out topics.

It's not very good at generating new data.

Here the model for us is just to help to

back up the topics and so on.

Uh, that model we can't use to, you know, generate

new data like.

OK.

So, um, imagine you have a corpus.

Corpus is just a set of documents like previously, um,

you have the documents.

you have the documents and then each document has ND

words, OK.

So, um, now you assume this weird statistical model to

have generated these documents that is absolutely not realistic, but

that model then gives you the ability to back up

the topic shares and then do topic analysis and so

on.

That's the whole trick.

So we say each document is contained containing weights of

topics like we saw before, genetics and computer science, and

then each topic is actually containing weights of words.

So the topic is made of weights over words.

We'll be clearer in a moment.

So let's, let's imagine like a super sty that corpus

first, um, with 5 documents.

Each document has the same number of words, 8 words,

that's it.

Um, and then also we assume there are 8 unique

words in that corpus, not more, like really simple, and

we say there are 3 topics.

How would that model here assume the data would have

been generated?

OK.

So for each of the 5 documents.

Draw its topic shares from this year originally, OK.

So we have 3 topics.

But first, the first document, we would draw um uh

this topic share.

OK.

So that document consists predominantly of topics um 2 and

then a little bit of 1 and 3.

And then for each of the topics, we now draw

from another another distribution, a topic vector.

OK.

So we have 3 topics, but each topic we set

contained 8 words.

So we now draw this vector here with the probability

of all the words, right?

So topic 2, for example, um, would be very heavily

uh putting weight on, on the words 8 and uh

the word, um the word set.

As you see here.

And now we can fill each document with words.

So for each document word position, um, so that's document

one, word one, document 12, document 13, and so on.

So we iterate all of these, we can fill them.

We, we, uh, define first of all, um, the, uh,

the topic.

So, uh, we, uh, we get the topic, so we

might say we draw a topic 2, and then from

that topic, we would draw the words.

So imagine the first word position and that document is

topic 2.

Now we know that vector that we find before and

from that vector, we draw that word.

OK.

Let me So imagine we start with the first word

and the first document.

OK.

So we know because we defined that before, that document

has um has uh these topic shares.

So let me first define which topic this is.

Now I draw from this.

So most likely this will be topic two.

So say I do that and now I have defined

this as topic two.

And now I turn to this vector here, which I

defined before, um, and I would draw the specific word,

most likely word 8, whatever that may be, that may

be hello hello or something like this.

So the first word would be.

And then I continue with the next word and so

on.

Um, where do these distributions come from?

Like, why is this, you know, 0.5, 0.4, and so

on.

We just assume whatever documents we see in front of

us have some underlying latent unobserved structure that has these

numbers.

Yeah.

So we say the documents we see come from some

structure like this, which is of course, you know, mad

in a way because the next word we would sample

might not need to form a coherent sentence and so

on, and this minimum amount of structure gives us the

ability to back up the topics.

OK, so we can like this is usually what you

would what you would see um more abstractly.

So for each document, I get, get the topic shares.

So I know document one consists of mostly topic, say

3, document 2 contains a topic one and so on.

And then for each of these uh of these um

specific topics, I draw from another dearishly, I draw the

word distribution.

So maybe to spend a bit more time on this.

What do I mean with this?

We look at topic 2, OK, and topic 2.

We say, um.

It's a distribution over all these 8 words, OK, so

I think I have this example on the previous slide.

So we have a bunch of 000.1.

014.

Alright, so imagining that that was how topic two was

generated, um, it would be mostly consisting of the last

two words and a little bit of a word.

Good.

So imagine, OK, taking a step back.

Imagine you see a library, what we said in the

beginning.

The assumption that this model makes is that the library

was generated according to this process here.

That's what the what the underlying model says.

The model says, say documents are books, OK.

So book one, say maybe pages are better assumptions for

the, for the mixture model, maybe book is too much,

say pages and books.

So each D is like a page in a book,

and then we go first page, first word.

Um, yeah, first of all, before we even start creating

the library, we need to define, uh, these, uh, these

shares for all of the pages.

So page one consists of so and so much topic

123, page 2 consists of so and so much topic

123.

And then we want to know, well, 123, are distributions

of our words, these topics, right?

So we also need to know, um, uh, how many

what proportions of words in these topics.

So for each of the topics, we generate these types

of vectors.

Um, and now we have all the proportions of topics

and documents and all the proportions of words within topics.

And now we can fill out our library, first page,

first word.

What is the topic?

Um, drawn from the topic.

And then what is the word, draw from the um

from the related vector that has all the words, but

the word there, go to the next position, fill it

out.

We make no sense in terms of English, but that's

the, uh, that's how we assume everything has been generated.

And the interesting thing is by making this very, you

know, unrealistic stylized worldview, um, you know, and making this

into a model, we can then back out pretty, uh,

pretty insightful topic shares often.

OK, do you have questions about this, even if they're

very broad or does this kind of make sense or

shall I repeat certain parts or everything?

Um, yeah.

Mhm.

Uh, so if I understand correctly, you're drawing from two

distributions, topics on a page and then words in a

topic.

So then if we're thinking when you mentioned before that

each draw from the simplex was a multinomial distribution.

I think I'm not understanding how these are connected.

Yeah, yeah, yeah.

OK, great.

So, um, first, so this is, this is the blueprint

of how we think our library came to life.

Um, you know, or how, how we model the library

came to life, OK?

And first of all, it's exactly like you said, there

are two division distributions.

One is topic shares within documents, and the other one

is word shares within topics.

The topic shares within documents are usually depicted as thetas,

and the word shares within uh within uh topics are

depicted as these betas.

OK.

So first of all, I have to, for every document

say these are all my pages in my library.

I have to say each of these pages has an

internal topic share distribution.

So I assumed someone who created the library had all

these vectors.

So they knew all the internal topic shares for each

of the pages.

And then I say, well, there are K topics wherever

K comes from, that's a huge thing to determine and

tricky and say I have 3.

I mean a library like say rather 3 million or

something, but like I have so many, um, I have

so many topics, and then each topic, topic is given

by the distribution over words.

See.

Um, and how do I, how do I imagine these

topic shares were created?

Someone who generated the library drew them from a Dirichle.

OK, now these are set in stone.

Each page has a topic distribution.

Each topic has a word distribution because someone drew them

from a Dirichle and now they sit there.

And now I can say with given these things, I

cannot do this huge loop over every single person.

In these pages, every single word position in these pages

determine for page one, the first word position, which topic.

Once I know the topic, sample the word, put the

word there, next one.

And now I'm actually, so my first step is to

sample from the Jewish to get these shares, and the

second step is then uh to sample from the multinominals,

which are these shares, to fill out the words and

the documents.

Other questions.

How is the topic distribution created like that.

Yes, so like I think great, great point.

So the uh the the trick here is to say

there is some, so these are what we would call

latent variables.

We don't know them, they are, they are, there are

some kind of, um, you know, unobserved variable.

We assume that our library was created by someone with

the true values of these.

So by having the text from the library.

And by assuming that structure, we can back out these

values.

That's the whole procedure.

So we have the text in the library, we assume

it's been generated this way.

So let us try to estimate all the theta and

all the betas um to uh to estimate all issues.

So it's like a maximum likelihood.

So there are there are different ways to do this.

some Asian procedures we won't focus on this, but yes,

like, yes.

So if you think of a Beijing, I don't know

whether you Beijing statistics, but there's a likelihood turn very

prominently in the bus area distribution.

So it's very related to what you're saying.

Good, so that's the general, that's the general case here.

Each document to share, each topic to share, and now

for each position, I draw the topics.

These are multinominal draw from the multinominal, have the word

next one.

And that's, that's just how I assume that library came

to life.

And by making that assumption, I can back out the

theta and the beta.

So in the paper, which, you know, it's like out

of all the readings, if you want to study this

more or you want to, uh, work with these things

in your research, then that's a great overview paper and

they often use this plate notation.

So I did all this like these previous slides for

this plate notation hopefully to make more sense.

These are the parameters that parameterize the two distributions.

This means where they Have their moulds, you know, think

of like this here.

These vectors, the alphas, you can also call it beta

or hello or whatever, right, but this vector here determines

where the mass of the distribution is in the corner

in the middle and so on.

So all that plate notation means really is you initialise

these two distributions with the scale parameters and now I

draw all the topics that exist from my library, um,

like all the distribution of the words and then the

topic shares within documents and then each word position in

the document is um is first I determine the topic,

um, and then once I have the topic, I sample

um from that topic with the specific word.

OK.

Good.

I repeat this for the documents and I repeat this

for N words in each of the documents.

So I have my two dear space.

I'm actually here, I imagine I have K of these

um uh because I I have all the different topic

shares and then I, um, I, uh, from the topic

distribution, I determine which topic the first uh the first

word.

like that word and that document is, and then I

sample the specific word from that topic using the beta.

So really I get the, the word in the documents.

So what the words in the first page on the

first position, I really get it only by knowing the

um the topic shares and by knowing the, uh, the

distribution of words within each topic.

Otherwise I couldn't get, I couldn't get that word.

OK, so now what I, what I said already is

like, you know, this is of course like extremely stylized,

but it gives us this ability to after running such

a model, getting this.

And this is usually the output that you would get

from these probabilistic topic models in certain variations.

Um, and from an applied perspective is that's what you're

getting out of them and that's what you're working with

and then say a research project or so.

Um, imagine this.

I say I have a corpus with 1000 documents, could

be 1000 pages, and then I say I have a

total vocabulary of 10,000 words.

OK, so there are 10,000 unique words thinking of the

DFM.

So what would be the dimension of the DFM here

if you think in terms of the previous lectures of

the bos?

I have that corpus, um, what's the, what's the dimension

of the DFM?

That's right.

1000 by 10,000, right?

I would have 1000 rows, one row for each document,

10,000 columns, a column for each word, OK.

So that would be my DFM here.

Um, and then I say, wherever that comes from, for

now, I just assume it, there are 3 topics in

there.

I know this, um, estimate this for 3 topics, and

that is a huge part of how to calibrate and

estimate more, of course, to know the K and or

pick a reasonable one because All of these are these

latent variables that are unobserved.

So we have to, to kind of use that a

couple of summaries that use a bunch of summary statistics

to select a reasonable case.

There's no um there is no way for us to

find um a true.

So once we run these models, we get these two

matrices, or, you know, less fancy golden tables.

OK.

So we get this out there and this year is

of dimension 1000 times 3.

And this year is of dimension 3 times 10,000.

And this slide is important.

So I hope that this makes sense for for everyone

when you think a bit about it.

So we have 1000 documents and we assume each document.

is made up of three topics, and these are the

shares.

Document one is page 1 is made up massively of

topic one.

On page 2 is made up largely of this, of

topic 3, and then a bit less or half that

size of topic one and a little bit off topic

two.

And then the last page is mostly topic.

So once we run this on our actual data set,

um, a book with 10,000 with 1000 with 1000 pages

and 10,000 unique words, we'll get this.

For every page, we get a distribution of what this

model says, uh, the topics are.

And now we have no idea though what is in

topic one, right?

So we need to know what's in topic one.

In topic one.

Are these words, that's the distribution of words in topic

one.

So, topic one has very high weight thinking that these

are 10,000 elements, very high weight on word one.

topic two has very high weight on the last word

that has a very high weight on the second word,

and so on.

So really, what topic shares are when you see topic

models, and that's also true for transformer-based models, they are

really just probability distributions over words.

So each topic, so question to you, is it true

that each topic actually contains all the words in the

corpus?

Looking at this, at this, at this, do you think

it's true, it's a true statement that each topic contains

all the words in the quotes.

Can we get 0 or is it that there is

a I mean look at this example.

These plugs, they never work for the the mic adapters,

see.

OK, um, the, the, the numbers in the the dots

here are the same, not the same, but like there,

there's nothing different to these numbers.

Yeah, so usually you, you might have models, you know,

where, where certain numbers here could be zero, but usually

they will be very close to 0.

So conceptually, each topic contains all the words.

It's actually a probability distribution over all the words.

It just has different points of mass in the probability

distribution.

So topic one.

Has more weight on the first words.

Topic 3 has more weights on, like, you know, the

second word, and topic 2 has a lot of words

on the last, a lot of weight on the last

word, whatever that word is.

You know, we would define these would be columns in

our DFM.

These would be what what these numbers are.

The columns, the, the tokens or features in our DFM,

the unique words.

OK, so each topic is a distribution over all the

words just in different proportions.

And then each document consists of is made up of

topics.

And by assuming the structure of how all documents were

created, we can then estimate this.

And that's what we get out of these topic models.

So we won't focus on the estimation, but a bit

to your earlier point, um, we do this usually in

the Beijing framework, but there are also variational methods that

like or you know, methods that uh that use other

forms of optimisation to get these, um, and you begin

with an initial parameterization of the the space, um, and,

and then, um, you use.

Phase rule if you've seen it and you keep sampling

from the data and you can use an algorithm where

once you keep drawing these and updating your distributions in

the end you converge to this estimation output that looks

like this here.

And that means that in the end you get these

2 tables, um, 2 tables given a certain library, a

certain data set, and these tables, uh, they contain the

shares of topics in each document and the share of

words within each topic.

If you're interested a bit in the technicalities, um, in

the Bay estimation framework, um, these are, uh, these are

means like, well these are the parameters in the, in

the posterior distributions.

Uh, these are all, if you think like that could

be a joint posterior distribution and then these are marginals,

but that will not be the the topic or like,

you know, that will be beyond the scope of this

course.

In the end, by assuming the initial structure, we estimate

the whole thing, we get these things out.

That's, that's what we're doing here, um.

So imagine this.

Um, actually, I ran a topic model on Newton's Pippia

because, you know, you could download it from Google Books,

uh, just, uh, probably one of the most famous books

in science ever written.

So I just thought it's, it's fun to look at

it to these newer methods.

Uh, and then I, I read the topic more and.

I assumed a certain amount of topics and then I

got that here.

So first of all, what is this?

This visualisation that you might see in papers in social

sciences, um, and then they, they say that's topic.

So what does that correspond to on the previous slides?

Yeah, so these are, I mean, it's also written here,

right?

So these each of these word clouds that you see

is a role in the beta matrix.

The beta matrix was of dimension K, how many topics

you have, and then B, how many words your vocabulary

has.

So the second row is this thing and it sums

up to 1.

So these are all individual words that might for example

be sun here.

And I depict these words in a word cloud, and

the size of the words is approximately the probability share

in that in that factor.

That's it.

So when you see topic plots, they are really just

weighted shares of the words contained in the topic.

Or shares of the words contained on the topic.

And then this year, say you could call astronomy, but

is there anything, and you might see this in topic

models, they run the say on political speeches and then

one is economic policy and one is health or something

like this.

But is there anything in these models that gives us

name for like in the ones we discussed so far

that gives us names for the topics?

Hm So where does the astronomy come from?

Why is that topic called astronomy?

For my dictionary.

Yeah, so you could, there are some ways in which

you could try to summarise them with dictionaries and so

on.

These days, actually, you could send the whole thing to

a language model and ask it what it would say

in terms of topic.

But thinking about it, that's exactly where this astronomy came

from, just that a human gave it that name.

So really these topic names, they come from people looking

at these plots and then saying, well, that's astronomy, or

you could also say physics or planets or I don't

know, it might not be you know accurate, but uh

you can give it whatever name.

So that's just like a more accurate name I could

come up.

Given what's actually in that specific topic.

Good.

So the topic model, what we have so far, like

that topic model and all these properistic topic models, they

don't give these these names to topics.

They just return the beta vectors, we put the beta

vectors, or, you know, look at the frequency of words,

and then we give names to these topics.

And even if we send it to a language model,

that would conceptually not be so different to a human

researcher just looking at the um at the uh at

the assembly of words and giving it a name or

I'd like to summarise it.

OK, a bit to your point from previously, if you

want to look this up, um, there are broadly two

ways.

One was what I was trying to sketch before the

Asian um methods where you, uh, where you have a

certain, um, algorithm in which you were a certain algorithm

with which you sample from these distributions, you assume an

initial scale parameter and you have a very Very clever

way to sample from them.

And by continuing this for a long time, you converge,

given the data to what the posterior distributions are, and

these are the the theta and beta matrices.

There are other ways with which you can approximately, I

imagine these posterior distributions, there are just some distributions in

some space.

So you can as well try to just approximate their

shape differently.

That's a different.

Bunch of methods.

That's then minimising the distance to some function that approximate

the shape of these distributions.

Why do we want this?

Only because we want the betas and the beta.

That's what we get out of this.

OK.

We assume the initial structure, we have the data set

that could be Newton's Principia or for library.

We run these estimation techniques and we get these, we

get these uh these numbers out.

OK, any questions?

Maybe let's, OK, let's do these 34 slides, um, and

then, uh, we take a break because I assume that's

just a lot of content, um, and a lot of,

you know, names and stuff like this.

But by the way, for all of these things, I'll

show you a quote later on where we actually apply

to, to, uh, things that we, you know, uh, care

about and that we could, uh, could be starting points

for, for little like research projects or so.

Good, so.

Extensions like this, is this really a canonical model if

you look it up, it's hugely cited the LDA, but

of course it's gated.

So then people, um, immediately started even like in the

times then following the years afterwards to work on, on

models that were refining some of these, um, things.

One is that, and we won't look.

In detail, we only look into the canonical model this

LDA, but like one thing is the correlated topic model

and really all that's doing, you know, to, to give

like broad level intuition is to swap this seriously distribution

with what is called the logistic normal and with this,

then you can compute correlation between topics.

So you put some more structure on the whole thing.

Our topics have correlations, and I can actually estimate the

correlations of topics.

I spoke with people like Pablo, which I'll show you

some work with later, and he worked in the department

a couple of years back.

He worked a lot with topic models.

He says he usually just uses plain LDA when he

works with these.

I found these correlated topic models or structural topic models

helpful.

Often these are empirical questions and you have to try

it out with something else.

Um, good.

So here we can compute some correlation.

That's the correlated topic model.

Other than this, it's that, uh, it's that, uh, design

from before, just swapping the drily with another distribution.

So now this is possibly more interesting for us, but

I mentioned the correlated because the library that we use

to estimate these, if we don't give any of these

covariants, it just is a correlated topic model.

That's why I wanted to mention this.

But more interesting is the so-called structural topic model, um,

and that's from a paper from Roberts, um, and Atal,

and they basically amend these topic models that we looked

at before by two types of covariants.

Yeah, so one type of covariate determines how common the

topic shares are within documents and you get it, the

other term of covariate determines how common the word shares

are within topics.

So now imagine I run the topic model over political

speeches, right?

And I get out some general topic shares per documents

and then word shares within topics.

But probably Democrats and Republicans will talk about similar things

very differently.

So how if I put a binary indicator, Democrat, Republican,

and just estimate sort of two different topic models almost,

but like in one goal to have some efficiency gain

where I still have some resemblance of the underlying topics,

but I, the, the, the speech.

of the Democrats can contain these topics in different proportions

and then in particular the way how Democrats talk about

certain topics might be different to how Republicans talk about

certain topics, and you can choose other covariants to make

this differentiation.

And I'll show you an example of this.

OK, again, in plate location, um, you know, this reminds

you a lot, I guess, of the previous one, with

the topic, um, with the topic shares, I determine whether

a word position what its underlying topic.

When I know the topic vector, I can determine what

one and document one.

Word two in document one, I need to know what

topic is it made of, um, and then I sample

from the topic, I put the word and so on.

The only difference to The previous plate notation is that

now how these betas come about and how these betas

come about, so the shares of topics within documents and

the shares of words within topics, they are determined by

some covariates, and they put some parameterization here.

So they say it's a function of a couple of

covariates and a certain functional form that, you know, happens

to work well and you can estimate reasonably well.

So now we're saying beta and theta are actually functions

of the data and the data could be, you know,

I mean they have been all along when we estimated

some of the underlying text data, but they are also

functions of some third type of covariant that could be

um party, you know, whatever, whatever kind of covariant you

you might want to use time could also be continuous

covariants and so on.

So now beta and theta can vary by.

Any questions?

Good.

And then there's other work on this in this overview

article on dynamic topic models.

Um, we won't focus on this, the structural, um, if

we have a, if we have a time component in

the structural topic model, we can get something similar to

here.

Also, we can, uh, we could even with a standard

LDA.

Imagine I have for all my documents, I have the

topic shares.

I could just sort my documents by when they were

published in terms of time, and then I could take

the average for each year and I could generate a

similar series.

Even with the standard LDA.

I have all my documents, I have the topic shares

in them.

So I know when they were published.

I just sort them by here.

I the topic shares and I can plot this, the

topic shares over time.

The the dynamic topic model makes this more efficient and

adds some more structure on it, somewhat resemblance of the

structural topic model, but now with time.

Um, but because we can get that also with the

simpler models, we won't focus on this, but you might

read it in some articles, so that's why I mentioned

it here.

OK, cool.

So now, um, before, uh, like after the break, um,

a major thing is obviously how do we select the

K, right?

So it's completely up to us, um, and we need

to from the data in some way, learn what a

reasonable K may be, and that's a tricky one.

And that's an important topic, not just for these probabilistic

models, but even if we say, um, we use some

transformer-based method, very often we implicitly set the amount of

topics, um, and a good question is how once we

found some topics, what is a good way to evaluate

later on whether that is reasonable or not, or this,

this choice is reasonable in terms of, uh, um, what

the estimation, um, yielded afterwards.

OK, and um so we take a break.

After the break, we uh um we uh continue with

the selection part and then look at implementations in art.

So I suggest um we uh.

Yeah, we can meet at, um, I'll start at 15

sharp, OK?

To No I.

So She's like I'm not here complaining.

The moment I saw 24 hours.

I get a lot of commute calls which I love

um like whenever.

Like this too.

64.

I'm not gonna lie, I have no idea.

We had to do it.

This is Surely for us it's just.

Uh, but I have not seen the day before, so

I don't know why we're actually like spring break, but

it's like so it should be.

Too many be electric shocks.

Yeah.

Hm.

Longer than they had last So it's it's just like

an extension of the last lecture.

I don't it doesn't take 18 hours and she was

like, I was like, oh I love every time like

it's a different way of being progressive about food.

But they all have them.

I was like, I try to.

And that became like so much trouble is about good

like I.

I know you can That's just that you're just like

trying to experience things.

So much just enjoy yourself, yeah, I don't.

I got so much back.

Yeah What.

OK.

OK.

A big Jewish family.

Yeah, yeah, I know that's like my mom's just I

mean she has a baby.

In of My mom likes to tell me what she

she's my age.

Oh, I don't wanna hear that.

That's her, and I'm like, he doesn't matter.

I'm like.

Yeah, that's so terrible, you know, I think it was

like a different what is off the ball and it

was like really common.

But I was like, I don't want I don't wanna

hear it.

The scene from Mrs.

Basel where she like takes her vitamins every single day

just like.

I don't be so sad.

You managed to only watch that.

Like, am I gonna have these two chocolate tarts and

yeah, it's like I'm gonna use the pastry as a

human.

She because she knows that diabet does so there's layers

to this you know what.

I I was like yeah.

I think genetically.

So now I understand the layers.

I can I can for like 20 minutes a day

because I still have the emails not subscribing, but they

still like I get like the um like they're completely

different.

OK, I didn't realise like how I like I don't

remember that much but like there was it was.

It's hard to, you have to look because they've driven

up.

I don't look people's head.

I can tell you how rich and you play like

a fine line.

that doesn't know it's funny.

I was trying to justify my head I lost tone

and then I thought I was out to because they

were like keto.

Stadiums at 34 metres by the beach.

I, I yeah, and um like because we have this

like celebration every year we feel like it's like, but

like they they in the government and the whole like

yeah yeah that's their coach and then our president.

But he lost 4-0 against Bayou, which is another team

this this weekend, so we play against them on Wednesday

yeah yeah yeah.

As long as we don't feel like.

I think it was, it was like probably not I

know it was like something like Wednesday 1 a.m. or

I guess Thursday I was like, I just remember there

being a lot of.

Yeah, we got up on Thursday.

I mean, yeah, last, last game, the one that we

lost 3-nil, again, it was from Wednesday to Thursday, and

we had the seminars the next day and I watched

the first time, so I went to bed we're losing

1-0, and I just thought as long as we keep

it like this, you know, like I, I can handle

10.

That's not terrible wake up 30.

He's unfortunately he's still.

I saw him and he's kind of weird like yeah

like I've never seen like I I I I met

a guy and I told him that I was feeling

a bit disconnected with football the past couple of years,

like, yeah, and then I was like.

Slowly, you know, started to watch every match again and

and and I was like, oh, but sometimes it's so

hard for the matches at one or two and then

he said, when I lived in New Zealand, the games

would be at 4 a.m. I couldn't fall asleep before

the games.

I get so nervous and I was like, and he

would just go, go to work the next day.

Um, and my wife would come into the room and

say, you need to go to bed.

You have work, and I told her to shut up.

I just wanna watch the game.

I know it's as a kid.

And that guy drinks like that he's a big guy.

He fixes elevator and the guy justnecks, you know, he

wouldn't have one of those like every 6 weeks.

Yeah.

I didn't realise it wasn't there like cold open, you

know, like I felt I was like every day I

wake up but that was like that was like I

don't even know I was like but then like.

Yeah, like, look at this about that where I was

like, I know that she wasn't doing enough, but there's

a big difference between not doing enough to stop it

and actively thinking it's like I like I I.

right.

Yeah, I think.

That nobody gives you.

I think, I think is quite chic.

Yes, yeah, actually I don't really know what he looks

like anymore, but I agree he almost killed my normally

I just I so so like Liverpool 2019, they had

and Alison, so I, so I was supporting them.

And now Real Madrid they had then or or was

the PSG had this, yeah, I, I guess um and

and and and and PS they had Netin.

Yeah like they had like a Twitter following and I

think you know I thought it was gonna be.

I thought it was the natural fort.

That I, I do think that is actually genuinely, they

are injuries, but so many guys like that must find

PSG they bought um um a defender of you called

Bal and and I was convinced that's.

So to to uh recap where where we at, we

have these topic models we did the the topic model

LDA, um, and then we briefly looked at the structural

topic model that gives us control over theta and beta

that currently, um, but all of these models have that

issue of selecting K, right?

So all that challenge.

Um, also, for example, the structure topic models, it could

be about covariances, we could also select other parameters like

visualisations and so on, but we're mostly focused on.

Um, and the key thing is that usually, um, the

way to go is to consult some quantitative metrics, but

also a lot of, um, a lot of looking at

documents, and I'll show you examples why both here and

in in coding.

All right, so very often when you read papers, people

look at a measure called health of likelihood or perplexity.

And then they choose a K, they show you some

kind of plot and they choose K that works very

well with uh with um getting a high perplexity value.

Conceptually what this is is how well would that model

do in predicting documents that hasn't, you know, documents, um,

in some data set.

Now, you will say pretty badly because the model is

just so far away from reality, but still different parameterizations

of the model, different K will be closer at least

to what two documents could look like than other parameterizations.

So you can pick K trying to predict uh documents.

However, you're always bound to that structure.

So you're not really predicting the documents.

You, you're still always in that mode of operation where

for each, you know, word position in the documents, you

first sample the topic shares, you sample, um, you know,

and then you sample the topic and then you sample

the word and so on.

So it turns out this does not work terribly well,

um, and I'll show you examples.

There's a great paper on this.

Um, OK, so generally there are also other measures, uh,

to this, many of them, and I'll show you literature

that you can look into if you want to work

with this.

Um, one common one is semantic coherence.

So that means So, um, how often do very likely

words in the topic then really also co-occur in the

same document, OK.

That should make sense.

If a document has a high share of a certain

topic, that subtopic has a high share in certain words,

these words should actually be visible in the document, otherwise

something is not working.

And then exclusivity, is it actually that our um topic

distributions are very well separated.

So is it the case that some topics uh contain

certain words and these words are then not always contained

in lots of other topics because then the topics aren't

well separated and it's all like very blurry, so we

want to have good separation of topics.

Um, there are other, like I already said, other automated

metrics.

These are the big ones, particularly the likelihood you will

see in lots of papers, um, uh, there's more discussion

in these papers.

Good, so on the adult likelihood then, um.

If we choose K, and you know, you will see

these papers, we chose K to maximise this next step,

um, but this has a lot of limitations.

Namely that um uh maximising that function may or may

not, um, Pablo, for example, said on the project I'll

show you later, they were guided a lot by the

perplexity metrics.

But in other examples when you just use this, then,

uh, then you might have topics that from a human

perspective are not very coherent and helpful.

So it really depends on the data set and on

uh on it is an empirical question, whatever problem you're

working on.

Um, we care if we particularly, uh, if we run

a topic model to then look at the topics, then

we care, of course, about whether they're coherent and meaningful

from a human perspective.

And there's this paper reading tea leaves, they contrast it

I linked it here.

They contrast these likelihood based metrics with human judgement and

how they did it is they, um they actually um

they perturb topics and topic shares with uh with certain

words that didn't belong to them.

Uh, and then they had humans try to detect the

topic uh with words that they added randomly, and it

turned out that the humans, um, often falsely then, uh,

you know, if in cases of very high perplexity or

something that was highly predictive, um, some of the topics

that the model generates already looked as if random words

were added to the topics, OK.

So actually in this paper, they find that human-based evaluation

of whether the words in the topic made sense and

had no random word intrusion in there like that didn't

belong to the topic or the topics in the document

made sense and there was not a random topic added

to it.

And these things actually correlated negatively with uh with these

perplexity measures of predictiveness and so on.

They're just being said, it depends on the, on the

problem and it's always good to actually to, to look

into these documents and as as Yeah, maybe unexpected as

it might sound, you, you think with these things, you

just automate everything, but it's really important to look into

specific examples oneself and try to see uh what sensible

values of K uh might be.

OK, so there's a lot of this that could be,

you know, a lot of, um, uh, we could have

a lot of slides on this, but there is a

good discussion in a couple of papers and particularly I

found the one in this Robert that out from 2014,

very good.

There's also further discussion in the vignette of the of

the SDM package, um, which I, which I link here,

and I also actually show you an example in the

coding, um, in the coding notebooks at the end of

the lectures.

Um, so usually what you try to combine are these

quantitative metrics with human reading and with reading, I mean

looking at topics, what words are in them, do the

topics make sense, these kind of workouts, but also actually

looking into documents that allegedly contain these topics.

That's really important.

Um, to see what that topic is flat to be

very high on the topic that that that document is

flat to be high on the topic that I might

call what we had before some kind of physics.

Now I look into it, is it actually about, about

what I think it may be about, uh, or is

it about something else hinting that my my model doesn't

work?

These are the takeaways, um, I, I basically, uh, um,

just said, uh, there's no unique quantitative metric that can

just replace human judgement, and I'll show you examples right

in the coding sessions, uh, how this, how this usually,

uh, can come up.

Um, it's just a site citation from, from this paper

here, um.

So, uh, people usually, uh, carefully read documents and then

combine them with, um, with quantitative metrics.

They are also, uh, there are also functions in this

library, which I'll, I'll use later in these examples to

actually look at example documents that allegedly contain topics in

high shares.

This is very insightful, um.

And then, like, this is the broader point here.

What is the goal of what you're trying to do?

OK.

So one could be, we want to, out of our

library, get coherent topics that makes sense from a human

perspective and give us a good overview of the library.

If that is the goal, we combine quantitative metrics with

reading a bunch of documents and topic shares and see

whether they make sense.

If our goal is only to put the topics into

a downstream regression model to then predict whatever, um, you

know, prices in a webshop or so given on the

description of the item, say, then really our own objective

function and that this world is to predict the price

well.

So really we don't care about whether the topics are

coherent for a human or so, but then we would

choose K uh to predict the outcome variable.

Imagine we could use the topic shares as an input

into another prediction model.

For example, to a commonly encounter thing in data scientists

say you want to do pricing, automated pricing and try

something in the workshop.

If using the description of the product helps you to

predict the price better if you use 27 topics rather

than 12, uh, then you should go with the 27

if the goal is to predict the price well, even

if these topics, if you then look into the individual

topics, uh, don't make that much sense from a human

perspective.

But if we don't have these kinds of What type

cases these days, we would also maybe use some other

embeddings to predict um the prices in that workshop.

Usually we use these models to understand the topics from

a from a topic perspective themselves, and they should make

sense for a human reader and then really it's about

using these quantitative metrics together with human reading.

OK, let me show you an example of a topic

model that works well.

Um, taking a step back, often in papers, you will

see people run topic models and show you 2 out

of 70 topics that look reasonable in these workloads, and

then you wonder where are the 68.

And often they are not there for a reason, uh,

because they might not look that, uh, you know.

That reasonable.

Some topics might not make sense at all, and so

on.

So it's very easy with these models to cherry pick,

but if you have a topic model that works well

at like in its entirety, there might be some topics

that are less sharp than others, but topics overall should

look coherent for for you to read and so on.

So Pablo has this paper here.

Um, where they looked at tweets in the, in the

US legislators and they actually had 651,000 tweets.

So to our point earlier, what what you said, the

mixture model for, for tweets is not that well suited

because these tweets at that time were very short.

So probably each tweet is about one topic.

So what they really did, and they told me they

found a big improvement with this is when they combined

documents, then the underlying mixture model assumption was more, uh,

more, um, satisfying or closer to reality of these documents.

OK, um, we can also, for example, look at individual

tweets, but then we would maybe need or possibly need

another topic model.

There are topic models, um, uh, where you can, uh,

for example, I link one here, the Sage models, uh,

where you can, uh, that are very good at looking

at shorter texts.

Also in this world with shorter texts.

You could use these transformer-based embeddings pretty well that I

showed you in a few slides where we get like

a really good embedding vector for um uh a short

document um and uh but this will clear after the

neural network parts of this course.

So let me show you this.

This is just plain LDA, OK.

That's what this is, not even structural model.

So.

These are all the topics.

One, you know, one great thing about this here is

that they show all the topics, right?

So there's no kind of hiding of topics that don't

work.

So if I go on this topics, why is, why

is it called small business because they gave it the

name small Business.

Um, and these are common words in that topic, and

then this is tracking the topic share over time.

See, there is no dynamic topic model that they use

here.

They just use LDA, but they have topic shares for

every document.

They know when every document was published, and by this

they can plot the topic shares all the time.

And here there was a lot of small business discussion

um in, in, this might be May or April or

so of the year.

And then here also they give you exemplary documents of

that topic.

So documents that are high in the in the share

of that specific topic, um, you know, minimum wage.

Here we go, very, you know, less spiky, more frequently

discussed, but also seems to come and go in waves.

Uh, these are example documents of, of where this where

this topic is high, and they, they look plausible largely,

right?

So these documents here, they, they discuss what you would

expect as a minimum wage topic and so on.

And yeah, and and all these, so where do these

names for the topics come from?

Well, they gave them, right?

So they looked at the workouts, what are the workouts,

they are just visualisation of the beta vector.

um they looked at these workouts, looked like minimum wage,

um, give the name minimum wage.

Good.

If you have time, you know, you can check it

out, but that's a very, very well working and calibrated

topic model.

But these models are brittle and it's not easy to

get there.

Um, but once you got there, you can track certain

topics all the time really well and so on.

So it's kind of, it's, it's uh it's quite interesting

what you can do with this, um.

Any questions on this?

OK, um, good.

So what implementations in art, um.

There, there are implementations of the, of the standard LDA

model, for example, in this library topic models, and it

also has the correlated one.

We focus on one, you know, library or package in

order for our coding samples to be simpler and more,

you know, homogeneous across the files.

So we focus on the structural topic model on the

package.

Without any added covariants, that was the topic model where

we could add things like party and so on.

This is just a correlated topic model, which is very

similar to the LDA makes this one different distributional assumption.

Um, and then we can also use these other libraries,

you know, to look at, uh, to analyse them more

further.

There's a, there's a decent ecosystem of libraries.

Uh, to analyse things.

If you're interested in that's an overview of that topic,

so you pre-process documents.

Actually, we can do it with Quantita, which we'll do,

we'll just create a DFMO with Quantita, um, and then

we can estimate the model and then we can search

for K with a bunch of quantitative metrics, we can

label topics and so on, uh, to understand things, we

can visualise, etc.

But even if we have some automated metrics for labelling,

there's always a degree of of human judgement being discussed.

OK, good.

So before I show you the code examples on probabilistic

topic models, by the way, because we assume they come

from all these probability distributions, right?

That's where really where the core of that generative model

stemming from, uh, from these probabilistic generation processes, that's why

we refer to them as probabilistic topic models.

Um, there are alternative approaches which will, uh, you know,

make more sense after we've reviewed neural networks and and

discussed transformers and so on.

But we already knew one actually that was a topic

model approach before starting this lecture, namely the one like

the last lecture from Ryan.

What you could have done is you could have created

the DFM with every page being one role in the

DFM and the columns being the the features, and then

you can Imagine, so I have, I have so so

many documents and then I have my features here at

the count of hello, I have the count of world,

I have the count of whatever word.

And these are document 1, document 23, then each of

these documents is actually conceptually a role vector of these.

I can normalise them and then put them in some

space and I can run clustering on them.

And that would already return clusters of documents, right?

So that would have been before all of that what

we did today.

You could have run like a really simple topic model.

Two limitations, it's still a bag of word meaning if

I swap these two columns, nothing changes.

I destroy the structure of the sequentiality of text.

Um, by, uh, by just counting it, but now you

might say that the LDA does the same thing, right?

I'm just predicting the frequency of words and documents.

It's also destroying the structure, that's true.

But the one thing that the LDA and the other

models do is they assume a mixture model of topics.

If a class.

That every single, um, if I get a vector of

every single document and then I cluster them like you

did in the lecture with Ryan, then um, then actually

I assume like each document is belonging to one topic.

I think of a cluster as a topic and one

topic only.

Does that make sense?

So each document is a raw, um, is a raw

vector in the DFM.

I cluster them and then I say, well, that cluster,

let me look at the documents in that cluster.

Let me create a work plot and say that cluster

is minimum wage, um, but then I'm assuming by how

I built this that each document is assigned to one

cluster only.

Um, whereas these models here, each document could consist of

multiple topics and that can, can often give a benefit.

OK, so that's why I'm here.

And then if you use like very recent, uh, um,

or more recent tools, um, you would get these types

of document embeddings, for example, from neural network-based models, and

they would just be better embeddings, um, but they would

be conceptually nothing different.

There would be these row vectors, and they would be

better, better row vectors.

Um, and then you, uh, you would cluster them and

maybe not with K means, but with HDB scan, that's

something that finds clusters endogenously as it determines how many

clusters you find.

Sounds a bit too good to be true because it

is, because you set some other hyperparametters that then defines

the number of clusters in the end again.

There is no magic bullet defining the number of topics,

but this does some of the work for us.

And you might have seen this.

I often see this in capstone in.

when we mark, uh, people use this topic.

Um, it's, it's less of a kind of, you know,

um, it's, it's more of an engineering solution to the

problem.

Um, it's getting transformer based embeddings, clustering them, looking at

the words in these clusters and so on.

Um, but it's using newer models to build, um, to

build topic models.

The way how we, um, you know, how we select

the hyperparametters here and how it determines how many topics

are in data.

resemble previous discussions.

Um, it does a bunch of the work for us,

but I wouldn't trust it out of the box and

always look into these documents, look at examples, um, and,

and do pretty much everything we've said for the probabilistic

model.

And then I wanted to add this here because this,

I mean, it's so obvious, but, uh, but, uh, it

should be added there.

You can increasingly do these things in natural language.

Imagine I just sent a document to check GPT and

ask you to tell me the topics.

Right.

And we will use JGBT via an API.

That means we can just send huge amounts of documents

via our programming language, say in our courses are, and

we can just send it if we have, you know,

enough resources or the or like an open source model

if we have enough compute, um, all the documents from

our library and for each of these pages tell us

the language model to tell us the topics.

Problem is like well we can buch documents and ask

it for topics.

The problem is how do we, you know, verify that?

We would have human labels that and correlate with human

labelers that find topics that we do it rigorously and

then also how do we standardise this across documents and

so on.

So it's not obvious, but, but you know, if you

set up a careful prompt and maybe preselects a set

of topics, that would be another approach these days.

And we'll, we'll discuss in the last lecture not only

how these models broadly work, but also how we can

use them via, for example, R or Python here we

use R at scale via APIs.

So we look into all these details, but we need

to revise the neural networks and so on.

Good.

Um, any questions before we do the coding?

OK, so big picture, we looked at these probabilistic topic

models.

Uh, we start with the LDA then we introduce these

models that can add covariants to it.

And all these models in one way or another give

us this data and meta output, uh, the shares of

topics within documents and the shares of words within topics,

what they return, and then we can plot them, track

them over time, and so on, um, and, uh, and

we'll look at more and more alternatives later in the

course when we, when we revise networks.

OK.

Good, then let me, let me show you uh the

code notebooks, um.

So usually, um, I'll try to illustrate things that we

discussed in the lecture then with some notebooks where we

try to build them or around them and, and so

on to, to make this um more applied.

And the first thing I wanted to add like basically

one, you know, minor document before we start, um, that

shows you how easy it is and art to for

example load PDF files, right?

Because often say text is sort in PDF files um

and how do we get the text.

Out of the PDF into our DFM and then do

the whole machinery that that you guys did in this

course so far.

Did Ryan already discuss a part of this or not?

OK, cool.

So in our there's a like a couple of options

for the nice libraries, PDF tools.

I need to install it.

Um, and then I basically linked here, you can either,

you know, a simple way to load an old book

would be via the Schoenberg art project where we just

basically have it readily stored in a, in a data

frame and then create a DFM.

But maybe more interesting is how could we get a

PDF file into a DFM to then work with it.

So let's look at this example of Newton's shipyard, um,

so we can, uh, we can go online.

Um, and then go on Google Books.

So I'll do this from scratch as well.

Feel free to run this on your computers in parallel.

You have all the data and all the code.

Um, what I do is I download the PDF.

I could also download the plain text, which would make

it simpler and so on, but I will stretch the

point how we can convert PDFs.

Let me download the PDF.

Here we go, right, we don't need these first pages,

but then.

Uh, we have the main document, OK, and this has

been OCR with optical character recognition because I can select

things here.

So it's like basically there is like the text is

stored in this file conscious an image.

Good.

So I need to come um to uh add this

to this folder.

Um Let me add this to here and bring it

here.

So now it's, it's in here, um, that's the book,

right?

OK, back to the file.

All I need to do is PEF test.

It gives me an error because of some uh um

font with issues in that document, but it works very

well, 607 pages.

OK, when you see the document here, it's 607 pages.

How does this thing pass or like you know read

this into R?

Well, this thing is just a character vector, like a

long list with each element and the list being a

page.

That's it, really simple.

Um, now I do remove some new line and so

on character characters, and then I delete the initial, um,

initial pages that that are just basically um a disclaimer

from, from Google because I don't want to run a

text analysis on that at least.

And well, yeah, now I can transform this into a

corpus.

And here I am, now I'm back in the quant

world.

You usually like in the in the quote with Ryan,

you use the library, is that right?

Or that's what, yeah, great.

So now I'm I'm basically I can create a printer

DFM.

Here we go.

You see, like text 12, these are the pages in

the PDF, um, these are the words that are mentioned

and the count.

The usual sparse matrix meaning lots of zeros, right, because

most of the words are not contained in, in specific

documents because the vocabulary is so large.

What did I do here?

I removed punctuations, I removed numbers, I removed soft words,

um, and then I, uh, I, um, I track Ngrams.

Do you guys know what Ngrams are?

Good.

Do you know why, um, what the padding here means

more, that's a tougher question, but do you know why

I see why it's important.

Because the n-grams rely on sequences of words, right?

If I remove the stop words.

Then I have these gaps in in in my text.

And if I were to just remove them without keeping

the gap, I would build wrong n grammes.

I would build nonsensical N grammes.

So I want to put a placeholder where I remove

the stop word in order not to create n grammes

of words that don't actually appear consecutively where there was

just a stop in the middle.

Good.

This is just up to bigrams sequences of two words.

Here our tokens are words.

um main term frequency means I only keep words that

are in at least 5 documents or at least 5

times, they occur 5 times.

Um, yeah, and then I have that matrix here with

606 documents because I removed some pages and then um

a residual amount of 4000 roughly features.

Good.

That's how I, like, you know, preliminary one, how do

I read PDFs into this.

Second, um, uh, is the thing that then you might

wonder, well, what if I have a PDF that actually

does not have, um, have, uh, Have already OCR text

but has an image.

So I went to Google because I'm an economist.

I just took a screenshot of the of the general

theory, and this is just an image.

Interestingly, these days, even Mac OS systems, OCR is on

the fly, basically.

So it's a bit like tricky for me to spot

that that's just an image because my operating system is

doing this.

But for a computer, I still need to OCR that

image, OK.

So on the computer, what do I do?

Um, and I just wanted to, you know, you might

use the APIs that do a lot of this for

you.

Google or Amazon and these big firms or smaller firms

have these APIs where you can also communicate with the

OCR software and natural language and so on.

But simple examples you can actually do very quickly for

free with um with open source software and there's this

library called Tesseract.

Uh, which you can use in R as well, um,

and I wanted to show you that it's actually.

Pretty simple to use.

I use this on the, on the general theory front

cover.

I set the amount of, uh, you know, those square

inches in terms of of the the granularity of the

image which English is which language this is.

Takes a little bit of time, but here we go.

So it's pretty simple and all done in art.

So now imagine you have a lot of historical, like

photos of historical books.

It's really only you need this work, this notebook to

transform it all into text that might be good enough

to do back of words analysis.

There might be some errors, but if you then need

the DFM out of this might be good enough.

Um, Good.

So, and what are these, so what are these end

things here?

Did you discuss it with with Rein?

So what are the best ends?

Why does this text look so weird?

Why does it not look like this here?

What's the end?

Isn't it like exactly, exactly.

That's a new line.

So I can use this or print function in R

and actually print this properly and you see it got

it almost perfectly.

Uh, has a comma here, which I believe there is

no comma.

Also, I made it a bit simpler.

There was like some kind of decoration on the front

cover around the text, um, and that threw off the

simple OCR and then I thought these were characters, but

it was just some drawing around it.

So I just basically took the screenshot of only the

text.

So you might need to have to do some processing

and so on.

But here we go.

Um, imagine you have an old book, you can take

a couple of photos and do this and you have

it in art, basically.

Um, cool.

Any questions?

I just think this is a kind of neat skill

to uh to know.

Um, good.

So next thing is like really simple topic model, let

me run this in one go.

And what I do is I now continue with the

print GPR.

Um, I just do exactly what we did before, uh,

create the DFM and then on that DFM uh run

the structural topic more.

That's it.

And then what I'm doing is I'm, I'm tracking these

topics over time.

Well, I mean, time here are pages in the book,

um.

And I'm depicting them basically.

So you see this takes a bit of time, even,

even with, uh, with only 3 topics.

Now, is 3 the right number of topics?

No, but you saw it already took a bit of

time, so I didn't want this to take ages, um,

and just to illustrate something that works reasonably well.

Also here I should aggregate more than one page and

so on.

Um, I should choose higher numbers of topics.

It would actually be, um, uh, be tricky with only

this one book to do this well, um, but, uh,

but it would be possible.

Um, or at least, um, more accurately than than this

year.

But this on the side is very quick for us

to analyse and it already carries home a lot of

intuition.

So, uh, let me, let me show you topic one,

these are the top three words.

In our beta vector, these are simply the top 3

highest probability items in the beta vector.

So topic one is a high probability on sun, Earth,

and motion.

Topic 3 is forced bodies, and topic 2 is given

line plus, OK.

So now let me plot the, the, um, the clouds

here, um, and I mean, see how simple it is

to take this, yeah, I mean, really important historical documents

and then just run something like a structural topic model.

So here I, I have a topic that looks like

this, so I give it this name, um, um, astronomy,

um, and then here there there seems to be a

lot of area line, right, and so on.

So I just call that math or algebra.

Um, and then, and then this year it says force

body and so on motion.

So let's call that mechanics.

OK.

Good.

So now I have labelled these things and now I

can actually track them over the pages.

And to our earlier point of automated reading, let's look

at the math topic, for example, you see here the

math share is pretty high.

Um, I can make this a bit bigger and it's

easier for you to read.

The mass share seems high, around page 180 or so.

So why don't we check this out, um.

Let's go to page 180.

Yeah, there we go.

OK.

That's kind of the, the, the flavour of all that.

Yeah, so I mean, yeah, pretty cool how quickly this

can be at least illustrated.

So here why like just to show you this is

the SDM package actually um estimates correlated topic models so

I can have correlation between topics um and as you

would expect, the plants and the mechanics are, you know.

Less like I'm more correlated than the other one.

Why they are all negatively correlated, they should be they

should co-occur, but I, I only have 3 topics and

I add the unit of a page.

So a page usually is on one of these things,

so they are negatively correlated.

They don't occur together in the way I set this

up.

Um, any questions?

Cool.

OK.

Then last example, I want to show you the um

structural topic model with like, you know, slightly more realistic

examples.

Um, and I use a sample of um of Facebook

posts from public Facebook pages from 2017 of US members

of Congress, and this is just a sample of 10,000

and that already ran a good amount of time on

my now slightly dated laptop, um, but I chose the

10,000 to make this manageable and That data you see

here has the screen name of of who posted this

the day when it was posted, the type of the

post, the message, whether it was like, commented, and like,

you know, the accounts of these, so that's a bunch

of information.

So I do some data cleaning and I keep that

variable here, Democrat or Republican, um, I, I get rid

of independent because I believe in that data it's mostly

Bernie Sanders.

And then um I, I have these two parties.

I saw the date and I saw the day of

the year, um, so I saw basically I extract from,

uh from the they call the specific day of the

year, um, I saw um agenda and party and Republican

and Democrat.

OK, so here I um.

I look at that data and I have screen name,

date, and message, and I do the topic more on

the message of the posts.

So now I create, I create my corpus, and you

see here still the whole kind of machinery.

My text is the message, and now I add dog

and you see where this is going because the structural

topic model will use these variables to then determine its

topics and I have some dogs now who posted this,

when did they post this party, etc.

Good.

So now I remove punctuations, I remove numbers, symbols, and

so on, and here I land again at a DFM.

Now there is this bridge function called converge, uh, convert

in the SDM package that now transforms DFM which is

which you have worked throughout this course into something that

this underlying topic model can deal with.

And that's it.

Now I run the model.

There is an automated function to search K, but as

said previously important to combine that with human reading, and

I'll show you a couple of examples in a minute.

I, you know, A reasonable set of topics here is

50 and that's still, you know, manageable and we can

run and that after you ran it this what comes

out of this.

In 2017, most discussed like in that data according to,

you know, the structure topic model assumes is um what

healthcare um.

As a topic and then a lot of discussion about

the persona of Trump, etc.

So now I look at a couple of topics and

some of these topics don't work terribly well, but we

already even with this relatively minimal tuning, have some uh

pretty well separated topics.

So here that's a topic that looks like um See

what I can increase this.

This is a topic that looks like healthcare, um, a

small business type topics, um, and.

Like opioid crisis type of topics, the opioid endemic and

so on, um, security and immigration topic, it seems law

enforcement topic, um, military and defence type topic.

So already finds reasonable structure.

So now comes this thing that I can't overstress.

Also, if you work with this or transformer based topic

models in your research and your capstones and dissertations, always

look at documents.

Because imagine I look at this and I say, you

know, great, that's the topic here, opioid crisis.

Let me check this all the time, let me analyse

it, then it was discussed like it was discussed here,

it wasn't discussed there, and so on.

But now if I actually look into these examples, let

me first look into a topic that works pretty well,

um, and that's the healthcare topic.

Um, this year is actually, you know, the um healthcare

discussion.

Um, example and a healthcare discussion example here for premiums

in California.

Um, these are just two, you can sample further examples

and you would look at a bunch of examples.

What is this, this, we could do this manually.

This is just document examples that contain a high share

allegedly of that topic, and that means a high number

of theta of that topic, right?

So whatever I label healthcare, that has a high value.

And now let me look at this, at this opioid

topic.

Um, and actually I see that, you know, when when

we looked at the workout we were thinking it could

be a crisis, but also actually contains a bunch of

stuff about net neutrality at all.

So it's not terribly well separated, OK.

Um, and if you look at example documents that have

high shares of this topic, uh, then it seems, well,

this is about net neutrality and now looking back at

actually the work out, you see that in there as

well.

So it's easy to overlook these things without looking at

specific documents that are high in these shares.

So I would have actually not tracked what I meant

to track.

That's why I get this example here to show you.

That's very common that that will happen when you work

with these documents and that's why it's important to look

at these um specific ones.

There are a bunch of functions in this, how much

I have 5 more minutes.

That's great.

There are a bunch of functions in this library.

So for example, we can see um and even estimates

some kind of error margin around it, whether a topics

lean, um, you know, certain covariants, and here I just

had the covariant that I added, you see when I

estimated this model.

So when I estimated the moral, I applied um two

covariants and I added the party and then I added

the day of the year.

OK.

So these are my two covariants.

To give you an example of one binary, the party,

and one continues the day of the year.

So now I can actually separate these things by um

by party and I see that the healthcare, the Russian

investigation topics they of course the Democrats in this data

set from 2017 and then uh you uh you have

other topics which um which lean Republican, uh, for example,

the.

Tax reform, law enforcement, immigration, and so on.

For some, of course, for some of the topics of

this model at that level of calibration, this might work

less well and and better, um, but, uh, you see

that's already picking a good amount of picking up a

good amount of structure here.

OK, so what we can also do is we can,

we can plot total proportions over time, um, and also

add some error bands on this why can I do

this because I added this covariant uh day of the

year.

So I could, even if my data was spanning multiple

years, I could know through that covariate analyse um certain

topic share that's the healthcare topic, um um over time.

Um, and actually over the years, so I could with

this for multiple years and see is across years that

topic discussed in certain parts of the year more than

in other parts.

There are other libraries that we can use our packages

like the visualisation package, that's amazing, so I um I

wanted to show you that as well.

First, I think I have to install that.

Let me run this and it should open the browser

window.

Yes, and here I can actually see my topics and

that's some kind of map that tries to compute the

distance between topics, um.

Uh, and it's, uh, like, you know, reduces them on

two principal components and then tries to separate them.

And that that's I don't have to uh have to

study the details here, but it shows like the difference

between topics that are more similar should be close in

that space.

And then we have that topic here, which is, um,

you know, heavy on, seems to be domestic stuff like,

um, um, residents and country and county and so on.

Um, here's another topic that seems to veterans topics, veterans

and service, uh, Vietnam and so on, and you can

actually analyse what this model is doing.

Another tool to to see uh what is what is

happening, um, kind of under the hood in these libraries.

OK, great.

Um, and now, so far, there were these two covariants

in these models.

One was to um to impact the um the the

word shares within topics, and then the other one was

to impact how often these topics occurred in documents to

begin with, right?

So there was theta and beta and one set of

varius impacted beta and one set of covarius impacted theta.

Um, and I that takes a good amount of time

to run, but I ran this before.

I now also um add another content covariant and I

separated by party.

And with this, then you see I still have it

keeps it very similar so I still have a healthcare

topic on topic 38 that depends, of course, on like

the initialization and so on, but Hopefully you should have

the same numbering.

um, and here I can see how um healthcare topics

differ in terms of the words by the covariants.

And it's kind of interesting here for the healthcare topics,

for example, you see a CAR and Act that's a

diagram, healthcare and so on, that's affordable, that's the nomenclature

or like the vocabulary of how Democrats discuss this and

then you have say Obamacare repeal and so on on

the on the Republican side in this 2017.

So we can also split this and see how they,

how they discuss topics differently.

And here I give you a lot of resources and

also kind of mirroring the slides.

Um, to, uh, um, um, to different approaches and how

you could, uh, you know, pre-select or select topics if

you were to run this in research and so on.

Good.

I think we reached 5 exactly.

Um, any questions?

Great.

Yeah, so I think we did a lot of stuff

in in probability topic models, lots of things you can

use if you want to.

And then, uh, next week we continue with, uh, a

bit of math and network revision to then to then

study more like very current approaches to to language modelling

and how they can be used in social science.

I'll stay a bit longer if you have any questions.

There's.

Mine told me it's probably desert.

Did.

See you.

I will.

Yeah, but there was one day that co-star early Saturday.

salad.

So rambling which is all I ever do dogs so

when I try to like, I think it's still working

in like, yeah, but I'm meeting with a friend.

So one way in which it works it works yeah

yeah yeah yeah you only ever applied to one job

and then you, you basically go into the the file

and also like you.

Same thing, yeah, the, this, yeah, try to see you

because you didn't hold the libraries, right, so that's a

different era.

So yeah, mhm.

Like you just got to, you just got there.

So by this there are other ways you can you

know there are some help files like that's a simple

way that's how I usually do.

Yeah, well, It's yeah I don't wanna do the stupid

desk job.

Yeah, you get degrees and things that you need that

you need.

It's right.

Yeah at that point when I was doing A levels,

I was like I want to go to university, this

is gonna help me to get to the next stage

of my laptop.

But with Yeah, let's think about the backup.

well that was.

Lecture 8:

Um, I'm trying to see if I can break to

get someone to fix this, but maybe that's too short,

so, uh.

Yeah OK, um, So I read the lecture for today

relative to recent years because the component of language models

and neural networks in the course is increasing due to

the obvious circumstances at the moment and um this became

increasingly difficult to to discuss without some kind of fundamentals

and you know, optimisation and then the algebra and so

on.

Um, and I opted this time for building like a

relatively big review, um, to, uh, to bring everyone on

the same page for some of you that might be

pure repetitions, for other, others not, and so on, and

then to start with the neural network section of the

course and then everything we discussed subsequently, I would hope,

I'll ask you again in the end is uh is

much clearer because of this review that we're, that we're

doing now.

Um, OK, so some quick overview.

So how many of you have worked with kind of,

you know, matrices have done a bit of matrix algebra

in the undergrad or the previous studies?

like kind of 2/3 to 5 or something.

OK, good.

So, um, let me know if I can, you know,

go through these things quickly, but, uh, but if you

haven't seen them.

And I would hope they're they're helpful for the materials.

Alright, let me see whether this works.

Um So today the lectures divided and brought in two

parts.

One is this method review, um, and one is this,

uh, this introduction to neural networks.

And note the, the, I've said this before, the overlap

with other courses is unavoidable because so much in computational

text analysis.

No network, neural network based.

So if you're taking neural networks courses, of course, you,

you will see some of this in these courses as

well.

However, if you don't take them, uh, you would miss

out of a major part of pretty much of everything

that's with current kind of very current text on us

methods if we didn't do neural networks.

OK, um.

Good.

So we start with like a review of like some

fundamental things with vectors and matrices, um, turning to like

some topics and calculus, and then we'll actually build some

gradient descent from scratch.

So basically, we'll quote unquote train our own, um, uh,

kind of function approximators with two simple examples.

So let's start with vectors.

The first thing is like, um, you know, if you

haven't worked with any other I think of vectors as,

uh, as lists of numbers, that's usually, uh, the way

to think of them in kind of computer science applications.

Um, however, you can also think of them as, uh,

as points in space, uh, when we look at a

couple of, um, ways to conceptualise in that way, um.

Also at points a few more maybe coming from physics

at points of direction.

OK.

So I assume that most of you will have seen

vectors, but maybe not everything I show subsequently.

So, um, we will use them in, in various models,

both to store data, but also to look at, at,

you know, a computer's encoding of meaning, uh, because that

will be encoded by a list of numbers.

Visual intuition for all of this is very helpful, um,

which I, I hope becomes clear in the subsequent slides.

OK.

So, first of all, where do vectors come from, from

vector spaces or where do they live?

Um, vector spaces have these two fundamental properties.

OK.

That's vector addition.

We can add up these lists of numbers and we

can scale them.

We're usually looking at the space with N dimensions, um,

you know, think of this as a 3D space, of

course, and like a plane as 2D, um you can

generalise this to N dimensions, and we have these two

fundamental operators.

One is, um, one is scalar multiplication.

So with just a number, we can scale the vectors.

So we take these lists of numbers, we multiply them

by scalar and each element in them gets multiplied by

a scalar.

And the second fundamental operations, if you, if you start

kind of discussing vector space is a bit more abstracted,

that's usually how you start.

It's just that you can add up factors, OK?

So we have two lists of numbers, we can, we

can add them up.

So let's look at, let's look at the scalar multiplication

first.

Imagine I have this vector at position 21, OK.

And then I have two scalars, um, 2 and minus

0.5, then that's the visual intrusion, which will guide us

through a lot of these things, um, and, uh, that

is uh just scaling the vector, and we can also

scale it with a negative number and that's just flipping

it in the origin and then scaling it on that

side of the coordinate system or easy.

Good.

So the next thing is vector addition, and actually we're

using the scalar multiplication right away.

So the nice thing about vector addition, first, it's very

easy.

You just add, like I said show you previously, every

position in these vectors is simple, but then, um, you

can visualise this, um, by just adding the vector that

you add to the other one, to the, to the

tip of it.

So basically, I moved that vector here, add it to

the tip of it, and then the, the, the kind

of added vector is up here.

That's a very neat way to visualise it and we'll

use this with, for example, studying word embeddings.

Uh, that's why we're revising it here.

OK, so simple.

But then, maybe you haven't seen the, I'll check the

next slide.

How do you conceptualise vector subtraction in this way?

Right.

So what you would do is you would first switch

the vector to the other side, multiply it by a

ne scalar, and then edit, and that would mean you

switch the vector and add it to the tip and

it will be somewhere here.

OK, um, and that's exactly how we can visualise that

track, OK.

To compute very simple, um, to add, uh, with the

first scale and multiply and then add, and here's the

final vector.

OK.

Quick recap, why do we care about vectors?

Because that's how computers process data.

You can think of your Y vector and, you know,

classification models such as the vector.

Obviously, like that stores your labels, but also, If you

think about the computer's attempt to encode the meaning of

the word, um, you know, inflation or something like this,

um, if you, if you turn on economics topics, then

usually that vector should be in some kind of space

and hopefully if that model works close to other vectors

on similar kind of economics topics.

OK, so and we'll use that heavily in the, in

the next week.

Good.

So how do we, um, what can we, you know?

Like how can we combine vectors if you saw one,

we can use this dog product.

And the dog product is defined as just multiplying every

element of these vectors and adding them up as a

dog product.

And more interestingly than just the formula, it's a general

measure of similarity between vectors.

Why is that?

So imagine I have two vectors, um, in, in our

notation, OK?

Um The dot product between these two here will be

higher than if I have, for example, a negative number

here.

OK, so say I have minus 7, and then these

two vectors that they like at the first position now

one has a negative number, and the similarity between them

will be lower.

So we'll look, I mean, you've checked with Ryan, I

think cosine similarity.

I'll reintroduce it a bit like with kind of the

fundamentals here, but actually the logic underlying cosine similarity is

just this that the dot product in itself is some

measure of similarity.

If we scale it by the length of the vector,

it's even exactly cosine similarity.

OK, onto matrix multiplication.

So matrices are rectangular rays with rows and columns, you

know, you can Think of them as these, as these

column vectors, OK, arranged.

Um, that's the matrix, um, and we can store data

in the tabular format.

Um, and then just to recap, a couple of properties,

if you haven't worked with them frequently that are maybe

less obvious.

One common one is that they don't commute.

That means that a matrix A times B is not

the same as B times A.

might not actually work even.

OK.

Um, I've brought a couple of properties down here.

Why do we care about them?

We care about And both for regression models and neural

network models, literally all the processing in these models is

done by matrices and arrays.

So how do we, uh, multiply two of these arrays?

That will be important for understanding how neural networks process

data and what's called a forward path later today, um.

So here are some examples.

That's the general formula.

Imagine I have these two matrices here.

I have a matrix set in the vector.

So what is the matrix, the matrix here is.

Let me show you this example A times X.

Say A is 1234.

Um, and then X is 5 and 6, and how

I like to kind of write this down and do

this when I do this multiplications, I write them in

this kind of coordinate system, uh, just, just for helping

me with the, with the computation.

And then basically the first position of the, of the

multiplication is 1 times 5 plus 2 times 6.

OK.

So I had 17.

And then 3 times 5, 15 and then 4 times

6, 24, so 39.

I didn't make a mistake, OK, um, and Which matrices

can you multiply those that have the same column, the

first one needs to have the same column number as

the second one's rose number.

Otherwise, we, this multiplication isn't defined.

And then if you, if you do it, you know,

without a computer, that's a neat way to, uh, to

write it down.

It's, uh, um, it's, um, yeah, uh, simpler than uh

than I find writing them side by side.

The same would work, you know, if we were to

multiply 22 by 2 matrices, we would have other numbers

here.

You know, That's, uh, let's do this as well, just

to be sure that everything subsequently is clear, so say

the matrix 12341234.

So what what's here?

Can you tell me?

What is the resulting, first of all, what's the dimension

of the resulting matrix?

Hm Yes, that's right.

2 by 2.

So I have 2x2 matrices.

Um, A is 2 by 2.

And these two went home.

I can quickly see the resulting dimension if I cancel

all these things in the middle.

They have to match, so the resulting dimension will be

2 by 2.

So what's the first number here of that of that

matrix product?

So I multiply it basically I multiply the matrix A

with itself.

Can you tell me what the first cell is?

I can also draw out of it.

Sorry, what did you say?

7.

That's right, 7.

All right.

2nd.

1 Cool, thanks else let me write a bit.

So these are the 2 2x2 matrices, OK.

Um, 12341234, and then 1 times 12 times 372 times

1 plus 4 times 2, 10.

What's this here?

15 Uh, for a pen.

I'll get one in the break.

So what did you say?

What's that number?

15?

Yeah.

So that's uh um 3 times 1 and 3 times

4, 15, and that number here is to make it

boring, is 3 times 26, 4 times 4, um 16,

so 22.

Yeah Good.

This will make it a lot easier for us to

imagine how data flows through a network.

Then, you know, networks have billions of parameters in real

world applications.

Uh, not all of these multiplications are done, uh, um,

on the scalar level, but the data is stored in

the matrices or the, uh, all the, um, parameters of

the models are stored in matrices, um, and then, um,

this is heavily reliant on matrix multiplication.

Um, do you know why like this is so helpful

for current so?

We'll discuss this in a moment, but some preview is,

um, current architectures rely heavily on paralyzation.

So a lot of the AI boom that you see

is based on these, you know, we might have heard

of graphical processing units, these GPUs, and it turns out

what they can do is they can really, they are

really good at matrix multiplication in parallel.

OK, so they can do a lot of these simple

operations that we just did on the whiteboard and parallel,

and that's one of the major drivers of, of the

increased intelligence that you see.

OK, but more details there.

So back to the methods for you.

Good.

So one cool thing maybe that you haven't seen, even

those of you that have done in the algebra, um,

is What is matrix multiplication?

OK.

So usually or very often, it's just taught us this

algorithm as this cooking recipes, that's how you get the

problem.

But it has a really nice visualisation.

If we think for example, of that matrix, then it

just moves vectors.

Like all other matrices and multiplication, move them from one

point in a space to a different point.

So if we apply that matrix, and I wrote it

out here, so 8 times, times 11 to this exemplary

vector, we'll get another vector, just how we did it

here.

But actually every vector in the original space by multiplying

it with the matrix, in this case, we sort of

rotate, um, and we scale, OK.

So that's what this is doing.

Yeah.

So really matrix multiplication is this linear transformation of the

space, and every vector from the original space is just

moved through the matrix multiplication.

And the, the really cool thing about this is, so

how many of you know what an inverse is?

Just for me to get an idea of how much

time spent on all this stuff.

So how many of you know what an inverse is?

Alright, so just for, for that group, later we revise

it in the, in the seminar, and inverse of the

matrix is simply the reverse rotation.

That's what it is.

So just visually it's like the the multiplication with A

is moving everything here and then in this case, it's

just the um uh just the uh just the um

rotation in the clockwise.

Good.

So usually these things get a lot easier with with

visual intuition and just by buying these kind of cooking

recipes.

OK, lastly, in the best of all vectors and matrices

is cosine similarity.

I believe you've already done this for like document vectors.

However, document vectors only have positive numbers, right?

So the ones you looked at because they just have

word counts.

But there's, if we have more complex document vectors, like

not the simple ones that you so far looked at

with word counts, but say some that come from transformer

models or so, they can have negative numbers, these, these

vectors.

They are also points in space, but in a different

space.

Also, if you look at word vectors next week, they

can have negative elements.

So it's important to realise that the course and similarities

defined between -1 and 1, right?

So it's not actually necessarily positive if you look at

more general vectors.

Good.

So that's the formula.

If you haven't seen a lot of linear algebra, it

might look a bit funky, but like what this really

just this is the length of the vector.

And then the main thing that I want to get

across here is it's just the dot product between the

two vectors, what we just did here.

And then each vector, we scale by its length.

OK.

Um, and that's the similarity between vectors.

So really the dot product is extremely close to it.

We just have to rescale the vector and what does

it mean to divide to, to, to bring this back

to our visual intuition.

Imagine I have different vectors and I divide them by

their length.

So the vectors, they are here, a negative number, here

is a vector, here's a vector.

What happens if I divide vectors by their length if

we think of that visualisation?

So I have these two, these lists of two numbers.

I put them in some 2D space.

And then I divide these lists, like, so there's each

vector has a length, this one here, you know, has

these, and then I divide them all by the vector.

And maybe maybe someone else, thanks.

So what happens if, uh, if I divide them by

this or if I normalise them?

Yeah, shrink them and how can I visualise this?

So if I divide a vector of length 42 by

its own length 42, then the resulting like this is

1, right?

So that means that if I do that division, every

vector will be a length one afterwards.

So what I'm essentially doing is I'm pushing them on

a unit circle, a circle with a radius one.

So they all live on that circle.

What happens if in a 3D space like this here,

I do this, I do this normalisation.

Where do the vectors sit?

On a, on a sphere, on the unit sphere, right?

And then we can generalise this to higher dimensions becomes

this this object called Nshere.

But like in 3D you would think of the unit

sphere and that's where all the normalised factors are.

By the way, this course and similarity is simply a

rearranging of that we can write the product as a,

as a function of the of the angle.

That's where it's coming from.

No magic there.

If you want to revise this or if you're interested

in this or need another course, that's where it's coming

from.

Very common in natural language processing, uh, because, you know,

we can compute similarities of vectors.

OK, again, visual intuition, that's kind of, uh, our main

focus in this course.

I have to practise.

I did the cosine similarity, get a number.

It's between +1, maximum similar, minus 1, maximum dissimilar.

First thing again, visual intuition.

What do you think with two vectors?

What's the cosine similarity of one?

What do we think?

Was the second victor.

Same direction, right?

But actually, it could be here or it could be

here.

It's just the angle, but it's the same direction.

What's the similarity of minus 1?

Exactly.

So it's like exactly 180 degrees.

So that's the cosine of that of 180 degrees, um,

and that will be, uh, that will be a cosine

similarity of -1.

So, just because when I started looking at this, I

really wonder about, OK, how do we get in terms

of, like, how do we transform this to like more

commonly kind of accessible, usual, you know, degrees and so

on.

And what you do is like, this is the cosine,

um, of, uh, of this angle.

It's like, um, this is the 0.85 here, OK.

Um, to try to get the angle in radiance, I

have to do the inverse of this.

Right.

Um, and then one like like, and this is, you

know, you have that defined in in your usual in

your usual software libraries.

Now I get this doesn't quite look like an or

like that angle here, right?

But when you then transform it into degrees, it, uh,

it resembles what you would think this looks like 30

degrees.

OK.

That's, that's why this is the angle, um, or the

degree of the angle between these two factors.

It's just in a different format, um, and radiance of

degrees.

All right, so it can be between -1 and +1,

um.

Yeah, follows from a definition of a dog product.

That's why the do right is so similar to it.

Any questions?

So when you do this similarity, do you have to

normalise based on the longest vector or can you on

on both so good point um.

You see here, I divide it by the product of

vector length, so I just normalise for both.

But I can also, that's why it's a good point.

I can, because these, these are just scales.

I can write it as an overall do product.

Between a normalised A and normalised B, that's the same

thing.

So that's why it's really just a dot product between

normalised vectors, and we'll use this heavily for uh for

vector embedding just to speed up our computations next week.

So I know, bear with me.

I know this is a bit dry, but take it

as a review of some fundamentals and everything else will

be easy because of this.

Um, any other questions?

And please ask questions.

Um, otherwise it's difficult to say for me whether it's

all obvious or anything is unclear, etc.

Be for now?

All right, cool.

So and let's continue with some quick recap of, of

some derivatives.

All right.

We'll look at, like, quickly look at um at univariate

derivatives of of simple univariate functions, um, and then, um,

and then at partial derivatives of multivariate functions, gradients chain.

Quick pole again.

So how many of you have done derivatives of multivariate

functions?

Again, like roughly half, OK, good.

And then, um, and then we need the chain rule.

How many have done the chain rule?

So, OK, great.

Cool.

Good.

So definition of the derivative, that's all from your package

is one class.

Um, what is it?

It's the, it's the slope and the point, right?

Um, We usually we like DFDX it's just a tiny

change in that divided by a tiny change in X,

uh, that's defined as or like usually denoted as this

flotation um and it's the slope of um of the

original function and a point.

OK.

That's the pure definition here, so.

Um, I add a little change in X to X,

see what the function does, take the difference in the

function, divided by that little change.

Um, and then I let that little change go to

zero, and it's this generalisation of the slope and just

to the point.

Some examples, um, that are good to to know because

we'll use them a lot later.

Um, uh, these things here, so what's the derivative of

EX?

That's just EX derivative of lock X is 1 over

X.

Um, that's the usual ones with these powers.

Um, it's linear, so if you apply it to a

to a summation or so it's the individual derivatives, um,

product will.

Um, of, of a product of two functions is here,

quotient rule, you've seen them in, in your introductory calculus

classes.

Um, OK, onto partial derivatives.

So that is like, you know, the analogous definition.

Now it's just a function of two variables, OK?

And I changed one and I divided by a change

of that one variable, and then I did, um, and,

and I take again the limit of that change to

0.

OK, and it's pretty simple if you haven't done this

before.

What you do is you take everything in that function

otherwise as given, and then you just take the derivative

with respect to that parameter.

OK.

So here, these are all parameters.

I could name them hello, right, whatever.

So these are X, Y, alpha and beta.

I take the derivative with respect to beta, X is

left, because in that derivative, I take everything else as

constant.

That's the logic, kind of standard derivatives, but you, you,

uh, you take everything else as constant.

We can combine these um these derivatives in, uh, in,

um, in what is called the gradient vector.

And that gradient vector is just um a way if

you want to store these derivatives of a multivariate function.

Um, this year is actually a transposed vector, otherwise it

would be 3 times 1.

I I just wrote it down like this.

Um, so quick question, what's the dimensionality of the gradient

of this function here?

4, that's right, because there's 4 inputs, so it has

4 partial derivatives.

Um, I can take 4 derivatives with respect to X,

Y, alpha and D.

OK.

Onto the chain rule, um, if I have chain functions.

I um And I need to take a derivative, for

example, in this case, I want to take a derivative

of the, of the chain function with respect to X.

The simple logic is to, if I differentiate the original

function by X, then it's the YDU because it's itself

a function of U and then the UD X.

So let's look at some examples.

This year, if I want to take the derivative of

this year with respect to X, then I first think

of this.

As the function of of like GX and squared.

So I can think of this as Y here.

So I take it the derivative with respect to why,

so it's 2 Y.

and then I take the inner derivative of that, that's

1 and I multiply with it.

So that is 2 times X plus 2 times 1.

That's the result of that, of that derivative, right?

So another one is.

No.

Um, 10 + X2.

Derivative, if I think of this as Y, the derivative

of this with respect to Y is just 1 over

Y and Y is 1 + X2.

Or X2, and then the inner derivative is 2 X.

So that's the entire derivative.

It's just the multiplication of the individual derivatives.

Why do we care?

You will see that because we want to compute these

gradients and to compute the gradients, usually we need chain

rule, um, and why do we need gradients to make

any kind of function learn in the sense that we

think of here.

OK.

So that's, that's why we're recapping this.

Good questions on this or or let's hear.

Cool, good.

Um, then let's look at gradient descent.

So first of all, why do we care about derivatives?

Because derivatives are the slopes of functions, where the derivative

is zero, the function has a minimum, a maximum or

a set of point.

OK.

The minimum is in like a convex area like this,

maximum would be in a concave area.

The set point is where it switches from one of

the two.

And usually we're, we're looking for minimised functions in, in

machine learning and natural language processing, and then we just

need functions that have this convex shape that they actually

have a minimum, right?

OK.

When the slope is zero, we, we achieve this, uh,

we achieve this minimum.

So why do we care in machine learning if you

haven't done any before?

Um, I mean, other than, you know, so some of

the, of the bits and pieces in this course.

Um, if you think of neural networks, they have this

error function, we call it loss function, and we want

to minimise that function.

We want to minimise their error.

And that's the whole magic of, of how, how they

learn is driven by this simple par.

So taking a step back, if you were to, um,

you know, say what the most powerful scientific ideas ever

developed were, I would say that the derivative is probably

among them.

Because as trivial as it sounds, but like if we

set the derivatives to zero, we have zero slope.

zero slope is the maximum or minimum.

All the current AI that you see is just minimising

errors.

How do you do that?

Setting derivatives to zero.

Simple as that.

But like all of that would be unthinkable without calculus.

Hm.

And not even fancy calculus, just the first order derivative.

The projector broke, so it's difficult to see from the

air.

Good.

So, so that's why we need it.

We minimise these loss functions, OK.

Um, and before we discuss more complex loss functions, I

want to build gradient descent from scratch for a um

for a linear regression and a simple classification problem.

OK.

That's the, that's the goal.

Um, so let's start with the linear regression.

Um, here is uh the, the model.

Have you done gradient descent with Ryan, you haven't, right?

OK, good.

And then with all these things like this is basically

not using all the parts we've reviewed so far, um,

and when we put this together, we'll just code it

up, um, and, and, and build it.

OK.

That's the, that's the approach.

So that's a linear regression model that you've seen in

kind of the stats prerequisites for this course.

That is clear.

Um, I assume that there are no math prerequisites.

That's why, that's why I did the revision.

If you think of linear regression, that's the usual loss

function, right?

So you want to minimise uh the sum of squared

residuals to all the points.

So once you want to draw a line in that

dot into that dotloud such that the squared residuals to

all these points are minimised as linear regression, right?

So here we go, like we need to minimise the

sum of squared residuals, how do we do it with

everything we discussed so far.

That's how we define the loss function.

And that is just the average over the sum.

You know, of square residuals, that's it.

That's the lost function.

Um, for one individual observation, if I take away the

sun sign, that's just that minus, um, I have this

linear regression with just a slope and an intercept minus

the 5th squared.

Um, do you have any questions about this?

So can you tell me, like, without looking at the

subsequent slides, just like from, from your inclusion, if you

already know it or so, how do we go from

here to learning Alpha and beta?

What are the next steps?

If you've seen this before, can you say it without

us discussing this now, to make this a bit more

interesting for those of you that know parts of this

already.

So how would you, if you were meant to call

up linear regression rate, what would you do?

depending on how large the losses in which direction you

want to change your parameters to make the loss closer

to zero.

So depending on where the loss is, you want to

adjust.

That's right.

So depending on where you are in this kind of

loss surface, I have a plot on this.

You want to adjust your parameters to make the loss,

that's just a mistake and fit.

That's how close is your curve to all these points.

You make it want to make it smaller.

How do you like how do you visualise this for

this, this curve here or like you know for this,

for this line with an intercept, right, where it intersects

the y-axis and with the slope that's the beer.

So I can move this curve around, shift it and

and pivot it, and by this I I find a

position where the sum of all these square residuals is

as little as possible.

Good.

How do I find your, your point of steepest, you

know, descent that you said?

That's the derivative.

That's the gradient.

That's exactly the definition of the gradient.

All right, so let's try to build this.

We first need the gradients, right, and that's pretty simple

um for linear regression, um.

It's, I want to have the gradient with respect to

alpha, so I take the chain rule 2 times this

thing, and then the inner derivative which is just minus

1.

So I have 2 times the minus 2 times the

residual.

The residual is Y minus the prediction.

OK.

I can take the derivative with respect to beta.

Um, it's 2 times the inner derivative, that's the inner

derivative minus X, so it's minus 2 times the residual,

simple.

That's my gradient vector.

These two derivatives, these two partials with respect to alpha

and with respect to beta, the two parameters that I

want to learn, and that's just the, the residual and

the regression.

Any questions?

What is the residual.

Um, and the residual is, is basically the distance to

individual points of my current prediction with my current alpha

and my current beta, and that's the distance to uh

individual points.

So it is Y minus whatever I predict.

Yes, please ask questions.

Otherwise, this is uh like a um.

Much harder to tell for me like where where issues

are.

OK, so that's E, OK.

We've done derivatives of the loss function.

The loss function is just loss function is just fancy

for I want to have small square distances to all

the points.

That's the, that's the average of all these distances could

also be the sum.

Um, and then I, uh, I take these derivatives and

now I can set up grade and descent.

Can you tell me, without skipping to the next slide,

how would you set up grade and descent?

So what's the logic here?

We want to learn.

So first of all, what we, what do we want

to achieve?

We want to achieve to, we want to get an

alpha and a beta that fits that curve optimally in

the sense that the square distances are minimised.

So what, how do we celebrate in decent?

Imagine you were to now tell me the pseudo code

for it.

You, I give you a data set.

That data set has 1 X.

Right, just one slope, and the why, that's what you

get.

And now the task is find alpha and beta viagra

in descent.

I'll use a formula in in uh in RSO but

like code it up yourself.

How would you do it?

Um, you could just start by setting your parameters to

a certain value.

You could start by setting your parameters to, uh, I

don't know, I sometimes 0 is fine.

That's right.

So for this one is, um, and then you can

pass through like one observation and calculate the loss so

you could pass through multiple observations and average the loss,

um, and then depending on what your loss is and

where it falls around like minimising it, so like more

than 0, less than 0, you can then take the

derivative of the loss and then multiply it.

By a scaling amount and Um, add that to the

parameters to shift them in whichever direction they need to

go and then you just keep doing that until you're.

That's right.

And the derivative, the gradient is the is the think

of a function that's the point of this is a

this is a 3D function here.

That's X, that's, and in that space is some function.

Imagine like some kind of corner or or volt or

something.

And at every point that function has a, has a

slope in multiple directions.

And the gradient is just the, um, the, the, the

direction of deepest ascent of that function.

So if we subtract that gradient from the parameters, we

can update the parameters to in that space go to

to the minimum and let me show you maybe the

visual intuition first.

Um So this is just the loss for that simple

problem here.

I have a linear regression regression where I choose the

alpha and the beta.

I have the loss.

It is on the, on the Z axis and the

loss is just the square, the sum of square residuals.

And imagine like this loss is in this room here

and I have my intercept here and my slope there,

and now I'm walking through this room and I try

to find the point of alpha and beta where that

loss is minimal.

What is the loss, the average or the sum of

all these square residuals?

And that's a function of alpha and beta, and I

try to minimise that function.

So I start somewhere, like you just said, I start

somewhere random, or at 0 or wherever, and then I

just go into this direction of steepest descent, and I

land at the minimum.

If the function is such a nice shape, which for

linear regression it has, then we can just walk and

we will hit the, we will hit the um the

minimum.

If it has a funkier shape, we might get stuck

in some, you know, local minimum or some saddle points

and so on.

But here we can just trust the process that we

reach it if we don't pick a bad learning rate,

etc.

So what are the steps sides between our moves towards

the, towards the final parameters?

This is the learning rate.

And this is really the fundamental of, you know, how

GPT, all these things learn.

It's this, it's great in the sense.

OK, so let's set this up, similarly to what you

just said.

First thing is we initialise the parameters, say at 0,

we can do this for linear regression.

Um, and then we choose this learning rate.

I call it ETA, OK, and that's just the step

size.

And then we repeat this for epochs, that means full

iteration of the data set is an epoch.

First thing we do is for the full data set

for our alpha beta, we predict all the residuals.

With these residuals, we compute the gradients, that's how we

do it before we we saw when we took the

derivatives, they are functions of the residuals, and we subtract

the derivatives from the parameter values.

And this is how in that space we walk towards

the minimum.

Any questions?

intuition.

So we are subtracting the derivative because for every step,

one unit that we take, how much are we changing

our parameters.

That's right exactly right.

So the new alpha is the old alpha minus the

subtracted derivative with the step size and the derivative is

that, you know, steepest ascent, we subtract it because we

uh we want to, we want to minimise.

Right, and if we see a big uh see a

big step reduction, that just entails that's the steepest.

Uh, so the, the, the, um, you know, the steepness

of the function at that point is given by the

derivative.

So if it's very steep and you take, so if

you take constant step sizes what this is, if my

ITA is a constant number, uh, then they are bigger

in the beginning because the slope is larger, right?

So that's why they're bigger in the beginning and they

get smaller and smaller because the slope isn't that large

anymore.

Exactly.

But one thing, for example, if we pick too high

learning rates, we can actually, we might never go to

the minimum, because what would happen in this function with

too high of a learning rates.

Too high of an eater.

Exactly.

So I would, I would do this kind of stuff

and I would never go to the minimum.

I might actually even go like this or something like

that, yeah, and you can see this with the code

we're building.

OK, other questions.

Let me show you the code and then we take

a break.

Good.

So always with these things, I find it, and here

you see that's a couple of lines of code.

That's how simple it is.

It's much easier to understand them if we build them,

right?

So what do we need to do here with this

code is, or what we need to do here is

to do two things.

One, Um, build data.

OK, so I don't have any data to to download.

I want to simulate it that I know what the

truth is, um, and then to try to estimate it.

Good.

So let me walk through this here.

I set a see such that you have the same

outcomes as me.

Kind of obvious.

And then I set the sample size.

I have a 1000 observations, and I create my axis,

and they come from a uniform distribution with support between

-10 and 10.

So all my axis will now be between -10 and

10, equally likely between -10 and 10.

I know the true intercept alpha, I know the true

beta.

Um, I create my error term, my epsilon, and then

I, I define my Y.

I say that is what people would refer to as

the GDP data generating process, so now I have my

Y.

It's alpha plus beta times X plus this error epsilon.

Um, I've simulated there.

That's it.

And now I can, you know, say I could plot

this.

Yeah, right, so pretty simple, like very little noise, easy

to learn.

OK, so that's all I thought about.

Um Good.

So I initialise alpha and beta.

I set a learning rate, that's the ITA from before.

I set a number of epoch, that's the iteration of

the data set.

I predict my values with the current alpha and beta.

In the beginning, this is just 0, right?

So it's like, it's pretty, I mean, my prediction is

0, right?

It's, it's.

So I subtract the zero vector, my residual is the

full outcome.

I can't predict the outcome.

Um, and now I compute the gradients, the two derivatives,

and I subtract them with a learning rate from the

parameters.

And here I give you updates every 10% of the

total epochs that I look at and I check out

what is the alpha, what is the beta value.

So let's run this.

Super quick.

You see, it converges pretty quickly.

It converges against an estimated alpha of 201, estimated 299.

It's very close to what we know the truth is.

Why do we not get it precisely, either because our

very descent doesn't work or because we don't use the.

Of the minimum in the, in the last surface, or

because of the noise, we can't get it fully, uh,

like exactly because we added some noise to that function,

right?

So what's cool here, I think, is if you then

try to just use ours function, um, and that's the

linear model, and to just run the regression, and here

you see, it's extremely close.

Right?

So, linear regression actually takes the analytical solution of this,

like, you know, we'll do this in the class or

if you want to, this is like one of the

exercises in the class.

Um, but by, by this building this simple function or

this, these two lines, of course, here, we've just replicated

exactly the outcome that you would get with the, with

the original function.

OK, and it's just the implementation of what we had

on the site.

Any questions?

Cool, then uh let's meet at 100.

I know.

I've been through this.

Uh It's so like.

To be nice to myself.

It was hard, but it like still feels like.

I sound.

I it's like yeah you'd have to be.

I like like you feel like the water is.

I explain the.

Yeah there's like maybe it's like really important for myself

to take advantage of the yeah that's like.

Yeah.

Yeah like every time.

Yeah, that's huge.

There's one today and there's another one next week.

A good officer.

It Yeah English.

Just please.

Oh, Wednesday OK, I get that.

Speaking, yeah, I know.

I.

That I.

Yeah, when I was a girl I had a full

like I was like.

Um, I went to international school, so Imagine.

kind of like I think maybe it's easy to play

and your loss is on and your product on you

just want to have an intersect also.

And so now you can shift out and now you

go up or down the loss.

Um, what is the at every point, the derivative is

actually the self of this function, right?

Um, and the derivatives might do you know, so at

this point, imagine at this point of the functions.

I want to.

I want that one?

I take my um 2 I mean yeah, but like

I yeah, the one I think it's I think it's

very much.

But it's like what is this I think that I

like um and if you think of like some space,

it was too long and it's the uh it's the

direction in that space for you.

to minimise the function to find for example so um

and then we look at the enforcements the score of

the game you want to.

The game one or if you play computer games, there

are also papers that you want to change your behaviour

that maximise the score.

So there you actually no, no.

So I think that's good to think about this like

it was the last I think it's really.

The cloud is like not enough.

Because everyone had a yeah I reckon like shut down

I mean you could prepare you 2 hours, right?

I think we've done it already like uh yeah I

mean eventually I just start working a little OK so

and then I I think.

I think there's like I started yesterday.

I feel like the landing rates have crashes and the

rest of the countries that are so um basically that's

what I did.

I guess now I like I.

You just like get so maybe the out the output

was not as big, but now I know where to

go it could have been could have been a.

Actually you're and I, I was just like yeah and

I'm like you can definitely right now.

I want to know like.

yeah like like like like that was like I slept

10 I woke up.

I'm lacking just like I'm just doing my best right

now.

OK, I could get some new pens, um, and, uh,

OK, cool.

So we'll do the same thing now with logistic regression.

Um, did you do logistic regression in the scores?

Or not so far.

Did you do, you did classification, right?

You did like text classification to say positive or negative

sentiment or something like that.

If you do this with logistic regression or no?

OK.

Um, how did you do classification then?

What use OK, um, so, OK, so, so you basically

did the much more complicated thing before, but if you

think of a losso, um, it has this penalty term,

right, uh, at, at the loss function.

If you set the multiplicative constant in front of that

penalty term to zero, what do you have?

Yeah, exactly.

And what if your, if your link function is not

just like the linear mapping, but it's actually the sigmoid

or inverse of this, then you will have logistic regression.

OK.

So you can have logistic regression or linear regression, and

then you can have these penalties, um, and you know,

they could be lasso or rich regression, but they try

to regularise if you have lots of covariants, which is

common here if you have lots of, you know, possible

words that you could count or so.

But really the fundamental thing is the logistic regression and

if you've done any kind of classification with these models,

you've done it implicit.

OK.

And now we don't look at regularisation, um, but we

can look at our kind of, you know, data set

here with just one X and one Y.

And first question is why do we not need to

look at regularisation?

So say I have 1000 observations of things to classify,

say texts, but for every text, I only have how

many words it has.

That's my.

I have 1000 texts and I try to predict whether

they are I don't know speeches from one party or

another, OK.

What how well this could work.

Um, why do or do I not need regularisation here?

I mean you only have one.

So I couldn't overt, right?

I just have one variable.

So if it helps me, but I wouldn't over on

like gazillions of variables.

Whereas if you look at the DFMs that you looked

at with Ryan, every column is conceptually a variable is

a word count.

So you can quickly have so many columns that you

just overfit everything and then it doesn't generalise well to

unseen data.

So that's why you regularise.

Um, that's the kind of big picture thing.

What we're trying to do now is we're building the

thing we just built but for classification.

Why do we do this?

Because if we look at supervised learning, that's where we

have cases of X, data and X and Y labels

or you know, positive or negative sentiment or or some

some outcome variable like say.

House prices in an area or something like this.

Um, those are the two big branches of supervised learning,

our regression and classification.

Regression we just did, classification we're doing now.

Each of them you can also do with neural networks.

Just then it gets a bit more complicated to build

it from scratch.

OK, so we do it um for these simple models.

So logistic regression.

Basically uses this logistic uh surprise function, um, and what

we do is we try to predict the probability, um,

of a certain item being, you know, of being of,

of a certain class, OK.

So say in a text classification, we want to predict

spam.

I think you look at that example, right?

So for something that's truly spam, we would like to

take a high probability for something.

That's not spam, you know, spam philtre, like email philtre,

we want to predict low probability.

How do we do this?

We actually wrap our, um, uh, you know, previous model

just into that function here.

So, um, here you see it on the slides like

mark.

Um, this model is just wrapped into the function here,

um, and then we, we basically can predict the number

between, uh, between 0 and 1 using our parameters alpha

or meta.

OK, good.

So, in order to do gradient descent, it's the same

thing as before, just the loss function changes.

Um, what's the fun for classification.

It's what people would call like in computer science, binary

cross-entropy, you do some statistics, it's just a logistic loss,

right?

It's just different names for things.

Um, we use this cross-entropy term and kind of when

we look at neural networks later that can have many

classes.

Here we only look at 2, so we have this

binary loss, right?

So, This is again the loss over the entire data

set, and that's the new loss.

It's not mean square error, but it's this weird kind

of binary cross-entropy, OK.

So why does that, and now I wrote it for

a single observation, OK, to make it a bit simple.

Why does this make sense?

Why is this something you want to minimise?

If you walk me through the terms here, why is

that like with the minus in front of it, why

is that a sensible loss, like an error function for

trying to predict ones and zeros as well?

Right, so now we don't try to regress, uh, like,

like fit a value by regression, but we try to

predict ones and zeros, classes, spam or no spam.

So why is this a sensible loss function?

Why do, why does everyone use it?

You can derive it from a distribution, but just intuitively.

like 1 and its all of that to 0.

And then it's it's perfect.

So that's just a neat version to write it.

Perfect.

So um I, I look at these two terms, depending

if my observation is spam, then this is one and

this collapses to 0.

If my observation is no spam, this collapses to 0.

OK, great.

But now we still have to look at the long

term.

So, we've now explained the scale in front of the

lock term, fantastic.

But like, why is this lock down?

Why does it make sense?

So first of all, how does the lock look like?

Um, the lock, now with the fancy new pen, uh,

hopefully more visible.

Um, the lock looks like this.

OK.

It's not defined for zero, but if you go to,

um, zero, so you walk with the lock towards zero,

what is the lock?

It's getting towards minus infinity.

So really small numbers when you have uh come out

of the lock when you put a number into the

log.

So why does that function make sense?

Yes.

If you're multiplying like probabilities or values close to zero

over and over and over and over and over again,

you'll get much, much smaller, smaller, smaller numbers, right?

You put the log there so that um you're not

kind of dealing with like the numerical underflow problem, I

guess, computationally makes sense to me.

So that's right.

So what you're saying is basically that's a bit of.

Imagine I have this.

So originally, this is if you did like a bit

more statistics, this comes from maximum likelihood.

When you multiply a lot of probability densities, if I

take a logarithm of the of the product, it becomes

a sum.

If I, if I multiply a lot of small values,

exactly like you say, at some point they become 0.0,000,

whatever.

And my computer with only so much precision cannot store

them anymore.

More at some point it's just 0.

So when I add them all up, then it makes

it numerically easier for my computer.

Absolutely.

Um, here I was thinking about something simpler.

I was thinking of why does that make sense to

learn that function here?

Um, why is the lock very sensible here?

So what you said earlier is, um, this year is

if the true class is one, it's spam, right?

So if the true class is one, I want to

predict the probability close to 1, right?

If my probability is 1, what's the lock of 1?

0, nice, no loss.

Great.

Now you're thinking, well, but no loss zero?

Yeah, because if you get a loss, it's minus almost

infinity, right?

So if I predict pretty low probability here, like say

the two class is 1 in spam, but I only

predicted 0.1, my lock is -10 or whatever, or like

pretty low.

And then I multiply it by -1, and I have

a massive loss.

That's the logic.

So if a true class is spam, I predict a

low probability, I multiply the whole thing by minus, I

get a huge loss.

That's why this function makes sense.

On the other hand, if I predict when something is

not spanned, yeah, if I predict the probability of zero,

when it's not spam, what I would like to do

eventually, that it's again a lock of 10.

Nice.

No, no increase in loss.

However, if I predict the probability close to 1, when

it's, when it's actually not spam, then I get again.

Close to the lock of zero, which is again a

huge negative number which are multiplied by -1, which is

a huge number, right?

So I want to predict a 1 when the true

plus is 1, and I want to predict a 0

when the two plus 0.

Unsurprisingly, and that function is just enforcing this.

That function is getting huge.

if I'm predicting a small number when I actually should

predict a zero, and that function is getting huge if

I predict a high probability when I should actually predict

a zero.

The logarithm is negative, but I just multiply the whole

thing 1.

So it seems.

Great, and that's like intuitively why this function makes a

lot of sense.

Now, I did the math for you, but I, I

skipped this because I think it's, it's too boring, um.

The key thing here, if we want to learn alpha

beta, we need to substitute alpha beta, right?

Um, so we, uh, we, um, substitute alpha and beta

into, into this logarithm here, um, and now the only

thing for us to make our lives a lot simpler

to realise this.

P is the probability is equal to just what we

said before, the sigmoid function, but 1 minus P is

just almost the same thing, but the E also at

the on top of the of the fraction.

Why?

Because this plus this will be equal to 1, right?

So we 1 plus E divide by 1 + E

to the.

So that's 1 minus P.

I substitute these things in here, I do some math,

I arrive at an individual loss of this year, makes

our life easier.

So that's now our loss for an individual observation our

data set.

To, you know, avoid all the sums and shipping.

Next slide.

Same as before, now that's my individual loss.

I have to take derivatives.

The derivative here is the chain ruler.

Now it's getting slightly more involved, but like, you know,

nothing complicated.

So here, um, the lock should have should have another

parenthesis here.

I take the derivative of this thing.

It's first treating this as the function Y thing, so

it's 1 over Y, one over the original thing.

Now, Another time, a chain rule, the derivative of e

to the X is just e to the X, so

like you know e to the ye to the, the

original, I relabel this thing as Y, which is the

original thing.

And now lastly, the inner derivative of this with respect

to alpha is just minus 1.

So that's just the concatenation of these derivatives.

1 over Y, E minus 1 over Z, E minus

Y, and then the inner of the last function which

is Y.

OK, this pretty like it's, it's amazing if you look

at that kind of function and kind of the algebra

you have to do to arrive at that simple form.

In the end, the derivative with respect to alpha is

just to predict the probability of minus the label.

Um, and then the predict the the derivative with respect

to beta also applying the chain rule, it's just the

predict the probability minus the label times X.

Is it clear how the chain rule of this gives

me these derivatives or not so clear?

OK, um.

Good, very for logistic regression then we again initialise it

after to beta 0.

We choose the learning rate.

We go for the the box now.

We compute not the, not the fitted belt, not the

Y, but we, we compute the regressional but we now

compute all the probabilities with all the probabilities we can

compute the gradients.

We do gradient descent.

Same thing as before, just different loss function.

That's how general this approach is, um.

Now, on to, uh, the coding example.

Again, we, we implement this from scratch, but here, to,

to estimate it accurately, I need a much larger sample

size actually.

So here I create a sample of 1000 observations, again,

my uniform from 10 to 10.

My, my true value is just to change things up

a bit, they are a bit different.

And now I compute the, the true probabilities with the

logistic function.

And then where's logistic regression.

Assume my ones and zeros come from.

They come from a binomial distribution here where I put

that probability.

So with the probability for every observation, I drew a

1 or 0.

Um, so in the end this year, if I run

it.

Why would it just be a vector of ones and

zeros?

Great.

And now I do gradient descent.

I initialise, you know, just our, our site initialise, pick

the learning rate here, pick a massive learning rate because

it actually works pretty well.

Um, the number of epochs I set it to 1000,

compute the probability, compute the gradients, subtract, etc.

Same thing.

I run this, runs a bit of time, but not

too long, converges to minus 2.8 and 0.8.

Again, I can use built in functions.

Um, I use the GLM that's short for generalised linear

model.

Um, I run the same thing.

I get almost the exact same coefficients.

So again, I replicated the estimation with basically my own

code with a few lines of code.

Using this fundamental principle of minimising the error.

You have questions about this.

Cool, OK.

Yeah, like.

Maybe that's a little bit before this.

How, like, can you know the loss function beforehand or

you just are discovering it like every time you calculate

the grey descent?

Like how do you know?

The the the the slope of the function if you

don't know the function beforehand, right?

Yeah, that's a good point.

Um, you know, right?

So you know the function is a function of the

data, right?

So you know the data, but now that's why you

initialise.

So in neural network comes as forward path through the

network here just forward path to function.

But if by initialising alpha and beta, you can compute

the outcome.

It will be a nonsensical outcome or like a bad

prediction in the beginning, but you get one, and then

you can compute the error and then you can do

gradient you update your alpha beta, repeat the same thing.

So how can you compute your loss or your gradients?

Well, by um by having the data and having a

current iteration value of alpha beta.

But if we didn't initialise, the whole thing wouldn't work.

Because we could never start computing diction.

Other questions.

Great.

OK.

Um, so this now allows us to, um, to move

through, uh, to uh neural networks, OK.

Um, and this will be um an introduction to them,

like the, the simplest multilayer perceptrons before in our networks.

This will be clear in a moment.

Um, and then we'll, we'll build on this, like we'll

use variants of them for word embeddings next week and

then we, uh, we expand them to kind of transform

us to this, uh, to study large language models in

the, in the last, in the last week of the

course.

And large language models that means something, you know, underlying

something like JGBT.

So some history of this, um.

From kind of like the current discussion, you might think

like these artificial neural networks are something very new, but

it's an ongoing like kind of old research project and

so on.

So there's this 20, 1943 paper, a logical calculus of

the ideas imminent and nervous activity.

It's pretty like amazing how long ongoing that research is.

OK, um, just they actually had neurons that were either

on fire or not 1 or 0.

You couldn't differentiate them, nor do gradient descent, etc.

Um, so there were, there were, um, there were clear

limitations of them, but people started thinking about this early.

Um, people also continued working on this in the 20th

century.

It's not something that, that, you know, researchers came up

with 10 years ago, but it didn't work.

Uh, very well, um, uh, at least I mean it

worked but not terribly well, not terribly much better than

lots of other approaches.

So people kind of lots of people lost belief in

this.

Actually, it's quite an interesting scientific story if you look

a bit into this, that there were a few people

keep like kept going with this and eventually created these

breakthroughs when, uh, when most people didn't believe these, these

models were, were very sensible.

Um.

Or at least lots of people believe that.

Um, if you think of like a big breakthrough moment

in in neural networks where probably was this Alexnet moment

of where a paper and by also Jeffrey Hinton, co-authors

who won the physics novel, um, last year, um, they

want this image classification challenge to predict certain images, whether

the image.

Contains a container ship or something like this, and they

were able to improve the previous classifiers massively and they

did this with what was called the convolution network.

If you're interested in this, check out 475, um, and,

uh, and at that time really were improving massively on

previous, uh, on previous predictions.

OK, so the key question is, if we work on

this for such a long time, why have they become

important recently?

Um, 3 main reasons.

There's much more data now, right?

So we have all the images that they were predicting,

but like check GPT we're predicting the next word on

the internet.

So we have all the internet texts, so we have

huge amounts of text.

Hardware, just more so, think of like how How um

there's this exponential pattern, um, in consistors, um, and how

our computers for, for a long time now, um, growing

fairly predictably along this law, um, then, you know, parts

of this are, for example, how we utilise these GPU

storage.

created for processing images, but very good at small computations.

What are our small computations matrix multiplication.

That's the small computation here.

That's why GPUs are used heavily from our networks.

And then increased software, right?

You have all these deep learning libraries now we'll use

um Tenorone and Pas.

Good.

So let me first show you the kind of simplest,

um, uh, simplest form of a neural network, and that's

a single layer perceptron.

OK.

Single layer is this thing, single neuron, because it's only

one of these, um, and we can define it like

this.

We say that is equal to the inner part of

a, of, um, of a vector and weight matrix, um,

plus a bias, that's just now machine learning terminology for

the slopes and the intercept.

And we need to learn the results and the intercept.

We wrap all this into an activation function.

So imagine like this is these are signals going into

the neuron.

We activate it, but we don't say 10 fire or

not fire because we couldn't take derivatives of this very

well and it would also like, you know, it would

be a step function by definition, so not smooth.

Um, And it's tricky to learn with this, um, so

we define activation functions.

Now imagine I defined a sigmoid activation function.

So I'm showing you the simplest neural network there is.

You have data flowing into this thing, computing, um, this

looks like a linear, you know, regression model, but then

we activate it, we run it through like a nonlinear

function, and then we have an output.

That's kind of the simplest neural network, one layer, one

neuron.

Um, what is this?

That's it.

So logistic regression is a neural network, OK.

So what we did previously was the simplest kind of

neural network.

Yeah, so this is just logistic regression.

Um, these functions, they don't have to be sigmoid.

The issue with sigmoid is very quickly the derivative as

you see here or here goes to zero.

So if we do gradient descent, we don't want derivatives

to be zero all the time because we can't learn.

So we want derivatives to, to actually contain signal and

not be numerically close to zero.

So there are other activation functions like this.

Now you might say, well, but this is 0 all

the time.

The derivative of a straight line here is just that

is like constant and 0 is 0 because it hasn't

absolved.

That's true, but in that part, the derivative here is

constant and that is not going to zero in the

in the activated part of this function here, and then

learning is much easier to achieve.

So if this year is like the simplest in our

network, let's look at the slightly more complicated one.

So now we have one hidden layer and in the

hidden layer, we have, so still one hidden layer, um,

or actually.

Now we have a hidden layer because previously we didn't

have one that's the usual terminology, we just had an

awkward layer.

So now we add one in the middle, um, and

we add one with in this example, imagine 7 neurons,

OK?

And then we have an outward layer with 3 neurons.

So now think of this intuitive example here with just

a single neuron, a single output layer.

Um, and how you were multiplying this into it.

Here we would need to multiply all these things into

the first neuron with different slopes and intercept into the

second neuron, with different slopes and intercepts into the third

neuron, with different slopes and so on, right?

So we would need to do lots of these computations

and then with different slopes and that, that, and that.

By the way, when do you think we will have

one output neuron and when will we have multiple or

are common cases?

Mhm.

Um, for like progression where the outcome is continuous, you'll

have like one prediction if you pass it a set

of predictors.

So let's say you're doing like multi-class classification where you

want like a vector probabilities, um.

So imagine our examples here that we look at this

text classification and we want to say spam spam or

we want to say sentiment positive, negative, neutral.

So then we need 3, OK.

Um, and if we wanted to do regression based on

text, because we want to predict prices in the webshop

to pricing based on product descriptions, then we would have

just one output.

OK, now the matrix multiplication that we that we went

through in the beginning comes very handy.

So once we initialise to your previous point of how

do we even get the value of that loss, once

we initialise all the W's and the betas, we can

compute data flowing through that network.

In logistic regression, we did this.

We obtained the predictions given like uh data, but in

our network, we can, we can do the same thing,

um, start with the axis and compute the outcomes.

This thing is commonly referred to as a forward path.

And this is done with matrix multiplication.

So imagine in the example here I have 5 inputs.

I multiply them with a hidden layer and then I

have 7 a vector of dimension 7 times 1 in

the middle.

I activate this thing.

I activate it like this activation function.

I apply it to every element of the vector.

It could be a sigmoid function or the redo.

There's other nonlinear.

Out of this comes the activated one.

I multiply again with the matrix to bring it down

to 3 dimensions at the slope, at the intercepts, activated

um finally to get the predictions.

Um, so you see this is simply It is simply

the form of this network.

Take a vector of length 5, bring it to 7

dimensions, bring it down to 3 dimensions.

All of that, that path, we can all these like

hundreds of multiplications here and decisions, we can just visualise

with with matrix multiplications.

I'm in the activation.

Yeah, so I think taking a step back.

What are we trying to build here?

Regressional classification models.

That's what we're trying to do.

Um, imagine like the with the function like that function

here seems like the dot cloud seems pretty linear.

So that we can do with a simple linear function.

But if the dot cloud has a funkier shape and

so on, we might increasingly non-linear function.

And if we add hidden layers and more more structure

to this network, we're simply creating a function approximator that

can fit more non-linear functions.

That's it.

And if we just remove the hidden layer, and we

just have a single output unit and we do a

sigmoid activation, we have logistic regression again, as you said,

and then we cannot fit very flexible functions.

But if we want to fit increasingly flexible function, we,

uh, we, this is the way in this like a

model framework to make functions more flexible.

And what exactly is the hidden layer in We have

our inputs we're multiplying that.

So this something is the weight matrix and the biases,

these are the slopes and intersects, and these we initialise,

yeah.

And once we have initialised them, we can get the

hidden layer, but the hidden layer, that's a great point,

is what we would refer to as a latent variable.

Some variables that we don't truly observe.

Um, we just create it and then we, uh, we

can, you know, we can, if you want to get

an estimate of it or so, if you think of

in terms of statistics.

Um, but that's not a variable that we, we, uh,

we observe and, uh, like in the real world, we

would just observe the input values, the access, and the

output values whether it's positive, negative, or neutral sentiment.

And but these hidden layers whether we as your point

is also how many do we add 75 or like

one hidden layer or what does it mean?

And that is just there to fit increasingly non-linear functions.

And if you think of something like GPT, what the

true next word is?

If I change one word in a huge input sequence,

that's a very nonlinear function.

That's a very kind of sensible function, a very, you

need a very expressive function that way I change like

one word, the predicted word of the next word changes

and so on.

So we'll, we really need these non nonlinearities later in

the course.

Otherwise, all of this wouldn't work.

a lot of questions.

OK, so.

To your point, we can create these things very flexibly,

right?

So we can, you know, add 3 hidden layers, um,

and make it more make it, you know, more flexible

as a function approximate.

First thing for regression, second thing for classification, for example.

The one thing we um We didn't discuss.

Uh, for now is this, how do we make this

thing in the end actually predict probabilities of classification, and

we need this function called softmax.

So imagine at the end, the better one, I imagine

at the end we arrive at 3 numbers out of

that network, so we have 3 numbers, um, and they

could be, you know, -10 + 42 and 5 for

one example of data.

So that's no probabilities.

How do we get this into proper?

Abilities, OK.

So how do we, out of these numbers that, you

know, from our network are the final outputs, uh, if

we take input and then we initialise weight matrices and

intercepts, we compute the whole thing and we get this

here, um, how do we make them probabilities?

And we make them probabilities by applying a final special

activation function, which is referred to as the softmax, and

the softmax of this thing here is, uh, the function

of its value.

Divided by the e function of all the values.

And here you see that's immediately normalising it between 0

and 1.

Like all of them will now add up to one,

and we can think of them as probabilities.

Why do we take the exponential here?

Why not just divide the values by the sum of

the values to normalise them between 0 and 1.

Is it some, is it like very inspired if you

have like sum them all up.

Think, yeah, but think even simpler.

Think of this overall thing, like what happens if I,

if I don't take the exponential of my 10?

What's the probability that I predict?

Yeah, so that ideal of probabilities.

So then, uh, this is exactly the reason.

So we wrapped it into the exponential because each of

the minus 10 is just now close to 1, close

to 0, right?

Um, and then this thing gets close to 0, and

if it's a very large number, the probability gets close

to 1.

OK, um, so now we've defined this thing, which is

just a sequence of matrix multiplications and at every hidden

layer, I also activate it.

So, you know, if you want to look at it

in detail after the lecture, if I do this matrix

multiplication, I arrive at the next vector, every every position

in the vector I activate, I apply this nonlinear function.

Um, and that nonlinear function could be either of these

here, like the logistic function, these relos, which is just

the max between 0 and 1 for every position of

the vector and so on.

That just adds the nonlinearity.

Um, and then I continue doing this for how many

layers are defined.

If I do, um, classification, I apply soft max in

the end, um, and I have probabilities.

So, you, you preempting a lot of the points that

I wanted to raise later.

So what we're doing here are just estimating function approximates

between X and Y.

Like we've done in this course and your stats one

on one course, it's just the same thing.

So neural networks, there's no magic about them in that

sense.

They're just this approximate between X and Y.

OK.

Um, however, because of their flexibility and particularly more modern

neural networks like the transformers that we're going to discuss.

In the last lecture, they can fit really non-linear functions.

And that's why they are great for sound, text, images,

you know, if some minor detail changes in images, like,

you know, whether a person smiles or frowns, but it's

still the same face, that changes the entire meaning of

the image.

So you need these kind of functions that they are

so expressive that they can pick up these small changes,

um, and that wouldn't work with like a linear regression.

Um, on the Pixel values in colour channels, you know,

or something like this, so that wouldn't be expressive enough.

OK, so that's just for, you know, if you're a

bit more interested in the technicalities, there's actually like a

um a universal function approximate type of theorem and under

some regularity conditions, these neural networks, if you add enough

neurons, um, they can fit any kind of function.

Yeah.

So people in the beginning got excited about this and

they were saying, great, I mean, we can fit everything

with this like that.

So that's the way to go.

But lots of actual function approximates have these kinds of

theorems going.

And the problem is how do you find that function.

The problem is not does it exist?

I mean, it's great if it exists.

That's a good sign for one of these classes of

function approximators, but the main challenge is how do you

find this?

Um, and then empirically it turns out if you add

a lot of layers and so on, like in modern

architectures, you can find pretty good approximations between X and

Y, even if they are very complicated.

OK, so we have weights and biases of the neural

network, but based on the intuition we have from the

gradient descent on linear regression and logistic regression, you sort

of know how gradient descent will turn out here.

You initialise the weights and the biases.

These are just the slopes and the intercepts of the

neural network layers.

Once you have initialised them, by the way, you can't

initialise them at zero, you initialise them at random values.

Um, why not at zero?

Because then you wouldn't learn in these functions like you

would always stay at these zero values.

So initialise them in small random values.

You can compute the predictive values, you can compute the

derivatives, you can do grascent.

Like all kind of, you know, now we have a

lot more slopes and intercepts, but the logic is the

same.

That's why it's so helpful to build these things in

minimal versions.

Great.

So loss functions.

No surprises here, mean square for regression.

And now this year is the generalisation of our of

our logistic loss for multiple classes.

So this year is um like the, the cross-entropy loss.

And This is just the, um, the generalisation.

So I have K classes now.

I have span or like positive, negative, neutral sentiment.

And for each of them is if it's positive, this

is one.

If it's neutral, this is one, if it's, um, um,

negative, this is one for its specific K 123.

Otherwise it's 0.

And then I better I want the predicted probability to

be close to 0, sorry, to be close to 1

for this logarithm to be close to 0, same thing

as before.

If the two label is 1, I want to predict

something close to 1 here because then the logarithm is

0.

If the two label is 1 and I predict 0,

at a hugely negative number here, multiplied by -1, a

huge loss.

And now really like kind of we, we're carried by

this intuition from, from the simple examples.

Here, to do the derivative requires a lot more chain

rule, because why?

Uh, this is like, you know, you think of this

like, you can think of it if or if you

take a course like 475, you know this underlying structure,

um, you can think of it like this computation graph.

So you must.

Apply it with the, uh, with the slab with the,

with the slopes at the intercepts, apply the activations and

you work through a graph, like through a little network

of computations.

OK.

And then basically, the, the concatenation of all these computations

to arrive at the output people refer to as a

lot of terminology here too as the forward.

Pass.

So our network that we now looked at in some

detail here, you know, we multiplied X by W1, added

B1, um, activated, applied the sigmoid or to every single

node, multiplied by B2, uh, and plus B2, arrived here,

applied softmax.

That was, that was how we could compute the outputs.

That's what causes what's called a forward pass.

So once we have this to get the ris, we

just looking at this year, we apply the chain rule.

So to get Um, to, uh, we, we wrap this

all into, you know, we wrap this all into our

overall loss functions.

So all the individual losses we can average over the

data set practise we look at the um batches of

the data set, um, because it's too computationally costly to

compute it for the entire thing.

Um, but once we have this, we just take the

derivative.

So through this whole form from X to the output,

we can apply the chain rule the other way around.

OK.

So if we have the output and we want to

know how does the output change not as before with

alpha and beta, but with like some like, you know,

weights in the first layer, we just do chain rule.

Um, we go from the loss function to the into

the slopes in the first layer as J with respect

to so J is the over loss function.

J with respect to a, a with respect to like

derivative to Z, Z with derivative to a, and a

with derivative to Z with derivatives to W.

So we do the chain we'll just do a longer

function.

We won't look in the details here because it would

require, you know, to do some more, um, uh, yeah,

some involved kind of like sometimes vector calculus if you,

if you do this with matrices and so on, but

the logic is really the same that we did before,

just the derivatives are are more involved.

Um, once we have defined the gradient.

This is this notion of computation graph.

Let me zoom in here.

You know, the Xs are multiplied.

We add they arrive at an A, we apply a

logarithm and so on.

So you can think of this whole thing here as

as this close structure that you can represent as such

a, such a computation graph.

This is a bit beyond the scope of this course,

but you can, you can check it out, um, if

you're interested in this or have a look at these,

at these other courses that focus on our networks.

Now we can again use gradient descent.

Turns out for neural networks, we can't use it on

the entire data set that we did before.

We now need to use it on little portions of

the data, um, because that's computationally feasible because neural networks

can have these billions of parameters.

We can't just compute the predictions for the entire data

set.

Our memory couldn't take this even on asset computers if

the models are large enough.

So we do it for small batches of data.

Turns out it looks almost exactly the same as before.

Uh, this is the gradient descent.

Uh, what we do is we train this thing for,

for EE box.

We randomly initialise the weight, we pick the learning rate

now called alpha, not ITA.

Um, and first thing, we shuffle the observations now because

we don't apply the full data set but we Do

this, uh, per batch.

And then we obtain that's the, that's, um, the ceiling

function, that means we round up.

So we divide the data set by, uh, by the

batch size.

Imagine we have 1000 observations, and each batch has 100

observations.

So we have 10 batches now.

We, we randomly divide, that's why we shuffled our data

in 10 batches.

We compute derivatives for each of These batches and update

the parameters, turn to the next batch and so on.

All of that is just splitting the data set.

That's the difference to our previous code where we computed

the gradient with the entire data set.

Here we compute it with small parts of the data

set.

That's it.

We create all these patches, update the parameters.

Theta as a gigantic vector with all the network parameters.

Um, and, um, yeah, and really at the moment.

Like, this is not what your software library would run

like this claim Manila gradient descent, but it would add

some terms like momentum for this grading to keep going

because in, in these, you know, high dimensional spaces, that

means where you have lots of parameters, lots of slopes

and intercepts, there are lots of settle points in these

spaces.

And it like, and you don't want to hang at

the set points where the slope is 0 or 2,

but you want to find something that is more closer

to like a minimum.

So you need to, uh, um, kind of, Progress in

these, uh, with your gradient descent, if you imagine how

we thought about this before over these lost surfaces, you

want to walk towards the minimum.

If they have lots of 7 points, you need to

keep going.

That's why um adding kind of momentum, meaning the next

step size depends on the previous, like or the step

direction depends on the weighted average of previous steps and

so on is very helpful.

But it's not a massive modification of this year.

It's still pretty close um to to what the plan

and liss are.

OK, so to avoid networks from overfitting, um, we would

um You know, you, you see what we discussed previously

your, your Lasso and rich regression penalty.

You can just add um the sum of all um

all parameters or the sum of all square absolute values

or the sum of all um square parameters to avoid

overfitting of the network.

Um, taking a step back, why do you think your

networks like so it doesn't make sense that I'm prone

to overfitting, thinking of this structure here.

Imagine how many parameters we have.

This first matrix here brings us from 5 dimensions to

where there's 7 dimensions.

So that's a matrix of dimension 5 times 7.

so that, you know, you can see the parameters, that's

35 parameters, um, then this is from 7 to 749,

49, like, you know, this is, um, each, and that

is like, you know, for, for each of these multiplications

here.

Um, all right, so, uh, you will just have a

lot of parameters if you add them all up.

That means no and that's, by the way, also why

people didn't really believe in these neural networks, why there

were these huge gaps in research, because people were in

the beginning of thinking you could fit a sensible function

if you had so many parameters and you actually had

fewer data points sometimes.

Yeah, so, um, you can still, like, for example, this

year was estimated on, I think, a million images, but

this had a huge amount of parameters, way more than

the images, and it still worked pretty well, and that

in the beginning surprised some people.

Anyways, they still tend to overfit too many parameters, and

we can use the usual stuff to prevent overfitting.

Um, and by this, we could, for example, um, we

can, for example, add these terms to the loss function.

Um, can you tell me, to not be too quick

here, why do you think this prevents overfitting?

We can revise the rest of the class.

There's there's no rush with this.

So why do you think this thing prevents overfitting?

Is it like regularisation or you think like that's right,

exactly, yeah.

So like intuitively, why does that prevent overfitting?

So if you, it kind of penalises having parameters that

don't add much predictive value uh to it because So

you take the absolute value of the.

The absolute value versus how much you're minimising like in

the last, the sort of cost benefit um translates better

when you add this penalty term.

Oh, yeah, yeah, yeah, I think that's like, did you

want to say something similar or you?

Uh, yeah, along the same lines that it just penalises

the overall size of the coefficient so that if you

are giving too much importance to too many things, um,

which is sort of what happens in overfitting, the model

will get weaker.

Yeah, so imagine like say we take a theta theta,

you know, 10, OK, and that's, that's one theta we

look at.

And imagine that theta is pretty predictive.

So it reduces our overall loss in square error or

cross-entropy quite a bit.

But now we add to the loss function, the absolute

value of that coefficient.

And now we can trade off these two things.

So it better reduce the other loss if it's a

big coefficient, because if we add that to the loss

function, it increases the loss a lot by its size.

Um, so, and this is basically saying as soon as

coefficient values are big, they mechanically make the loss larger.

So on the other hand, they need to bring our

fit, um, you know, we need to improve our fit

as well and bring down mean square error or across

entropy parts because otherwise they are not worth adding.

That's why this works here.

Um, OK, um, other options are, they're all there to,

to prevent overfitting dropout, that's setting random weights to zero.

and we can also just stop grading the scent early.

Uh, and that is pretty clever.

It's like basically you look at the prediction on some

unseen data.

And as soon as because As soon, like if we

overfit the training data, the prediction on the data we're

training on will go down and down.

We'll see this in the code example, but it won't

work very well on new data.

So you just track how well you predict new data

and you stop training as soon as you stop predicting

the like improving your predictions on the new.

That's also one tool to overfit this, or you can

add noise at various steps and so on.

Um, good.

So I'll stop here.

We, I show you that, I mean, it's only 5

slides or less or so that the rest of the

lecture, but I don't want to rush this, so we'll

discuss that.

And then I'll show you how to implement the neural

network in in all that's, uh, like, like, you know,

a bit more involved than the functions we built from

scratch.

um, and then we have a couple of exercises in

the class.

Um, any questions, I'll just stay here a bit longer

if you want to ask any questions.

I see.

I see I see.

to find You I get a whole lot of time.

Yeah, um, so I still.

That's right.

That's right.

Uh, yeah, so the, the parameters are really these multiplications

that bring you from one to the other layer.

So like, so Ws and Bs, they bring you from

one layer to the next layer.

Before moving on to the next thing.

No, so like we set them exactly, we set them.

Imagine this neural network to arrive from like to go

from the beginning to the end of the network from

the forward path from the input to positive negative, neutral

sentiment that takes you to compute everything in that network.

That's called this forward path.

So I need to initialise all the slopes, all the

intercepts.

Once you have initialised.

I get a really crappy prediction, so I get a

really bad prediction, but I get a prediction and because

I get a prediction, I can then apply gradient descent

and then I update all of them at the same

time, and these predictions hopefully get successively better and better

exactly.

And I go through the entire network.

So with derivative with respect to the first intercept in

that layer, I would have to with respect to the.

First with respect to the second with respect and then

with respect to the parameters, so you would need to

do the chain depth to the network and more flexibility

to the function approximate to be able to fit more

nonlinear functions or more complex functions.

Finally, I guess my question would be in terms of

say we're doing very simple I don't know if that's

a good example, but these things are basically they are

they are covariates.

So imagine we have, um, we have, we have, um,

you know, before we would have a single note here.

Um, that would be our X1, 2345, and so on.

These are just depending on how you transpose it.

So they are usually the columns in your frame and

then the rows are your observations.

Yeah.

But like in like what you did with Ryan, each

of them could be a word in the DFM.

So if your DFM 200.

you would have 200 Xs going into the network.

I'm right, if you think of it in the in

the people transpose it, but they are the variables whether

your roles are the variables or your columns are the

variables.

They are the variables Xs are the variables.

Yeah.

Every node or network I mean.

Functions, so there are so many different functions, yeah, like

how, how does the process.

function and it's like so the functions, the activations, I

mean, I think there are many levels to that question.

So the activations we choose would be Sigal, it could

be uh like the real or whatever, and we initialise

the weights and the biases, the slopes and the intercepts,

and we define the activation functions.

Once we've said this, we've defined the overall function.

How, I mean, based on what do we find it

functions.

Yeah, so I mean we could find them, we try

different ways because like you probably what you're aiming at

is we could have many different ways that we do

this, how many neurons, how many layers, I mean so

many degrees of freedom and what we would do exactly

which activation function.

So there are heuristics of things that work pretty well.

I think the bigger picture answers there are many, many

ways to fit X on one, right?

So many ways because neural networks work pretty well because

also they work pretty well with our current architecture, the

matrix multiplication, the GPUs, and so on.

All right, so there, there's also luckily this universal function

approximate type theorem behind them that we can fit anything

if we only find it.

The problem is to find the way to the biases.

That's the problem, but they work reasonably well.

Um, and then, you know, how do we set them,

the activations we use ones that work really well, guided

by some principles.

For example, the sigmoid isn't a great activation because the

derivatives are close to zero.

We don't learn very much.

So we want activation functions, activation functions with larger derivatives

because then we learn that.

These kinds of things, but then it's, it's trying out

a lot of things and probably zillions of functions work

and we find one out of the space of functions

that's good enough for us, and the, the model itself

tries.

Lecture 9:

Maybe you know some black peppers, but I feel like

that's like a solid you have to really.

Yes.

My dad wears a black cat but hopefully, yeah, for

the older generation, but like, yeah.

Yeah, you have to earn the flat.

You got through.

Um You, you went to LSC.

OK.

do So cute.

Yeah, super, especially the black like the lady, the barber

jacket and the black caps, yes.

Um, Yeah they were they were gonna come.

And then, or no, they're gonna come with me back

and like.

OK.

And then It's like, well For some assignments and then

also like.

It's very sad, so like they had to put this,

um, it was like expected so.

Um, yes, and they didn't come and then they're like

maybe gonna come in April.

I was like, You probably be the worst month of

my life living hell, yeah, like I'm actually not gonna

see you.

Studying a lot, so like you can come, but I

can't guarantee how much time I'll have to raw it.

Um, like if you do come like.

Yeah, like so that you can buy my groceries like

it's just too much, um.

And yeah, I don't Who Yeah, I think I'm fine.

Are you guys discussing what happened with that kind of

we're talking about if we're when we're gonna see our

family again.

OK, OK, OK, well, that's happier.

That's happier, yeah, yeah, I just assumed because everybody's like

two weeks we're gonna see I'm like that's objective.

I think I think we'll all be around, we'll be

around.

I'm gonna be around.

I have no idea.

My lease ends in September.

I'm in no rush.

Are you guys thinking of doing that.

it's very weird.

I.

Yeah, I, I just feel like I need to like

my, it's still som like I need to continue assessing

the situation, yeah, like it's like so much changes.

Apply for the grad visa gets approved in like 4

days, so it's like you can yeah you can be

spontaneous about it.

There's another one that I was thinking about applying for

I I need to apply for it.

I was gonna do it to if you graduated from

a list of colleges it's a lot cheaper than.

Can you do both in succession and see what happens?

No, because the HPI the graduate, but HPI is like

I'm so old like it it expires because I graduated

undergrad so long.

Oh, I guess I should have liked that like I

totally missed out, yeah, we just did, um, so that

would expire.

So I'm like I think I have to do a

postgraduate though.

I just like don't go.

Yeah, it's kind of a, they're searching homes at the

border to see if you're like talking shit.

God forbid we have to see.

I think I don't know, but I don't my mom

reads the news, but sometimes.

But she was saying that it was like a French

guy couldn't.

He went to as well so to come again some

set some stuff up.

Yeah right.

Why that's so odd.

I'm just, I, but I think everyone's like, oh, like

the, the courts, that's.

What what are you gonna do about it there's no

I think it's great.

I mean, can anyone because that's like the accent is

very deceiving, um, so I don't really understand how any

of this works.

Like when when Trump does things like sign executive orders

to like make things like there's the fucking the legal

companies and no one stop him.

like to judges.

I know I don't have, but the problem is that

he just doesn't like the deportation yeah they just didn't.

That's the only one so far that they were like

you have to bring the workers back and the workers

were like no we're good.

A lot of people are quitting.

Yeah, a lot of people are making sure we're not

doing the same.

What is it going to do with the now like

a lot of people don't want to go back to

the USA because they're gonna do all this bullets.

My uncle and they were like, oh, you can stay.

He was on the list, but they replaced his direct

boss and somebody who helped writeject 2025, so he was

like, I'm gonna take that out.

Thanks so he was like I'm just gonna like.

I don't.

and he's like I mean like up in that position.

Or a scrap and like apparently like that.

The child and I'm like.

Why could you I'm so I only have like would

you ever think that that was a good idea for

like I'd rather be sure of your family.

Let me tell you, the young, but I have to

like.

Foundation because nobody wants to work.

Yeah Yeah Hey Oh What about?

I just like do this stuff.

Oh, you do it for like.

3 more years But Yeah, yeah, yeah, just a minute.

I mean, I'll, I'll have to stick him in, but

I'll try like I think they're likes to get along.

Cyril or maybe like they all look the same.

It's like a hybrid.

I worry.

Matthew's really worried that like he has on him.

What?

I I feel like I think, yeah, also the, yeah,

and I was like if you, if they really think

it's bad, like do you wanna work there like I

put it on my cos like if they ask you

like why you stop working on it she's like I

just knew that she frustrates me.

We came here, so it's supposed to start at 5

right.

But like it's just, yeah, I don't think why are

people doing do fucking movies because it's progress.

You ordered like.

Yeah maybe lucky that I got out, yeah.

OK, let's go.

So, um.

Today we're getting into this kind of yeah.

Neural network based language processing territory.

Um, and in this course, we have, uh, we have

kind of a good amount of time and we can

really go into this in detail and get some intuition

on, on how these things work.

And before I start with the slides.

I'd like you to imagine someone presented this kind of

quote unquote, you know, AI model here to you and

and the capabilities it has, right?

So obviously I won't show you anything that has the

capabilities of check GPT, but you will see that it's

really quite capable in some ways.

Um, and then basically this lecture, I'll see around trying

to understand how this works because it's simple enough to

understand relatively easily how this works and in particular, um,

that we can build it ourselves, OK.

Um, and I often think this way when you see

some kind of new, new tool in this space and

this kind of a high space or language, it seems

almost like magic and then um a good first step

is always to try to understand the the basics and

then they usually um uh at least seem seem less

obscure.

So same thing here and I hope that's kind of

a takeaway from this lecture.

So this model can actually look at At different terms,

um, and it can tell me similar terms to terms.

So now here I talk like some flying dinosaur because

it was just a niche term and even that level

of term, look at the other ones sensible other similar

terms, OK.

So that model can find similar, uh, similar, um.

Similar concepts to farms.

It can also tell us, give us answers to something

like this.

So I created these functions.

We're going to discuss these functions, but now just take

them as given.

So I'll have here, I have a bunch of physicists

and Mozart, and I'll see if it can tell me

who's not a physicist and it picks Mozart, OK.

Um, now you might say that's a bit too easy.

Let's make this a bit harder.

So what the next example is a bunch of physicists

and then Fleming who developed penicillin.

So they're all scientists now.

Is it still feasible?

And it's no problem, OK.

So, um, then who is not on the moon?

Armstrong, Aldrin, or Einstein, and now again making it harder,

saying that that Einstein, you know, is a physicist as

well, or an astronaut, um, still possible, um, who's not

a writer, Austin Armstrong Shakespeare, you kind of get the

idea.

OK, so it's pretty strong at this, uh, even when

differences are quite subtle.

Um, and, and I said, like this is at a

level that we can do this.

Um, it just, and, and the really interesting thing about

this, it's all relying on some geometry of vector space

that we're going to discuss.

Um, and that's really intuitive, not technical, and so on.

So like maybe most surprisingly, this thing can get analogies.

OK.

So I'll show you a couple of them, you can

try them out later.

Um, it doesn't work for everything, of course, these things

here are picked, but it works for a lot of

things.

So I say Austria Switzer what Italy is too.

Um, and it gives me a bunch of, uh, sort

of sensible results.

The top one would be lasagna.

OK.

Um, other analogies, Einstein is the scientist or Picasso is

too, and it gives me painter.

Now maybe these you find still reasonably easy analogies, but

we can, we can make this tougher.

So we can use this to get, um, you know,

um, to look at the adjectives.

Um, and tallest to taller, what's smartest to smarter.

So it also gets this, um, here fast is to

faster what good is to to make it one step

harder again.

Not good about that.

I am OK.

Adverbs, it also gets slow, slowly, careful, carefully.

Some to show you something, it messes up.

For example, tall tall is quick, timely, OK, still reasonable,

but like not what I was looking for.

OK, we can extend this, you know, swim, swim, walk,

and then get past tense.

Um, we can use this to get capitals France, Paris,

Peru, Lima.

Cameroon.

Um, we can even find, you know, England is Shakespeare

or Germany is to, we can find, um, writers this

way, be good, um, we can do this for Spain

to get Cervantes, you know, so like it works really

for a wide range.

So not to make this like this is reasonably simple,

so that should give me basketball, but now let's make

it harder yet again.

So David is to Goliath or small is is it

aware of like kind of the logic of David and

Goliath, um, and it's true, you know.

And then lastly, um, to, like, you know, kind of

one step yet is say A and Hector are the

enemies in the Iliad.

So can I transfer that to football, OK, something, something

even more kind of experimental and not perfectly, you know,

um, but it gives me like, uh, um, a lot

of football, OK, right.

Um, it doesn't give me or something like, you know,

like kind of, but it gives me a lot of

um.

Actually, I mean they lost against Valencia a lot of

times in the Euroleague, so I mean this this is

reasonably sensible.

Um, so yeah, quite, quite mad at this, uh, how

well this works.

Um, and now the full lecture we'll be trying to

build and first the theoretical understanding and then actually these

functions, the, the cool thing about them is this is

not from some library or some package, but I worked

to myself and we can like go through them and

really, you know, build this from scratch.

That's the, that's the plan for the lecture.

Do you have any, any questions?

OK, great.

And let's get started.

So that's where we are.

Um, we're now at this point of um of static

word embeddings.

Those were kind of the things powering what I just

showed you, um, and this will give us a great

basis to then do this introduction to large language models.

We built on the math review and on the neural

networks, um, and then we're kind of combine this all

to study the language models.

OK, so that was the demo I just showed you,

OK.

Um, and then that's the outline for today.

I'll quickly introduce the topic, then I'll show you two

very salient kind of algorithms to get these vectors, these

embeddings.

Then we discuss about, discuss the geometry in more detail,

show you some application, and importantly the biases that that

results from, from training these, and then we look at

coding examples, both building these functions with pre-trained embeddings, but

also training our own embeddings on textual data.

OK, so, so far, um, until last lecture, we, uh,

and even last lecture, we predicted based on the DFM

and the DFM is a bag of words, um, you

know, based approach.

Why?

Intuitively, because I could swap columns in the DFM and

it wouldn't really matter.

They were just, they are just variable to these variables

to these models, but I don't preserve the order of

sentences.

Even the topic models, they were just raw count based

on raw counts of words in the documents, so they

wouldn't take word order into account either.

Now if we take word order into account like with

neural networks iterating over sentences step by step, like, you

know, GPT would do other neural network architectures, and usually

these words or fractions of words are processed as so-called

embeds.

So what are embeddings?

They are numerical vectors, OK, and they are these numerical

vectors in the um in this machine learning kind of

terminology embeddings, um, are trying to measure the meaning of

a word.

OK, so you need some numerical representation of what the

word means, otherwise what I just showed you would be

uh not working at all.

And this is actually pretty close to this old notion

here.

Of, um, is quite, um, well defined by words that

are similar to it.

OK.

So if you think of a vector space, imagine that's

a vector space with three dimensions X1, X2, X3, then

we would want, uh, like what say I have two

words that are on similar topics, basketball and football.

I would want them in similar parts of the space.

That's the full logic.

OK.

And then other words that are.

Education, I don't know, degree or whatever, they should be

in other parts of the, of the space and uh

and hopefully close to each other if this works.

After training, yeah, after training tomorrow.

Good.

So, um, One thing, this has been an ongoing research

topic for a long time, a normal um probabilistic language

model that's actually also interestingly talked about this in a

bit more detail last lecture.

The paper was predicting the next word.

So this is not just, you know, JGPT coming up

with this in 2022, but this is an old objective

function.

Just the model wasn't very good at it, but that

that idea of predicting.

The next word has been around for a while.

This also gave you embeddings for words, but they weren't

very good and so did, you know, other methods that

will also look at some kind of it during the

talk when we look at the glove that builds on

old methods.

These ones here, they were from 13 and 14, and

they, you know, the wet embeddings are publicly available.

They were able to improve these embeddings dramatically.

So what is their main limitation if they work well

in many ways?

Their main limitation is that they are static or one,

they have lots of limitations in particular to the respect

to biases and so on that are ingrained in them,

but we'll look at this in more detail later.

So one kind of technical limitation is that they are.

So every word just has one vector, OK.

So the vector for inflation will be the same um

if it's in a sentence with monetary or if it's

in a sentence with grade, you know, but obviously it

will mean different things, um, and this vector, however, will

be some average for all the meanings of these words.

OK, so now you might say that's a pretty strong

assumption and that's true and modern models, they have context

specific invents.

So each, each embedding each has a different value depending

on which words are around it.

But then we cannot really work with them equally well

just in.

um, because they change all the time depending on the

sentence.

These modern models they actually create embeddings by tokens, these

are fractions of words, um, and, uh, we can thereby

not not work with them as kind of naturally as

with these, with these embeddings over um the unit of

the word.

Good.

So let's get some inclusion on this.

So say we trained on a word tobacco glove model,

um, and then we have these kind of, you know,

bank loan, football, basketball type of bettings, and then if

this were like, you know, after training, um, when I

train these, like say initially.

and then I trained them, then we should have um

football and basketball being closed in space and bank loan

being closed in space, you know, out of these four

words um and that's just what we, what we uh

what we thought about so far, um, that's the visual

intuition of this.

Do you have any questions about this?

OK, um, good.

So how do we get these?

So that's the intuition, um, that's how they would look

like in 3 dimensions, probably have higher dimensions because they're

more accurate, but how would we get them?

Um, so maybe that's the most salient of the algorithms,

that's the word to that one.

and the key thing here is they obtained these embeddings

from a method that was predicting, um, context or centre

words.

So imagine you take Wikipedia and you just change all

the documents, a bit like your language model we're talking

about next week, just, you know, looping through all the

words in the documents.

So here I Combine all the documents uh that are

say in Wikipedia.

Um, and that's, by the way, feasible.

I've trained the word vector model just to try this

out a while back on the computer class at LSE

unlike Wikipedia.

And actually even Wikipedia is like, I think back then

the Wikipedia had maybe like as it compressed with some

deletion, like, you know, some cleaning text file.

I don't think more than 15 gigabytes.

So just to give you an idea, and I mean

how humongous this is but text is is is a

very kind of, you know, it doesn't take much space

to store it.

Um, good.

So I took Wikipedia and then I concatenated all the

documents and then I could look through these documents, right?

And I could, for every, say I like my point

has no an apricot say here, you know, in this

example, and that's my target word.

And then I have a context around it.

So here it's a tablespoon of apricot jam and pinch.

So that's like a recipe.

OK.

So now tablespoon of jam and uh all appear in

the context of tea.

But also tea is at the centre of these words.

Right.

So we can either try to predict words that are

in the context, or we can take all these words

and try to predict the words at the centre.

Slightly confusing here is the centre and context both start

with C, so like here the target words say, um,

but there are two approaches how we could, for example,

um, Try to try to um use a prediction model

here, unlike you know the next word prediction that that

we would say use the GPT um here we want

to see uh broadly in what context the words appear

so we could predict from the context what the centre

word is it apricot or is it football or from

apricot the uh the um the words.

Context around it.

And these are really the two models that are in

work to pack.

OK.

So in that paper, they had these two models, um,

one is, you know, CIO and Skipgram.

CEO was exactly doing the target word prediction.

So I took the words in the context, predicted the

words in the middle, and the skipgram from the word

in the middle predicted the words in the context.

These are the two options.

So we will focus on the, on the, on the

skipgram and then like we will focus on a particular

way to get a solution to it, OK, because at

the time in particular, but even now they are computationally,

the basic problem is computationally quite heavy, so they had

a they had an algorithm to uh to have a

much simpler way to compute these synthetics.

If you want to look at the paper, that's all

in the two models, different algorithms.

We'll look at the um at the skip gramme and

we look at the uh negative sampling.

And the skipgram works pretty well for rare words and

that's why we're using it.

We're we're in particular interested in getting goods for for

white set of words.

Alright, so let's introduce this.

So I concatenate say my Wikipedia, OK.

And now Wikipedia is a sequence of training words, right,

from word one to word capital T, which is a

very large number.

So I can just like go to Wikipedia.

I have concatenated all the documents.

Um, and I'll say I start like at 5 or

so if I have a context window or 10 if

I have a window or 5 around it, um, and

We, uh, we start and to to get going, we

do something similar to the neural networks we've discussed in

the past.

We first randomly initiate the parameters.

So we would randomly initialise vectors and we have two

types of vectors, OK.

We have a vector representing a word when it's at

the centre, the Two, or we have a vector representing

it.

When it's around the centre, relatively simple.

So either you represent it when it's at the centre

or when it's around the centre.

We randomly initialise these vectors, so now we have 2

times the number of words, unique words in that vocabulary.

After some cleaning, you know, I might remove words that

are very rare, these things that I don't have a

word vector for a word that occurs twice.

Um, and now I have two vectors for words.

So the next thing we need to recap is the

soft max.

I remember the softmax has this notion of normalising outputs

such that they become probabilities.

Imagine I have this kind of output here like a

list or a vector of numbers of 1012, minus 5,

and 7, and now the soft max of this would

be the exponential of 10 divided by the exponential of

10 plus the exponential of 12 plus the exponential of

minus 5 plus the exponential of 7.

OK, and of the 5 that would give me E

to the -5.

And the same thing here again.

Yeah.

So, what is the probability of the -5 associated with

the -5, roughly, can you see this?

So what probability comes out of when I rip the

-5 into a soft knife?

I mean, what's each of the minus 5 for you?

Close to 0, yeah.

I mean, each of the each of the minus is

a number um smaller than 1.

All the E's in the in the um Nominated here,

you um you would divide it by a really large

number, so you divide a number a bit smaller than

one by a really large number, so that thing has

a probability close to zero.

but what you have is you have say a vector

or a list of real number outputs, this could be

5.7, and then um we uh um we can wrap

them into something that's like a probability.

And we'll need that here too.

Why?

Because we want to predict the probabilities of words being

around words.

That's it, OK.

Good and that's a slightly kind of looking objective function,

but let me walk you through it and we'll actually

solve this in a, in a very simple and intuitive

way.

Alright, so the intuit the objective function is this, and

by the way, we maximise this with gradient descent.

So we use the whole methodology that we discussed in

the previous week.

And why does that make sense?

Um, so recapping, we're here, uh, like we're training or

estimating the skipgra model.

We want to predict which words are around the world.

That's, that's our goal.

So we want to learn what factors that are good

at this.

OK.

Great.

So the probability.

It's my soft max and I want to, let's first

take it, like, you know, ignore the structure, say I

have one tool with a probability.

So given a word that position T.

I want to predict the probability of a context word,

OK?

And I do this for, I do this for words

like in the context of, you know, minus and, you

know, say 5 left to the word and 5 right

to the word.

And then I want to predict this probability here.

And I want to predict, make this probability very large,

OK, so I want to for the true context words,

I want to predict a large probability.

Why wrap the lock around it?

It comes from, you know, you had a like a

product of probabilities and you took the lock of the

whole thing, and it became a sum.

But intuitively, it's also, if I predict a small probability

here, it's close to minus infinity because the lock has

the shape here.

The logarithm goes to one.

And then towards minus infinity, um, uh, so like a

small predicted probability for a true context word will be

pushing down my objective function a lot will be going

to minus infinity, but I want to maximise this, right?

So for me, in order to maximise this, I want

to maximise this probability.

So then how do we do this?

Well, um, I structure this probability as a softmax, and

now I have these word vectors and now we, you

know, it's a bit annoying with the, you know, the

sub-indices here, but it's very intuitive.

So the vector V is of the the current representation

for the word at position T.

I've just initialised it randomly, so I have a vector

for this.

That's the vector representation for the word that position T.

And I take the dot product, the inner product, um,

with, uh, with the vector that um it could possibly

be in the context of, so in the context of

it.

So say that's Apricot and that could be baseball, OK.

Um, and now down here, it's the vector of apricot,

dotted with every single vector in the data set.

And that's why it's computationally intense.

OK.

So here I would do the vector of apricott, the

vector of every single data set, um, you know, the

vector of every single word in the data set.

Um, so to compute this takes a while.

OK.

Um, and that's why they opted for another, another algorithm

at the time.

Good.

So what is the dot product, product between two vectors?

We revised this last time, right?

So imagine I take these two vectors, 123, and then

0 to 5 or something like this.

So the dot product between the 0 times 12 times

2.

And then um 3 times 5, which is 19.

OK.

So that's the top right between two vectors.

So I have a real number coming out of this,

could be negative, it could be positive.

This is not a probability, so I need to wrap

this by the same logic into a soft max to

get that probability here, OK.

Um, and I have the vector bedding for the word

at position T product with anything that could be around

it, that will give me the probability of word position

T, given that any word is in like it's in

its context.

So for, um, for baseball, I take the word vector

for baseball, the word vector for apricot, or the word

vector dotted with apricot.

I have a number.

Um, if my model works, that number should be low,

ideally, right?

If I have jam, then I got the vector for

jam with the vector for apricot over all the vectors

again, um, and then that number should, should ideally be

high if my training works.

Um, my computer crashed.

Let's see whether I can recover this.

OK.

Go.

OK.

So that's, that's what, that's the objective function.

We want to maximise the probability of the words that

are truly in the context of target words.

That's our goal.

And then what do we do?

We want to, to get these vectors.

We want to get the U and the V vectors.

A question to you, why do we have two vectors?

Why do we have U vectors and the vectors?

What's what's the deal with this?

I was on the and like the previous slide.

Mhm.

Yeah, exactly and it just works better, yeah.

So like this whole thing, if we give the model

a bit more flexibility and give it a like a

different vector for works in the context, if it's at

the centre, it just works a bit better.

How could we proceed at the end depends a bit

on the algorithm.

We could average over the vectors for each word, or

we could pick only the target vector it's at the

centres that are gradient, that means that every context vector

and every like centred vector for every word in Wikipedia

is.

So what you're learning here.

It's a great point.

So what you're learning here are two massive tables.

So that's the V table and that's the U table.

And each of them has Say like this.

You know, like, let's, let's use them.

Let's, let's, uh, we have it and then we transpose

later.

So say these are the embeddings.

So this, this is my uh V4 1,42, and so

on, and this could be, you know, the embedding for

hello, embedding for world.

And then there's another you table where again I have

an embedding for hello.

Betting for world, betting for LSE, whatever, right?

So I have these role factors.

Um, and this is the embedding for when a word

is at the centre, and this is the embedding for

a word when it's at its context.

And this is just a lookup.

I have them, I initialise these matrices randomly.

Say I have word vectors of dimensions, say 300, then

if that's, for example, 300, then that matrix would be,

um, say I have a million words in my vocabulary,

so it would be 1 million times 300.

That's the table, OK.

So each word, 1 million words, 1 million rows, would

have a 300 dimensional vector embedding.

And how do I pick the 300?

That's just something that works well, but it's computationally a

bit more intense than if I take 50 or 100.

The ones I showed you on the computer were 100

dimensional.

OK.

So that's just how much information do you want to

soak up.

Yeah, so like there's no DFM here.

Like the DFM would destroy the structure like the sequence

of words.

So we can't have a DFM.

How we get that training is by looping through all

of the.

Documents and try to predict what is around words.

So it's inherently something that at least once you train

while training, it takes the order into account.

So how do you train it?

We, we're not even there yet, but we can say

what we want to maximise and what we would do

is we would initialise these two matrices with random numbers.

At the point where we have initialised them with random

numbers.

We can compute any of these probabilities, right?

because we take the dot product between these vectors and

we develop by all the dot products which we can

compute, um, and then we, we have that for every,

every word that possibly around every word.

So I go through my entire context and like I

go through all the words that are really around words

and try to maximise the overall probability of a query.

Does it make sense?

And DFM we don't need to count words here, so

like this is a different approach.

In the end, by the way, if we have these

embeddings and you have a sentence, and then, you know,

your sentences I'll say, and I give you two embeddings,

then of course you can shuffle and so on.

So the embeddings themselves, they are just points in space,

but how we get them, how we train a model

to give us sensible embeddings, that model needs to look

at position of words.

Yeah.

Uppercase T would be like the last position of the

entire Wikipedia corpus.

The, yes, I mean, not quite.

I mean this is this is a bit like, you

know, um uh like the formulation because you need to

keep the context, right?

So say it's the last minus whatever context you need.

OK, but then when you go through the summation of

these, you would see the same word more than once,

right?

Yes, yes, um, you will have that, but like it's

a good point, but you know, the same word can

have different words in its context.

So what you want to have is you want to

have an average over everything, right?

So because that's like going to the limitation of this

as well, because that word embedding will give you an

average of the context that keeps.

It won't give you, you know, like a context-specific embedding

when it's next to a specific word, but it will

give you an average.

So if, if the same phrase like in the same

context happens many times.

It will lean towards this exactly, exactly.

So if inflation is more next to monetary than it

is next to rate, it will also be in the

final vector space or it should be closer to this.

Yeah, uh-huh.

Have you also had a question or no.

All right, cool.

Any other questions?

Yeah, these are great points.

Um, OK.

And the last thing like computation wouldn't you have another

table with the inner products?

Yeah, so I mean we could, you mean to store

it because to keep all of that like Txt or

because we would store this, then we don't have to

recomput it at every step.

That's your point, right?

That's right.

But still, the problem is if we learn them by

a gradient descent, after every update, we have to do

that in a product between these gigantic tables.

And that's why originally they took a different.

Algorithm, OK, which are in in a few slides.

Now, the first thing, before I show you the algorithm

is why does this work?

So when I first saw this, I saw these examples

of, you know, schnitzel and lasagna, and so on.

And then I was like, OK, that's the objective function.

Why, why, you know, how could a word be similar

to a word with that with that objective function?

Because we're just trying to predict words around words.

But the key thing for understanding this and maybe it's

kind of obvious to you, um, but it wasn't to

me at the time, is to think of what the

do product means.

Overall, the product is already some measure of similarity because

if all the elements are positive, I'll have a high

number.

If all the elements say I'm negative, I have a

high number, but if one vector with negative and one

vector with positive, uh, you know, elements, it won't be

such a high number, OK.

So it will be, it will be, you know, some

positive, some negative, so some will net, uh, there will

be netting out of positive and negative terms.

So people think overall of the product as a measure

of similarity and actually for us having, you know, studied

cosine similarity a bit more last week and so on,

once we normalise a vector by its length and just

to define again what its length.

So say I have this vector here, it's 02, and

5 is my vector, OK.

Can you tell me how I compute the length of

that vector?

Anyone you square the terms and you sum them so

you basically go 0 squared plus 2 squared plus 5

squad.

I guess there are different definitions for length, but that's

right, that's the R2.

So that's X.

And then That definition here, specific, you know, definition, um,

is doing this here and by the way, this is

a generalisation of the Pythagorean theorem.

That's what this is because in two dimensions that would

be exactly, um, you know, think of, think of a

two piece space.

I have a vector here.

Let's make this a bit larger.

So say I have a 2D space and I have

a vector here.

Well, what's the length of the vector?

If you think of kind of high school mathematics, it's

A2, B squared, and the, and the square root of

this, right?

So that's, and then we're just generalising this to higher

dimensions.

That's where this comes, that comes from.

OK.

So what this is in the end is a number,

OK?

Um, and that here is like, what is this square

29.

Um, so that's the square 29.

So we divide every position.

So what would be X divided by um by the

length of X would be um every position 0 divided

by square root 29, um, 2 divided by square root

29.

5 divided by square root 29, etc.

OK.

So that would be the the the vector.

Now, we can just do this and then store our

updated vectors on the table, right?

We've now normalised them, but now the cool thing is

if we take do products between vectors, they are just

immediately cosine similarities.

So that also shows you why is the dot product

broadly measure of similarity, because we only have to normalise

the length of vectors.

It's, it's the definition of similarity.

All right, so now if that's the case, Then do

you see now why this algorithm works?

When am I predicting a truly high probability of a

word appearing in the context of a target word?

Maybe someone else, but like anyone else like an idea.

So when do I predict a truly high probability of

that's my goal.

My goal is to predict truly high probabilities or to

predict high probabilities when words truly appear in the context

of words.

That's my whole objective here.

When do I achieve this?

I initialise my vectors U and V.

I got this now.

And when do I predict high probabilities?

What do I, what, what does need to be between,

or like, you know, what does this, uh, what do

these two vectors need to satisfy?

Just mechanically looking at this function.

Forget this, that's just a normalising constant.

Think of this as 10,000 or whatever, uh, you know,

um, and, uh, and then how do I get it,

how do I get a high probability?

Mhm.

That's right.

How do I get a high probability.

That's the, the sole, you know, way to get a

high probability.

How do I get like when the top is decide,

what does that mean?

The vectors are similar.

That's it.

That that's what I'm what I'm maximising for here.

I'm maximising for vector similarity.

That's why predicting things around the world with that kind

of structure will give me vectors um that are more

similar if they are truly next to each other if

they associated words are truly next to each other.

Does that make sense for everyone or you know, shall

I repeat this or any questions about this?

Mhm.

Uh, I, it's difficult even to, to, to make the

question, uh, because the similarity measures the distance between the

angles of the vector, the angle, sorry, the answer, but

it doesn't matter what's the distance between the end of

the vector, right.

Don't you do something there.

Yeah, so I mean, when I, you know, you might

think the same.

I thought like, why does everyone in natural interest in

use this cosine seminarity?

What's the deal?

Why don't they just go Euclidean distance or something because

all of these, you know, we have these nice.

And the nice thing is if we go back to

the DF or to think about this in the DFM

world, which is counting words, OK.

So we have, we have only a corpus with two

possible words and that's hello and world.

Yeah And we have, we have documents that either contain

hell ones and world ones, so they sit here.

Because roles in our DFM are vectors of of documents

now of documents, not words.

Um, and now we have another document that contains hello,

hello world word.

So just each word twice.

So that vector will be here.

Right.

So if I take Euclidean distance, I will think these

are very different.

But if I take the angle between them, not so

much.

Now, with twist, if I normalise them to a unit

length, like they live all on the circle now.

And then it's kind of the same, whether I take

Eli distance or course on similarity.

But cos and similarity is robust to these differences in

like scaling of factors.

And that's why people generally use it.

But you, like, you know, what works best for your

specific application if you're building something in a firm or

research, I'll just try different things.

But usually the way to start here is cause on

similarity.

OK, so we want to predict, we want, we want

to predict a high probability when a word is really

around the target world.

How do we achieve this high do product?

So we need to change our vectors such that they

give high do products.

How do we change the gradient descent.

Um, that's, that's the deal.

And that's like, you know, how we did this, how

we discussed last lecture how to train models, um.

So now let's try to think this through.

Um, tiny example, we, we only have 10,000 words.

And then after optimisation, uh, we would have um 20,000

word vectors, one for each word when it's like these

two matrices, where it could be at the centre, where

it could be.

Good.

We can take the average to obtain final word vectors.

Now, like the challenge here is that after each update

to UNV, we would have to recomute.

That didn't like that number here nominated the normalising constant,

right?

And that is at least costly, you know, you know,

whether the computer currently available, it's, it's getting feasible, but

it's for certain data sets, but it's getting, it's very

costly.

So now these guys then thought of a really simple

way to um to approximate this, um, and they use

the negative sampling algorithm and just to give you an

outline, um, because it's so intuitive and it uses the

same, you know, product type logic that we looked at

before.

So create a supervised learning problem as the target, right,

uh, of, of this and a really simple one.

So sample one word at a position T, I, OK,

right, we sample something from Wikipedia, and then we get

a bunch of words from its context, and we set

what the context needs, maybe 10 or 5 words.

That's what the context window is.

And you'll see that's a hyperparametter in these models.

So now we take words that are around that word.

But these are true words, true context words, and now

we also sample um negative cases, meaning nonsensical context words.

OK.

And you see we create data sets like this here.

We have, we picked the word in the and then

we have two words around it, and then we have

nonsensical words around it that didn't really appear.

And now we get sample words in Wikipedia football.

We have some true context words and some negative examples

that were never really around.

OK.

We do this for lots of different random words.

At some point we have a gigantic data set.

With, uh, with, um, the word here and then, uh,

like, you know, the, the, the tired word and then

the true context words and um and false context words,

so positive or negative examples.

And now we can run.

What do you think we can run some variant of

what we want to predict, is, is, you know, if

it were truly in the context?

Yes or no?

What variants like I think we will run of what

model?

Yes, you would use a variant of logistic regression with,

you know, the kind of conceptual twist that you need

to wrap your head around is that there's not really

an X here.

So, you know, when we had the logistic regression, we

were thinking of X and beta, and we were saying

beta is the parameter I want to estimate.

X is my data.

OK, so we went to get beta with gradient descent,

and here it's a little bit odd first, but But

the, it's just a logistic function again, what we discussed

last time, the sigmoid function.

So you have 1/1 + E to the minus, and

now it's not to the minus of X beta, like

we had in our example, but it's to the minus

of the dot product, surprise, surprise of these two vectors.

OK?

So I have, um, vector of the word that position

T apricot, and then the vector of like a word

around it.

OK.

And I want to predict a high probability.

If that word around it is true, um, and I

want to predict a low probability if it's, um, if

it's, uh, if it's not truly in the context.

And what do I do?

I just do learn both of them via gradient descent.

So this is my objective function.

I just learned, um, U and V by a gradient

descent and my labels from that data, um in that

supervised learning problem are are the ones and the zeros

that I know.

Is it truly in the context?

Is it not truly in the context?

So I initialise both both word vectors and these matrices.

I know which combination of word vectors is a 1

and which is a zero.

I know that from my sampling algorithm.

Um, for each of them, I can, I can compute

a predictive probability, I compute a logistic loss, what we

did last time, the logistic loss.

I do gradingscent on that loss, I get two vectors

out.

The the only difference to last time that I don't

have X and beta, but I just have to work

vectors.

I need to be able to compute the output of

that function.

Well, how did I do this last week?

I did this with the X I had from the

data set, and with the beta I initialised.

How can I compute the output of that function?

I need to know you and V.

So well, I initialise these matrices randomly, so I can,

I can compute that.

And then I can run right into this.

Um, any questions?

Is it like a different optimisation problem for each word?

Like for each word, you have the true label and

a sample of 5 or 10 negative samples.

And you solve an optimisation problem for that world, the

world, and then you move on to, it's like one

gigantic data set and that data set has pairs of

words and whether these pairs of words are 1 or

0.

The, the, the data set looks a bit like, you

know, think like this.

So my data set is, that's my Y column, OK.

Yeah, and my Y column is 1 or 0 or

something like this, OK.

And then my column here is which words, so kind

of, you know, uh.

If you think of it X1 and X2, but these

are really the word vectors.

So really say um U and V and then these

are the associated word vectors with this, with this example.

That's my training data set.

But which are the parameters and the variables so like

with my data set.

Basically, I have my data set looks like this.

I, I have word factors, um, and then I know,

do they truly co-occur or not in the same context,

right?

That's like that I know.

I just, you know, sample.

I sampled Apricot and looked what was truly around it.

So I have positive and negative examples.

And then for each of them, I have the U

and the V and I just initialise them randomly.

Yeah, when I initialise them randomly, that's the only thing

that I met wrapping one's head around.

It's not, it's not like having X and beta, but

here I have sort of two betas, OK.

So I initialise them randomly, um, and then with each

of them, I can compute the implied probability, what the

logistic function would tell me.

Is it 0 or is it 1?

Is it 0.25 78, or is it 0.93 or something

like this?

And then I'm just tweaking these UNV vectors to grade

in descent to predict the one when it's truly one

in order to minimise the loss function from last lecture

that we did in the review.

No, so I mean, imagine like I, I, you know,

let me, you know, I concatenate all of Wikipedia.

I just randomly pick a word, and then I just

move left and right of it, and I know what's

truly around it.

How do I get negative examples?

I randomly pick other words.

And yet, like, you know, occasionally they might truly be

in the context of, of why by extreme chance, but

then I can maybe annually review them just keep them

there.

But yeah, my positive examples are what is really around

the world, around football, and my negative examples are just

random words from Wikipedia.

Then I have my training data set to whereent on

it, um, and that's how they, with that algorithm got

to work like this.

And then I could average them and, and then I

have the vectors.

OK, um, when you talk about this is like back

and maybe I missed it, but when we were talking

about soft back out for like the final context it's

like the, the words in the context, do we just

get a probability for every word in the corpus and

then we just like rank the highest probabilities, or do

we like output?

You mean here?

Yeah, like all the way.

Back, I think even, yeah, for a skipgram model because

it outputs the words that you think would be surrounding.

So, OK, this is basically the skip model is what

we're looking at.

We're looking at, we are looking at this problem of

predicting contexts.

That's our objective.

This is how we would state that objective and then

how we de facto solve it is with the with

the negative sampling algorithm.

You can show that you get a solution to that

objective with it.

I think my question was more like how many context

words do you predict?

Yeah, yeah.

So, um, like, uh, first of all, the context window

this M is a hyper parameter we said commonly it's

set to 5 or 10, for example.

Um, and then here this is just to go through,

you know, every word in this corpus and then go

5 left to it and 5 right to it in

terms of words.

I know that these are truly occurring like words, truly

occurring in the context of a given word.

So I want to maximise the predictive probability of that.

Um, and that probability, I'll, I'll basically, you know, define

through the software functions as it's normalised between 0 and

1.

And how do I compute the softmax I have with

my initialised tables a word embedding for every time.

In in every context where I compute it for a

case of two words and then I just normalise it

to every single possible dot product of that type with

every single word that could be in this context.

So that's a huge normalising constant with the only purpose

to normalise these things between 0 and 1.

And then I want to tweak the vectors such that

my predictive probability gets high for what is truly co-occurring.

So if my context is 5 words, then ultimately like

at the very end of this, I put in Afrikaan

I should get 5 words that come out as like

being around the context of Africa.

I see what you mean.

So it's not like so this model doesn't tell you,

you know, it's not mapping words into like giving you

a number 5 or 10 or something like this, but

I'm just saying what still is my goal to predict.

So by giving it so thinking of the negative sampling

algorithm, how do I build that data set here?

I need positive examples of, you know, the word here

and then what's in this context, and I need negative

examples of what's not in this context.

How do I even define what's a positive and what's

a negative example?

I need to first define the context length.

So once I've defined the context lengths to be 5

or 10, then I can build that data set.

So once I have defined the context length to be

5 or 10, I can actually in a meaningful way

compute that objective function, because I only want to produce

a high probability for words that are around the word,

um with an area of 5 to 10 words.

Yeah.

And that's just for us then in, in the end,

I'm not predicting that area.

That's only like, like that's, that's the amount of words

around the world about I'm much caring or about which

I'm still carrying, basically.

Um, and then, uh, here I am just trying to

change these vectors such that I'm predicting high probabilities where

they truly were, uh, in context.

And it turns out by the logic on the next

slide, because this is essentially similarities, uh, we then produce

word vectors that have all these quite amazing properties.

But actually when people were, they got these word vectors

out and they built this for a reason, right?

So they wanted vectors that were in similar locations if

they were in similar context.

That's easy to see from this, but that they actually

had this geometry that you could do these analogies and

so on, that was a lot less obvious.

Yeah, OK.

I just.

Could you just repeat what each of the Vs and

E's are are vectors representing words in that vector has

no meaning.

They are just a point in space.

So what are the like in the dimensions.

If I have 3 dimensions X1, 23, it's a number

in.

OK.

Um, the meaning, like the meaning of these words is

that I want words that, you know, have similar meaning

in language to be close in that space.

But like it's not that the that the numbers are

word counts or something like this.

My example with the sna was for documents.

So basically if the first two lines have the exact

same.

Numbers and on the same, yeah, exactly, unless they have

the same word would be undesirable and the dimensions of

this should be the same number as the words right

so I have the ass as I have unique words

after some feeling, and I have columns as many columns

as I have dimensions and those I said that's set

to 300 if you download data.

OK, great, um.

Maybe like maybe let's take a break.

I think this might might have been a lot, so

that's uh it's.

Starting at 50.

Ah Quickly.

OK.

And pay attention, but um.

OK I go.

So you wanna um.

I.

Yeah Oh that's nice.

You you you it's like.

me because like he has a say in this too,

um, but also he said so many times I felt

like I had to.

I was really nervous.

I'm like, did I agree to a second date a

second date.

I have to because he's insecure.

And so that just.

I feel like his willingness to go on a second

date and very little to do with me.

Oh.

Like.

They like its not like.

Yeah you just.

yeah.

I like a job interview that's supposed to be about

like.

like if it's like you.

That's more so than the, yeah, exactly, that's like a,

this is like a 50/50 yeah and so it just

felt like he like I could have been anyone who

they could be like.

No.

But I can be I think you could do it.

Oh, that's just that's fine.

I think so yeah.

It's like.

No It's so what what I think is is that

thought.

Yes.

You know, We have um we have basically solid London.

OK.

You He said he had a kind he did yeah.

And conversation was good because.

repeatedly actually it was like a little.

I don't like the first one was like I had

like hot chocolate.

And I was like excuse, no, he didn't like he

wasn't like he's like on me and I was like

she loves, he's like like you were trying to transfer

and he didn't test it, um, little, yeah, and then

he like doing similar things and like I absolutely could

have said.

Like I was like I like felt like I had

to, but it's like like he was like oh I'll

have to kissing style, and I was like, well, I

both agree that was awkward, but also like way, way

to reinforce that.

So I was like.

Mhm.

Oh.

Yeah, I like Like There's not anything behind it.

And the name.

Yeah, that was, that was wrong.

No.

Yeah I will say just like um I don't know

honestly if he was just like going with it to

see how far I would go, we got pretty far

into a story of you robbing a bank for tuition

money.

Before he's like I have to know, yeah, he was

like, and he was like, come closer so like no

one overhes and I was like, are you just like

trying to get closer because there have been like a

couple of times where he tried to like get me

to come sit closer or whatever um but then I

was like, I'm not.

Um, but I was like, there is no, like I'm

not, I'm not coming up the story of how I

robbed this bank.

I like didn't, I don't know if he can get

started with, uh.

A job that way I know something about going to.

I like which is funny, really funny just of all

the words exactly massive and you know how it unfolds

into you, but it's just it's all.

I'll say it's a great point.

I, I um, but it's always the combination of just

to function with respect to every single parameter you just

say that's why all the practise one huge factor.

I like bluffing I like fluffing.

OK, this isn't kind of about.

This isn't fun anymore.

I need to like come up with the story of

a bank.

If a defensive because like you got your Jewish mother

and the pastor be like she's gonna be a defensive.

Uh, or you're gonna have to pull over or whatever,

but as I'm saying he's like, well, I guess there's

that stereotype but he's like money, and I was like,

that was over the line.

He was like, you robbed a bank and you're saying

that was over the line.

Why you rob a bank?

No, that's that's.

Is it not Yeah.

Sounds weird and it was I like didn't like I

like didn't entirely hear it, but he was like, always

in the states I feel about that.

I was like, I actually don't you thought it plausible.

I said there's a stereotype, and I was like, I'm

not, I don't know the connotation of your statement.

I'm like OK, I'm glad I'm not crazy.

Yeah, sometimes I like things slowly.

I mean, I did like I did like fully call

it out.

Rely, but yeah.

Yeah.

um yeah, that's not it.

Yeah That's a crazy Yeah, I didn't, I don't admittedly.

Exactly what he said.

There was something about stereotypes and Jewish.

But like I didn't hear the exact sentence.

you Then I ate, or at least I like there

was a line out my breast.

We're not seeing.

people don't believe me and I'm like I'm like I

thought I thought it was like close enough to but

like I like it because I don't like try to

make things believable when I say things like that like,

oh yeah, I had like.

Sponsorships and that's when people are like, no you did

not.

We're not a corporate sponsored bowler you know it usually

doesn't get it like I mean this is just that

they gave us.

It was the.

Um, you should do we all get turtles.

practise, practise, joint.

You know you're supposed to practise for yeah like.

have custody.

and we have like an extended.

I would like go through like the process like.

Now but like he had cut the red string on

the turtle and then you'd like, but then at the

end of it he was like, so are we adopting

the turtles or yeah, but then if you like had

a turtle, I would have been like.

That yeah.

I think like.

Yeah, Jewish stereotypes like.

Oh yeah.

Um, any residual questions?

That's OK, so what's the remainder of the lecture?

We're still um in the quest of getting word embeddings.

Um, the next method we look at, but only briefly

to give you an idea of that, this is not

the only way to get embeddings that work very well

as glove.

Then we study the geometry.

Why does this work that we saw in the initial,

you know, demonstration, these analogies and so on in particular.

The closeness of words is more obvious.

And then we look at a few applications and and

discuss biases and we go into coding and try to

build these functions.

Um, OK, so next, this is the next way to

get sensible word embeddings, um, you know, or well working

word embeddings.

Um, good, and that's, um, that's called, um, uh, global

vectors for representations.

OK, cool.

So.

Um, people ask me what are these thetas and the

objective functions because our vectors are U and V.

That's just combining all the vectors into one huge vector

theta for ease of notation and then doing gradient descent

over theta, the usual kind of notational trick from, from

these networks also from the previous lecture just to make

me not having tracked many, many parameters.

Think of all the parameters that are maximising over wrapped

into theta, um, a very long vector of all these,

and then you can, you know, after you, you unfold

them into these, these other matrices.

OK, good.

So I want to show you one word, you know,

one kind of very obvious way to get worth vettings.

It just doesn't work very well, yeah, but like some

super obvious ways.

What would be to build the core current table.

um, OK, and say, um, this is taken from, uh,

from a, from a course, say there's this, uh, this,

um, this sentence I like deep learning, um, and then

I like NFP and I enjoy flying and then they

build this core current matrix.

Let's make this simple.

Let's say um.

I say London School of Economics or something.

And if I check all the words here, London.

And then the school, um, uh, the words say, um,

say off and school.

Here and say my context window is only one, right,

which usually I wouldn't pick.

I have London here, I have often in school here.

Then if that's my corpus, only one document saying London

School of Economics, then London, um, and off they don't

co-occur, and London and school, they co-occur once, OK?

That means within the window of one, they co-occur once.

So, in this example here, they also look at the

window of one and see how often these terms are

next to each other.

And then they track and what will result, uh, like

track this and account, uh, the score occurrences and create

the symmetric, um, the symmetric table.

OK.

Um, yeah, and, and then I have this basically, a

gigantic table with every single word in my corpus here.

Every single word to my corpus here, and then how

often they're next to each other in a context window

of 1 or 5 or 10, usually 5 or 10,

right?

So really, really simple.

I'm just counting co-occurrences, table with all the words in

my corpus, all the words in my corpus, huge table,

and then I'm counting how many times they co-occur.

Um, OK, so who in this kind of, who of

you knows things like principal component analysis?

Have you heard of this, like this dimension reduction?

Um, so I just it doesn't work very well, but

just to show you how simply we could get word

embeddings now, we would simply push down the dimension of

the columns and then we would have all the words

here.

We would push the dimension of the columns down to

say 100 or 300 and then we have words.

Again, given from the information of how the court occurred,

and it is really simple and brute force way.

So we built that table, we pushed on the dimension

to 300 or something, and then these here are still

the words.

These are now combinations of the accounts of corecurrences, and

we have vectors.

Turns out it doesn't work very well.

But what, and that's kind of the old methods and

what love then did.

If they combine this with kind of newer approaches around

gradient descent and defining some objective functions.

Again, my theta are all the vectors and again I

have vectors U and V, OK.

Now what is XIJ?

That is just the cell and the co-currence matrix, as

simple as this.

It's just how many times do these two words I

and J co-occur.

I have a sum over every single possible word combination,

right?

How they could co-occur.

This is how many times did they co-occur?

Why do I take the lock to, to, you know,

squash things a bit and have extreme coreoccurrences like, you

know, the and certain words or something like this, not

completely dominate the objective function.

So I, I take the lock and it brings everything

closer together because, um, um, because of its properties.

So I take the lock.

And then I basically subtract that from my prediction of

the co-occurrence.

That's this part here.

So I'm trying to predict how many times these two

words co-occur and how do I do this?

I do this with an inner product of, you know,

surprise two vectors.

What is it doing?

It's getting similarity.

If it's high, if the co-occurrence is high, I want

this in a product to be high and that's in

a product of two vectors.

And then these are just intercepts to make the optimisation

a bit more flexible and I throw them out afterwards.

So these are just intercepts.

So this then is a mean square error loss, like

in the linear regression that we looked at, like the

mean square error, and I'm trying to minimise the square

distance between my predicted cocurrence and the lock of the

actual core occurrence.

Um, and I square this to have negatives and positive

net, uh, like, you know, um, I become positive square

distances.

So lastly, what is this F function then?

This F function is a weighting function.

Um, that, that in particular does two things.

One, it caps the, like, you know, the, the importance

because the objective function is just the sum overall every

single word.

So it just scales down the importance a little bit

of words that co-occur all the time.

Because otherwise, that what would happen if you just did

PCL a matrix.

Some of these core occurrences are so frequent, they just

dominate everything.

So this matrix is like this function.

scales down some of the points in the, in the

optimisation, some of the word combinations that occur all the

time.

So it's for, you know, two words that occur all

the time, um, they might have a co-occurrence of 40,000

in a corpus or something like this, and the scaling

function makes this like a relatively small weight.

Um, and then lastly, what it does, it, whenever a

words, words don't occur, which will free Frequently happen, right?

Because two words will never be in one.

If I list all the words on the rows, all

the words on the columns, um, certain words will, there

will be lots of zeros in that matrix.

So words will not co-occur.

The lock of zero is not defined because the lock

goes towards like minus infinity when it goes towards zero.

So I, I had this heck that actually, whenever this

thing is zero, I just weight the whole thing with

0.

Alright, and what do I do?

I initialise UNV.

I know for a given U and V can compute

the value of the objective function.

I can take the derivative just in our examples with

linear regression and so on, and I do great in

descent on this.

I try to minimise the objective of grading dissent.

In the end, I average over the UMV um to

have my, uh, to have my finals.

That's love.

I mean, we won't go into all detail.

We won't build this optimisation algorithm.

I, I hope that through the methods we uses and,

and, you know, discussing this function, the intuition at least

is quite clear on this.

And like, why does this again give me a sense

of the word vettings?

Because I'm doing nothing else but trying to predict with

the dot product with the similarity again between vendors, the

coreccurrence of them.

That's, that's why this should work.

And then I'm doing some hacks by scaling down certain

things that co-occur all the time and get a better

optimise.

And in the end we have pretty good works.

But Yeah, could you comment on what's the main difference

between this and where to work?

Yeah, I mean, it's coming via like, I mean, so

it has that same notion of how the words occur

in context, but it comes from a very different angle.

It builds up this table and tries to predict elements

in that table, whereas the other thing is just iterating

over the corpus and trying to predict words around words.

But the interesting thing is it gives you super similar,

I mean really powerful words.

But it's, it's, and why?

Because it's both both leveraging this idea of predicting core

occurrences in windows.

He's predicting cells in that table and the other thing

is predicting it a bit more directly, um, but they,

they both, they both work by the same mechanic.

Yeah.

And then, but in gloves, the uses and Bs are

still like context.

A word in context and then so they, they are

um also in love.

I have these two tables which initialised for each word,

um, here they are rows and so I guess what

I guess I see what you mean they refer here

to rows and columns.

So yeah.

OK, um, good.

So now I have these, I have these words ready.

Um, and I want to, um, I want to visualise

them, OK, um.

And How do I visualise them in the higher dimensional

space?

I can't really, so I like, you know, I look

at lower dimensional spaces and then I use this dimension

reduction.

Um, we won't cover the details here, so, you know,

this is not.

Uh, the topic of this course, but we need to

bring this like these 300 dimensional embeddings down to two

dimensions to look at them and get some intuition more

than 3 dimensions.

And the main ways to do this or or ways

that are often used is either PCA principle component analysis,

uh, you know, if you want to.

more about it during the last lecture of 474.

Um, this is nonlinear way to do this via what

is called TSE.

Um, what is this doing?

It's, um, it's preserving clusters that occurred in the high

dimensional space.

So when you see these TE plots, um, and you,

you visualise things in two dimensions, you often have some

kind of clustering in these two dimensions.

And with PCA, it might look a bit more like

this.

Yeah.

And the TC tries to in the lower dimensional space

because it's for visualisation, tries to push things that were

clustered in the higher dimensional space.

So you have these clouds in the higher dimensional space,

tries to push them away also in lower dimensional space

to have better, um, better visualisation.

Um, but by that it's distorting things, um, matter, you

know, in terms of their numbers, but it gives us

a nicer visualisation.

There are other things like UA that do very similar

stuff to TC they faster.

So there are a nice illustration of this, um.

That they run on tensor flow.

Um, Tensorflow's website, so.

Here, like these are all the words and benefits, OK.

So like all of them are, uh, that's the space

and then like, you know, you can zoom into this

and so on.

You can can have a look at this.

Kind of it's a nice visualisation.

You can um you can, you know, you can dimensionally

use it with these different algorithms.

It just takes a while to compute.

So like the website is pretty slowly, they will put

only limited compute resources behind the website.

But um yeah, so there's a nice visualisation.

Um, good.

Um, so on the similarities, like, you know, this should

now be relatively easy.

Um, we, uh, we use the, uh, we use the

original vectors to compute the similarities.

These are also once I pushed them in the 2D

space, these are, these are 2D vectors, right?

So, say, with two dimensions now.

So I could also compute similarities with them, but they

have been, uh, you know, men, they have been dimension

reduced.

They lost a lot of information to the.

dimension reduction from 300 to 2 dimensions and also they

have been uh dimension reduced with the objective of visualisation,

so they're not greater for computing similarities.

Don't use these for computing similarities, use the original ones.

Are we similarities with close and similarity?

Um, why do I write it just, you know, see.

Not asleep, so why do I write it like this

year?

Um And I said before, like in one term, and

I said before that it's just the dot product between

normalised vectors.

And now I write it in one or like, you

know, I write in one formula.

So.

I guess the question is why can I write it

like this?

When these are vectors.

It's too obvious, but like, yeah, do you know what

I mean?

So why can I go from that step to that?

So I wrote before like this.

I wrote, first of all, with the transport, right?

The transpose, um, I'll need to do the transport to

take the dog product.

So he actually, that's a bit of a typo I

should put a transpose here, OK?

Otherwise, I couldn't take the dog product, of course.

Um, so now, um, I have the transpose, and why

can I go from here to here?

Why can I, why can I separate them?

I think it's probably too obvious because the length of

the vector is just a number.

Uh, so if these were vectors as well, I could

just separate this whole thing.

I couldn't even, like, you know, I would have to

do an inverse and so on.

But here, because these are just numbers, I can, element

wise, divide that vector by the number.

It's just a one over scale up of the vector,

what we talked about last week.

But that's to your earlier point with your distance.

Great.

And now we look at these cool analogies.

Why do they, why do they work?

Um, all right, so let's, let's try to get this

like clear in a rigorous way.

Um, usually you see these types of plots, OK.

So man is to woman, uh, what king is to,

da da da, and so on.

And then they write it like this and it points

to some direction, that space.

And if you only take that vector and add it

to king, you arrive at queen.

But when we think about how we, like, if we

think about this a bit more rigorously, vectors start in

the origin.

Of like a space.

So that to me, I always found it like intuitive,

but, but not very like intuitively interesting, but I wonder

like how would that translate into the, into the vector

notation we did previously.

So first of all, try, let's, uh, let's think about

the, the computation we need to do here.

So to do this analogy or to get this analogy,

what we do is we take the vector king from

the vector king, we subtract the vector man.

That means every element in the vector we subtract, and

then we add the vector woman.

Why does this, why does this, you know, work?

Because we, we sort of, we, we take men out

of this king vector, we're left with, say, royalty or

something like this, that, that's what this should then encode.

We add it to woman and we arrived at queen.

So our algorithm would be to to create some synthetic

queen vector.

By computing king minus man plus woman, and then do

similarity of that synthetic vector with everything in the database,

with every other vector, and then pick them all similar.

That's how this works.

OK, so build a synthetic target vector, take similarity with

everything, um, and then, uh, and then basically uh pick

the top ones.

And there are some hacks that I'll show you later,

remove the same vectors, etc.

but visually then, because like these things, you see them

all over the internet and so on, but trying to

bring this into the slightly more rigorous way of thinking

about this that we did previously.

So say man is here.

Um, woman is Europe.

Um, King is here.

And then queen is here.

Something like this, this parallel run.

Great.

Um, and then they basically look at the, at the

distances between these, but let's say, let's say we do

this computation step by step.

Right, so we, um, we have the vector for king,

which is here, right?

That's the vector for king.

Now, we need the vector for man and we need

to subtract it.

How do we do this thinking about the methods review,

we take this vector here, we multiply it by -1,

and now the negative man is here.

In that space, right?

So now, how do I do vector addition?

I move tip of one vector to the tip of

the other.

So now I move this negative vector to um to

the, uh, so I have the uh the king minus

the minus man.

So right here.

Like I don't, I might mess this up with my

growing abilities, but like I will probably be here.

That's, that's, um, that's man minus, uh, that's, that's king

minus man.

OK, and now I have the woman vector here, and

I add the woman vector to this.

And it brings me to be.

Good.

So, how do I, how do I subtract man from

king?

I first do the, the negative scaling, it's here.

How do I add vectors in two different, you know,

imagine I have these two vectors.

I add the one by adding it to the tip

of the other one, that's usually the visualisation of vector

edition.

So I scale it, add it here, then the then

the synthetic queen, like the the the royalty kind of

vector would be here, and then I add the vector

for woman.

This year is the the vector for woman, and then

I arrived at Queen.

So and that's kind of linking it kind of full

circle to last week to these, to these uh reviews

and that's why we did them.

But then if we just look at, you know, like,

more easily, we can just take this entrance it here

and that's where we would land at queen and this

is usually what these, what these directional plots are.

Um, great.

OK, cool.

This is how this works.

This is why the, um, why the all these analogies

with, with tensors and so on were.

They were leveraging the structure of the, of the geometry

of these vectors.

This is kind of, you know, um, man, woman, king,

queen, this kind of, you will see this, um, like

always as this canonical example about walking, walk, swimming, swim,

um, the, the capitals, etc.

So they all go from a certain point into the

same direction, um, and vector space.

And if we get that direction and add it to

another, um, to another, um, to another country, we get

another capital.

Any questions?

Mhm.

So, when you're thinking about, like, the swimming and swim

and where they're plotted, and like, why they would be

plotted, why would they be plotted close to each other

if they're probably not going to be next to each

other in, like, any sort of like Wikipedia or something,

because they're different tenses.

They're gonna be very far away, I mean.

Yeah, yeah.

So, yeah, like when you're doing the math, why do

they end up near each other in terms of I

don't know if that makes any sense.

I might not understand, so you're saying, are you saying

that men, king queen are probably not so close in

a high dimensional like in whatever space like swimming and

swam because if you're taking them like in a real

context, they probably wouldn't be close to each other because

they're different tenses they're far apart.

So when you're creating the vector, why do they end

up close to each other mathematically.

Um, because they are close to a synthetic vector.

So they are.

So if you think of this in queen example, Then

King and, and, um, you know, King is not particularly

close to woman.

Um, man would be, you know, this is why these

are slightly, you know, man would be closer, but this,

they actually do this here, right?

So man is probably in that space closer to women

than to king and to queen because there are different

parts of the space.

But, what I'm doing is, um, in order for trying

to find the queen vector here, I'm trying to build

a simple.

A queen vector.

How do I do this?

I subtract man from king.

I add woman, and then I have this, this combined

vector out of made out of other vectors, and that

new vector, I take distances to everything I have, or

like similarities with everything I have, and it turns out

very high up in the list will be queen.

So by, by this kind of thing, I'm scaling, I'm

adding and so on, I land like I'm moving into

a different parts of the space and I'm landing close

to where my target vectors.

So the original vectors don't have to be closed, but

the combined vector, the synthetic vector will be close.

Yes.

So is the synthetic something that you are like developing

each one separately based on like your own expertise or

whatever, or is there part is part of the process,

like, identifying?

like where does the synthetic vector come from, I guess.

Yeah, so it depends on like these analogies.

So you see here, this is A is to be

what C is to, right?

Um, and if we go with, um, I'll show you

this in code, but if you go with man is

to woman what king is to queen, then each of

them is, is one of his A, B, C, and

D, and then I can just write a function that

does this for A, B, C, and D more generally,

and then I do it with other words, and that

function we look at in detail, yeah.

But like it will always be that computation and the

analogy always has this form.

A is to B, or C is to D.

That's always the analogy, and here it is, um, you

know, say, man is to woman or king is to

queen.

Right, but it could also be, you know, uh, Goethe

to Germany or Shakespeare is to England or something like

this.

And then I can write a function that just like

once I define these vectors, does this always, and then

if it works, sometimes it doesn't, it gives me sensible

words.

And it's always creating this synthetic vector, and then basically,

uh, with this synthetic vector takes differences to others, um,

and then, uh, and then, um, and then gives me

the most similar vector in the actual database or in

the table to this to this newly created vector.

How about questions.

Yes.

Can you throw more than one outcome, for example, I

think in the in the Liverpool like many exactly.

So what you would do is you would build the

synthetic one, you would take some similarities to everything in

your in your data, to all the other vectors, and

then you the top XYZ more.

But the first one is more or it's like they

have equal.

There will be an ordering.

So if I just saw the table unless the vector

sit all at the same point, there would be some

will be more similar to what I have synthetically and

others will be less so.

Other questions.

Is this clear?

Could everyone kind of, is this clear?

OK.

It's just building on the uh on the Um, You

know, maybe, maybe let's do this again.

And try to drive it.

The larger the drawings.

It's building on on our method review on what it

means to um add and subtract vectors.

So I have these vectors here.

I have um then.

And woman, and now let me also like draw them

a bit more closely than king and queen and then

King.

I mean And then I'm trying to do that computation.

I subtract the vector man from King.

How will I do this?

The vector man is here, so the negative man vector

would be somewhere here.

So that will be minus the man vector.

Um, I add this to the vector, like this negative

vector I add that to king, that's the same as

subtracting.

So adding this to king will move it somewhere here.

So now I'm there and now I'm adding the vector

woman to this year, which is this direction.

So any woman, and then I land at a synthetic

vector that's here.

I'm taking cosine similarities of that vector with everything else

and it should be pretty close to the queen vector

now, and it should be close to the queen vector

by that logic.

Questions?

All right, cool.

So, um, yeah, let's look at a few more examples

and then let's do this.

Um.

Great.

So some applications, you, you might have some papers in

mind or you might have seen some papers here quick

selection.

Um, you can, uh, you can look at, uh, at

the geometry of terms like geometry.

Why?

Because now you can represent all of these terms and

some vector space and see how they relate to each

other.

Um, there's this, uh, this paper by Kozowski, um, you

can look at semantic shifts, so you train word vector

and or like word vector models through time.

You just need to make them comparable, but there are

tools to do this and then you.

You can, uh, you can check, for example, how things

work like awful meaning, uh, meaning something very different in

different, um, periods in the 1850s, 1900s, and 1990s and

so on.

So that is just like by looking at what is

close to these words in corpora from different times.

The only thing is all your different estimated models you

need to make comparable, um, because, you know, they have

random initialization, etc.

um, but there are ways to do this, um, yeah,

and then you can look at the semantic shifts.

Do you have a question or no?

OK.

Uh, any questions?

Good.

One thing to be aware of, um, and now hopefully

from the underlying mechanic, it's clear why this takes in

a lot of the biases from the data into these

embeddings, right?

So you can actually, if you take these kind of

analogies, you can create a lot of um of uh

these kind of gender stereotype analogies and so on if

the underlying data uh encodes these stereotypes.

Um, so not just, not because this is an algorithm

or something, this thing is neutral, obviously, it's just encoding

everything, all the biases that are in the training data.

Um, and there's been work on this and other work

in the NIS and um and in science on the,

on these topics, um, yeah, but I, I mean, from

the underlying, you know, if you imagine what is around

words and what context are they mentioned depending on how

biassed the data is on which you will train, these

will also be these biases will be encoded in the

vectors, um, by the sheer, um, you know, optimisation problems

that the problem that these things are are solving.

Any questions.

Yeah, so that's kind of um that's a big issue

with these actors that's important to be aware of the

social scientists and also kind of discuss uh if you're

using them and relying on the analogies, etc.

Um, good, um.

Cool.

So now, uh, onto the coding, any, any other questions

about that?

Great.

So let's look at the code.

How do we, how do we build this?

Um, maybe if you're running this on your computer, wait

for a moment to just scroll through everything and read

it.

Let's try to, let's try to kind of come up

with this.

I think then um you will have a much easier

time remembering this.

Um, so the first thing is I load the library

TC, that's the standard reduction, you know that retain the

clusters for later.

First thing is I need to download this embedding.

I can go to this website here and then I

download just the Wikipedia and Giga word.

These are, you know, massive trained or massive text corpora.

Um, I download the embeddings.

They all come in the zip file.

I just take the 100 dimensional embeddings because they are

pretty fast to load and to compute with.

I use a library or a package or data table

because it has this faster every function because these are

pretty big tables.

Um, and then I, I wrote them, I wrote them

into, uh, into my art, um, and what will they

be?

They will be tables.

That have, you know, the vector names here.

Hello.

Well, these will be role names and then each role

will correspond to an embedding of a vector.

That's how these tables would look like.

So in order to make my life simpler, I now

just um um divide each row vector.

Um, through the length of that vector again.

Why am I doing this?

Because like I can just take products afterwards and have

courses and similarities.

No need to use a course and similarity formula from

any library or so.

I just, by doing this, I can just now compute

course and seminarities through simple products.

So that's my preparation, OK.

So the first thing that we looked at just to

remember like remind you, were these illustrations like what is

similar, you know, who is not a physicist, uh, even

like, you know, adding a scientist from a different field

in here.

So how do you think, like, you trying to develop,

so actually these functions go back on when you use

libraries like the word library in art or the Gen

library in Python, they have these functions, they are implemented

in these libraries.

Um, and we could have just used them out of

the box, but the purpose of building them here was

trying to see how they really work.

And if you were to build this function, does not

fit yourself as I did in this kind of notebook

here, how would you build it?

How do you think this works?

I give it certain terms and it tells me which

doesn't fit.

If you like, without looking at the code if you

can, like how would you try to do this with

the vets and now with everything you know about it?

There are many ways, but like what, what, what could

be ways to achieve this yourself?

Mhm.

Yeah, do you have any idea or you can maybe

see how far away each of them are from each

other and that's cool.

Imagine I give it a very long list um then

it would be a lot of pairwise comparisons, right?

So it's exactly right, um, but like it can be

computationally.

So what is like your most accurate?

What is like a simple way to try to do

this?

One issue with what you're saying though is it's away

from what?

Is it away from one specific actor in this group,

or is it away from the group, you know, so

that's one thing you will still have to solve.

What is the simple notion to getting like to getting

at what's away from the group or similar to the

group?

Yeah, exactly, exactly.

So probably what these functions will do is they'll take

an average factor of of all the vectors that are

fed into them.

They compute similarities of that average vector with every with

every single vector, and they throw out the vector that's

mostly.

That's really how, you know, this AI or whatever, you

know, it tells us like it has no idea that

Fleming did like you know, so it's just like basically

that vector is further away from the other vectors and

that's like by the coming from the strain.

So let's look at this first function.

Um, it's exactly like you say, we take the word.

This is just like the role here, and the cool

thing is we can now use this batterized code and

we can just do a single, single, like, you know,

a single product of all embeddings with that single embedding

from our um um.

Um, from our, uh, from our um.

Let me see.

We we were here with do not fit.

Let's let's look at this like I, I, I should

start with let's look at this first.

So we, we have basically, we take all the words

that we just looked at, um, and then we, uh,

we wrap them into the embedding matrix.

So this gives us all the word vectors.

Now we just take the mean of these vectors and

we take the similarity of that, um, that mean vector

with with the vector, um.

Um, with the vectors of the words, OK.

So this, this is the, uh, this is the mean

over all the words that were supplied to the function,

this list of words, um, and we take the similarity

with, um, uh, with respect to every word that was

supplied to the function.

Um, and this is similarity to the mean.

Why, why is this here?

Can you like, I mean, even Rising this being a

bit boring, but why is this the similarity just again?

How comes this this single language goes on similarity?

It's, it's a product and the vectors are normalised to

you.

That's why.

So I've normalised them before, otherwise this will be causing

similarity.

It's goes in similarity because they're normalised.

This is the mean vector of the, of the words,

you know, like the Einstein and so on, and this

is, um, these are all the individual word vectors.

I compute the similar to the mean, I sort the

thing, um, I report the word um furthest away from

the mean, and that's here.

That's how this works.

So in simple what I what I skipped, imagine like

we want to create a similar function, but that's really

obvious now.

It's like I add a word back to that function.

I take similarity to everything in my database and I'll

report the talk um more similar.

That's it.

That's really just the the completely mechanical.

Um, OK, so here, how do I do this?

I take the word I want to compute the similarity

with.

This here is my word vector.

This is my matrix of word vectors.

So can you tell me what are the dimensions?

Um, what is this embedding matrix?

What are the dimensions of this?

The rows are the number of words in your vocabulary

and the column dimension is the size of the embedding

that you chose 100 perfect.

So what was the dimension of this year?

It's just like the row of the word, right, like

100, and actually what is doing here in the transposing

this for me because otherwise the matrixation wouldn't be defined.

So what this is is like these are all my

embeddings, um, and they go with 100 times, you know,

the, the, say the number of words I have the

vocabulary, and I'm dotting this with this single vector of

101.

So that's my single embedding.

So what I will get out of this again, so

I'm taking embedding matrix.

Which is 3 times 100, and I'm dotting this with

a single embedding 100 times 1.

So I'm getting out.

V times 1, a huge vector, doesn't make sense that

I get only a vector out of this?

Why or why not?

So what what is contained in that vector?

What has its store now?

So what I did is, what does this do?

Um, each row here is an individual word, right?

So what I did is every step in this vector

is a dot between word 1 and the the word,

word two and the word, word 3 and the word.

So this is what's in there.

So then what are these elements in the new vector?

What would we call them?

OK, so I have this matrix E, OK, and he

goes like this, um, he has row vectors.

And these ones here correspond to words.

The first one might be the vector of hello, the

second might be the vector of world, etc.

OK.

And now I want to find the most similar words

to say hello, OK.

And what I do is I compute the part between

E and the embedding of hello.

Um, and how does that look like that looks um

to this multiplication.

If I use my trick from last lecture to to

make this a bit more intuitive, the multiplication, so this

is the huge matrix.

And then this is my betting for pedal.

That's a bunch of numbers.

So what is the, what is the first number of

this um of this multiplication?

It's the dot product between the first row vector here

and hello, so that should have what value?

We already know what value it will have, um, do

you, do you see this?

Because they are normalised to unit length, so what value

will it have?

One, because that's of course a similarity of one.

So that won't be a one.

And now we basically have a final vector where we

arrive here that stores all the different posts and similarities

with a single dot product between the matrix, the table,

and the individual vector, we computed all the courses and

similarities.

All we have to do is we still have to

sort it.

Then we removed the very first one because it's, you

know, just hello again, right?

So that has a scenario of one, and then we,

uh, we look at the other most like the most

similar to to um plus 1, and these are our

top ends.

This is how this works.

OK, and then the last function was this analogy thing.

Right.

And that we discussed in more detail already.

So we have, uh, say this is Einstein to scientists

what Picasso is to, and this is, this is the

structure of A to B, what C is to, OK?

And now I I wrote this into a, into a

general function.

OK.

And here I I gave it like kind of an

overall explanation and inputs and outputs again.

And the example is that we had in the lecture

was man is to woman.

this should be, uh, should be an A here.

um, what uh king is to u.

OK.

Great, so now, um.

So man is to woman, what king is to queen.

Great.

So then we need to get these vectors, yeah.

So we need to get the man vector, the woman

vector, and the king vector, right?

So that's how we get them from the table.

They are rows on the table.

So here we go.

We have these three vectors.

And now we want to get our kind of synthetic

target vector.

We do exactly what we did in the lecture.

C minus A plus B.

Now just generically labelled.

OK.

So but if we look at this, we go king

minus man plus woman.

OK, so we do exactly this here.

Any questions?

No?

OK.

So here we created the synthetic vector, and now we

do the same, we, we do the same logic as

before.

We dotted with every, every single vector in our database,

do the dot product here, as you know from the

last seminar, that's the notation for the dot product.

Um, we then, um, or we saw all these similarities,

um, and we pick the, the, um, 1 to top

end plus 3 words.

Now, this seems a bit strange.

I, I want to look at the top 5 or

top end analogies here.

But why do I do + 3 here?

Um, you know, if you look at the next slide,

do you see what it's doing?

I say remove all original words.

So what seems to be happening here?

It turns out that if you use von Gim like

to get to do these analogies or so that actually

among the top most similar to the synthetic because that

space is so dense, will sometimes be the original vectors,

which will then look a lot less good than like

these similarities look like.

So what you need to do is to among like,

you know, you do the target vector, the synthetic one,

you've dotted with everything in the database you sort it,

you keep the most similar ones.

But among these more similar ones, the original vectors might

be among them again, so you just remove them, and

there can be up to 3 original vectors A, B,

C, right?

So I removed them again, um, and, and then I

only return the topim among the ones with then removed.

So that's why, why this works so well.

So this is kind of the vector space, everything is

so close that these errors happen.

Great, OK, um, questions.

Yeah, I mean, if you study these functions, you have

questions, let's discuss them in the next um seminars or

um you know, um, you can.

Focus on the office hours or uh etc.

if you have questions on any of these contents.

Um, now we, let's try to see if we can

visualise this stuff in two dimensions.

So I'll take some words here, um, 12345, a couple

of sports and a couple of economics terms.

And this is really interesting.

Now I brought this into two dimensions and I use

this technique which is very good at maintaining clusters that

are also in high dimensional space, right?

So I reduced this to 2.

So here, recession and inflation are really quite close in

a different part of space than football and basketball.

So this works really well.

Um, the numbers, this is also quite interesting.

543, and 2, they are quite close together, but one

is somewhat seems somewhat away from them.

And that's because one can have different meanings, right?

So one doesn't like this.

lecture or one likes this lecture, right?

So and then um this year this is exactly representing

one can have different words around it than the other

ones.

The other ones really are mostly numbers or mostly used

for counting.

Um and economics is somewhere in the middle and so

on.

So what TCE does is these things, these individual clusters,

it tries to maintain the structure that is local in

these clusters.

But the overall thing, like, you know, how far is

this away from economics and from football, basketball, that's what

it's distorting.

OK, so it's maintaining small groups to show us the

clusters that are in high dimensional space, um, but it's

not so good at, um, uh, you know, or it's

distorting distances between these groups.

We can thereby also try to Look at these analogies

and look how well this works.

So for a specific group like a queen, man, woman,

king, um, like we have almost a perfect parallel in

2D.

So this will be perfect in higher dimensional space, otherwise

this wouldn't work.

Otherwise we, uh, we couldn't, we couldn't get these um.

Um, these, these analogies so accurately and here's just an

illustration, you know, just trying to continue to build this

stuff um with all the capitals and you see it's

always kind of the um uh a similar direction, you

know, going basically, um, kind of yeah, southwest from the

um from the country.

So Spain goes to Madrid, uh, France goes to Paris,

Germany to Berlin.

You see here like, you know, it's not perfect again,

it's only 2D representation, China, Beijing, uh, Hanoi for um.

So that's why this works and as much as intuition

as we can get into the um.

Cool.

I complete our trying to build all of this, OK.

Um, and I know it's a lot of content, but

I think it, it kind of, um, uh, yeah, uh,

can give quite a valuable inclusion.

Um, OK.

And then lastly, because I know people, you know, use

this in their capsules, etc.

what if you want to train your own models?

So here, I, um, I actually used one that was

trained on a lot of data and that I didn't

have to run to that myself, but I might want

to run this on a specific purpose of documents I'm

studying maybe because of the semantic shifts, etc.

and then how could I do this?

If you know Python, there's a pretty good library Python

against it to do this.

That's, you know, that's very good.

But there's also a library in R if you want

to stick with R here, you know, these are the

familiar terms we keep the skipgra model, we do the

dimension, we keep the window around the world, etc.

And here we run this.

Here I run this on the good old Newton book

that we kind of, you know, looked at in previous

lectures, um, and I try to see if I can

get anything meaningful out of it.

Of course, it's a small seed like this already runs,

you know, a short amount of time even on this

tiny book, so I couldn't run this on like, uh,

on a good chunk of Wikipedia.

That would take too long for the lecture.

And then what I do is I look at more

similar terms and I try some analogies, and then I

saw the the model um to this basically and I

leave the.

I link the vignette here if you have further questions.

So more or less trained, um, this is how it

looks like, you see, again, a row is the name

earth, tangents, or, you know, um, medium and so on.

And then, um, and then the rows are the dimensions

of the embeddings.

If I look at stuff that's similar to Satan, I

get like even Jupiter, you know, not bad, um, Mercury,

Mars, etc.

But then if I try to do analogies.

So sun is to start what, um, you know, uh,

what Earth is to say I am, I want to

get a planet, uh, then this doesn't really work.

So like the data is too small, like it doesn't

have enough power to learn these analogies.

But if I had trained them on Wikipedia, I tried

it and it gives you a lot of sensible analogies,

even with basic hyperparometer settings.

It's just not necessary.

You can just download them, uh, pre-trained unless you want

to train them on your own course, but for Wikipedia,

they retrained.

Um, yeah, you take like a Wikipedia trained version and

then like tune the parameter try to find it with

like a specific thing like if you wanted to train

it on this book.

Yeah, so I mean, it's like there might be options

like, I don't know whether to continue to train options,

maybe there are some, but of course, then it's the

question, how much do you train it and what do

you want to get out of it?

If you want to really encode and you are embedding

some semantic shifts that happen in a specific data set,

it might not be sensible to anchor them with some,

you know, values from another data set.

But if you want to have more accurate analogies, then

maybe it is.

So it depends a bit on the application that you

look at.

Um, other questions.

Great, OK, I'll, um, I'll stay here a bit longer

if you have any questions you want to ask, uh,

feel free to drop by and um I hope you

find this interesting and otherwise, uh, and then I'll see

you, I'll see you next week to discuss our kind

of points.

OK Yeah.

OK.

But by the way, about the evaluation, you don't have

to fill them out now.

I give you some time in the last seminar because

my part isn't finished yet and so on.

So if you want to get feedback on everything, then

I'll like you have some time in the last to

do this, but they always trigger them this week, we

can't change these things even if we have two parts

and courses.

This um.

yeah.

Yeah, no, she's just very soft.

like I said and then like.

I miss I miss that.

Absolute apathy for that um.

This There's not Does.

the one dimensional.

I just want to confirm what I understand is correctly

so for here, um, you firstly do a small deal.

Uh, supervisor.

No, like, so I mean I'm training work like this,

right?

Yeah, so I mean like like it was like one

week you know.

Yeah, I didn't have one course, but you know it

was like so you first need to train.

So at that point in time we trained all the

word vectors.

So at that point in time we have all the

word vectors.

How do we train the word factor?

Check out the video of the lecture and how we

get, right?

And at this point we have them.

So as a pointed space, we have that, yeah.

No training, no supervised learning required anymore, nothing.

We have these factors.

They are, they are, yeah.

So now if we put them into space, they look

somewhat like this and then to do these analogies we

can just leverage like the geometry of that space.

So what we can do is we can take the

vector from in, we subtract the vector.

We add the vector for, then we have a vector

vector.

We take similarity of that target vector with everything, all

the other vectors, and then we find something that is

probably queen.

So like these things have been trained already.

Um, we do vector mice and plus will be some

sort of.

Proximate queen vector.

We take that vector, take similarities with every vector in

the data, and um very will be to the queen

vector.

Does that make sense?

So in the vectors we got from the steps before.

So we got the vector with for example glove.

OK, and the other question is, um, I'm not sure,

but I just want to say is there.

He applies things to more strength, so it's like.

I'm thinking in the context of my research.

I want to like because people usually mention one direct

in relation to.

So you could then maybe your own word factor, the

data you have, and then you can do these things.

Um, but with just words, it wouldn't work.

You need like some numerical representation of the words.

And that you would get with the vector, and that

you would need to get to for example what to

or glove.

OK, so you could maybe on your on your data.

You can try it out with the code I I,

yeah, OK, thank you.

Uh, we were wondering if.

During April.

Yes, there will be no during April there will be

a break after the term that's April, no.

But then once the once the break is over there

the next term starts and then there will be office

hours every week.

Yeah, I don't quite know the definitely be in May

and I don't quite know when it starts.

Yeah, when is the 4th of April.

So when does the, when does the the spring term

start?

Early May, you know, so and then basically for the

whole of spring term, there will be office hours every

week, um, and then kind of your exams are probably

May, June something they already 14th of May.

Yeah.

Lecture 10:

That's That's good.

OK.

How's your Oh, it was funny oh no I just.

I It So.

It's just not We Yeah I feel like What All

of those cases where so here.

to the.

It was no that's one I mean.

He's supposed to be.

I I.

Yeah.

Uh, the one.

and that's like the.

Yeah.

It's really strange and I don't.

you should I think you Yeah.

I like Yeah, I go ahead.

Yeah, yeah, we planned a few.

Um, we're talking.

Who's who will be very cool, nice.

Um, here or not here, OK, that's beautiful.

Is it like it's like a cottage, yeah, it's um.

What the groceries like is it more?

Yeah, it's like Rolling Hills New English, yeah.

Yeah it should like the drinking about 2026.

like did you send your application.

Is that legit uh.

Challenging remote or.

trying to fast track my application I don't care.

And this is like you you turn in your.

Very good.

It's pretty stressful, but.

I think I want.

And, but yeah, you want the optics to to work

yeah this is like it's yeah but like is here

and then this one is like.

My brother is gonna do he's very it's gonna be

I'm hoping to be like.

Sure.

At the end, someone gets married, that's.

That yeah.

she OK, that's really good to know.

she told me that she would.

She's a DJ now and I can.

but she said she's beautiful.

That's very sweet, she's like I don't.

Right.

I.

by, but I think if you get used to it

compared to the day to day, and you can still

access one and stuff like you're like go to a

show or go to like a randomly specific restaurant, you

can still achieve that.

So yeah, yeah.

like You do like juice I think there's depends on

where you go.

There's some it's like, oh, OK, yeah, and I, but

then a lot of them.

I know.

Sure basically like Spike Spike told me what he like.

cutting just to get like.

I used to do it in like 70s, yeah, right,

like if he's able to spot.

and No.

But OK.

Yes, and I'm finding it's a lot of I I

I do.

Oh my God, you.

I So let's go, um.

So today, today is uh I think very exciting topic.

Uh, it's, it's one lecture with a high level overview

of how current language models work and relative to last

year because things are so quickly moving in this field,

I've essentially invented this.

Um, I've tried everything that I find quite interesting at

the moment, um, let's see how much time we have.

To, um, to kind of, I don't want to rush

through any of this, but we can also, um, potentially

discuss other like you know, residual parts of the lecture

in the past.

So any questions you might have about this when I

go through this, please ask, uh, we have enough time

in total and it's much more important to understand these

things and then, you know, trying to, trying to rush

through them.

OK, um, so that's where we are.

Um, and that's, that's today, OK?

So I said, it's an introduction to LLMs and the

first thing that I, I'm sure many of you will

know already.

Um, anyhow, is that at a fundamental level, um, LLMs

are just this function.

They just take a sequence of, you know, let's say

for our words from, uh, you know, a start to

an end to the sequence, and then they predict the

probability of what could be the next word.

So that really at a fundamental level is what a

what a current large language model is.

So if you write a prompt into JGBT, you give

it a sequence of words and then it predicts the

probability of the next word.

Um, the next word is sampled from that probability put

there.

This is roll one forward it's met with like all

regressive here.

You know, you like the word the word that JGBT

just created, it's appended to the sequence, the probability of

the next word is predicted and so on.

OK.

At some point, JGBT or clause or whatever will predict

an end of statement token, OK, and then it's so

sampling.

That's how that's how one step and one step interaction

with JGBT works.

All right, so given this is a lot of content,

I'll speak about many different parts briefly, but I'll uh

provide a lot of resources if you, you know, if

I want to study in more detail.

Any questions about this kind of high level of gold?

So yeah, today will be about trying to make sense

of this function here and that function is um is

a language model.

Good.

So that's the outline.

Uh, before we even start with the function, I think

it's good to spend a bit of time on tokenization.

I know you've done a good amount with Ryan here,

you know, you used the token function in 20 all

the time and so on, but I think it's good

to think about how tokenization works for these language models

at the moment.

That makes also some common failure mode of these models

a lot clearer that you might see on online and

on social media and so on.

So then we, uh, we look at the fundamental architecture

behind these models, time permitting, of course, but I want

to show you the two main reasons, you know, the

two main reasons why this underlying transformer architecture is so

powerful, um, and then we look at the current, um,

steps to obtain models like state of the art language

models, which you can roughly, um, you know, separate and

pre-training and posting.

So to, to start with the poll, how much?

Like how many of you knew that SGBT is just

predicting the next word?

I assume by this at this stage everyone sort of

know.

All right, good.

So how many of you are familiar with pre-training, post-training,

these kinds of steps to obtain a more like?

OK.

So let me like, like we're now talking about language

models and so on, but really this is the underlying

paradigm powering current AI, right?

So when people talk about the progress and intelligence and

so on, it's extremely closely connected with everything we're talking

about here.

Um, so that's why I think these, uh, these, these

topics are so exciting.

Um, OK, good.

So let's start with the organisation then.

Um, the key question remains, you know, what we had

in the course so far, how do we move from

text to numerical representations, right?

That's the key.

All right.

So, and I created a corpus of for, you know,

patents because I worked on patents, um, and just imagine

these are, these are patents and these are simple enough

for us for us to just tokenize the entire caucus,

OK, and that's what a tokenizer is.

If we assume tokenization at the word level, which, you

know, current models don't, but I'll show you in a

few slides, if we for now assume tokenization at the

word level, this corpus has 5 tokens, and then I

just put them into a so-called dictionary, and I say

that could be a JSO file here or something like

this, and I could, um, I, I say, um, these

typos.

I could say 0 is is represented by, uh, it

is the token integer A1 is the token integer for

invention, 2 for better, etc.

right?

OK, so, um, what is the um, you know, this,

this tokenization dictionary then it's a mapping from these integers,

um, to, to the tokens to the underlying words, right?

So then if we have a sentence, um, or you

know, a document, say a good invention that would internally

be just represented as a sequence of integers 031.

OK.

Does that make sense?

Great.

So that also already shed some light on, on our

function, because the function actually, if you write something into

JGBT, it just gets a sequence of numbers as input

and it predicts the probability of what could be the

next number.

And these numbers you then map back into text.

So this is not, you know, how, how would you

even enter text into this?

It gets a sequence of numbers and then it predicts

the next number.

With the tonization dictionary, we can just um switch back

and forth between numbers and texts.

OK.

But this of course is a is a yeah, so

that means that for example Je GPT only works with

a specific embedding method.

Like they have only one way to transform a word

into a numerical vector.

Yeah, so we haven't even talked about mapping for now.

All I'm saying is we have a mapping between numbers

that make it easier for this thing to think about

tokens or words and the underlying words.

But you're absolutely right.

Each number here is then represented by an embedding.

So for example, there is the embedding X0, which is,

you know, representing the word 0, which happens to be,

you know, internally, but we'll, we'll speak about this in

a second.

So for now, we're just switching between integers and words.

Good.

So how do we get tokenized, um, or like how

do we do tokenization and kind of state of the

art um environments.

The cool thing in this course is that um you've

done some work on on character encoding, right?

Um, so you know, like, you know, that some workings

of this and internally, um, a computer.

Um, would encode characters, of course, as, as zeros and

ones like everything.

And we can think of a sequence of 80s and

1s, um, as, uh, you know, 0 and 1 we

can think of as a bit, one, you know, binary

digit, and 8 of them we would call a byte.

Um, and then internally.

Um, texts are simply representations of these zeros and ones.

Have you seen these examples of um of Unicode, how

it is variable byte length in coding?

Um, has Ryan showed you an example of this in

action, how like if you store files with different characters

or something like this?

Yes, OK, awesome.

Cool.

Um, great.

So now, actually, the modern tokenizers are so-called um by

pair encoding tokenizer, OK.

What they do is they look at a lot of

text and then they simply bundle commonly occurring sequences of

bytes.

That's what they do into tokens, OK.

So it's a holistic process in a way.

There's no ground truth the so.

That's why a token is a fraction of a word.

The computer doesn't see the word.

It just, you know, sees sequences of fights that are

commonly occurring in texts, you know, imagine all of Wikipedia

and more, and then it's bundling them.

And it turns out, for example, GPT40 is a tokenizer

at the moment with roughly 200,000 tokens.

And each token is a sequence of bytes, usually a

fraction of a word to a word would be a

wide space in a word, stuff like this.

But that's why it's a white space in a word

or a fraction of a word, because these are just

sequences of zeros and ones that come in the ground.

It's groupings.

So, and we can, for example, then if we use

the tokenization tokenizer of TPT 40, uh, we would tokenize

London School of Economics into this year, um, and we

can actually check this out.

Um, so let me write this London School of Economics.

You see, this becomes 5 tokens, OK.

Um, yes.

Now that the tokens actually make a more.

Sorry?

that the tokens.

Yeah, I mean, the model is operating on tokens and

that's already our first, you know, bigger insights because we

can understand some limitations of these models.

So that's a great point.

It's not like a lot of, you know, I'll show

you in a second.

I think one thing that relates to your point.

OK, so first thing to note this year, this is

quite the sequence of, you know, 4 words, fair enough,

gets 5 tokens, but one token for L.

Why?

We see this in action because lowercase London is just

very rare.

Upper case tokenizes London a single token.

OK.

That's why this is kind of a nice example.

OK, and here you can try out things, you know,

we, we could write any kind of sentence and then

we have the integer sequence that flows into this model

and then it predicts the next integer that we can

then map back into whatever it is.

That's, by the way, also why if you write to

CBT, the answer is arriving sequentially.

It's sampling token by token and then appending each other

like, you know, to, to some kind of oper.

OK, great.

Um, any questions about this?

OK, let me, give me one thing here to illustrate

this further.

OK.

Good.

So this is how we break sequences of words into

tokens.

Tokens are represented internally as integers.

That's just a mapping, you know, a back and forth

uh function between integers and words.

So now, That's an approximate process.

Many people aren't happy with it, um, you know, people

are working on this, uh, but it's not super obvious

how you represent input text into something a computer can

deal with, um.

It can explain what you already were looking at.

I mean, a bunch of these models.

For example, the models aren't great at counting, but imagine

if I gave you a huge text with, you know,

and it said the word LLC in various places, counting

that word if it's not necessarily a unit of operation

for that model is not obvious, or counting the word

like a letter or something if it doesn't even operate

on a letter level.

That's why it's so tricky.

Also, for example, swapping letters.

So let me show you something.

Sometimes this works, sometimes this doesn't, because these things are

probabilistic, but I take like a pretty strong moral, OK,

the version of UBT40, and I say, can you swap

the first?

And hold on and say, OK.

See, and it, it immediately watches it, um.

Yeah, so it swaps the end here, but then it

ends with an R, and that's, you know, has a

bunch of reasons, but one major reason is that it's,

you know, it's not working on the character level tokenization.

It's working on, on these fractions of words.

So, um, now we can say, uh, great, can you

count the number of os and that should work because

the word is really small, but let's see.

Can you count how many.

Times the swat word contains the letter.

Oh, OK, so I think now, see, see what it's

doing, see what it's doing.

It's already doing tools.

OK, so that's interesting because we're talking about this later.

But let me, let me do this again.

OK.

Can you swap the first And the last letter in

Holborn.

OK.

Different version, like still, um, you know, now the R

is gone basically.

OK, so great.

Can you um count how many times the swap word

contains the letter O, do not use tools.

OK, here we go twice.

If it was a longer word, you know, there was

this discussion of how many eyes are there in strawberry

and so on.

You might have seen this with the one models of

Open AI, um, it will struggle with this.

OK.

Um, the bigger the model, the more capable it is.

But if you, you know, try different versions, you will

find some of these kinds of models.

Many of them are linked to, uh, are linked to.

I think to tokenization.

One very interesting thing was that the model used a

tool.

It ran something in computer report before and then it

could answer it more easily.

And for this, the model needs to be aware of

certain things that we're actually discussing in the lecture.

So that was quite fitting.

Um, great, cool.

Um, so then models like check BPT input and output

of words but token IDs.

OK, we got this.

And then um the text from this, uh, is, um,

transformed into tokens, and then the models, model responses.

in responses in tokens and transforms these back to text

and internally in the model, what you are already saying

is these token IDs are not internally integers multiplied by

numbers, but they are vectors.

For us that's, that's, you know, good news because we

studied this in a lot of depth.

These embeddings actually in these models, models have lots of

embeddings.

They are sort of Learned.

They are, they are learned while actually achieving something else,

namely put it in the next word, OK.

But fundamentally, all of this is working on embeddings and

that's how it's having such a rich understanding of, of

language.

But here it's not the sole objective like it was

in word to work or something like this.

Here it's just learned on the way to be better

at predicting the next word.

OK, questions and tokenization.

Good.

So now, um, let's think about what this function, you

know, is and how it's parameterized and so on.

The first interesting thing when I first looked into this,

um, I thought is that this is really nothing new.

OK.

There's this paper by Bio and from 03 on lower

language models, and they just predict the next word, right?

What they do is they have a very short context,

um, uh, but they already named the same context and

then they predict the next word.

Um, the key today will be to use modern architectures

and better, um, better, you know, more flexible, more expressive

functions, but to do the same thing and much more

data and much more, of course, right.

Um, but that's not new.

So here, the goal of today is, is, you know,

it remains the same to build and train um this,

this function here, but today's language models are based on

these transformer architectures from this paper, um, attention is all

you need.

You might have heard about this in other machine learning

classes.

you an idea, this paper, I think is cited, so

it's published 17.

I now cited maybe 120,000 times or something like this

just for the scale of this, and it's underlying literally

every of these models, lots of current machine learning architectures

and so on.

They're all based on this architecture, on on this underlying

model.

Right, to also give you an idea of how many

parameters this have, think of like linear regression with the

slope and an intercept, what we had in gradient descent,

right, we had two parameters.

It's not official, but models like 40 or so might

have close to 1 trillion parameters.

Um, that's how flexible this function is.

That's why it can, you know, that's why when you

write 100,000 words, if you change one word and ask

it to respond in uppercase letters, there are good chances

that will do it.

That function needs to be incredibly, um, you know, nonlinear.

Expressive and so on.

Also context window if we think of this, um, if

we look at the current Gemini Pro model, they have

like a very long context window, they can take around

2 million tokens into account, OK.

Um, so pay attention, you know, to um to a

very large amount of tokens sitting in here to predict

the next token.

Good.

So in the following, we'll discuss the simplified version um

of these of these models broadly along the lines of

GPT 2.

There are different reasons.

One, from GPT 3.5 onwards, these architectures in the case

of Open were not public anymore.

GPT 2 was an open source repository that you can

just check out.

However, if you look at the models like LA or

Deep Sea and so on.

This this underlying architecture um in very, very similar terms

is still powering like the current models, OK, the ones

that are open source, it does power.

There are a bunch of adjustments, you know, hacks and,

and, and, and tweets and so on, but the underlying,

um, the underlying model is very similar, just a lot

larger than GPT2 and trained on a large data and

with a lot more and we will look into this,

into this later.

OK, great.

So that's the GBT2 architecture and that looks a bit,

you know.

Uh, maybe a bit complicated, but we'll walk through these,

you know, uh, through these fundamental parts.

This is an attention block.

This is, uh, an MLP block like the MLPs we

already looked at, just a multilayer perceptron.

Um, and then there are a couple of, of, um,

of tweaks in the middle, and then you stack a

lot of them.

So let's take a step back and think of it

more simply for now.

So let's think of this example.

Language models are interesting, OK?

And really what this transformer is, it's a function that

takes in these four words via embeddings and then um

returns a probability distribution of what could be the next

word.

OK.

That's what this function is, um, and that's how they

set it up for GBT2, right?

But, but really more fundamentally, that's what, uh, what we're

after here.

So orange is the transformer, this is the input that's

the, it's really not the, it doesn't output um next

token, but it outputs the probability of our next tokens,

and then you sample from, uh, from that distribution.

So imagine you have 3 tokens, it might output.8.

15 for the second is 0.05 for the third token,

and then it's sampling from this, and most likely you

see the the first token if the vocabulary only has

three tokens, but you might see the other ones.

Um and that's why if you ask JGBT something you

never or usually never get the same response twice, because

underlying is like an outputs of the probability of the

next token samples from that probability.

Sorry, yeah, so the size of the output of the

model is always the same as the size of the

token vocabulary that's it.

So if we have 200,000, that model outputs a multinominal

distribution, think of like, you know, the lecture on topic

models, these models.

Nominals, you know, you can think of this as a

as a as a multinomal distribution just as like a

bar chart, you know, something like this, um, and, and

it also puts a multinominal over 200,000 potential text tokens

and samples from this.

That's not what the model does exactly.

And maybe it is but.

Does he look back?

Or or it operates greedily.

You know what I mean like It always produces the

word, the token with the highest probability, but then down

the line.

Maybe it was better to do something else.

Yeah, so that's I think hinting at a lot of

things.

One thing I guess you're saying is this is like

mathematically we call these approaches greedy.

It's one step look ahead, OK.

So it has a sequence of inputs, next topic.

One further like you know.

include the token into the sequence, next token and so

on.

That's not really forward looking.

And what is powering the, the very current, uh, you

know, um, models like the reasoning models, and I think

what makes people in this field very excited at the

moment is, is relaxing that assumption ultimately.

Um, 12 is also it's moving, right?

So this context say, you know, this model might have

a context of 200,000 tokens, I think clot and GPT

4.

So now you write a prompt that's clearly not 200,000

tokens usually, and then it predicts the next one.

And then it's concatenating them and makes this the next

context.

At some point, um, if the context is longer than

200,000 tokens, it will, you know, drop off the first

tokens can't, you know, pay attention to them anymore.

So that's also how the sliding window works.

I mean it looks in a sense of, you know,

that prediction sort of look back here too because depending

on these tokens.

Yes, that's what I mean.

So once they put the token thing is set in

stone.

Yeah.

This committing on something and then iterating forward is a

fundamental property of these auto regressive processes of just, you

know, putting like, you know, XT and then XC +

1, XC + 3, and so on, but that's arguably

a limitation here.

And there's any of you, the first of 5 people

I know, but other people, do you know diffusion models?

OK, um, but there like there is, you know, there

might be people are working on this.

There might be other architectures that that actually also create

tokens for words, but don't commit on the sequence, but

this one it does and it just works very well.

Auto regressive processes works super well for language.

I guess, you know, it's how we, uh, create and

speak the language as well.

All right, good.

So on to back to the transformer.

Um, the transformer, I said, contained two key parts that

I want to discuss here.

OK.

And let me first At a very kind of high

level.

So I have my input and I think we said

uh language.

Morals are interesting, OK.

And now I look these things up and let me

say I don't use the massive token numbers in this

like true dictionary.

I say X1, X2.

X3, X4.

These are my, these are my token embeddings for these

tokens, OK.

Um, so language models are interesting and now my whole

goal here is to create a function.

That ultimately outputs.

What could be the next token, OK?

So the probability of X5, given that sequence.

That's what I'm trying to do here, OK.

So I input these embeddings and then I want to

get the probability of what could be the next one.

And I'm relying on two things here, um, on, uh,

on fundamentally on attention and feed for neural networks.

Any questions?

Good.

So then let's go into attention and attention is, is

really key here.

The paper, the transformer paper that I said I talked

about earlier, that paper is called Attention is All You

Need because they were able to reduce architectures that were

much more complex to something that was fundamentally relying on

this mechanism that is called attention.

Um, fun fact, that was like, you know, Several years

ago, that was in the paper on recurrent neural networks

named attention sort of last minute.

Uh, they first wanted to name it, uh, I think

RNN recurrent neural network search or something like this.

So this, this word that is, um, you know, mentioned

in all these models is, um, uh, it's just like

kind of um a label for something much more fundamental.

All right, good.

So that's our example.

Input sequences language models are interesting.

If we really, I looked this up in this tokenizer

of 40, we would get these token intes, we have

4.

OK.

So now what we try to do is we try

to think of this model and try to.

Probability, OK.

That's our task now.

We try to understand this function.

We input this and we want to get the probability

of the final one.

The first thing that we do here is each of

these words has a, has a, has an embedding, right?

And that embedding comes from one of these gigantic matrices

that we talked.

About, um, in, you know, working back, for example, um,

it has as many dimensions as we have uh uh

embedding dimensions.

For example, it could be 10024 or something like this

or we had 300 before and then it has many

column as many columns as we have tokens in our

vocabulary.

Right?

So each of these could be an embedding.

I think in the last lecture we, we, we swapped

it, we thought about it this way, this doesn't matter,

right?

So this is just a gigantic lookup table where for

every single possible input word, I find a vector.

That thing I just initialise randomly.

Sounds familiar because that's like in other neural networks, we

initialise these parameters randomly and then uh begin to train

them.

All right, so we're thinking this to how to arrive

at the probability for the next token.

We're getting for every, for every input token we're getting

in embedding, what we're actually doing is we're adding another

embedding just additively that encodes the position because it's not

just that the first word is.

Language the second is models and so on, but it's

also that language sits at position 1 in the sequence,

models at position 2 in the sequence and so on.

OK.

So in the resulting things, we have these embeddings that

I already do down here.

I say I added already position, so plus position, plus

position, and then we arrive at X1, X2 X3, X4.

OK.

So when they enter the first attention mark, and now

comes this thing.

Which looks a bit funky at first, but it's extremely

simple.

Um, what this is is we're updating these vectors to

be weighted averages of the other vectors.

That's it.

OK.

And now look, let's look at this in a bit

more detail.

So the first thing is we transform these things with

the weight matrix, OK.

So we have to say these embeddings are of dimension

1024, say times 1.

So now we have the wave matrix.

Um We value of 1,0024 times 1,0024.

Um, and this thing is multiplied with each of the

axes, and it gives you just a linear transform of

the axis, which is still 1024 times 1.

So each of these vectors just gets transformed linearly.

That's the first step.

It's multiplied by this matrix WB.

Why?

Just to add a more flexibility to the model.

How do we get the W?

We initialise it randomly and then we learn it again.

But really like just to make these embeddings slightly, you

know, more versatile, they enter this so-called value embeddings, but

they've just been multiplied by a number if you don't

like a table, OK.

Good.

So now we have this and then all that attention

does is it takes away the average.

So the The updated um embedding for X4 it's just

a weighted average of these and that's it.

So, um, that's where the weight, Alpha 41.

With the weight Alpha 42, Alpha 43.

Alpha 44, and that's the alpha 41 times this plus

alpha 42 times this plus alpha 43 times this, Alpha

4 times this.

That's the updated method.

How do we learn the alphas?

Great in this sense.

What are the alphas really?

They are functions of other matrices, queries and keys.

In this course, I don't want to look into this,

but I know some people are are curious about these

things, so I added like an appendix that derives this,

OK.

But in this course like course, I think that we

don't, we don't have to bother about this because really

this thing is just learning the ways to multiply them.

That's it, OK.

Good.

So we have the input vectors.

They are averaged into the new vectors.

Um, you, you guessed it, X3 is also um a

uh a weighted some of the other ones.

However, um, to your earlier point also, in these language

models that predict the next word, it's only a function

of the ones behind it in the sequence.

And this is called not fully connected attention, which would

be also here, but this is called causal or mass

detention.

That means that attention is backward looking.

That's it.

Because why, if we want to predict language models are

and we want to predict interesting, it can't depend on

interesting, right?

Um, so that's why this is this is only backward

looking here.

And why self?

Why self attention only self because originally the paper, um,

originally the attention is all you need paper, it had

one transformer which was an encoder.

And one transformer, which was a decoder.

And what this one was, you had a sequence of

Xs that went into the encoder and the sequence of

X is that came out of the decoder and that

was for machine transaction.

So you, for example, had an English sentence saying in

Spanish or something like this, um, and this year.

Um, in here, um, the bettings pay attention to each

other, self-intention, but they also across these two parts had

cross attention, you know, between the decoder and the encoder

part.

Each of them were transformers.

These days, just like a bit again beyond the scope

here, but the encoder transformer is What you see for

bird and so on, uh, when you use birds and

and and bird topics and these kinds of things, and

and the tension in the Eco transformer is just both

ways, uh, words left and right of other words, and

then gives a very rich embeddings of these words.

Um, It's always the issue here of like the.

power supply.

OK, back here.

Um, whereas in, in, in uh current large language models,

they are based on these decoder parts of transformers, um,

and their intention is this, this call of tension only

looking backwards.

OK.

And the reason is we want to predict the next

word, we can't depend on the next word in the

sequence.

We want to learn to only depend on uh what's

previously.

In the sequence.

Good, but, but that's attention.

It's just like learning these alphas to, to compute, you

know, weighted sums of, of embeddings.

Uh, and we know this.

You can scale a vector, you can add it to

a scale, different vector graphically, but really, it's just a

vector multiplied by a constant plus an vector multiplied by

other constants, etc.

Um, and that gets us a new vector.

OK, great, and that's attention.

Oh, and then, um, there's also, um, I'll um show

this in a few slides.

This vector is also added to this vector directly.

So this is a weighted average of them and then

I also add X4.

This is a weighted average of them and I also

add X3 directly and so on.

Um, but, but these are most implementation tricks to make

these models.

So, um, here we go.

That's attention.

The second fundamental part is a feed for network and

we already know this.

So that's, that's our MLP from the methods review.

And now, whereas this is part of the model where

everything communicates with each other.

So here, all the, all the tokens depend on each

other and they, you know, um, language models, they can

be updated such that, what do you think after one,

after one layer of attention?

OK.

So that's attention here.

What do you think will happen to the embeddings of

language and model?

Um, what will happen after one attention, um, attention block?

Think of them in space.

So we have language sitting here and models sitting here

or something like this.

Language sits in a crowd of, you know, linguistics terms

or whatever, and the model sits there.

Um, and then if we do this here, the new

embedding for model will be a linear combination of all

of these, and our, our transformer will learn the alphas

through training.

Probably, it will move these two embeddings closer to each

other, because, um, model next to.

Language is, is, is now getting closer to language mark

and so on.

So that's what's happening here.

These like graphically to imagine this for us in these

attention layers, we're moving these embeddings closer to each other

depending on how they relate and.

That's that's that's likely how the transformer is going to

learn these these alphas.

OK, out of this come 4 new embeddings.

We, we input it 4 embeddings, we get 4 embeddings

out.

Um, We get X1, X2, 3, X4 out.

And now each of them goes.

Individually through a multilayer perceptual and neural network.

Um, and that's just a neural network like the ones

we had before and to draw this out more.

So, for example, the first vector goes through this thing

and this thing comes at dimension 10024.

Times one, it goes into a neural network with one

hidden layer.

See, you know, that's up to specification, how many neurons

it has, and then it's going back to an output,

not of one, this is not regression or so, but

actually to the same dimension.

Um, again, 1024 times 1.

Mhm Let me see if I can find another plaque.

OK, great.

OK.

And what are we, what are we doing here?

We're still trying to get this function in which we

input 4 embeddings and we get a probability of what's

the 5th.

The first thing this function is doing, it's still giving

us 4 embeddings, but now these are weighted sums of

the other embeddings.

And that's what you mean, you know, with attention, they

are communicating or paying attention to each other because they

are, they are linearly combined.

And now Now embedding by embedding is passed through an

MLP like this network that we discussed last, um, in

the last weeks.

Um, and however, this is a network that keeps the

input dimension, um, in this example, OK.

So you, you run this through here, you arrive here

after multiplying with the weight matrix here, right?

And then you arrive here and then you activate.

And the activation could be the relu activation that we

looked at is max of zero or X, but actually

in Transformers, it's something called Glu often.

I show you, I mean, LA, it's a different activation,

but I'll show you on the next slide.

They look almost the same.

Um, what is this?

This is crucially um a nonlinear transformation of this whole

thing.

There is some nonlinear transformation in these weights as well,

but very minor.

The key nonlinear transformation in the transformer is this year.

Um, we have one step where we have all embeddings

communicate, one step where we pass them embedding by embedding.

They don't communicate here, but now we nonlinearly transform them.

Out of this come all the new embeddings, basically.

OK, so now we have X1, X2 double prime.

We still have these embedders, we just have transformed them.

Yeah, can you remind me where the 1024 comes from?

Uh, yes, good point.

So that's just something I choose.

That's just a parameter of this model.

Um, you know, this depends on the language model, but

you see here, um, it determines a lot of the

parameters in this model.

If I have 10024.

My weight matrix has more, has more rows, and so

on.

Um, my initial lookup table has more, has more problems

in rows or rows depending on how I transpose it.

So this will immediately impact the transformers' parameters, total parameter

count.

Um, but modern transformers actually have embedding dimensions of more

than this.

Um, and, uh, you know, thinking of the word to

that we had was like just 300, right?

But like this year, the higher this dimension, the, the

Richer you can encode the meaning of language, and you

have to figure this dimension out.

Yeah.

So this is not even like so you only have

a decoder encoder and a decoder in translation.

You encoded the English sentence and you decoded it into

Spanish.

Here, what this is, it's, it's actually from the original

paper.

They wrote this paper, I think, more like, you know,

uh, thinking about machine translation probably, because that was the

application.

And at the time, it looked like, um, um, an

interesting machine translation paper that could, uh, give you extremely

good performance on machine translation.

These days, people don't really use this architecture here with

encoder and decoder, but they either use the encoder for

bird or they use the decoder for LLMs.

The key difference between encoder and decoder is if attention

goes both ways.

For the decoder, attention is backward looking, for the encoder,

it's going both ways.

Why?

Because if you think of bird, if you know bird

or these kinds of models, they give you encodings for

sentences.

If you want a rich encoding for a sentence, you

want attention to go into all directions.

You don't care to predict the next word.

You just want very good encoding for the sentence.

That's why here attention goes in both directions.

For the decoder, for the language model, um, I want

to not know what I'm predicting.

I want to only depend on the previous tokens of

words.

Then my question will be, where's the supervised part in

like we're, we're making our way there for now we're

just having a silly fun, you know, a function that

with a lot of random initialization is able to take

4 vectors, processes them through like this year is, by

the way, one transformer blog.

If we add a couple of more acts like these

skip connections of just adding the thing again and so

on, but fundamentally just neural network and attention.

Um, and then this is repeated through current language models.

But let's, let's, you know, that's not adding much intuition

for us.

Let's say we only have one transformer block, um.

Without repeating this, eventually, we take this year, OK, so

we take only the final embedding, which is still, you

know, of dimension 1024 in our example times 1, and

this thing then we map into the vocabulary and that's

it.

So what do we do?

We take 10024.

Times 1 Um, and this thing is mapped into or

multiplied um with a, uh, with a matrix that maps

it into the vocabulary I say 200,000.

OK.

Um.

So it would Multiplied with the matrix 200,000 times 24,

these will go and this thing will become 200,000 dimensional.

So we take the final embedding this this vector of

in our example, 11024 dimensions.

That's X4.

Of a prime say here, and we scale this up

into a vector, two matrix multiplication, we transform this into

a vector of 200,000 dimensions.

That's just a matrix, a single matrix multiplication.

Um, so here in the end, we have 1 or

2 200,000.

Now, these things are just numbers, that one could be

-75.

This could be 2000 or something like this.

What do we do?

We apply a softmax to this, now we have probabilities,

now we sample the next word.

That's how we would get the next word.

OK.

So here on the slides, like this is, this is

just this, but I have everything on the slides as

well.

The MLT block, MLT block is defined like before with

slopes and intercepts, and then an activation.

If you see these papers, you might wonder what Glu.

You see here, it looks almost the same as the

relo function.

All it does is it has this, this shape here,

which has non-zero derivatives around 0.

that can help with learning.

This looks funky, but what this really is, it's if

you want, it's the cumulative distribution function of a normal

times X.

That's the activation.

It turns out to look like this.

It looks almost like the hockey stick shape from, uh,

from the relu, um, uh, just that it avoids having

zero derivatives left of zero.

That Yeah.

So you don't worry about the um the the math

here, but really what is interesting to us is the

shape and that's almost the same as the ones that

we looked at before.

Good.

That's the whole thing that I, that I drew out

here.

Plus, you know, now some, some things added, you would

normalise these things to have at various stages mean zero

standard deviation one, for example, you would actually apply a

bunch of these things in parallel.

You would have multiple attention heads.

So you would have, uh, this and another attention.

And later you would combine the vectors.

But, but conceptually it's not very interesting.

This is the key thing here that it's a weighted

sum of, of the previous inventings.

Um, you would then have these skip connections where you

add the original thing back to the weighted thing.

um, here's the feed forward block, but that's really just

with a couple of added hacks what I drew there.

And for the next block, if we had another block,

we would simply set this back to X4, and we

would start here again.

That's it.

So we would set that to X4, and we would

start again.

Weighted some, running through a neural network, weighted sum, running

through a network, etc.

and we would repeat this.

At some point we stopped.

We take only the final one, which is also crazy

in a way, right, that all the information we need

to predict the next token is just now condensed in

one vector, basically.

That vector, we scale up to the size of the

vocabulary.

We have a bunch of numbers.

We apply the soft max.

And we sample from it and we put the next

one.

And that's what I wrote here, um.

You're able to uh sentence, you look up the static

input embeddings, you add the positional embeddings.

Now you have these initial vectors.

You apply entrance former blocks, which are all multiedit tension

and feedforward neural networks.

You arrive at this in the end, you take the

final embedding.

This is for inference, um, and, uh, and you have

a um a vector of 200,000 dimensions with it, uh,

to that one, you apply these are logits, people call

them this way usually you apply the soft text, just

real numbers here, the soft next to this, you have

a probability sample.

That's it.

That's the next word.

So now we have a model in which we input

the sentence and we can sample the next stop.

Good.

So if we randomly initialise the parameters across the transformer,

we could now make a prediction.

It would just be a bad prediction, right?

We would initialise all these weights, all the um in

the neural network, the the the intercepts, all of that

for all these blocks.

We could run this in there and we would get

a pretty good probability, um, just that would be a

bad probability.

And now we need to, uh, to learn all these

parameters and that's again, done with gradient descent fundamentally like

in the linear regression.

Um, but just now with a lot, a lot of

parameters.

OK, questions.

Mhm.

So for the example of language models are interesting, what

the expected output is like a full sentence or just

the next word, the expected thing here is, it's um

I could be one of 200,000 in my vocabulary if

I use the tokenizer that actually really GPT.

So they have uh roughly 200,000 tokens.

So what this model is giving me, it's giving me

a predicted probability for each of 200,000 tokens.

So I input the sentence, language models are interesting, and

I get a predicted probability of 200,000 tokens.

Most of them will have very small predictive probabilities, some

might have.

predicted probabilities and so on, right?

So, and then I, I have that distribution, this multinomal

distribution here, and then I sample from that.

That means I just make a draw from that distribution.

Most likely I draw this, this or this token, but

I might also occasionally draw an unlikely token.

Um, and then I get one token out of all

of this, and that one I would append to that

sequence and I would repeat.

So the key thing is like, you know, this builds

a super flexible function, but if you think about how,

you know, what the level of intelligence is that you're

seeing in these systems, and, you know, I know this

is a lot, but we can still discuss it in

the space of one lecture, the fundamentals of it, right?

So it's, it's just the way that some more factors

and then the nonlinear transform of factors.

And you just stack these things and you add a

bunch of things to make training more stable, but then

you're sort of there.

Yes.

So after it finds the probability.

It picks one word based on that probability.

Yeah, so like that's a good point.

What would be one simple way would be let's just

pick the word with the highest probability.

Let's just always put that there.

Um, that might be language models are interesting because or

something like that, but that that wouldn't give you that

always relies differently to your prompts.

So there must be something probabilistic going on there.

So what it's doing, it's drawing a word from that

distribution.

So most likely it does because, or most likely.

word, but it might occasionally just put a word that

that is much less likely.

Um, but yeah, what the function is doing, it predicts

a probability of what everything that could be number 5,

and then it's drawing from that distribution and puts the

word there and then its one.

But if we just had let's say two words instead

of 20,000 we just had the whole vocabulary because and

and then it was 99.

1%, I mean that 1% of the time you predict

that's right.

Yeah, exactly.

Um, however, if we had two words in the, I

mean then the sentence, let's say we had this, this

sentence because a chair or something like this, then, um,

then chair should ideally after training have very low probability

because otherwisePD wouldn't work.

But there is nothing in GPT constraining it to give

you sound English or something like this.

It has just learned these patterns from data.

It has just picked all its parameters to sample sensible

next words.

Uh, there's no encoded grammatical rules or something.

It's just all in the language that it has been

trained on.

Exactly, and then, and she would just have like a

tiny probability after the training.

Um, other questions.

Yes, that.

instead of just picking the word.

Yes, so we, we, um, we just make our outputs.

There is usually in this whole framework, it's, it's imagining

language creation as probabilistic.

Um, so it's, uh, if it was always to commit

on the most likely token, um, that's not necessarily the

right token.

No token will have a probability of one.

So rather than committing on something that's not the right

thing always, it then samples, you know, out of, out

of a set of possible next answers to how sensible

they are.

Other questions.

OK, um, Great, so yeah, that's um.

Let's take a break until It's the time Until 10

past, but then I start 100 sharp, um, and then

we're discussing how we actually get that function to work

well because for now we've defined it, we've defined all

the parameters, we can predict what's the next word, but

we get that predictions.

And the really exciting thing is how to make these

predictions work well, um, and work well to the degree

that you see more of like, you know, etc.

um, OK, cool.

So I'll see you at 10 past.

Yeah big.

And.

probably like 12.

there's.

Looks uh as well.

And usually like 12.

And 2 the girls are afraid one.

I guess I'm, I'll tell the girls.

It March is, you know.

Like that Oh.

Yeah, I think that's a matter about not staying awake

I tell myself I want to do things when I

feel like preceding 6 hours, I've suggested that I'm not

gonna do things.

um.

And.

She's in like.

You have.

Um, and are you still at the part in the

car, um, to help the.

Uh I have.

Yeah, I also like mostly on one and then I

decided to try and today they send the pencil breakdown

text where I got the email from Apple and then

I the spasm in here I wasn't I even saw

the I think I like double clicked the the cursor

happened to land on it.

This wasn't you cancel your um account.

Is what I'm doing the first this is my 4th

attempt at this doesn't look great, so that's only my

mom and I texted you about this guy.

That's.

um while she's at the Apple store trying to get

my phone.

She got a call that uh my grandma had fallen,

so she's been like, I mean, she's fine now, but

she's also 96, so it's just been a lot, but

my mom was like, you should know I accidentally lost.

And we just smash all phone to them and engagement

ring because the jeweller that she's gonna take her ring

to be repaired to is next to the Apple store,

but she didn't have time and then she went to

get a coffee and left the bag.

In hospital coffee shop I only realised because the bag

also had like scar in it so she got back

with the copy she's like, where's my and she'd also

lost an iPhone and then gave her um a level

of chaos that in case you're wondering why I am

mentally the way I am as a person, like, right.

like It you Oh.

If you look, what do you know about what you

have to put on the file for D because I

think that class combined with sitting down.

Like I can't think of a less relevant way to

respect, but I can't think of a way, yeah, they're

like it's actually, have you like watched for a lecture

and like totally hurt someone and then like.

No, I haven't.

I heard of it.

I think that a lot.

Does not happen here yet.

I don't know.

I feel like I've been, yeah, I have.

I feel like they're little.

So there's no sample.

Is it still?

multifaceted.

It all.

OK.

That Yeah.

I won't And I don't know, but maybe actually I

didn't want.

Yeah yeah first.

I need to get.

You.

Yes.

Big 12.

Mary and they got, they got about like the city.

This is like um if you wanna have some walking

time this week, I'm trying to do that so I

can like actually enjoy travelling.

Yes, that's OK.

I did.

I can show you the screenshot.

No, I can't because, well, I can find them again

shortly.

It wouldn't be that hard for the travel website, but

like the ones Albania were uh and then the US

but also like fairly comparable to like France.

Um, like.

Is I'm, I think, but there was also something about

like some places along the border that still have landmines,

and I was like watching out for those, right, um,

but it's fine.

She's gonna, she's scheduled type 31.

For like a friend to come to specifics after our

weeks so that she knew she had in the city

of Santos in the state of.

mostly Sometimes, sometimes she cares.

So the biggest.

What you.

Uh, Saturday So she was on Saturday morning.

Alright, let's continue.

Oh So we're done with the technical part of the.

And I, I, I know this.

Some of you might find this more interesting than others,

but it's, I think it's good to go through this

to see at least in broad strokes what architecture accomplishes

this, um, and now it's about learning the parameters of

that architecture by a gradient descent.

And then we look at a lot of kind of

different.

Um, yeah, I mean, use cases and tweets to this

architecture, which I think illustrated, uh, illustrated in many different

ways.

Um, OK, so the first stage, this is usually the

setup how you would obtain something like um current GPT

or clot or or Gemini or whatever or LA or

or Deep Sea.

Um, there's a pre-training and a post-training stage, OK.

Um, the pre-training stage is, is really kind of, it's

um computationally extremely expensive and also money wise, very expensive

and energy-wise, um, and that's trying to learn ultimately a

lot of basic facts about the world.

Um, so how is this working?

Um, so we're training the, the LLM.

Um, to, uh, um, you know, predict the word, just

that's what we always talked about on very large amounts

of curated internet text data.

OK.

Um, conceptually, this is a classification problem.

Um, can you tell me, just to, you know, recap

after the break, why is this, why is this a

classification problem?

Why is this a classification network here ultimately?

And why is the next one is a classification problem?

Maybe someone else, yeah, but.

So why is this a classification model?

And why is predicting the next word a classification problem?

Mhm.

Because ultimately your output, which is a vector of tokens,

is categorical as opposed to like an actual numeric.

Exactly, exactly.

So you're predicting the probabilities for 200,000 things that could

be the next token, but you know the token.

It's like a categorising sentiment of a text into positive,

negative, neutral, but just here, uh, we have 200,000 potential

labels, which are the next token, and then we have

the true next token and the data, right?

Um, so we would optimise this conceptually in the same

way that we optimised our logistic regression.

Um, we could write, we would write down a loss

function, uh, we could do, uh, we would do gradient

descent, etc.

All right, so to like um a classification model because

this output to the softmax is not over 3 classes

but over 200,000 if you look at this tokenizer, um,

and classification problem because there is a true next word

on the internet.

Great, so.

This is often said so, you know, nonchalantly in a

way, you know, we just trained this on the internet,

but there is a lot of good and a lot

of not so good information on the internet, right?

So and how do you train this, how do you

try to select, um, select the data that's helpful for

training and and ensure it has sufficient quality.

There aren't that many public data sets of the scale

that are used for LLM training, but there is one

by by hugging face, um, that you can check out

here.

Um Uh, that is open source, OK.

Um, and that gives you an idea of the, of

the sheer scale of this in, in different ways.

OK.

So the entire data set, which you could use to,

to train, you know, pretty much state of the art

models, um, has 42, 44 terabytes.

So, OK, whatever.

emphasise that this is both large and small in some

ways.

It's small because we could buy a couple of hard

discs, uh, on Amazon and just store this thing on

there, right?

We don't need a massive computer cluster to store this,

or so we can just, you know, buy, buy a

few hard discs and then just store that.

But just for text to give you some perspective, um,

I downloaded the full Wikipedia to train a word vector

model on it once to try to see how this

works.

Full Wikipedia had roughly like 15 gigabytes.

OK.

So just this gives you an idea of how humongous

this is in terms like this is a good chunk

of what is, what is available as internet text.

OK.

Good, um, because text is just not very expensive to

store.

Um, and then this, they, they go through a lot

of examples here of how they were like a lot

of discussion, how they arrived at the final data.

Um, and broadly, you know, they, they look at certain

URLs, they extract the text from these URLs.

They philtre for certain languages, right?

If you don't want to a model that speaks a

lot of different languages, but it's better at one, you

maybe just want to have English text.

Um, you know, you You, you do, uh, you do

further philtres, um, to remove, for example, personal information, but

also problematic content, etc.

um, and then in the end, You arrive at at

at the final data set, OK?

Um, and there's lots of discussion and, uh, um, of,

of this process here.

That's not our focus, but just imagine you were saying

one of these firms and you want to train the

language model, just the, you will have an entire massive

team to build that data set.

It's very compartmentalised.

So there's a team maybe for pre-training, a team for

different stages of post-training.

There will be just a team to assemble that data

set.

Because that's what goes into the model that's the information

it's acquiring.

OK, um, good.

Any questions about this?

Great.

Now we have a good amount of clean internet text

data and um we tokenize it and now we predict

the next, uh, the next token.

OK.

After we have trained our model this way, and we

do this with gradient descent of a, of a cross-entropy

loss just like we did before.

OK.

Now just with a much larger model.

Um, once we have trained the model, we can, um,

generate text with what is called inference based on the

model.

So you infer future texts based on the trade model.

Now, continuing our example, we might have an initial prompt

that is encoded in these tokens, and that actually Its

language models are interesting.

And now the next token that we might sample, now

I looked at these numbers for the, for the, um,

for the, um, um, GP4 tokenizer, um, that could be

because, OK, so we add this to the sequence, um,

language more is interesting because we sample the next token

they, we add it to the sequence, we sample the

next token R.

and that's how we would create an answer to a

prompt in this model, the model has been trained, and

now we can, uh, we can use it to, um,

to autocomplete.

However, I chose that example here.

We're not really asking a question, are we?

So this is really just all completing and what we've

trained so far is a glo glorified autocomplete engine, right?

So we've just predicted the next token on large amounts

of the internet.

Good.

And that's, I think some kind of interesting background discussion

of this.

So we have obtained the so-called base model that just

predicts the next token, right?

Um, that is an autocomplete, or you know, you might

call it the internet text simulator.

You can just put like an initial prompt and it

will just keep going and simulate internet texts.

Right?

Um, but maybe more interesting, uh, what this is doing,

it's compressing all the information that it has seen into

its numbers into its parameter values.

So it is conceptually like a lossy compression of all

the internet texts that it has seen via the slightly

obscure objective of predicting the next token into All its

parameter values.

So now all the parameter values of the model you

store the information to predict the next token.

They can't store all the internet text, but in, in,

you know, now 1 trillion floating point numbers, each number

could be, you know, 1 point, whatever, how many floating

point precision, uh, how much floating precision we have here.

And we have condensed all that information into these numbers.

OK.

So that's what's happening here on the conceptual level.

We're, we're reading all the, the knowledge about, you know,

um, major parts, say, um, uh, of, of the world

that is, that is distributed across these texts, and we

condense them into these numbers.

And to give you an idea, if you look at

the LA 2 model, which that's a smaller one, smaller

one relative to like state of the art models at

this point, um, but like the 3.3 or 70 billion

parameters actually very, very capable.

So say, um, 70 million parameters.

You can train that or this thing was trained on

10 terabytes of text, so actually much smaller than the

than the fine web data that was 44.

And then we combine all these like 10 terabytes of

text into only 140 gigabytes and that now we could

really go on our computer.

So we compress all that information from the internet text

state into a file that you can store your computer

with, um, you know, 70 billion floating point numbers, 70

billion numbers like this here, 1.7839 or something like this,

and you can store it on your computer and that's

the compression of that information, the lossy compression, not the

perfect recall of the text, but that stores a lot

of the information from the internet.

I think that's quite interesting, um, uh, um, what this

is really what this is really doing.

And to give you an idea of like these things

come from this lecture here, these numbers, um, if you,

if you were to, you, you wanted to obtain that

model at the time, that required 6000 GPUs, you needed

to run that for 12 days.

To give you an idea, that was 2 million to

get that model.

So that compression is very costly, both in terms of

energy or like in terms of energy, in terms of

money, in terms of time, everything.

OK, um, and that's why these base models are difficult

to come by.

Uh, Lama open sources them and so on, but it's

very expensive to get good base models.pa models meaning the

models that predict the next token on, on, you know,

large amounts of texts.

Um, Good.

So now, there are some providers where you can actually

pro-based models, OK?

But these space models are just these internet document simulators,

so we shouldn't expect them to um to answer questions

very well.

OK.

Uh, and here for example is a is a is

a chance to to prompt, um, the lama like um.

The Lama, the biggest Lama 3 days model, which has

400, roughly 400 billion parameters, and we can ask it

questions now and then run inference on it.

So say I want only say 20 tokens or something,

and I ask it, why is the sky blue?

What do you expect will happen now?

So that's the model that comes out of pre-training, OK.

That's not GPT.

That's the model that comes out of pre-training, the first

stage.

If I ask it, why is the sky blue?

What do you think it will do?

What could be a sensible auto compete?

The sky is blue because an answer, but it could

also be, you know, uh, why are trees green or

why are leaves green or something like this, right?

So it doesn't necessarily need to answer my question.

Particularly this is, this is a question that is generally

just asked often as a as a generic kind of

question.

So let's see um what it does.

Why is grass sweet?

See.

OK.

So, and then lots of random auto completion.

OK.

Um, this is not answering questions.

It's just all competing stuff.

But can we actually get at what's stored in its

parameters?

And one cool example is to take data on which

it's probably trained a couple of times.

So in these training data sets, high quality data is

iterated over more times than lower quality data.

So something like Wikipedia, very Likely high quality data and

iterated over a bunch of times.

So this model has probably seen Wikipedia for a bunch

of times.

So what we can do is, you know, we revisit

this obscure dinosaur example from the, from the word embedding

discussion last week, and I just start with the first

paragraph, OK, and I paste it into this Lama model.

Um, the website is a bit kind of buggy, but

I think I don't have an empty line here.

So I just paste this in here and then I,

I run an inference say on 500 tokens or something

like this.

Uh, let's go.

This crash.

I am OK.

Flying dinosaurs and so on, um.

Let me see.

I should be able.

Let's maybe pick one with fewer of these citations.

Let's pick this part here.

Yeah, OK.

See?

No, it doesn't like this.

So what I, what I'm getting at here is you

can try.

To have it um recite certain parts of Wikipedia.

It's usually capable of doing this.

The question is a bit do you prompt it in

the same way as Wikipedia looks to it.

Let's let's take the other one.

Let me see if I can get this running.

Mhm.

OK.

Mm OK, here we go.

So I'm pasting this into the model.

OK.

And then the next predicted sentence here is they could

take, well, like the actual Wikipedia sentences, they could take

off from the ground.

So now the model is doing this.

They can take off from the ground and fossil, um,

track will show at least.

Here you go, um, and fossil track will show at

least.

So it is capable of just reciting that article.

So in all these parameters that make up this model,

there's so much information stored the data that it has

seen a couple of times, it has perfect recall over.

OK, so we could keep going here and at some

point this would.

Um, this would break this pattern.

Let me see, let's let's say Max Token 2000 or

something.

So here we're talking wingspan, let's see.

Hm So wingspan is where is the serum.

There we go.

So at least very similar or reciting, at some point

it will get derailed and it will, you know, predict

the sensible other article and so on, right?

Um, but it will, it will just recite this particularly

and that's why if you ask JGBT something, it has

perfect recall over some things that I've seen.

So some things um are up to a point recitable

through the information and its parameter values.

So that's how, how much information it has actually stored,

OK.

Um, good.

However, if I now gave it like, let's say some

time, but if I gave it the article, I did

this, like, say, of, of the Paris Olympics, this model

has been changed until December 2023.

If I gave it the first paragraph of the Paris

Olympics article, it would just invent the Paris Olympics, right?

It would just autocomplete this.

simulated the document that looks like the Paris article, although

it cannot know this, right?

There's nothing for now, um, telling me, you know, whether

this is factual or it's inaccurate, it's hallucinating and so

on, right?

It's just all completing internet texts and simulating sensibly looking

internet texts that matches the distribution of what it has

seen.

Um, great.

Uh, when the model is training, how much context does

it consider like over each iteration, so is that like

Always predicting the next word, using all the information from

the beginning of the article to where you are now,

or is it like paragraphs, sentences?

Yeah, so that's a great question.

So what is actually happening here is when it's training

in inference, we take this and we take the very

last of betting and we take the next word when

we generate an answer to.

However, in training, we would, with this year, predict model.

Models with this year, we would predict art, and with

this year, we would predict interesting.

You see, so with the sequence length of 1, we

would predict models with a sequence length of 2, we

would predict R with a sequence length of 3, we

would predict interesting.

So we are still training, training many different sequence lengths

from a single sentence.

And the sequence length that it can pay attention to,

um, is as long as it's a context window.

That's by the way, where, why, you know, clot has

200,000 tokens.

What does that mean?

It can pay attention to 200,000 tokens in the context.

Um, but it also works on, um, on shorter sequences.

Other questions.

OK.

Cool.

So how that is, that's also quite fascinating, um.

Our eventual task is not to simulate internet documents, right?

Our eventual task is to display intelligence and solve a

bunch of problems, answers to our questions, help us with

stuff, etc.

However, there are these so-called scaling laws in in AI

research and they have been, you know, Very, uh, like

they have, they have, you know, held for, for a

while now.

And what do they mean?

If you look at the test loss, that's just, you

know, imagine our cross-entropy loss, like in our logistic regression

on some holdout data, some holdout test data.

If you increase the compute that you have at your

disposal, these are log scales.

If you increase the data set size, if you increase

the parameters, and actually they say in the paper, you

have to increase them all at the same time.

You can't just increase one.

Then that loss is really predictably declining.

That means you're getting better and better and better at

predicting the next word.

Interestingly, your downstream capabilities of mathematics and all that kind

of stuff that you get out of these models later

on is correlated with how good they are in the

base uh prediction and how well they predict the next

word on the underlying data.

OK.

And the key question in the current paradigm that everyone

is wondering about is how long will these things hold?

When will they level off, and so on.

At what point, you know, will we not see this

sort of mechanical prediction.

But this has been basically the, um, you know, the

playbook for, for a lot of AI research because they

have just, um, they have just ever increased their data

sets, their model parameters, and then they could expect if

these things held that the training, um, that the test

loss would go further down.

Um, and this was published in 2020, and part of

the story of OpenAI is arguably that they completely committed

on this.

There's all these patterns, and often such patterns break, you

know, the next iteration, but these ones here didn't break,

and they just did, you know, invest everything in language

models to scale these things in terms of commute parameters

and dataset sizes.

Um, and it turned out these scaling laws held for

longer.

And and now essentially powering a lot of what we

see in the intelligence, um, that these models display.

Another interesting fact is Compute, you, you might know this

Moore's law, Moore's law, right?

So the, the amount of transistor cores, um, doubling and

so on.

Actually, this is just like a regularity that is by

virtue of an amazing amount of scientists working on this,

but maintained for now, but we're arguably running out of

data set size.

So if we want to write these scaling laws, you

know, like, um, you know, we need to increase these

all combined.

Compute.

Moore's laws will further increase.

But like our data set size, at some point, we

don't have more internet data, right?

So we're, we're at some point we, we run out

of internet data.

That's why there's currently so much discussion of trying to

use uh textbooks data and trying to, you know, reach

agreement with textbook providers.

Um, there's also the whole discussion of synthetic data.

You could have LLMs generating data.

They won't generate fully new information, but some new information.

Um, etc.

But this is, this is like, uh, the scaling law

is the, is the kind of underlying, um, uh, underlying

logic driving this discussion.

If computers ever increasing, we need to increase these all

at the same time.

Parameters of a model, we can increase a compute and

memory is decreasing, but we need more data.

Uh, and that's a major problem for the current, um,

paradigm at some point that we might run out of

this.

And obviously how long we so is is unclear.

Um, But that this line exists and it doesn't cut

to this line is in itself fascinating.

There's a lot of research on this.

Um, OK, uh, questions.

Cool.

I mean, I try to, like, I mean, maybe it's

obvious to you, but imagine you went to one of

these AI pioneers in the 20th century, you showed them

chat GPT, um, I don't know, maybe they would have

guessed it, but like my feeling is it's so obvious

that this thing is just predicting the next word, and

that it's all based on this, that if you scale

the models and scale the data and scale the compute,

um, you have these emergent properties of all this intelligence

basically uh emerging um from, from the underlying underlying function.

Good.

Questions.

Cool.

So now we have an internet document simulator and autocomplete

model, like the one we saw, not very helpful, but

that step is crucial for, you know, ingraining all the

information on the, on the, uh, you know, in the

underlying text into the model's parameter values.

And now we need to actually, uh, do, Post-training to

make it, you know, an assistant or to, to make

it actually useful in tasks.

And broadly at this point in time, this is divided

in two parts, supervised fine tuning and reinforcement learning.

OK.

So first supervised fine tuning.

um.

Pre-training stage stored just a lot of information in the

parameter values that we've discussed in detail now.

Um, how can we make this model answer questions?

And the simple, simple solution to this is you create

data sets where you write questions and answers, you concatenate

all these data sets, you continue training the model on

predicting the next token on these data sets.

These data sets need to be much smaller than the

internet from pre-training, and all of a sudden this thing

starts answering questions.

OK.

So we have the base model predicting the next word

on the internet.

That was there to read the internet and learn all

the, the world knowledge.

And now we give it much smaller data sets of

questions and answer pairs.

Um, we repeat the same training, predict the next word

on these data sets.

Um, and we do this for diverse questions, but we

won't have all the questions.

But at some point, the model will have generalised what

it means to answer questions and also start answering questions

in completely different domains based on its underlying knowledge of

the world.

All right.

This was pioneered in this paper, um, in 2022, um,

that they took GPT 3, and they, they created such

a data set of prompt and answer pairs and GPT

3 was obtained just from predicting, scaling GPT2 and predicting

the next token.

Um, and then they did this, and this is really

underlying what the initial check GPT was.

OK.

Um, the 3.5, um, likely was based on, uh, on

a similar pipeline.

Um, right, so what you do is you create prompt

and answer pairs.

Um, and how do they do this?

This is just a case study from that paper.

The, the GPT 3 models at the time at 1.36

and 175 billion parameters already.

They took a data set of 13,000 trading prompts.

They had that from the API.

So people asked them all different things.

They, you know, sent requests to the API.

So they had 13,000 of these.

And then they just wrote answers themselves.

So label us, you know, or people that they hired

and their teams were writing, um, Um, were writing answers,

just a plain answer to a question, few shots, that

means giving a few examples and so on.

Um, and then, um, uh, they were, they were trying

to, uh, to write the desired answers from users, etc.

In the end, that's a data set of a few

10 thousands of such observations which, which have prompt.

And then answer.

Write a poem about the LSC answer and then it

takes a human a long time to write a poem

about the LSE, but it just needs, it's just there

to show the model what it needs to answer a

question.

It's already knowing how to write a poem.

It just doesn't know how to answer a question.

And if we show them the model enough answers to

questions and just continue the initial training on this, um,

it can already get much better at at answering them.

So this process is actually not just used for teaching

the model to answer questions, but also to align it

more broadly, to display ethically desirable behaviour, uh, refuse certain,

um, answers, etc.

And there's no, you know, there's no way to hardcode

every single Example and say, if you hear this, don't

respond if you hear this.

So you need to rather show these models specific patterns,

um, and then they will generalise these patterns, um, if

this works, OK?

And there are again, not that many open source data

sets, but this is one.

OK.

Um, so we can check this out from like an

open source model, um, Tum.

And that's the data set.

And here you see just like I think about a

million data points, um, yes, about a million samples with

prompt and answer questions, um, you know, prompt and answer

pairs, um, and, and that's the data here.

You have content, um, the, um.

This year that the user says and then content that

the system replies with, that's the answer.

Um, and then it's, it's trained on on that data.

What is also interesting in this year, sometimes you see

in the early stages, still today, you might see some

people asking JGP to see who are you, right?

If you think of the base model, the base model

would just autocomplete who we are.

It's just statistically predicting patterns that sound reasonable given the

data it has seen.

doesn't mean that it has a knowledge of itself or

something like this, right?

When you today you ask Chechi P, who are you?

We would say, I'm a large language model from Open

Air or something like this, but how if you look

into these data sets, actually there are these hardcoded data

sets and then you know, content telling you about yourself

and then it says hi.

I'm a chat for assistant created so so so they're

actually showing the model how to respond to these questions,

OK?

And then the model responds this way.

Another way is to, um, add that information as part

of a system from, the problem that you don't see.

So they might add today is, you know, the, um,

Um, the day X, Y, Z, and you are, you

know, a more creative, but hopefully AI or something like

this, and then it's more completing the sentence based on

this and giving answers that, that, um, you know, are

more what people might expect.

That's basically how agents work in.

I mean, you know, that you could create like a

math agent.

It's simply giving a prompt a I look I look

into, yes, so the system prompt is simply a prompt

that's always kept a memory.

That the user doesn't see, that's it.

But it's just like you could like more conceptually these

models work in prompt and answer system and user prompts

they are all combined ultimately in one prompt and put

it to the model that predicts the next token, you

can predict the next tonne, etc.

But exactly agents are many essentially like, you know, instances

of LLMs, um, all with different prompts arguably, yes, exactly.

OK, but we're not quite there.

We look into all of this actually.

Um, uh, and, um, and we'll definitely, um, you know,

be able to, uh, to finish this in like the

first, uh, half of the, of the seminar.

By the way, uh, we can do this this week

because in the seminar in the second half, we're looking

at practise problems for the exam.

OK.

That's this week's seminar.

So that won't take up 2 hours.

That's why I could add more content on, on LLMs

because I think that's probably the topic people are most

interested in in this course.

Um, good.

Um, cool.

So that's these SFT data sets, examples of prompts and

answers.

We continue training the model on them.

The the interesting thing is we can't show the model

all the questions and answers, but we really only need

to show it across a diverse set of topics what

it means to answer, and And it knows what to

answer from the base training because it has all that

information stored in its barometer values.

It just didn't answer, you know.

OK, so there are many challenges here, and you, you

will think about them already.

So we align them to preferences in that SFT data

set, but that might not be the preferences of someone

else, right?

You could also like, um, malicious actors might align it

to to wrong goals, etc.

yeah.

Um, and then also you might have seen these things

on social media again of, of jailbreaks of these models,

uh, try to, um, try to trick them into responding

to questions that they have been in line not to

respond to, etc.

So, um, this is also not, um, not absolutely proof,

uh, for these kinds of attacks.

Um, great.

So, you might think of this as a 2022 paper,

and you might think of this, you know, why do

we create all these answers with humans?

Can't we just let other LLMs write answers?

Because they already know how to answer things.

So we could just create these data sets with LLMs.

And the data set that I showed you here, the

201.

It's partially created with LLMs, but we still need humans,

um, to, to curate that and to actually look at

the information, um, and that the information is what should

be in that, in that pre-staining stage.

Um, we could like, you know, do this fine tuning,

um, the supervised fine tuning with open weights models ourselves.

If we had, you know, downloaded some like LA-based model

and we had enough compute, then we could also tune

it on these question answer pairs and we could make

it into an assistant.

Um, usually this is done for us, but sometimes the

commercial providers allow us still to tune on some data

and that you might have seen in data in your

private sector work or papers and we fine tuned the

GBT model.

That really just means you take the already instruction line

GPT model that already answers questions, but you show it

a few more prompts and answers, and you just make

it, you push it a little bit more into that

direction.

You know, for example, opening ISA you can upload 100

prompts and answers, which is nothing, um, and then just

like this displays already somewhat more behaviour that is in

these asks.

Generally, one interesting thing here as well is that these

data sets are really tiny relative to um to the,

to the pre-training.

Even the one I showed you.

Yeah, this one, has only a million examples.

If you think in comparison to the data that this

has been trained on the day training, that's nothing.

Also the compute you need for this is much lower,

etc.

because this is not about acquiring information per se.

This is more about acquiring, um, um, knowledge of how

to behave and what to do.

It still sounds very expensive for them to allow you

to fine tune them because if, if the parameters of

a model are like 150 gigabytes, if they let you

fine-tune then it's, it means that you're going to modify

those parameters.

So you must have a copy of every.

Who that fine tunes that's fine.

But like, OK, the copy is super cheap because it's

so little data, fair enough.

Um, however, um, you might not, I mean that they

don't share this, but you might not actually change all

the parameters of the models.

Maybe you share some you change some final layers.

Great.

So this gives me a good segue into discussing one

thing that is often confused and that is, that is

really kind of important to understand here is there are

two different ways to customise outputs of these models.

One is called in context learning, and the other thing

is fine-tuning.

So sometimes and, and when you work with this as

a researcher or as a practitioner, if you want to

use a model like this for a certain use case,

you know, I, I worked on, for example, um, uh,

a research paper on using these to conduct interviews, qualitative

interviews.

We had this decision, do we find you?

the thing to be an interviewer, or do we just

give it more information as part of its prompt?

That's sort of always the trade-off, or you could do

both, um, but often people try to not fine tune

it and then give it a lot of examples as

part of the prompt, and that's in context learning.

OK, and that's just extending the pros, OK.

So fine tuning updates the parameters exactly like you said,

but in context learning doesn't, OK.

So without updating any parameter of the underlying function, you

simply add more information to the context.

You give it examples and stuff like this.

via its attention mechanism that we discussed, it can pay

attention to all these things that you can.

You can give it examples of how you want the

output to look like, etc.

You don't change any of the parameter values, you just

give it more examples.

If you don't, if you prompt it again, it has

forgotten all of this.

It's not in the parameter values.

It's just part of the prompt.

Um, and this can also improve answers, and then there

are these words that you might see or will have

seen already, zero shock prompting, that's just, um, uh, only

the instructions.

So what's 2 + 2?

OK, that's zero shock prompting.

And then the model needs to figure this out.

Um, one shot prompting is, um, for example, um, To

give, to give an example of this, to continue what

I just said, 3 + 3 is 6, what is

2 + 2, and then see what it relies.

Future prompting is to give more examples.

That's all part of the context window.

no fine tuning, no update to parameters, just adding information

in the context window.

So a model like GPT has been instruction trained with

SFT and other approaches anyways.

That's why it answers questions, right?

It's not the base model.

But we can still supply more information to the model

as part of the prompt.

And this is really interesting here.

We can actually even make a base model into an

assistant through in context learning.

And that's how we can try to do this.

So let's, let's try to do this.

We take a base model, which is just this auto

completion thing, right?

This internet document simulator, and we just give it a

sequence of things that we could hide in the prompt

that a user doesn't see, um, and then we ask

it, um, this.

Um, we, we prompted this way, OK, and let me.

Let me combine this text without spaces.

So I still, you know, remember my initial question Why

is the sky blue?

And then it says, why is grass green and so

on.

I now wanted to actually answer my question.

So what could I do?

I I could create a sequence which if it autocompletes

it, it implicitly then answer my answers my question.

And this is just, you know, a way to set

up such a sequence.

First I say you are a helpful AI assistant answering

questions.

Then comes user question What is the abbreviation of the

London School of Economics and Political Science?

You answer LSE Uer question when was the LSE founded?

Your answer.

1895 user question, why is the sky blue?

You answer, OK.

And then a grower.

This is not updating any parameters, nothing.

It's just adding more information to the context.

Um, and let's restrict this maybe to 20 tokens or

something like this, but if we run this.

There we go.

Right?

Because the way light scatters in the atmosphere, it knows

it.

It just doesn't know how to answer a question.

And that's one way to use in context learning to

make an answer a question, not very practical.

That's why we would do supervised fine tuning and we

would show it examples of questions and answers.

But we could already achieve this by setting up the

context in this way.

So that's kind of like a nice example of how

all of that is already in the parameters of the

model.

It's just all a question of um making this model

respond in a way that that meets meets certain criteria.

Any questions?

Mhm.

Can you like train it in the negative in the

same like with examples in the same way that you

can give it positive examples.

So, presumably chat GBT is designed to have guardrails against

like people asking for instructions on how to do something

violent or something like that.

So is that trained only by showing Examples of how

it wants GBT to respond or can you give it

like a loss penalty for others?

I mean like I don't know how they do this

right um a lot of examples.

That refuse answers to such questions, and then the model

generalising this to other um things in similar ethical domains.

But then probably they add a bunch of hardco guard

rails on top of this, um, because otherwise these systems

wouldn't show the degree of reliability.

But the, the amount of precise hard coded examples you

could put on top of this, you can't come up

with every example, right?

So you need to rely on some underlying teaching the

model a certain system of ethics.

But for social scientists, that's such an interesting area of

research as well, right?

Because what is that said like, to whose preferences do

you align this to, um, etc.

There, there's lots of interesting research should be done there.

Um, other questions.

Great, so For now, it's very intuitive why these things

hallucinate, right?

Because they are autocompleters and if we tell them, Um,

uh, to, um, to, um, to answer questions in a

confident way.

They will also answer questions in confident ways, not necessarily

depending on whether they know the answer or not.

OK.

So what we did so far is we supplied more

information as part of the context, right?

And we could actually use this to decrease hallucinations.

We could supply certain documents as part of the context.

Imagine I have a small LLM who doesn't know the

Wikipedia article for 5 hours or something.

What I could just do is I could add that

article to the context, and then it has much better

recall of the context and from its parameter values, um,

and it will, uh, it will give me better, better

outcomes.

You can think of the parameter values in a way

like, you know, all the, the nuts and bolts in

this brain, and the context is a bit like a

table in front of that model with documents, and it

can pay attention to these documents.

And it has some fake recollection of these documents and

its parameter values, but not perfect recollection.

The bigger the model, the better the recollection.

Um, but giving stuff in the context basically puts the

documents in front of the model.

OK, and you might have seen this people ask me

to quickly talk about this in the lecture.

You might see this in job descriptions and so on,

these rack system retrieval augmented generation.

And they are systems to try to increase the reliability

of these models with questions that are not, for example,

in its training data or that it doesn't have perfect

recall over, etc.

So people ask a question, you simply find similar documents

and you put it into the context of the model.

That's it.

And then the model has these documents in its context

and it can answer more accurately.

That also helps with hallucinations because it now has the

relevant information in the context and might not hallucinate, OK.

Mhm.

I, how does it?

Because, uh, I don't know, maybe it's way beyond the

content, but for rag it compares the similarity between the

prompt and the vectors of an encoded knowledge base.

How do you summarise.

The chunk of sex in the knowledge base or the

prompt into one vector so you can um compute similarity.

You have an encoder transformer.

You have an encoder transformer that creates an embedding of

the question that the person asks, and then there are

Vector database storing embeddings of lots of chunks in your

database.

You compute, like you said, course and similarity with the

prompt and every embedding.

You take the ones that are more similar, re-rank them

maybe with a slower model, and then put them into

a context of a chat model and have the chat

model answer.

It's a bit beyond like the scope here, but if

you want, like we can discuss after the election.

Um, great.

So, To close this, um, uh, and then the rest

will be in the seminar, is this is a very

clever way to improve factual accuracies of these systems.

So to recap, they are predicting the next word.

There's nothing stopping them from making up stuff.

If we paste the article of, uh, the France Olympics

into a base model, although it's not been trained until

2024, it would just make up the France Olympics.

The thing is, though, what is really interesting is it

probably knows that it doesn't know.

It is just fulfilling the objective.

By autocompleting, it's fulfilling the objective of base training, and

by giving us a super confident answer, it's mimicking how

supervised fine tuning people answered.

So we were, we fine tuned it on examples of

labeler saying, what is a poem about the LSE Here

you have a poem about the LSE and so on.

So we have these confident responses on which we find

you in this, and this model will just simulate labels

now.

Now we're just confident responses about two questions.

But whether these are true or not, it's not, you

know, that's not so far its objective.

So how could you do this?

And what you would do is like one way to

do this is you need to find out what the

model doesn't know, OK?

For example, um, take the, like take a Wikipedia page

on Aminuta here that's like a um a really important

mathematician, um, and in here there is some information of

who wrote the dissertation, I believe.

Here, under the supervision of Paul Gordon, she wrote her

dissertation, um, on complete systems of invariant, um, invariants.

OK.

So if we take a model that is not as

large, let's not take deep see, but let's, let's take

a simple model like, um, like one of the small

Microsoft models here.

Um, this one, for example.

And we asked it what was the title of Emmy

Nurta's dissertation.

Let's see, um.

What are responses.

Uncomplete systems are invariant, see.

There you go.

It got this, OK.

So now what we would do is we would go

through successively more complex things here, and we would ultimately

find the question that I can't.

Actually this question, I think, I believe if we ask

it again, it might already mess this up.

Let's see.

Mm, it's all good.

Let me see.

Ah it says I think it has memorised this.

Yeah There you go.

That's already wrong.

OK.

So ideal theories of rings.

OK.

That's wrong.

So here it's already, it has imperfect knowledge of this,

and you will try to find little questions where you

see the model doesn't give you the right answer, and

you can verify this.

And now you can build data sets of models to

of questions to which you know a specific model doesn't

know the answers.

And how you create a data set of these questions,

and this might be, you know, I don't know when

did an obscure team win certain trophies or something like

this?

You just need examples of what it doesn't know, and

then you write answers to these questions, saying, I don't

know.

Because probably the model has a feeling that it doesn't

know these things.

There's some neuron in all its layers being activated, for

example, and signalling that it doesn't know the answer, but

it's just auto completing in a confident way, and you

want to teach it to in these cases, not do

this.

So first, you need to figure out the cases when

it doesn't know.

This you find out this way.

And then you have a data set assembled of cases

it doesn't know, and then you can do fine tuning

on these cases with your own written answers saying, I

don't know, sorry, or something like this.

And then the model is becoming factually more accurate.

And you see how many things this combined.

Still the fine-tuning.

It's getting at this major problem of factual accuracy here

and hallucination, but it needs to figure out first of

all, what the model doesn't know.

Um, and then you can write, um, desired answers, teach

the model, um, and then, uh, the model will, will

likely display this.

OK.

Um, any questions, let me know.

And otherwise, we continue with the, in the classes.

Somebody has that.

Hope that I like cool.

That's it's it's like.

So we don't like it's you know it's.

I Um.

I exactly.

I just like I feel like I.

CBG 907 didn't even realise CBG was that big.

I mean, yeah, sounds wrong, no.

OK.

I like I like I I need to like yeah

it's terrible.

What do you start working.

No, it's like it's not like they were like, is

there, is there some like rand's done it for you

that's good though, yeah, like I like I like.

was the right question or you have any follow up,

yeah, yeah, I forgot that there was another embedding model

involved I have more questions you have, you have an

embedding mode and All the check in and then you

write the, the, and then have some more similar ones.

Then you probably apply some slower re that's a bit

better than they already got slower.

That's some slower competition so.

But you, you like post and similarity, if you have

a pet then you have millions of others it's easy

to be fast, but like you will not work.

So what usually do is you do similarity you may

have your top 100s and then you again more actual.

And then you take the top or you can check

with I mean there's lots of refinements.

Exactly, exactly.

And then the point you might say, OK, these are

your related things from the database, please answer the following

questions when you say slow.

Some people face in the city I think it's the

other way around like if you need to.

Because for example we we've been matching names of uh

database that we have in the US we done and

brought the data and it's like it's like balance sheet

data we need to match names of businesses.

Uh, but it's not an exact match.

So the best example was how do you match like

YMCA to young association.

So you can use an embedded model and simply compare

similarity.

There's also in Python pretty good like fuzzy was.